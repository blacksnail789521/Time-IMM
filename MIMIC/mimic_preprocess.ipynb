{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic-IV Preprocessing\n",
    "Code originally taken from GRU-ODE-Bayes Preprocessing, simplified and adapted by Neural-Flows, and finally simplified and adapted for Time-IMM Benchmark.\n",
    "\n",
    "MIMIC-IV-Note dataset was added and preprocessed for Multimodality.\n",
    "\n",
    "### Data Selection and Inclusion criteria:\n",
    "- Include only patients in Metavision system\n",
    "- Retain only patients with a single admission\n",
    "- Select patients whose admission duration is over 50 days for longer time-series\n",
    "- Remove patients who are younger than 15 years\n",
    "- Remove patients without chart events data\n",
    "- Remove patients that are not included in MIMIC-IV-Note dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure to download these two projects:\n",
    "- [MIMIC-IV-Note](https://physionet.org/content/mimic-iv-note/2.2/)\n",
    "- [MIMIC-IV](https://physionet.org/content/mimiciv/3.1/)\n",
    "\n",
    "So under the folder MIMIC, you would have:\n",
    "```\n",
    "mimiciv/3.1/\n",
    "mimic-iv-note/2.2/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Time Series Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"mimiciv/3.1/hosp/admissions.csv.gz\"\n",
    "adm = pd.read_csv(fn)\n",
    "adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only patients present in patients data\n",
    "patients_df = pd.read_csv(\"mimiciv/3.1/hosp/patients.csv.gz\")\n",
    "patients_df[[\"subject_id\", \"anchor_age\"]].head()\n",
    "adm_dob = pd.merge(patients_df[[\"subject_id\", \"anchor_age\"]], adm, on=\"subject_id\")\n",
    "\n",
    "df = adm.groupby(\"subject_id\")[\"hadm_id\"].nunique()\n",
    "subj_ids = list(df[df == 1].index)\n",
    "adm_1 = adm_dob.loc[adm_dob[\"subject_id\"].isin(subj_ids)]\n",
    "print(\"Number of patients remaining in the dataframe: \")\n",
    "print(len(adm_1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of stay in ICU\n",
    "adm_1 = adm_1.copy()\n",
    "adm_1[\"admittime\"] = pd.to_datetime(adm_1[\"admittime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "adm_1[\"dischtime\"] = pd.to_datetime(adm_1[\"dischtime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "adm_1[\"elapsed_time\"] = adm_1[\"dischtime\"] - adm_1[\"admittime\"]\n",
    "adm_1.head()\n",
    "adm_1[\"elapsed_days\"] = adm_1[\"elapsed_time\"].dt.days\n",
    "\n",
    "adm_2 = adm_1.loc[(adm_1[\"elapsed_days\"] > 50)]\n",
    "print(\"Number of patients remaining in the dataframe: \")\n",
    "print(len(adm_2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only patients older than 15\n",
    "adm_2_15 = adm_2.loc[adm_2[\"anchor_age\"] > 15].copy()\n",
    "print(\"Number of patients remaining in the dataframe: \")\n",
    "print(len(adm_2_15.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"mimiciv/3.1/icu/chartevents.csv.gz\"\n",
    "# this file is huge, we need to read in the data in chunks\n",
    "# chartevents = pd.read_csv(fn, compression='gzip')\n",
    "\n",
    "# workaround: (takes about 7 min)\n",
    "ids = np.array([])\n",
    "for chunk in tqdm(pd.read_csv(fn, chunksize=1000000), desc=\"Processing chartevents\"):\n",
    "    ids = np.append(ids, chunk[\"hadm_id\"].unique())\n",
    "    ids = np.unique(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_2_15_chart = adm_2_15.loc[adm_2_15[\"hadm_id\"].isin(ids)].copy()\n",
    "print(\"Number of patients remaining in the dataframe: \")\n",
    "print(len(adm_2_15_chart.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"raw\").mkdir(exist_ok=True)\n",
    "adm_2_15_chart.to_csv(\"raw/admissions_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_3 = adm_2_15_chart.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose previously selected admission ids (takes about 30 seconds)\n",
    "inputs = pd.read_csv(\"mimiciv/3.1/icu/inputevents.csv.gz\")\n",
    "adm_ids = list(adm_3[\"hadm_id\"])\n",
    "inputs = inputs.loc[inputs[\"hadm_id\"].isin(adm_ids)]\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep columns of interest\n",
    "inputs_small = inputs[\n",
    "    [\n",
    "        \"subject_id\",\n",
    "        \"hadm_id\",\n",
    "        \"starttime\",\n",
    "        \"endtime\",\n",
    "        \"itemid\",\n",
    "        \"amount\",\n",
    "        \"amountuom\",\n",
    "        \"rate\",\n",
    "        \"rateuom\",\n",
    "        \"patientweight\",\n",
    "        \"ordercategorydescription\",\n",
    "    ]\n",
    "]\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(inputs_small[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item ids for inputs\n",
    "item_id = pd.read_csv(\"mimiciv/3.1/icu/d_items.csv.gz\")\n",
    "item_id_1 = item_id[[\"itemid\", \"label\"]]\n",
    "item_id_1.head()\n",
    "\n",
    "inputs_small_2 = pd.merge(inputs_small, item_id_1, on=\"itemid\")\n",
    "inputs_small_2.head()\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(inputs_small_2[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each item, evaluate the number of patients who have been given this item\n",
    "# Select only the inputs with highest occurence\n",
    "pat_for_item = inputs_small_2.groupby(\"label\")[\"subject_id\"].nunique()\n",
    "frequent_labels = pat_for_item.sort_values(ascending=False)[:50]\n",
    "inputs_small_3 = inputs_small_2.loc[\n",
    "    inputs_small_2[\"label\"].isin(list(frequent_labels.index))\n",
    "].copy()\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(inputs_small_3[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(inputs_small_3.groupby(\"label\")[\"amountuom\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cleaning the Cefazolin (remove the ones that are not in dose unit)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"itemid\"] == 225850) & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Cefepime (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Cefepime\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Ceftriaxone (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Ceftriaxone\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Ciprofloxacin (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Ciprofloxacin\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Famotidine (Pepcid) (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Famotidine (Pepcid)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Fentanyl (Concentrate) (remove the non mg)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Fentanyl (Concentrate)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"label\"] == \"Fentanyl (Concentrate)\")\n",
    "    & (inputs_small_3[\"amountuom\"] == \"mg\"),\n",
    "    \"amount\",\n",
    "] *= 1000\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"label\"] == \"Fentanyl (Concentrate)\")\n",
    "    & (inputs_small_3[\"amountuom\"] == \"mg\"),\n",
    "    \"amountuom\",\n",
    "] = \"mcg\"\n",
    "# Cleaning the Heparin Sodium (Prophylaxis) (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Heparin Sodium (Prophylaxis)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Hydromorphone (Dilaudid) (remove the non mg)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Hydromorphone (Dilaudid)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Magnesium Sulfate (remove the non grams)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Magnesium Sulfate\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"grams\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Propofol (remove the non mg)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Propofol\") & (inputs_small_3[\"amountuom\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Metoprolol (remove the non mg)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Metoprolol\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Piperacillin/Tazobactam (Zosyn) (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Piperacillin/Tazobactam (Zosyn)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Metronidazole (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Metronidazole\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Ranitidine (Prophylaxis)(remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Ranitidine (Prophylaxis)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Vancomycin (remove the non dose)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Vancomycin\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning the Fentanyl. Put the mg to mcg\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"itemid\"] == 221744) & (inputs_small_3[\"amountuom\"] == \"mg\"),\n",
    "    \"amount\",\n",
    "] *= 1000\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"itemid\"] == 221744) & (inputs_small_3[\"amountuom\"] == \"mg\"),\n",
    "    \"amountuom\",\n",
    "] = \"mcg\"\n",
    "# Cleaning of the Pantoprazole (Protonix)\n",
    "# divide in two (drug shot or continuous treatment and create a new item id for the continuous version)\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"itemid\"] == 225910)\n",
    "    & (inputs_small_3[\"ordercategorydescription\"] == \"Continuous Med\"),\n",
    "    \"label\",\n",
    "] = \"Pantoprazole (Protonix) Continuous\"\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"itemid\"] == 225910)\n",
    "    & (inputs_small_3[\"ordercategorydescription\"] == \"Continuous Med\"),\n",
    "    \"itemid\",\n",
    "] = 2217441\n",
    "# remove the non dose from the drug shot version\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Pantoprazole (Protonix)\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"dose\")\n",
    "    ].index\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Preprocessing for MIMIC 4 items\n",
    "# Cleaning the Acetaminophen-IV (keep mg)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Acetaminophen-IV\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the D5 1/2NS (keep ml)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"D5 1/2NS\") & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the Dexmedetomidine (Precedex) (cast all to mg)\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"label\"] == \"Dexmedetomidine (Precedex)\")\n",
    "    & (inputs_small_3[\"amountuom\"] == \"mcg\"),\n",
    "    \"amount\",\n",
    "] /= 1000\n",
    "inputs_small_3.loc[\n",
    "    (inputs_small_3[\"label\"] == \"Dexmedetomidine (Precedex)\")\n",
    "    & (inputs_small_3[\"amountuom\"] == \"mcg\"),\n",
    "    \"amountuom\",\n",
    "] = \"mg\"\n",
    "\n",
    "# Cleaning the LR\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"LR\") & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the NaCl 0.9%\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"NaCl 0.9%\") & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the OR Crystalloid Intake\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"OR Crystalloid Intake\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the PO Intake\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"PO Intake\") & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning the Pre-Admission/Non-ICU Intake\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Pre-Admission/Non-ICU Intake\")\n",
    "        & (inputs_small_3[\"amountuom\"] != \"ml\")\n",
    "    ].index\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(inputs_small_3.groupby(\"label\")[\"amountuom\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing for inputs given in rates\n",
    "inputs_small_3.groupby(\"label\")[\"rateuom\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of Dextrose 5%  (remove the non mL/hour)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Dextrose 5%\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning of Magnesium Sulfate (Bolus)  (remove the non mL/hour)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Magnesium Sulfate (Bolus)\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning of NaCl 0.9% (remove the non mL/hour)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"NaCl 0.9%\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning of Piggyback (remove the non mL/hour)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Piggyback\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "# Cleaning of Packed Red Bllod Cells (remove the non mL/hour)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Packed Red Blood Cells\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# additional cleaning for mimic4\n",
    "# Cleaning of Acetaminophen-IV\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Acetaminophen-IV\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mg/min\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning of Fentanyl (Concentrate)\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Fentanyl (Concentrate)\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mcg/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning of Phenylephrine\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Phenylephrine\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mcg/kg/min\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "# Cleaning of Sterile Water\n",
    "inputs_small_3 = inputs_small_3.drop(\n",
    "    inputs_small_3.loc[\n",
    "        (inputs_small_3[\"label\"] == \"Sterile Water\")\n",
    "        & (inputs_small_3[\"rateuom\"] != \"mL/hour\")\n",
    "    ].index\n",
    ").copy()\n",
    "\n",
    "\n",
    "# Check if a single unit per drug\n",
    "inputs_small_3.groupby(\"label\")[\"rateuom\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the entries which are spread in time.\n",
    "We chose the duration window for the sampling. here we choose 30 minutes. So every entry which has a rate and with duration larger than 1 hour, we split it into fixed times injections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = inputs_small_3.loc[\n",
    "    (inputs_small_3[\"rate\"].notnull())\n",
    "    & (inputs_small_3[\"rateuom\"].str.contains(\"mg/min\"))\n",
    "].copy()\n",
    "df_temp[\"computed_amount\"] = df_temp[\"rate\"] * (\n",
    "    (\n",
    "        pd.to_datetime(df_temp[\"endtime\"]) - pd.to_datetime(df_temp[\"starttime\"])\n",
    "    ).dt.total_seconds()\n",
    "    / 60\n",
    ")\n",
    "\n",
    "# Check with a 0.01 tolerance\n",
    "assert (\n",
    "    len(df_temp.loc[(abs(df_temp[\"computed_amount\"] - df_temp[\"amount\"]) > 0.01)].index)\n",
    "    == 0\n",
    ")  # OK\n",
    "\n",
    "# Third check the kg/min units\n",
    "df_temp = inputs_small_3.loc[\n",
    "    (inputs_small_3[\"rate\"].notnull())\n",
    "    & (inputs_small_3[\"rateuom\"].str.contains(\"mcg/kg/min\"))\n",
    "].copy()\n",
    "df_temp[\"computed_amount\"] = (\n",
    "    df_temp[\"rate\"]\n",
    "    * (\n",
    "        (\n",
    "            pd.to_datetime(df_temp[\"endtime\"]) - pd.to_datetime(df_temp[\"starttime\"])\n",
    "        ).dt.total_seconds()\n",
    "        / 60\n",
    "    )\n",
    "    * df_temp[\"patientweight\"]\n",
    ")\n",
    "\n",
    "# Check with a 0.01 tolerance\n",
    "assert (\n",
    "    len(\n",
    "        df_temp.loc[\n",
    "            (abs(df_temp[\"computed_amount\"] / 1000 - df_temp[\"amount\"]) > 0.01)\n",
    "        ].index\n",
    "    )\n",
    "    == 0\n",
    ")  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_small_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_split_hours = 0.5\n",
    "to_sec_fact = 3600 * duration_split_hours\n",
    "\n",
    "# split data set in four.\n",
    "\n",
    "# The first dataframe contains the entries with no rate but with extended duration inputs (over 0.5 hour)\n",
    "df_temp1 = (\n",
    "    inputs_small_3.loc[\n",
    "        (\n",
    "            (\n",
    "                pd.to_datetime(inputs_small_3[\"endtime\"])\n",
    "                - pd.to_datetime(inputs_small_3[\"starttime\"])\n",
    "            )\n",
    "            > timedelta(hours=duration_split_hours)\n",
    "        )\n",
    "        & (inputs_small_3[\"rate\"].isnull())\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# The second dataframe contains the entries with no rate and low duration entries (<0.5hour)\n",
    "df_temp2 = (\n",
    "    inputs_small_3.loc[\n",
    "        (\n",
    "            (\n",
    "                pd.to_datetime(inputs_small_3[\"endtime\"])\n",
    "                - pd.to_datetime(inputs_small_3[\"starttime\"])\n",
    "            )\n",
    "            <= timedelta(hours=duration_split_hours)\n",
    "        )\n",
    "        & (inputs_small_3[\"rate\"].isnull())\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# The third dataframe contains the entries with a rate and extended duration inputs (over 0.5 hour)\n",
    "df_temp3 = (\n",
    "    inputs_small_3.loc[\n",
    "        (\n",
    "            (\n",
    "                pd.to_datetime(inputs_small_3[\"endtime\"])\n",
    "                - pd.to_datetime(inputs_small_3[\"starttime\"])\n",
    "            )\n",
    "            > timedelta(hours=duration_split_hours)\n",
    "        )\n",
    "        & (inputs_small_3[\"rate\"].notnull())\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# The forth dataframe contains the entries with a rate and low duration entries (< 0.5 hour)\n",
    "df_temp4 = (\n",
    "    inputs_small_3.loc[\n",
    "        (\n",
    "            (\n",
    "                pd.to_datetime(inputs_small_3[\"endtime\"])\n",
    "                - pd.to_datetime(inputs_small_3[\"starttime\"])\n",
    "            )\n",
    "            <= timedelta(hours=duration_split_hours)\n",
    "        )\n",
    "        & (inputs_small_3[\"rate\"].notnull())\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Check if split is complete\n",
    "assert len(df_temp1.index) + len(df_temp2.index) + len(df_temp3.index) + len(\n",
    "    df_temp4.index\n",
    ") == len(inputs_small_3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then process all of these dfs.\n",
    "# In the first one, we need to duplicate the entries according to their duration and then divide each entry by the number of duplicates\n",
    "\n",
    "# We duplicate the rows with the number bins for each injection\n",
    "df_temp1[\"Repeat\"] = np.ceil(\n",
    "    (\n",
    "        pd.to_datetime(df_temp1[\"endtime\"]) - pd.to_datetime(df_temp1[\"starttime\"])\n",
    "    ).dt.total_seconds()\n",
    "    / to_sec_fact\n",
    ").astype(int)\n",
    "df_new1 = df_temp1.reindex(df_temp1.index.repeat(df_temp1[\"Repeat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then create the admninistration time as a shifted version of the STARTTIME.\n",
    "df_new1[\"charttime\"] = df_new1.groupby(level=0)[\"starttime\"].transform(\n",
    "    lambda x: pd.date_range(\n",
    "        start=x.iat[0], freq=str(60 * duration_split_hours) + \"min\", periods=len(x)\n",
    "    )\n",
    ")\n",
    "# We divide each entry by the number of repeats\n",
    "df_new1[\"amount\"] = df_new1[\"amount\"] / df_new1[\"Repeat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the third one, we do the same\n",
    "# We duplicate the rows with the number bins for each injection\n",
    "df_temp3[\"Repeat\"] = np.ceil(\n",
    "    (\n",
    "        pd.to_datetime(df_temp3[\"endtime\"]) - pd.to_datetime(df_temp3[\"starttime\"])\n",
    "    ).dt.total_seconds()\n",
    "    / to_sec_fact\n",
    ").astype(int)\n",
    "df_new3 = df_temp3.reindex(df_temp3.index.repeat(df_temp3[\"Repeat\"]))\n",
    "# We then create the admninistration time as a shifted version of the STARTTIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new3[\"charttime\"] = df_new3.groupby(level=0)[\"starttime\"].transform(\n",
    "    lambda x: pd.date_range(\n",
    "        start=x.iat[0], freq=str(60 * duration_split_hours) + \"min\", periods=len(x)\n",
    "    )\n",
    ")\n",
    "# We divide each entry by the number of repeats\n",
    "df_new3[\"amount\"] = df_new3[\"amount\"] / df_new3[\"Repeat\"]\n",
    "\n",
    "df_temp2[\"charttime\"] = df_temp2[\"starttime\"]\n",
    "df_temp4[\"charttime\"] = df_temp4[\"starttime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, we merge all 4splits into one.\n",
    "inputs_small_4 = pd.concat([df_new1, df_temp2, df_new3, df_temp4])\n",
    "# The result is a dataset with discrete inputs for each treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_small_4.to_csv(\"raw/inputs_processed.csv\")\n",
    "inputs_small_4[\"hadm_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = adm_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 4 min\n",
    "df = pd.DataFrame()\n",
    "for chunk in pd.read_csv(\"mimiciv/3.1/hosp/labevents.csv.gz\", chunksize=500000):\n",
    "    adm_ids = list(adm[\"hadm_id\"])\n",
    "    chunk = chunk.loc[chunk[\"hadm_id\"].isin(adm_ids)]\n",
    "    df = pd.concat(\n",
    "        [df, chunk[[\"subject_id\", \"hadm_id\", \"charttime\", \"valuenum\", \"itemid\"]]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose previously selected admission ids.\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(df[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item ids\n",
    "item_id = pd.read_csv(\"mimiciv/3.1/hosp/d_labitems.csv.gz\")\n",
    "item_id_1 = item_id[[\"itemid\", \"label\"]]\n",
    "item_id_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of administered items\n",
    "lab2 = pd.merge(df, item_id_1, on=\"itemid\")\n",
    "lab2.head()\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(lab2[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only top 150 most used tests\n",
    "n_best = 150\n",
    "pat_for_item = lab2.groupby(\"label\")[\"subject_id\"].nunique()\n",
    "frequent_labels = pat_for_item.sort_values(ascending=False)[:n_best]\n",
    "lab3 = lab2.loc[lab2[\"label\"].isin(list(frequent_labels.index))].copy()\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(lab3[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the subset that was used in the paper (only missing is INR(PT))\n",
    "subset = [\n",
    "    \"Albumin\",\n",
    "    \"Alanine Aminotransferase (ALT)\",\n",
    "    \"Alkaline Phosphatase\",\n",
    "    \"Anion Gap\",\n",
    "    \"Asparate Aminotransferase (AST)\",\n",
    "    \"Base Excess\",\n",
    "    \"Basophils\",\n",
    "    \"Bicarbonate\",\n",
    "    \"Bilirubin, Total\",\n",
    "    \"Calcium, Total\",\n",
    "    \"Calculated Total CO2\",\n",
    "    \"Chloride\",\n",
    "    \"Creatinine\",\n",
    "    \"Eosinophils\",\n",
    "    \"Glucose\",\n",
    "    \"Hematocrit\",\n",
    "    \"Hemoglobin\",\n",
    "    \"Lactate\",\n",
    "    \"Lymphocytes\",\n",
    "    \"MCH\",\n",
    "    \"MCV\",\n",
    "    \"Magnesium\",\n",
    "    \"Monocytes\",\n",
    "    \"Neutrophils\",\n",
    "    \"PT\",\n",
    "    \"PTT\",\n",
    "    \"Phosphate\",\n",
    "    \"Platelet Count\",\n",
    "    \"Potassium\",\n",
    "    \"RDW\",\n",
    "    \"Red Blood Cells\",\n",
    "    \"Sodium\",\n",
    "    \"Specific Gravity\",\n",
    "    \"Urea Nitrogen\",\n",
    "    \"White Blood Cells\",\n",
    "    \"pCO2\",\n",
    "    \"pH\",\n",
    "    \"pO2\",\n",
    "]\n",
    "\n",
    "lab4 = lab3.loc[lab3[\"label\"].isin(subset)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab4.to_csv(\"raw/lab_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 1 min\n",
    "# only choose previously selected admission ids\n",
    "presc = pd.read_csv(\"mimiciv/3.1/hosp/prescriptions.csv.gz\")\n",
    "adm_ids = list(adm[\"hadm_id\"])\n",
    "presc = presc.loc[presc[\"hadm_id\"].isin(adm_ids)]\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(presc[\"subject_id\"].nunique())\n",
    "presc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select entries whose drug name is in the list from the paper.\n",
    "drugs_list = [\n",
    "    \"Acetaminophen\",\n",
    "    \"Aspirin\",\n",
    "    \"Bisacodyl\",\n",
    "    \"Insulin\",\n",
    "    \"Heparin\",\n",
    "    \"Docusate Sodium\",\n",
    "    \"D5W\",\n",
    "    \"Humulin-R Insulin\",\n",
    "    \"Potassium Chloride\",\n",
    "    \"Magnesium Sulfate\",\n",
    "    \"Metoprolol Tartrate\",\n",
    "    \"Sodium Chloride 0.9%  Flush\",\n",
    "    \"Pantoprazole\",\n",
    "]\n",
    "presc2 = presc.loc[presc[\"drug\"].isin(drugs_list)]\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(presc2[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(presc2.groupby(\"drug\")[\"dose_unit_rx\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units correction\n",
    "presc2 = presc2.drop(presc2.loc[presc2[\"dose_unit_rx\"].isnull()].index).copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[\n",
    "        (presc2[\"drug\"] == \"Acetaminophen\") & (presc2[\"dose_unit_rx\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "presc2.loc[\n",
    "    (presc2[\"drug\"] == \"D5W\") & (presc2[\"dose_unit_rx\"] == \"ml\"), \"dose_unit_rx\"\n",
    "] = \"mL\"\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[(presc2[\"drug\"] == \"D5W\") & (presc2[\"dose_unit_rx\"] != \"mL\")].index\n",
    ").copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[(presc2[\"drug\"] == \"Heparin\") & (presc2[\"dose_unit_rx\"] != \"UNIT\")].index\n",
    ").copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[(presc2[\"drug\"] == \"Insulin\") & (presc2[\"dose_unit_rx\"] != \"UNIT\")].index\n",
    ").copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[\n",
    "        (presc2[\"drug\"] == \"Magnesium Sulfate\") & (presc2[\"dose_unit_rx\"] != \"gm\")\n",
    "    ].index\n",
    ").copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[\n",
    "        (presc2[\"drug\"] == \"Potassium Chloride\") & (presc2[\"dose_unit_rx\"] != \"mEq\")\n",
    "    ].index\n",
    ").copy()\n",
    "presc2.loc[\n",
    "    (presc2[\"drug\"] == \"Sodium Chloride 0.9%  Flush\")\n",
    "    & (presc2[\"dose_unit_rx\"] == \"ml\"),\n",
    "    \"dose_unit_rx\",\n",
    "] = \"mL\"\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[(presc2[\"drug\"] == \"Bisacodyl\") & (presc2[\"dose_unit_rx\"] != \"mg\")].index\n",
    ").copy()\n",
    "presc2 = presc2.drop(\n",
    "    presc2.loc[\n",
    "        (presc2[\"drug\"] == \"Pantoprazole\") & (presc2[\"dose_unit_rx\"] != \"mg\")\n",
    "    ].index\n",
    ").copy()\n",
    "print(presc2.groupby(\"drug\")[\"dose_unit_rx\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid confounding labels with labels from other tables, we add \"drug\" to the name\n",
    "presc2[\"charttime\"] = pd.to_datetime(presc2[\"starttime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "presc2[\"drug\"] = presc2[\"drug\"] + \" Drug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presc2.to_csv(\"raw/prescriptions_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.read_csv(\"mimiciv/3.1/icu/outputevents.csv.gz\")\n",
    "outputs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose previously selected admission ids\n",
    "adm_ids = list(adm[\"hadm_id\"])\n",
    "outputs = outputs.loc[outputs[\"hadm_id\"].isin(adm_ids)]\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(outputs[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item names\n",
    "item_id = pd.read_csv(\"mimiciv/3.1/icu/d_items.csv.gz\")\n",
    "item_id_1 = item_id[[\"itemid\", \"label\"]]\n",
    "item_id_1.head()\n",
    "\n",
    "outputs_2 = pd.merge(outputs, item_id_1, on=\"itemid\")\n",
    "outputs_2.head()\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(outputs_2[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the n most used items\n",
    "n_best = 15\n",
    "pat_for_item = outputs_2.groupby(\"label\")[\"subject_id\"].nunique()\n",
    "frequent_labels = pat_for_item.sort_values(ascending=False)[:n_best]\n",
    "outputs_3 = outputs_2.loc[outputs_2[\"label\"].isin(list(frequent_labels.index))].copy()\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(outputs_3[\"subject_id\"].nunique())\n",
    "print(\"Number of datapoints remaining in the database: \")\n",
    "print(len(outputs_3.index))\n",
    "\n",
    "print(frequent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_label_list = [\n",
    "    \"Foley\",\n",
    "    \"Void\",\n",
    "    \"OR Urine\",\n",
    "    \"Chest Tube\",\n",
    "    \"Oral Gastric\",\n",
    "    \"Pre-Admission\",\n",
    "    \"TF Residual\",\n",
    "    \"OR EBL\",\n",
    "    \"Emesis\",\n",
    "    \"Nasogastric\",\n",
    "    \"Stool\",\n",
    "    \"Jackson Pratt\",\n",
    "    \"TF Residual Output\",\n",
    "    \"Fecal Bag\",\n",
    "    \"Straight Cath\",\n",
    "]\n",
    "outputs_bis = outputs_2.loc[outputs_2[\"label\"].isin(outputs_label_list)].copy()\n",
    "\n",
    "print(\"Number of patients remaining in the database: \")\n",
    "print(outputs_bis[\"subject_id\"].nunique())\n",
    "print(\"Number of datapoints remaining in the database: \")\n",
    "print(len(outputs_bis.index))\n",
    "\n",
    "outputs_3 = outputs_bis.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification that all input labels have the same amounts units\n",
    "outputs_3.groupby(\"label\")[\"valueuom\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_3.to_csv(\"raw/outputs_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_df = pd.read_csv(\"raw/lab_processed.csv\")[\n",
    "    [\"subject_id\", \"hadm_id\", \"charttime\", \"valuenum\", \"label\"]\n",
    "]\n",
    "inputs_df = pd.read_csv(\"raw/inputs_processed.csv\")[\n",
    "    [\"subject_id\", \"hadm_id\", \"charttime\", \"amount\", \"label\"]\n",
    "]\n",
    "outputs_df = pd.read_csv(\"raw/outputs_processed.csv\")[\n",
    "    [\"subject_id\", \"hadm_id\", \"charttime\", \"value\", \"label\"]\n",
    "]\n",
    "presc_df = pd.read_csv(\"raw/prescriptions_processed.csv\")[\n",
    "    [\"subject_id\", \"hadm_id\", \"charttime\", \"dose_val_rx\", \"drug\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of amount. Valuenum for every table\n",
    "inputs_df[\"valuenum\"] = inputs_df[\"amount\"]\n",
    "inputs_df = inputs_df.drop(columns=[\"amount\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df[\"valuenum\"] = outputs_df[\"value\"]\n",
    "outputs_df = outputs_df.drop(columns=[\"value\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presc_df[\"valuenum\"] = presc_df[\"dose_val_rx\"]\n",
    "presc_df = presc_df.drop(columns=[\"dose_val_rx\"]).copy()\n",
    "presc_df[\"label\"] = presc_df[\"drug\"]\n",
    "presc_df = presc_df.drop(columns=[\"drug\"]).copy()\n",
    "# Drop rows with non-numeric values\n",
    "presc_df = presc_df.drop(\n",
    "    presc_df[presc_df[\"valuenum\"].str.contains(\"-\", na=False)].index\n",
    ")\n",
    "presc_df[\"valuenum\"] = (\n",
    "    presc_df[\"valuenum\"].astype(str).str.replace(\",\", \"\", regex=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to distinguish between lab and inputs events\n",
    "inputs_df[\"Origin\"] = \"Inputs\"\n",
    "lab_df[\"Origin\"] = \"Lab\"\n",
    "outputs_df[\"Origin\"] = \"Outputs\"\n",
    "presc_df[\"Origin\"] = \"Prescriptions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both dfs.\n",
    "merged_df1 = (pd.concat([inputs_df, lab_df])).reset_index(drop=True)\n",
    "merged_df2 = (pd.concat([merged_df1, outputs_df])).reset_index(drop=True)\n",
    "merged_df = (pd.concat([merged_df2, presc_df])).reset_index(drop=True)\n",
    "assert merged_df[\"label\"].nunique() == (\n",
    "    inputs_df[\"label\"].nunique()\n",
    "    + lab_df[\"label\"].nunique()\n",
    "    + outputs_df[\"label\"].nunique()\n",
    "    + presc_df[\"label\"].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"charttime\"] = pd.to_datetime(\n",
    "    merged_df[\"charttime\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timestamp as the time delta between the first chart time for each admission\n",
    "merged_df[\"charttime\"] = pd.to_datetime(\n",
    "    merged_df[\"charttime\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "ref_time = merged_df.groupby(\"hadm_id\")[\"charttime\"].min()\n",
    "merged_df_1 = pd.merge(\n",
    "    ref_time.to_frame(name=\"ref_time\"), merged_df, left_index=True, right_on=\"hadm_id\"\n",
    ")\n",
    "merged_df_1[\"time_stamp\"] = merged_df_1[\"charttime\"] - merged_df_1[\"ref_time\"]\n",
    "assert len(merged_df_1.loc[merged_df_1[\"time_stamp\"] < timedelta(hours=0)].index) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label code (int) for the labels.\n",
    "label_dict = dict(\n",
    "    zip(\n",
    "        list(merged_df_1[\"label\"].unique()),\n",
    "        range(len(list(merged_df_1[\"label\"].unique()))),\n",
    "    )\n",
    ")\n",
    "merged_df_1[\"label_code\"] = merged_df_1[\"label\"].map(label_dict)\n",
    "\n",
    "merged_df_short = merged_df_1[\n",
    "    [\"hadm_id\", \"valuenum\", \"time_stamp\", \"label_code\", \"Origin\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_df = pd.Series(merged_df_1[\"label\"].unique()).reset_index()\n",
    "label_dict_df.columns = [\"index\", \"label\"]\n",
    "label_dict_df[\"label_code\"] = label_dict_df[\"label\"].map(label_dict)\n",
    "label_dict_df.drop(columns=[\"index\"], inplace=True)\n",
    "label_dict_df.to_csv(\"raw/label_dict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_short[\"valuenum\"] = merged_df_short[\"valuenum\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed for final dataset\n",
    "merged_df_short.drop([\"Origin\"], axis=1, inplace=True)\n",
    "merged_df_short.dropna(inplace=True)\n",
    "complete_df = merged_df_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create value- and mask- columns and fill with data\n",
    "labels = complete_df[\"label_code\"].unique()\n",
    "value_columns = []\n",
    "mask_columns = []\n",
    "for num in labels:\n",
    "    name = \"Value_label_\" + str(num)\n",
    "    name2 = \"Mask_label_\" + str(num)\n",
    "    value_columns.append(name)\n",
    "    mask_columns.append(name2)\n",
    "    complete_df[name] = 0\n",
    "    complete_df[name2] = 0\n",
    "    complete_df[name] = complete_df[name].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 2 min\n",
    "for index, row in complete_df.iterrows():\n",
    "    name = \"Value_label_\" + str(row[\"label_code\"])\n",
    "    name2 = \"Mask_label_\" + str(row[\"label_code\"])\n",
    "    complete_df.at[index, name] = row[\"valuenum\"]\n",
    "    complete_df.at[index, name2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all unneccesary columns and do sanity check\n",
    "complete_df.drop([\"valuenum\", \"label_code\"], axis=1, inplace=True)\n",
    "# merge duplicate rows using hadim_id and time_stamp as keys\n",
    "complete_df = complete_df.groupby([\"hadm_id\", \"time_stamp\"], as_index=False).max()\n",
    "for x in mask_columns:\n",
    "    assert len(complete_df.loc[complete_df[x] > 1]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Value_label_X to NaN where Mask_label_X is 0\n",
    "for i in range(109):\n",
    "    value_col = f\"Value_label_{i}\"\n",
    "    mask_col = f\"Mask_label_{i}\"\n",
    "    if value_col in complete_df.columns and mask_col in complete_df.columns:\n",
    "        # Set Value_label_X to NaN where Mask_label_X is 0\n",
    "        complete_df.loc[complete_df[mask_col] == 0, value_col] = np.nan\n",
    "# drop rows with missing values\n",
    "# Drop all Mask_label columns\n",
    "mask_cols = [col for col in complete_df.columns if col.startswith(\"Mask_label_\")]\n",
    "complete_df = complete_df.drop(columns=mask_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_ids = complete_df[\"hadm_id\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 30 seconds\n",
    "rad_df = pd.DataFrame()\n",
    "for chunk in pd.read_csv(\"mimic-iv-note/2.2/note/radiology.csv.gz\", chunksize=500000):\n",
    "    chunk = chunk.loc[chunk[\"hadm_id\"].isin(hadm_ids)]\n",
    "    rad_df = pd.concat([rad_df, chunk])\n",
    "rad_df = rad_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_df[\"charttime\"] = pd.to_datetime(rad_df[\"charttime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "rad_df_1 = pd.merge(\n",
    "    ref_time.to_frame(name=\"ref_time\"), rad_df, left_index=True, right_on=\"hadm_id\"\n",
    ")\n",
    "rad_df_1[\"time_stamp\"] = rad_df_1[\"charttime\"] - rad_df_1[\"ref_time\"]\n",
    "rad_df_1 = rad_df_1[[\"hadm_id\", \"time_stamp\", \"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only hadm_ids that are in both complete_df (time series) and rad_df_1 (text)\n",
    "save_hadm_ids = np.unique(rad_df_1[\"hadm_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change time_stamp from day time to '2000-01-01 00:00:00' format, where all time stamps start from 2000-01-01\n",
    "base_datetime = pd.Timestamp(\"2000-01-01 00:00:00\")\n",
    "complete_df[\"time_stamp\"] = pd.to_timedelta(complete_df[\"time_stamp\"])\n",
    "rad_df_1[\"time_stamp\"] = pd.to_timedelta(rad_df_1[\"time_stamp\"])\n",
    "complete_df[\"time_stamp\"] = complete_df[\"time_stamp\"] + base_datetime\n",
    "rad_df_1[\"time_stamp\"] = rad_df_1[\"time_stamp\"] + base_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_hadm_id = rad_df_1.groupby(\"hadm_id\")\n",
    "for hadm_id_float, group_df in grouped_by_hadm_id:\n",
    "    if hadm_id_float in save_hadm_ids:\n",
    "        hadm_id_int = int(hadm_id_float)\n",
    "        folder_name = str(hadm_id_int)\n",
    "        os.makedirs(folder_path + folder_name, exist_ok=True)\n",
    "        file_path = folder_path + folder_name + \"/text.csv\"\n",
    "        group_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_grouped_by_hadm_id = complete_df.groupby(\"hadm_id\")\n",
    "for hadm_id_float, group_df in ts_grouped_by_hadm_id:\n",
    "    if hadm_id_float in save_hadm_ids:\n",
    "        hadm_id_int = int(hadm_id_float)\n",
    "        folder_name = str(hadm_id_int)\n",
    "        os.makedirs(folder_path + folder_name, exist_ok=True)\n",
    "        file_path = folder_path + folder_name + \"/time_series.csv\"\n",
    "        group_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root_dir = \"raw\"\n",
    "\n",
    "# Dictionary to store row counts and missing rate\n",
    "entity_stats = {}\n",
    "\n",
    "# Traverse each subfolder (entity_id)\n",
    "for entity_id in os.listdir(root_dir):\n",
    "    entity_path = os.path.join(root_dir, entity_id)\n",
    "    time_series_path = os.path.join(entity_path, \"time_series.csv\")\n",
    "\n",
    "    if os.path.isdir(entity_path) and os.path.exists(time_series_path):\n",
    "        try:\n",
    "            # Load time_series.csv\n",
    "            df_ts = pd.read_csv(time_series_path)\n",
    "            time_series_count = len(df_ts)\n",
    "\n",
    "            # Calculate missing rate: total NaNs / total cells\n",
    "            total_cells = df_ts.shape[0] * df_ts.shape[1]\n",
    "            missing_cells = df_ts.isna().sum().sum()\n",
    "            missing_rate = missing_cells / total_cells if total_cells > 0 else None\n",
    "\n",
    "            entity_stats[entity_id] = {\n",
    "                \"time_series_rows\": time_series_count,\n",
    "                \"missing_rate\": missing_rate,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {time_series_path}: {e}\")\n",
    "\n",
    "# Sort by time_series row count and take top 20\n",
    "top_20_entities = sorted(\n",
    "    entity_stats.items(), key=lambda x: x[1][\"time_series_rows\"], reverse=True\n",
    ")[:20]\n",
    "\n",
    "# Now, count rows in text.csv and add it (using pandas)\n",
    "for entity_id, stats in top_20_entities:\n",
    "    text_path = os.path.join(root_dir, entity_id, \"text.csv\")\n",
    "    try:\n",
    "        df_text = pd.read_csv(text_path)\n",
    "        text_count = len(df_text)\n",
    "        stats[\"text_rows\"] = text_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {text_path}: {e}\")\n",
    "        stats[\"text_rows\"] = None\n",
    "\n",
    "# Prepare DataFrame\n",
    "top_20_df = pd.DataFrame(\n",
    "    [\n",
    "        (eid, stats[\"time_series_rows\"], stats[\"text_rows\"], stats[\"missing_rate\"])\n",
    "        for eid, stats in top_20_entities\n",
    "    ],\n",
    "    columns=[\"entity_id\", \"time_series_rows\", \"text_rows\", \"missing_rate\"],\n",
    ")\n",
    "\n",
    "entity_list = top_20_df[\"entity_id\"].tolist()\n",
    "entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature_names = [\n",
    "    \"Dextrose 5%\",\n",
    "    \"Sterile Water\",\n",
    "    \"Fentanyl\",\n",
    "    \"Heparin Sodium\",\n",
    "    \"Solution\",\n",
    "    \"Propofol\",\n",
    "    \"Phenylephrine\",\n",
    "    \"Foley\",\n",
    "    \"Norepinephrine\",\n",
    "    \"Midazolam (Versed)\",\n",
    "    \"pH\",\n",
    "    \"Base Excess\",\n",
    "    \"Calculated Total CO2\",\n",
    "    \"pCO2\",\n",
    "    \"pO2\",\n",
    "    \"Glucose\",\n",
    "    \"Sodium\",\n",
    "    \"Bicarbonate\",\n",
    "    \"Chloride\",\n",
    "    \"Urea Nitrogen\",\n",
    "    \"Creatinine\",\n",
    "    \"Potassium\",\n",
    "    \"Vasopressin\",\n",
    "    \"Anion Gap\",\n",
    "    \"Magnesium\",\n",
    "    \"Calcium, Total\",\n",
    "    \"Phosphate\",\n",
    "    \"Gastric Meds\",\n",
    "    \"Insulin - Regular\",\n",
    "    \"Piggyback\",\n",
    "]\n",
    "important_features = [\n",
    "    \"Value_label_\" + str(label_dict[label])\n",
    "    for label in important_feature_names\n",
    "    if label in label_dict\n",
    "]\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(entity_list, source_root=\"raw\", target_root=\"processed\"):\n",
    "    \"\"\"\n",
    "    Copies folders of selected entities from source_root to target_root.\n",
    "\n",
    "    Parameters:\n",
    "        entity_list (list): List of entity IDs (integers or strings).\n",
    "        source_root (str): Path to the source root directory (e.g., 'raw').\n",
    "        target_root (str): Path to the target root directory (e.g., 'processed').\n",
    "    \"\"\"\n",
    "    os.makedirs(target_root, exist_ok=True)\n",
    "\n",
    "    for entity_id in entity_list:\n",
    "        src_path = os.path.join(source_root, str(entity_id))\n",
    "        dst_path = os.path.join(target_root, str(entity_id))\n",
    "\n",
    "        if os.path.exists(src_path) and os.path.isdir(src_path):\n",
    "            try:\n",
    "                shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
    "                print(f\"Copied {entity_id} to {target_root}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to copy {entity_id}: {e}\")\n",
    "        else:\n",
    "            print(f\"Source folder not found for entity {entity_id}\")\n",
    "\n",
    "\n",
    "def process_csv_files(entity_list, root_dir=\"processed\"):\n",
    "    \"\"\"\n",
    "    - For text.csv: renames columns and sorts by date_time, keeps all columns.\n",
    "    - For time_series.csv: renames columns, filters to important_features + date_time + record_id, sorts and saves.\n",
    "    \"\"\"\n",
    "    for entity_id in entity_list:\n",
    "        entity_dir = os.path.join(root_dir, entity_id)\n",
    "\n",
    "        # Process text.csv\n",
    "        text_path = os.path.join(entity_dir, \"text.csv\")\n",
    "        if os.path.exists(text_path):\n",
    "            df_text = pd.read_csv(text_path)\n",
    "            df_text.rename(\n",
    "                columns={\"time_stamp\": \"date_time\", \"hadm_id\": \"record_id\"},\n",
    "                inplace=True,\n",
    "            )\n",
    "            if \"date_time\" in df_text.columns:\n",
    "                df_text.sort_values(by=\"date_time\", inplace=True)\n",
    "            df_text.to_csv(text_path, index=False)\n",
    "\n",
    "        # Process time_series.csv\n",
    "        ts_path = os.path.join(entity_dir, \"time_series.csv\")\n",
    "        if os.path.exists(ts_path):\n",
    "            df_ts = pd.read_csv(ts_path)\n",
    "            df_ts.rename(\n",
    "                columns={\"time_stamp\": \"date_time\", \"hadm_id\": \"record_id\"},\n",
    "                inplace=True,\n",
    "            )\n",
    "            # Determine columns to keep\n",
    "            cols_to_keep = []\n",
    "            if \"date_time\" in df_ts.columns:\n",
    "                cols_to_keep.append(\"date_time\")\n",
    "            if \"record_id\" in df_ts.columns:\n",
    "                cols_to_keep.append(\"record_id\")\n",
    "            cols_to_keep += [col for col in important_features if col in df_ts.columns]\n",
    "            df_ts = df_ts[cols_to_keep]\n",
    "            if \"date_time\" in df_ts.columns:\n",
    "                df_ts.sort_values(by=\"date_time\", inplace=True)\n",
    "            df_ts.to_csv(ts_path, index=False)\n",
    "\n",
    "\n",
    "extract_data(entity_list)\n",
    "process_csv_files(entity_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeimm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
