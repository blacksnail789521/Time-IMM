date_time,record_id,text
2017-01-18,pytorch/pytorch,"The discussions primarily revolve around accessing and extending PyTorch's source code, with a focus on improving distributed communication via in-process groups and exploring RDMA, MPI, or custom TCP options. Users report issues with specific functionality, such as LSTM requiring `.contiguous()` in batch_first mode, and express interest in updating legacy optimizers. There are concerns about hardware backend support, particularly the lack of official OpenCL support due to performance and development complexity, with a preference toward AMD GPUs and future HIP support once ready. Additionally, there is debate regarding the feasibility and strategic direction of supporting alternative hardware platforms outside CUDA, HIP, or MKLDNN, with some users advocating for broader hardware compatibility efforts. Overall, key themes include source code access, optimization fixes, and strategic hardware backend support."
2017-01-19,pytorch/pytorch,"The discussions highlight challenges in supporting diverse hardware backends, particularly the complexity of implementing and maintaining OpenCL and AMD GPU support, with community suggestions favoring SPIR-V and SYCL as future directions. There are technical issues with building dependencies, such as MKL libraries and pthreads on Linux, requiring environment configuration adjustments for successful compilation. Some users inquire about Windows support, with indications that adding core support requires dedicated developer effort since existing team members primarily use Linux. Additionally, there are ongoing efforts to improve build systems, dependency management, and API stability, including deprecation plans for legacy functions like `is_tensor`. Overall, the core concerns revolve around enhancing multi-platform compatibility, streamlining build processes, and expanding hardware support through community contributions."
2017-01-20,pytorch/pytorch,"The discussions cover several key technical concerns, including ensuring proper use of `super().__init__()` in `nn.Module` constructions, and considerations for supporting Windows, macOS, and variable-length sequence processing in PyTorch, emphasizing the implementation of efficient masking and input descriptor handling for cuDNN. There is ongoing dialogue about integrating cuDNN v6 support, managing CUDA compatibility across different hardware and software versions, and the challenges associated with coexisting code paths for different cuDNN versions. Additionally, questions are raised regarding image rendering issues in documentation, JavaPyPI packaging strategies to facilitate easier releases, and GPU-specific settings such as data types in convolution operations, indicating a focus on both usability and performance optimizations. Overall, the conversations highlight efforts to improve cross-platform support, library usability, and compatibility with evolving hardware capabilities."
2017-01-21,pytorch/pytorch,"The discussions primarily address issues with reopening PRs from private branches and build configuration problems, particularly relating to compatibility with specific GPU architectures and compiler setups. There is a focus on optimizing RNN performance by reducing kernel launches, with suggestions to pre-allocate buffers and perform in-place calculations, though the impact of memory allocation bottlenecks appears less significant due to improvements like the CUDA caching allocator. Masking strategies in backpropagation, especially using sentinel values versus indexing, are debated, with indexing identified as a faster and less error-prone method. Several comments highlight the importance of proper environment setup, such as setting correct compiler variables and choosing between source installation and Conda binaries for compatibility and performance. Overall, key concerns include ensuring build correctness, optimizing GPU computation, and implementing reliable sequence masking techniques."
2017-01-22,pytorch/pytorch,"The discussions highlight concerns about the lack of contiguity checks in PyTorch, with potential risks of incorrect results or internal errors when using non-contiguous inputs or weights, particularly with cuDNN modules. There is ongoing debate about the naming conventions for GPU support APIs, specifically whether to deprecate `.cuda()` in favor of a more generic `.gpu()` to better accommodate multiple backends like OpenCL and AMD's HIP. Support for Windows remains a challenge, requiring community contributions to achieve stable compilation and testing, especially with C++11 dependencies and compiler compatibility issues. Additionally, issues related to cudnn backend errors, API consistency, and kernel behavior under different hardware configurations are noted, with some uncertainties around the causes of specific errors and the best way to handle tensor device transfers across multiple backends. Unresolved questions include how to best enforce data contiguity, manage API naming transitions, and ensure robust compatibility across diverse hardware and software environments."
2017-01-23,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for improved GPU memory management and workspace allocation strategies in cudnn, with suggestions to fallback to less memory-intensive algorithms upon failures (issues #552, #554, #560). Compatibility issues related to CUDA versions and compute architectures are also emphasized, especially regarding K10 GPUs requiring CUDA 8.0 (issue #553). There are questions about proper usage patterns to ensure variables and tensors are on the GPU for CUDA operations, such as moving tensors into GPU memory correctly (issue #547, #560). Some discussions focus on optimizing or fixing specific functionalities, like porting certain loss functions (issue #551) or improving the performance of initial CUDA calls (issue #537). Unresolved questions include handling of intermediate gradients, efficient workspace estimation, and support for certain hardware or libraries like libdnn and OpenCL (issues #544, #562)."
2017-01-24,pytorch/pytorch,"The discussions highlight several key technical issues: First, ensuring contiguity of tensors, weights, and auxiliary buffers is critical for CuDNN modules to prevent incorrect results or internal errors, with suggestions to improve checks and error handling. Second, there is interest in expanding support for CPU functionalities like im2col/unfold through autograd integration and clarifying the distinctions between legacy 2D softmax/logsoftmax modules versus functional APIs. Third, handling of reproducibility and consistent naming conventions is discussed, including renaming files for compatibility and clarifying API usage to avoid user confusion. Fourth, addressing resource management and error handling, such as OOM errors in CuDNN and unpredicted behavior of embedding modules on CUDA, is important. Lastly, various usability improvements are proposed, including better documentation, code style enforcement, and structural enhancements like making `output_size` a non-optional parameter in pooling modules."
2017-01-25,pytorch/pytorch,"The discussions primarily revolve around optimizing GPU computation and library support in PyTorch, including removing unnecessary `.zero()` calls on gradient inputs to improve efficiency, and supporting multi-backend GPU operations (`.cuda()`, `.cl()`, `.gpu()`) for flexibility with different hardware like AMD and NVIDIA. There are concerns about proper handling of convolutional operations, particularly the ambiguity in `MaxPool2d` with `output_size` and the need to document and possibly enforce correct usage to prevent errors. Compatibility issues are addressed regarding cuDNN versions and dynamic loader problems, with suggestions to verify installations and manage environment variables like `LIBRARY_PATH`. Additional topics include planning release timelines (notably v6), Dockerfile organization, and handling evaluation mode limitations in cuDNN’s backward pass. Overall, the discussions focus on improving robustness, flexibility, and clarity in PyTorch’s GPU and library support."
2017-01-26,pytorch/pytorch,"The discussions highlight a range of technical concerns, including the integration of optional parameters like dilation in convolution routines, which is considered minimal yet potentially beneficial. Several issues relate to improving robustness and correctness, such as asserting support for backward passes in cuDNN evaluation mode and adding device-side assertions for out-of-bounds errors in modules like Embedding. There are ongoing efforts to enhance compatibility and usability, exemplified by plans for supporting Python 3.6, resolving environment-specific bugs (e.g., UCS2 Python 2.7), and revising autograd internals to handle multiple parallel hooks. Memory management on CUDA, especially regarding memory oversubscription and consistency between PyTorch and Lua Torch, remains a concern, with reports of out-of-memory issues under certain configurations. Lastly, some feature improvements are suggested, such as replacing certain tests with `unittest` and clarifying usage patterns for functions like `torch.cat`."
2017-01-27,pytorch/pytorch,"The discussions highlight several technical concerns, including memory management and cyclic references in CUDA-based RNN implementations, which cause excessive memory allocations; resolution suggestions involve explicit garbage collection and fixing cyclic references. There are also questions about API flexibility, particularly regarding merge functions in RNN layers, weighing simplicity against the need for configurable options. Additionally, efforts are noted to improve build efficiency through tools like ccache, and there are ongoing discussions on code style preferences, such as line length limits. Overall, the conversations focus on optimizing memory usage, API design, build processes, and code consistency."
2017-01-28,pytorch/pytorch,"The discussions primarily revolve around bug fixes and feature enhancements in PyTorch, including addressing issues with negative strides, cuDNN-related performance inconsistencies, and tensor operation behaviors that lead to unexpected Python number conversions. There is concern about cross-platform build compatibility, especially with macOS and LLVM/Clang compilers, as well as linker problems affecting CUDA integration on macOS. Other technical points include potential bugs in tensor concatenation (notably with singleton tensors and transpositions), and configuration management for code style enforcement tools like pep8. Overall, ongoing bug fixes, platform-specific issues, and build environment compatibility remain key areas of focus."
2017-01-29,pytorch/pytorch,"The discussions cover several technical concerns, including a suggestion to improve weight initialization in PyTorch by passing initializer functions directly to layers for better clarity and error handling. There are ongoing challenges related to compatibility and performance on AMD CPUs, with specific issues around vectorization and instruction set support, possibly requiring compiler adjustments. Bug reports highlight problems with out-of-bounds indexing on CUDA, specifically with sparse modules currently CPU-only, and potential indexing errors when switching models to GPU. Additionally, suggestions include adding support for advanced RNN features such as initializing hidden states and supporting batched inputs, along with ensuring thorough testing of these functionalities. Unresolved questions involve system compatibility, especially with different hardware and CUDA versions, and the need for more comprehensive test coverage for new or altered features."
2017-01-30,pytorch/pytorch,"The discussions highlight efforts to improve PyTorch's initialization mechanisms, favoring passing initializer functions directly to layers for clarity and flexibility. Several issues address build and environment compatibility, especially regarding CUDA support on AMD systems, distribution packaging, and supporting diverse hardware architectures with minimal effort. There are technical concerns about tensor device management—ensuring tensors are correctly moved to GPU—and about compiler optimizations that may affect performance on specific CPU architectures. Additionally, questions are raised about maintaining portability across platforms, dependency management (e.g., CMake detection of CUDA and cuDNN), and coding style standards like line-breaking conventions. Unresolved topics include integrating different architectures seamlessly, handling AMD hardware differences, and aligning external library usage across Torch7 and PyTorch."
2017-01-31,pytorch/pytorch,"The discussions highlight issues with missing libraries such as pthread and libm during the build process, which can be mitigated by adjusting environment variables like LIBRARY_PATH, and are slated for fixes in PR #650. Several bugs involve CUDA-specific behaviors, including unguarded cuDNN version checks, sparse tensor operations only supporting CPU, and potential out-of-bounds indexing issues that require device-side assertions. There are concerns about build and dependency management inconsistencies, exemplified by problems with the conda package for torchvision, and environment setup, which are addressed through recommended reinstallation steps. Additionally, some code design considerations are noted, such as improving DataParallel support by dynamically constructing parameter lists, and clarifications on function argument conventions to prevent syntax errors. Overall, the main concerns revolve around ensuring proper library linking, CUDA compatibility, robust error handling, and clearer code practices."
2017-02-01,pytorch/pytorch,"The discussions highlight ongoing efforts to improve CUDA compatibility and detection, with specific workarounds for architecture detection and build issues, such as modifying CMake files and handling FP16 support. There are concerns about proper system library detection (e.g., libgomp) and ensuring smooth compilation, especially when building from source or within Docker environments. Code development updates include the addition of functions like `torch.unbind` and fixing documentation formatting, while structural issues involve merging branches and managing conflicts. A significant area of focus is the lack of lazy evaluation in PyTorch, with suggestions to implement a lazy engine or symbolic variable system akin to Theano or TensorFlow to facilitate graph compilation and deferred execution. Overall, the conversations emphasize resolving build and environment compatibility problems, enhancing core functionalities, and exploring architectural changes to support more flexible computation paradigms."
2017-02-02,pytorch/pytorch,"The discussions highlight several technical concerns, notably issues with PyTorch installation from source versus binary, including import errors linked to missing modules such as `_C`, and potential conflicts caused by residual old installations. There are ongoing efforts to provide experimental deb packages for CPU-only configurations on Debian sid/testing, with considerations of CPU architecture compatibility and build flags like `-march=native`. Discussions also address limitations of PyTorch’s current imperative execution model, with suggestions for implementing lazy evaluation to enable graph compilation akin to Theano or TensorFlow, and potential integration with Keras. Testing and validation challenges are evident in the need to verify bias configurations in convolutional modules, as well as ensuring compatibility of shared code like `torch/nn` with PyTorch. Lastly, performance bottlenecks, especially related to HDD disk I/O during training, are examined, with recommendations to optimize dataset loading, though some issues remain unresolved."
2017-02-03,pytorch/pytorch,"The discussions primarily focus on improving flexibility and usability in PyTorch, such as implementing a more user-friendly initialization mechanism for layers—suggesting passing initializer functions directly or supporting string/class name identifiers, with debate over the preferred approach. There are concerns about cross-platform compatibility, especially regarding the handling of data types like `long` versus `int64_t` in C/C++ code, prompting patching strategies to ensure platform independence. Some issues address enhancing integration and extending functionality, such as wrapping external libraries (warp-ctc), supporting persistent data iterators in DataLoader for long-running or continuous processes, and handling biases in cuDNN RNN implementations. Additionally, configuration challenges like environment setup for CUDA and handling binary compatibility are raised, alongside proposals for more elegant, robust solutions to these technical problems."
2017-02-04,pytorch/pytorch,"The discussions focus on transitioning from using `long` to `int64_t` for cross-platform consistency, with the suggestion to centralize type definitions for maintainability. Some contributors prefer to preserve macros and the current approach for simplicity and clarity, especially aligning with existing types like `THHalf`. There are concerns about the impact of `sizeof(long)==4` on Windows, but it is deemed manageable if tensor types other than `LongTensor` are used initially, postponing the change. The serialization process, which writes tensor dimensions as longs, raises worries about compatibility issues across platforms. Overall, the main unresolved question concerns the timing and scope of converting tensor dimension storage to ensure portability without complicating serialization."
2017-02-05,pytorch/pytorch,"The discussions highlight issues with mixed-precision FP16 support, notably that batch normalization fails with FP16 inputs due to backend limitations and key errors, indicating incomplete FP16 functionality in batchnorm. There is concern about the compatibility and support for GPUs with low compute capability (e.g., 2.1), questioning whether such hardware can run current PyTorch builds. Clarifications are sought regarding the accuracy of existing comments about CUDA implementation status, especially user tests involving `.cuda()`, and whether to manually migrate code from `torch/nn` to the main repository. Additionally, there are ongoing efforts to merge modifications via git subtrees, and a specific PR addressing shared code has been linked for integration. Overall, core issues revolve around FP16 support inconsistencies, GPU compatibility, and proper code migration practices."
2017-02-06,pytorch/pytorch,"The discussions highlight several technical concerns, including the limited support for fp16 in cuDNN convolution and batch normalization, with potential accuracy degradation and performance issues on older GPU architectures. There are challenges in supporting older CUDA compute capabilities (e.g., 2.1, 2.0), which affect compatibility and performance, along with considerations for rebuilding PyTorch from source for such hardware. Questions arise about extending functionality, such as implementing probability distribution functions (pdfs) requiring special mathematical operations like gamma functions, and the necessity of adding features like infinite data iterators or worker initialization functions in DataLoader. Unresolved issues include handling fp16 support on various backends, supporting legacy GPU architectures, and implementing mathematical functions like lgamma with proper autograd support for CUDA. Overall, the main points concern hardware compatibility, numerical stability in floating-point operations, and enhancing framework flexibility through new features."
2017-02-07,pytorch/pytorch,"The discussions highlight concerns about the mutability of `Sequential` modules, with some advocating for immutability to maintain invariants and prevent confusion, while others favor mutable APIs for user flexibility, especially regarding module addition/removal. There are technical considerations around replacing `long` with `int64_t` for tensor dimensions, balancing portability and compatibility, notably on Windows, and the need for careful serialization handling with `__setstate__`. Test failures in RNN and module behavior are partly attributed to code changes, with suggestions to modify the dataset input dimensions for certain layers to ensure correct operation. Additionally, questions arise about comparison operators like `==` with `None`, emphasizing adherence to numpy-like behavior and avoiding deprecation issues. Overall, the conversations revolve around API design decisions, type portability, serialization robustness, and maintaining compatibility and consistency in the framework."
2017-02-08,pytorch/pytorch,"The discussions highlight performance improvements to file I/O by rewriting load/save functions to avoid tar, and address tensor handling in GPU operations, specifically noting that THCPAutoGPU only processes one nesting level in lists. A significant concern involves cache inefficiencies in CPU implementation of `expand`, causing substantial slowdowns during backward passes, with proposed workarounds like replacing `expand` with `stack` and `mm`. There are questions about whether the `reserveSpace` used in cuDNN RNN operations can be relied upon across multiple backward passes, with clarifications suggesting that `reserveSpace` is mutable with `backwardData` and should be cloned if reused. Additionally, some comments hint at ongoing debugging efforts to improve cache utilization and operation speed on both CPU and GPU. Overall, the main focus is on optimizing tensor manipulation, ensuring correct reuse of workspace memory, and maintaining performance consistency across hardware."
2017-02-09,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's robustness and compatibility, such as making CrossEntropyLoss more universal and merging 2D and 2D variants of loss functions, with some acceptance of input shape modifications. There are technical challenges related to performance bottlenecks, notably CPU cache issues with the `sum` operation and slow memory operations for backward passes, with proposed temporary fixes like replacing `th.bmm` with `th.stack` and `mm`. Compatibility and environment setup are concerns, exemplified by installation issues due to missing or outdated helper modules (`tools.setup_helpers`) and driver version conflicts on CUDA, with suggested troubleshooting steps involving driver upgrades and reinstallations. Certain questions, like how to ignore specific linting errors, remain unresolved, hinting at the need for tooling adjustments. Overall, the focus is on fixing bugs, enhancing compatibility, and optimizing performance, with some unresolved configuration and environment issues."
2017-02-10,pytorch/pytorch,"The discussions highlight installation and import issues, particularly with Python 2.7, Anaconda, and module path conflicts, emphasizing the importance of proper environment setup and directory management to resolve module import errors such as ""No module named _C"". Users seek guidance on specific functionalities like implementing minibatch discrimination and combining tensor operations, with practical code solutions provided. There are technical challenges related to cache performance in tensor computations and the complexities of module registration within PyTorch's neural network framework, including potential manual module registration or automatic handling via `nn.ModuleList`. Package distribution and installation hurdles, especially with pip and the need for correct pre-built binaries, are also addressed. Overall, these threads underscore the importance of environment configuration, code correctness, performance optimizations, and proper module management in PyTorch development and usage."
2017-02-11,pytorch/pytorch,"The discussions highlight ongoing efforts to deprecate legacy functions like `is_tensor` and `is_storage` in favor of standard `isinstance` checks for `torch.Tensor` and `torch.Storage`. There is attention to ensuring consistency in handling input biases and weights between cuDNN and PyTorch RNN implementations, with suggestions to manage bias propagation and maintain intuitive behavior when skipping input weights. Some issues related to system library conflicts and environment setup suggest adopting conda packages for stability. Additionally, there is acknowledgment of upcoming feature support requiring modifications to C backend infrastructure, with some fixes scheduled for implementation soon. Overall, the focus is on refining API consistency, handling system-specific issues, and planning backward-compatible enhancements."
2017-02-12,pytorch/pytorch,"The discussions highlight concerns about API consistency and usability, particularly with module management in sequential containers, where there's debate over whether these should be immutable or mutable, and how to handle module naming conventions. There's also consideration of deprecating legacy functions like `is_tensor` and `is_storage` in favor of newer ABC-based checks, amid ongoing API stability questions. In the cuDNN context, the approach to handling input biases and flags like `skip_input` is debated, especially regarding tensor optionality and backward compatibility. A bug related to serialization shows the need for a type-casting fix in a `min` function call. Additionally, a workaround for runtime errors in dependency engine issues involves re-packaging tensors, with unresolved questions about proper long-term solutions."
2017-02-13,pytorch/pytorch,"The discussions highlight ongoing efforts to redesign PyTorch's RNN modules for improved API consistency, modularity, and compatibility with cuDNN, emphasizing the removal of legacy code and the use of dedicated cell modules (like `LSTMCell` and `RNNCell`) that can be optimized independently. There is mention of challenges maintaining full cuDNN support with main classes such as `LSTM` and `RNN`, suggesting possible workaround strategies like substituting forward passes with cuDNN-based functions. Additionally, several technical issues related to build processes are addressed, including compatibility fixes for MacOS deployment targets and type adjustments for buffer size variables to prevent compiler errors. Concerns around build efficiency and resource utilization on platforms like OS X are also raised, questioning the cost-benefit of maintaining certain continuous integration tests. Unresolved questions remain about how best to balance API flexibility, backward compatibility, and performance, especially concerning cuDNN integration and the modularization of RNN components."
2017-02-14,pytorch/pytorch,"The discussions highlight ongoing enhancements and fixes in PyTorch, including support for strides in tensor creation, with non-negative strides being prioritized. Several issues address CUDA and MKL compatibility, driver requirements, and proper configuration of environment variables to resolve linking errors. There are technical clarifications and considerations regarding the use of cuDNN's reserveSpace for RNN backward passes, emphasizing the need to clone reserveSpace when backpropagating multiple times. Additional concerns involve reverting certain changes for linear and padding modules to support batched input modes and understanding the behavior of `output_padding` in transposed convolutions, with suggested documentation updates. Overall, the focus centers on improving tensor manipulation flexibility, ensuring seamless CUDA/MKL integration, and clarifying RNN backward operation dependencies."
2017-02-15,pytorch/pytorch,"The discussions highlight several key technical issues including the need for clearer error messaging, such as specifying expected tensor types in exceptions, and handling optional parameters like weights and biases in RNN cells while maintaining compatibility with saving/loading mechanisms. There are efforts to patch or extend existing functions, like `_view4d`, to fix bugs or enhance functionality, with some fixes merged into master. Concerns about automatic network conversion for batch mode in `load_lua`, especially regarding different tensor dimensions and index conventions, indicate ongoing challenges with backward compatibility. Additionally, questions about tensor device queries and improvements to loss functions for stable gradients are raised, with some enhancements scheduled for upcoming releases."
2017-02-16,pytorch/pytorch,"The discussions reveal ongoing technical challenges such as modifying `Tensor.cwrap`'s transpose function, with a preference to implement the fix within the Python wrapper to avoid changes in C libraries. There are concerns about supporting optional parameters like `w_ih` and `b_ih` in RNN cells, especially regarding the proper handling of tensor storage and backward pass saving. Multiple comments address performance issues, including cache inefficiencies in summing operations, with plans for algorithmic improvements via upcoming PRs. Support for multi-GPU data parallelism in RNN modules is identified as inconsistent and potentially as a bug, necessitating careful argument packing. Lastly, common usability questions include tensor contiguity after transpose and correct argument ordering in `torch.save`, pointing to ongoing refinements in function behavior and user guidance."
2017-02-17,pytorch/pytorch,"The discussions highlight challenges with hardware-specific issues, such as incompatibilities between binary optimizations and CPUs like AMD architectures, leading to potential crashes (e.g., illegal instructions) and the need for source builds or compiler adjustments. There are ongoing efforts to fix bugs related to tensor detachment and dependency management in autograd, with fixes committed in recent PRs. Several feature requests are noted, including adding support for complex tensors, extending torch.Size functionalities, and ensuring proper weight initialization in RNNs. Additionally, version inconsistencies in CUDA tooling and the need for alternative installation methods (binaries, source, Docker) are discussed to mitigate hardware/software conflicts. Lastly, there are proposals for optimizing bitwise operations in C backends, with an emphasis on balancing implementation complexity against performance gains."
2017-02-18,pytorch/pytorch,"The discussions highlight concerns over CPU instruction set support, particularly the lack of SSE4_1 and SSE4_2 on certain AMD Athlon processors, leading to potential compilation and performance issues; one proposed solution involves shipping binaries with different instruction set optimizations or utilizing compiler features like multiversioning. Additionally, there are challenges related to the installation process of PyTorch, specifically errors due to missing modules like 'tools.setup_helpers', which complicate building or installing from source. Performance optimization suggestions include merging memory operations into single kernels for efficiency, though some argue the gains may be negligible due to hardware granularity. There are also updates and questions about code improvements, such as porting tutorials to sphinx and addressing backward compatibility, with some coordination around task prioritization and contribution. Overall, unresolved technical and logistical issues persist around build configurations, binary distributions, and performance enhancements."
2017-02-19,pytorch/pytorch,"The discussions highlight several technical concerns, including troubleshooting a CUDA-related bug where the loss returns zero on the GPU, and addressing dataset implementation errors that stem from returning NumPy arrays instead of tensors—suggesting the use of `torch.from_numpy`. There are also significant discussions around compatibility issues, such as segmentation faults caused by import order dependencies between torch and scientific libraries like scipy and sklearn, possibly related to numpy initialization or BLAS/MKL configurations. Additionally, suggestions are made to refactor test code to utilize the `unittest` framework and to improve code inheritance practices, as seen in dataset class fixes. Unresolved questions include how to effectively address environment-specific crashes and ensuring dataset code returns correct data types for seamless PyTorch integration."
2017-02-20,pytorch/pytorch,"The discussions highlight the need for native broadcasting support in PyTorch, with suggestions to use functions like `repeat` and `expand` as interim solutions. Users inquire about best practices for API compatibility and implementation details, such as annotating arguments in cwrap or initializing bias parameters in LSTM/GRU cells. There are concerns about testing frameworks, particularly integrating unit tests with `unittest`, and ensuring CUDA compatibility and debugging GPU-related issues, such as zero loss values. Some discussions also focus on data inspection techniques (e.g., examining min/max values) and clarifications about library functions, error handling, and documentation consistency. Overall, the conversations emphasize improving user experience through better API design, clearer documentation, and robust testing/support for GPU operations."
2017-02-21,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's tensor functionality, such as adding support for strides in tensor initialization, with initial efforts resulting in non-negative stride support being merged. There are concerns about incomplete documentation for functions like `unsqueeze`, leading to suggestions for improvement. The issue of dataset handling is addressed, emphasizing the need to convert numpy arrays to tensors using `torch.from_numpy`, and a move towards supporting ndarray types more generally. Shared memory limitations in Docker environments affect data loading performance, prompting users to increase shared memory size. Lastly, there's debate on adding certain modules like `PairwiseDistance` to core PyTorch, with suggestions to keep them in user utilities due to their simplicity, and some discussion about proper handling of embedding lookups for `Variable` tensors."
2017-02-22,pytorch/pytorch,"The discussions primarily revolve around problematic behaviors and potential improvements in the PyTorch framework, including issues with the output indices in nn.MaxPool1d and the depreciation path for old behaviors, as well as CUDA extension interfacing and kernel JIT compilation. Several comments address challenges with gradient computations when slicing tensors or using in-place modifications, suggesting that proper handling of non-contiguous tensors and in-place ops is essential. An ongoing focus is on improving user experience through clearer error messages, more consistent API behavior (e.g., variable immutability and batch processing limitations), and enhanced support for extension development, including CUDA code interfacing via tools like PyCUDA. Some issues are identified as bugs requiring fixes, notably with tensor dimensions and method support, with efforts underway to implement these solutions or clarify design choices. Overall, the discussions highlight a need for more robust, user-friendly, and flexible functionalities in the PyTorch ecosystem."
2017-02-23,pytorch/pytorch,"The discussions address several technical topics including ensuring compatibility in tensor serialization across different sizes, particularly in data writing and deserialization, and fixing the behavior of MaxPool1D by potentially replacing it with a MaxPool2D with a kernel height of 1 to match the expected W dimension indices. There are questions about leveraging cuDNN support for variable-length sequences, and suggestions to improve argument annotations in cwrap for cleaner API modifications. Issues around proper testing, including adding relevant test cases and handling dependencies like scipy, are also highlighted. Additionally, there’s discussion on integrating new functionalities such as `torch.dist` alignment with existing modules, and technical details of internal implementations like storage sharing and multiprocessing reductions."
2017-02-24,pytorch/pytorch,"The discussions highlight several key technical concerns: the need for proper version tagging and release management in Issue #752; suggestions to improve code maintainability by adding argument annotations or `before_call` modifications in Issue #792; issues with test reliability and edge case handling, such as disabled index checks in MaxPool1d tests (Issue #832); dependency management challenges, notably the optional use of scipy for statistical tests and the associated installation failures on Travis CI (Issue #833); and performance/oom problems related to in-place tensor modifications on GPU, as observed in Issue #839. Additionally, there is a discussion on manual configuration of MKL threads for improved performance when using PyTorch alongside numpy (Issue #841). These points reflect ongoing efforts to optimize code quality, testing robustness, and environment handling."
2017-02-25,pytorch/pytorch,"The discussions primarily revolve around issues related to GPU compatibility and tensor operations, including ensuring tensors are properly moved to the GPU to avoid runtime errors (e.g., issue #631). Several threads address the need to modify or extend existing functionalities, such as adding `None` handling in tensor iteration (#658) and fixing the MaxPool1D implementation to match Torch7 behavior (#771). There are concerns about CUDA memory management and potential memory leaks due to in-place tensor modifications (#839), as well as discussions on software maintenance tasks like patching optimizer `__setstate__` methods (#774) and improving code robustness. Support requests and clarifications about system configuration, driver setup, and model training stability are also present. Overall, the issues highlight ongoing efforts to address hardware compatibility, API correctness, code consistency, and stability improvements."
2017-02-26,pytorch/pytorch,"The discussions highlight concerns about GPU memory management and in-place tensor modifications causing memory leaks or OOM errors, with suggested fixes involving avoiding in-place ops or modifying in a way that prevents reference cycles. There is debate over proper handling of tensor broadcasting and expansion, especially optimizing pointwise operations and considering automatic broadcast addition in future updates, with suggestions for efficient CUDA implementations. Several issues pertain to maintaining code stability and compatibility, such as rebasing and squashing commits, integrating new features like optional weights, and handling merge conflicts. Questions remain about the best practices for model serialization/deserialization, including exporting models for production and supporting multiple initialization variants (fan_in vs. fan_out). Lastly, some discussions address improving testing and documentation, ensuring code correctness, and clarifying the fragility of model saving mechanisms compared to parameter storage."
2017-02-27,pytorch/pytorch,"The discussions encompass a variety of technical concerns, including the stability and reliability of CUDA support in the current PyTorch alpha, with some tests passing and others failing, highlighting the need for further debugging and testing. Improvements to optimizer implementations are suggested, such as enhancing the `__setstate__` method for better state management and referencing relevant research papers in docstrings. There is also a recommendation to create functional counterparts for all `Module` classes to promote modularity, exemplified by pixel shuffle. Additionally, issues related to in-place operations on CUDA tensors, multinomial sampling inconsistencies, and handling of incomplete `state_dict` objects are addressed, with suggested fixes and workarounds provided. Overall, the conversations indicate ongoing efforts to refine core functionalities, ensure compatibility, and improve usability."
2017-02-28,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's CUDA support, including debugging and fixing compatibility issues on Windows and enhancing performance for functions like `sum` via cache-aware algorithms. There is active work on refactoring serialization (`load`/`save`) for efficiency, as well as expanding initialization methods (Kaiming, Xavier) with configurable options like `mode` and `fan_in/out`. Open questions remain regarding the support for different initialization variants, such as incorporating `use_fan_out`, and whether features like the `gain` parameter should be retained. Additionally, debugging and reproducing issues like the multinomial sampling bug and multi-GPU handling remain priorities, with community contributions and patches being developed to resolve these challenges. Overall, the focus is on stabilizing, optimizing, and extending core functionalities while addressing existing bugs and unanswered design questions."
2017-03-01,pytorch/pytorch,"The discussions highlight several technical concerns, including installation difficulties due to missing modules like 'tools.setup_helpers,' which require proper wheel or binary packages and clear installation instructions. There are questions about code structure and organization, especially regarding loss layer implementations and the addition of features like fan_out modes in initializers, with suggestions for argument options and code refactoring. Challenges related to model stability and GPU kernel performance are discussed, notably for small batch sizes and compatibility with CUDA versions, including issues with CUDA 8.0 and hardware-specific performance bottlenecks. Additionally, there is deliberation on integrating tutorials into documentation versus maintaining separate files, balancing ease of access and maintenance efforts. Unresolved issues include ensuring proper setup for different CUDA versions, optimizing GPU operations for small tensors, and clarifying code organization for easier development and troubleshooting."
2017-03-02,pytorch/pytorch,"The discussions highlight technical challenges such as installation issues with older versions of PyTorch using `pip` and the `ImportError` related to `tools.setup_helpers`, suggesting potential errors in package setup or version mismatches. There are concerns about incompatible input tensor dimensions in models, specifically the need to modify network inputs (e.g., changing to 4D tensors) and adjust model components like `nn.Linear`, indicating possibly rigid model architecture assumptions. Efforts to optimize batched matrix multiplications (`bmm`) on CUDA involve leveraging external libraries like MAGMA or fallback strategies like expanding and summing tensors, reflecting performance and API constraints. The conversation also touches on extending data types (adding `ShortTensor` support) by enabling appropriate NumPy conversions and the importance of correct Dockerfile modifications for reproducible environments. Unresolved questions include proper handling of sparse gradients, choosing meaningful naming conventions for new batched operations, and ensuring compatibility and stability across various installation and execution contexts."
2017-03-03,pytorch/pytorch,"The discussions reveal ongoing efforts to modify and deprecate certain behaviors, with careful consideration of fallback strategies and communication plans. Several issues focus on debugging and optimizing CUDA operations, notably addressing gradient initialization problems, batching small matrix multiplications, and ensuring compatibility across different hardware architectures. Algorithmic and implementation challenges are discussed around model statefulness, module flattening, and reducing code ugliness, especially for RNN operations, with proposals for restructuring internal buffers and workflows. Troubles with setup, build configurations, and ensuring consistent environment behavior (e.g., Dockerfile modifications, Python process handling) are also prominent. Unresolved questions remain about improving code clarity, performance on diverse GPUs, and handling complex autograd behaviors, indicating active development and refining of PyTorch's core functionalities."
2017-03-04,pytorch/pytorch,"The discussions primarily focus on improving the flexibility and usability of loss functions and RNN modules, such as modifying the `size_average` parameter to return a per-sample tensor, enabling masking in cross-entropy losses, and handling variable-length RNNs with masking capabilities. There are ongoing efforts to refactor data loading and collection code, including adding support for bytes objects and removing redundant cases. Additionally, there is consideration of changing underlying C/CUDA implementations, especially for RNNs, with challenges around maintaining statelessness, buffer management, and compatibility with existing user code. Support for CUDA versions below 7.5 and installation methods (binary vs source) is also discussed. Overall, questions remain on how to balance backward compatibility, ease of use, and performance optimizations in core library features."
2017-03-05,pytorch/pytorch,"The discussions highlight several technical concerns, including proper implementation of custom modules like Maxout, where initializing layers within the forward pass is discouraged and constructor-based setup is recommended. There are questions about performance impacts of specific calls, particularly in relation to small LSTMs, and whether its optimization is necessary. Debugging issues such as segmentation faults are addressed via stack traces pointing to potential interactions between PyTorch, OpenCV, and OpenCL, with suggestions to import OpenCV prior to PyTorch to mitigate conflicts. Additionally, there are maintenance and integration questions, such as resolving conflicts in PRs, fixing build issues like GCC installation on OSX, and addressing the breaking changes caused by upstream optimizations."
2017-03-06,pytorch/pytorch,"The discussions highlight several technical concerns, including the inadequacy of the current `pip install pytorch` method, which leads to errors due to missing setup files and unsupported installation from PyPI—suggesting the need for clearer instructions or pre-built wheel files for ease of installation. There is a focus on performance optimization, particularly regarding the overhead of small operation calls like repeated cudaGetDevice calls and microbenchmarking unrolled LSTMs, indicating potential improvements in CUDA call efficiency and code batching. Discussions also touch on the importance of robust testing, particularly in data loading (`DataLoader`) modifications, to ensure correct functionality across different data types and Python versions. Additionally, issues related to environment setup, such as dependency management with subtrees or external libraries like openMP, are addressed, emphasizing environments' stability during installation. Overall, the consensus points toward enhancing user guidance, optimizing low-level CUDA calls, and strengthening testing protocols to improve PyTorch's usability and performance."
2017-03-07,pytorch/pytorch,"The discussions highlight a range of technical concerns, including the need for better dataset download management and handling of partial downloads, as mentioned in Issue #740, and the potential for extending functionalities within `torch.nn.functional` and `torch.nn` to improve API parity, as discussed in Issue #835. There are questions regarding environment setup and module path configurations, emphasizing the importance of correctly installing and utilizing PyTorch within virtual environments (Issues #909 and #943). Testing and stability issues are evident, such as occasional failures in legacy neural network tests and the necessity of adding comprehensive tests for new modules like PReLU (Issues #942 and #946). Additionally, there are architectural considerations about tensor representations, like differentiating dense and sparse tensors for better flexibility (#947), and concerns about memory constraints affecting CUDA operations (Issue #953). Overall, key suggestions include improving error messaging, refining API consistency, ensuring environment correctness, and enhancing test coverage for robustness."
2017-03-08,pytorch/pytorch,"The discussions highlight various technical challenges including improving error message clarity, especially for tensor type mismatches, and handling out-of-memory errors on GPUs, often related to memory management and insufficient resources. There are issues with certain operations like transposing 1D tensors wrapped in Variables, causing confusing runtime errors, and OpenCV integration conflicts potentially due to interaction with CUDA or OpenCL. Several comments address ensuring proper dependency management, such as using conda to install dependencies like OpenMP, and fixing build issues in specific PyTorch versions. In-place operation caveats and zero-strided tensors are acknowledged as low-priority but important considerations for users. Overall, the key concerns revolve around debugging, clearer error reporting, resource management, and library compatibility."
2017-03-09,pytorch/pytorch,"The discussions cover various technical concerns, including the implementation of `__setstate__` in an optimizer to handle parameter group defaults, with uncertainty about the origin of the ""centered"" parameter, and considerations for documentation references. Several issues relate to resource management, such as reducing test script size and mitigating excessive GPU memory consumption during model training, with suggestions to replace data generation with more efficient tensors and to set parameters for memory control. Compatibility and reproducibility challenges are also highlighted, such as converting old Torch models via LuaTorch and addressing non-reproducible errors with backward passes in complex models. Additionally, debates focus on testing strategies across data types (float, double, half), ensuring consistency in functionalities, and fixing recent code changes that cause build failures. Overall, the issues emphasize improving robustness, efficiency, and compatibility within the PyTorch ecosystem."
2017-03-10,pytorch/pytorch,"The discussions highlight ongoing efforts to unify tensor implementations across CPU and GPU through templating or traits, aiming for cleaner, more consistent code. There are concerns about compatibility and proper loading of legacy Torch models, with suggestions to re-save models in LuaTorch or update PyTorch versions. Performance issues, particularly with multi-threaded backward passes on CPU, suggest that process synchronization and thread management need careful handling, possibly through `set_num_threads`. Reproducibility of bugs raised in DataLoader and convolution operations is complicated by hardware and software environment differences, indicating a need for minimal scripts. Additionally, some features like `nn.functional` tensor support and new argument documentation, as well as model serialization compatibility, remain unresolved or under active development."
2017-03-11,pytorch/pytorch,"The discussions primarily address performance variability and optimization challenges across different GPU architectures (Kepler vs Maxwell), with benchmarking code highlighting the impact of specific methods such as batched GEMM and element-wise multiplication. Questions are raised about the consistency of performance differences across hardware and driver versions, especially regarding CUDA version effects on Pascal and Maxwell GPUs. There are concerns about the proper handling of negative padding in transposed convolutions with cuDNN, and whether regular padding should also be checked for negativity. Additionally, linking and library dependencies, particularly related to MKL and threading libraries, are discussed, emphasizing the need for proper linking configurations to ensure consistent performance. Lastly, there's mention of API design considerations for dropout semantics, suggesting clearer parameter handling and documentation."
2017-03-12,pytorch/pytorch,"The discussions highlight challenges in extending RNN implementations, such as integrating cuDNN support for `LSTMCell` and `RNNCell`, with considerations on compatibility and API stability. There is concern about handling negative padding in transposed convolutions, especially when using cuDNN with newer versions like v6, which may introduce breakages. Users are emphasizing the importance of proper recompilation and configuration of cuDNN libraries to ensure compatibility with CUDA and cuDNN versions. Additionally, there's an ongoing effort to improve code modularity and extensibility, including more transparent cell classes and flexible parameter types, such as for numerical inputs. Overall, unresolved issues include balancing performance with API stability, ensuring compatibility across CUDA/cuDNN versions, and clarifying user configurations."
2017-03-13,pytorch/pytorch,"The discussions highlight several key technical concerns: ensuring backward compatibility and correct implementation for functions like Local Response Normalization (LRN) and custom autograd functions; the necessity of proper recompilation and environment configuration when switching cuDNN versions, especially with CUDA/cuDNN library paths and installation details; challenges with migrating legacy Torch models and their compatibility with PyTorch, emphasizing the need for updated serialization methods; performance discrepancies across different hardware setups, linked to threading and library linking issues, suggesting the need for better build and environment control; and the importance of clear documentation and interface consistency for new arguments, such as dropout modes and tensor argument types, to aid user implementation and avoid runtime errors."
2017-03-14,pytorch/pytorch,"The discussions highlight performance issues with dilated Conv1d operations on CUDA, where dilation significantly slows execution, prompting suggestions to optimize by replacing dilated convolutions with multiple 1x1 convolutions and additions. There is ongoing effort to improve Windows support for PyTorch, particularly CUDA GPU support, with inquiries about source branch access and port status. Build consistency and threading performance are concerns, with observations that certain recent commits (e.g., @761d679) yield better multi-threaded behavior and benchmark results despite similar linking, indicating underlying build or environment factors. Compatibility issues with Python shared libraries are also noted, emphasizing the importance of building Python with shared library support (`--enable-shared`) for successful dynamic linking. Overall, the key themes involve optimizing CUDA kernel performance, enhancing cross-platform support, and ensuring reliable build configurations."
2017-03-15,pytorch/pytorch,"The discussions primarily focus on technical issues related to low-level hardware compatibility and performance, such as broken VSX interfaces due to PR changes, and discrepancies in convolution performance likely stemming from suboptimal cuDNN utilization or environment setup. Several contributors address bugs in autograd operations, notably in reduction functions, emphasizing the need for correct gradient propagation and robust testing. There is also a recurring concern about supporting old Python class types, with suggestions to enhance compatibility for legacy classes. Additionally, there is discussion about the relevance and benefits of Java extension support, ultimately concluding it’s not a development priority given the current core focus on Python/C++. Overall, unresolved questions include environment setup troubleshooting for GPU performance and ensuring proper handling of legacy class objects in serialization."
2017-03-16,pytorch/pytorch,"The primary technical concern revolves around CUDA and cuDNN compatibility, specifically issues with non-contiguous tensors causing errors such as `CUDNN_STATUS_INTERNAL_ERROR` and `CUDNN_STATUS_BAD_PARAM`, which have been addressed with buffer checks and PR fixes. Several discussions highlight challenges with GPU performance, including difficulty achieving expected speedups, potentially due to incorrect environment variables or library paths, with suggestions to rebuild clean Docker images or remove conflicting cuDNN versions. There are also questions about correct model evaluation practices, emphasizing that models should be invoked with `model(x)` rather than `model.forward(x)` to ensure hooks are executed, potentially affecting model performance or accuracy comparisons. Additionally, issues related to dataset integrity, such as non-image files causing errors, are mentioned, alongside improvements like cloudfront caching for faster package downloads. Unresolved questions include ensuring proper setup of environment variables, library paths, and confirming that build and runtime configurations are aligned for optimal performance and compatibility."
2017-03-17,pytorch/pytorch,"The discussions highlight several key technical concerns: the absence of GPU support and autograd integration for CPU-specific functions like `unfolded_copy` and `unfolded_acc`, with a specific request for implementation of im2col and col2im support; the need to add padding capabilities to `nn.AvgPool3d` akin to `AvgPool2d`, requiring modifications to both CPU and GPU code; and the task of developing an `nn.Bilinear` layer using autograd, paralleling existing `Linear` implementations, to enhance modularity. Other issues include handling dataset download errors, code refactoring requests (e.g., removing certain changes), and clarifications about layer operations (e.g., `Linear` vs. `ConvNd`). Some comments also point to implementation details such as the use of im2col in convolution operations, and discuss current limitations or bugs, like the improper behavior of `masked_copy`. Overall, the community emphasizes extending functionality, ensuring proper autograd support, and refining implementation details."
2017-03-18,pytorch/pytorch,"The discussions highlight ongoing development efforts and technical challenges in PyTorch, including the need to implement implicit zero padding for `nn.AvgPool3d` by referencing `nn.AvgPool2d` code, and the importance of modifying both CPU and GPU implementations. There are concerns about handling numerical stability issues such as the 1-norm backward pass and potential refactoring to improve underlying implementation robustness, especially for functions like `div` and `pow`. Compatibility and version detection issues are apparent, such as ensuring correct cuDNN versions are recognized despite environment variable configurations and the implications of GPU assertion errors potentially requiring device resets. Additionally, a noted limitation in serializing large byte objects with Python's pickle, particularly on macOS, calls for potential workarounds due to OS or Python version bugs. Overall, these discussions emphasize both feature enhancements and deep dives into stability, correctness, and compatibility challenges within the PyTorch ecosystem."
2017-03-19,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's compatibility and functionality, such as implementing padding support in `nn.AvgPool3d`, addressing GPU reset issues post-assertions, and improving user interface elements like exposing tensor properties (`size`) through proper Python methods. There are notable community contributions focused on stabilizing GPU support across different hardware, fixing binary distributions for Windows, and enhancing API usability by allowing keyword arguments in C extensions like `addbmm`. Several comments address the deprecation or limitation of certain features, such as the complex number backend (`ztorch`) and unofficial modules (`nn.Inception`), indicating ongoing priorities. Unresolved questions include the best approach to masking in `cross_entropy`, handling GPU assertion failures, and clarifying the behavior of functions like `masked_copy`, all pointing to areas for future development or documentation improvements."
2017-03-20,pytorch/pytorch,"The discussions highlight several technical concerns: performance issues and feature requests such as Negative Sampling in W2V models; handling of gradient stability and implementation intricacies via custom Functions; memory bottleneck and batch size limitations related to cuDNN; inconsistencies in tensor indexing and maximum function behavior, especially with slicing; and errors stemming from device mismatches (CPU vs GPU) in LSTM inputs. Additionally, there are questions about the proper use of `unfold` versus dedicated `im2col`/`col2im` implementations for differentiation, and version compatibility issues affecting model serialization/loading across different PyTorch versions. Several suggestions involve improving test coverage for edge cases, refining underlying function implementations, and ensuring consistent device usage to prevent runtime errors."
2017-03-21,pytorch/pytorch,"The discussions highlight a need for enhancements in PyTorch, such as modifying the `size_average` parameter to output batch-wise losses for better flexibility, and adding masking options to cross-entropy loss for variable-length RNN support. There is concern about proper documentation and user experience, especially related to API behavior and installation procedures, with suggestions to clarify function dispatch and improve package availability. Compatibility issues are evident with hardware-specific functions (e.g., VSX) and different environments like PowerPC64le, indicating a desire for broader platform support and troubleshooting guidance. Users also seek more control over batch normalization and CUDA configurations, as well as clearer guidance on setting parameters like `cudnn.benchmark`. Overall, the discussions emphasize improving usability, flexibility, and platform compatibility in PyTorch's ongoing development."
2017-03-22,pytorch/pytorch,"The discussions highlight several technical concerns including proper package support and compatibility issues, such as difficulties with PyPI packaging, supporting CPU-only versions, and handling large tensor serialization due to OS or Python limitations. There are performance-related questions about CUDA and cuDNN support, specifically around thread management, build inconsistencies, and CUDA version compatibility, with suggestions to rebuild from source or upgrade cuDNN. Some issues focus on correctness and behavior, such as bugs in `torch.max`, handling of padding in convolutions, and implementation details in autograd functions, emphasizing the need for proper testing, code fixes, and documentation clarity. Additionally, there are questions about module attributes, data parallelism, and environment configuration to optimize multi-GPU performance. Unresolved topics include standardizing build processes, managing environment variables for threading, and fixing bugs related to tensor operations in various frameworks."
2017-03-23,pytorch/pytorch,"The discussions highlight significant concerns about maintaining backward compatibility while introducing new features, notably in broadcasting and module API changes, with debates over API modifications like `add_module`. There are ongoing efforts to fix bugs related to tensor operations, such as max index retrieval and handling specific hardware (ppc64le, CUDA), with fixes committed recently. Several issues involve system compatibility and performance optimizations, including the management of CPU-GPU memory transfers, interfacing with BLAS libraries, and ensuring consistent behavior across hardware and software platforms. Additionally, there's interest in standardizing and verifying loss function implementations and improving testing practices for robustness. Unresolved questions remain about the best ways to evolve APIs without breaking existing code and handling hardware-specific quirks."
2017-03-24,pytorch/pytorch,"The discussions primarily revolve around implementing and optimizing PyTorch functionalities, such as memory-efficient backpropagation methods for reduction operations (`std`, `var`, `renorm`), and refining tensor operations like `addr_` versus `expand_as` for biases on the GPU, highlighting size-dependent performance. There are concerns over the correctness and testing of new features, including using `gradcheck` for validation and ensuring proper handling of `Variable` behaviors in autograd. Several comments address installation challenges, especially in virtual environments or macOS, suggesting alternative pip install methods. The overhaul of module introspection methods (`modules()`, `named_modules()`) and clarifications on GPU tensor usage indicate ongoing API refinement and usability improvements. Finally, issues regarding process handling during data loading and the deprecated status of certain components (like `Trainer`) hint at maintenance and future-direction considerations."
2017-03-25,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch functionalities, including adding support for masking in cross-entropy loss to handle variable-length RNNs, and implementing implicit zero-padding for `nn.AvgPool3d`. There are technical tasks such as modifying the CPU and GPU versions of volumetric pooling layers, with directives to analyze existing implementations like `AvgPool2d` for padding support. Concerns also include ensuring compatibility with specific compiler environments (e.g., Anaconda GCC) and maintaining clear documentation to prevent user confusion, particularly regarding learning rate behavior in optimization routines. Additionally, there's emphasis on proper testing, including adding new test cases for serialization and pooling layers, and some discussion around code version management and development workflows."
2017-03-26,pytorch/pytorch,"The comments address several technical issues: difficulties with legacy behavior and parameter scaling in dropout functions, and the desire to maintain consistent attribute types across pooling layers, specifically noting that `AdaptiveMaxPool1d` and `AdaptiveAvgPool1d` should return a tuple rather than a single integer for `output_size`. System and environment concerns are raised regarding GPU/kernel errors, such as device-side asserts, memory limitations, and the impact of swap space, along with troubleshooting guidance like setting `CUDA_LAUNCH_BLOCKING=1`. Additionally, there is discussion about ensuring correct local installation and build verification of PyTorch, emphasizing the importance of clear communication to avoid duplicated work on tasks and proper task assignment acknowledgment. Overall, these comments highlight ongoing debugging, consistency improvements, build verification, and effective collaboration practices."
2017-03-27,pytorch/pytorch,"The discussions highlight concerns about the complexity and inconsistency of PyTorch's tensor constructors and data handling, with calls for unifying and streamlining tensor creation interfaces across CPU and GPU (Issue #1081.0). Performance and stability issues in data loading, especially related to multi-worker dataloaders, are noted (Issue #973.0), alongside the need for better debugging tools or timeouts. There are suggestions for more flexible gradient manipulation methods, such as hooks or custom functions, instead of dedicated modules, to enhance gradient reversal workflows (Issue #1110.0). Some discussions address software versioning and build concerns, specifically ensuring the correct package URLs for different platforms and Python versions (Issue #1096.0). Finally, there is interest in simplifying or clarifying internal behaviors, such as print verbosity and batch collation, to improve usability and code maintainability (Issue #1114.0, #1115.0)."
2017-03-28,pytorch/pytorch,"The discussions highlight performance considerations in tensor operations, particularly comparing `addr_` versus `expand_as + add` for small/medium sizes, with findings that `addr_` can be faster under certain conditions, though complexities in linear layers may warrant fixing. Error handling and debugging problems, such as device-side asserts caused by invalid labels or synchronization issues, are emphasized, with suggestions like using `CUDA_LAUNCH_BLOCKING=1` for diagnostics. Efforts to optimize gradient computations, especially for `cumsum`, explore methods like explicit summation versus reverse algorithms, with benchmarking indicating that the clone method is generally more efficient than reverse-based approaches. Model and API documentation management, including versioning and module exposure, is also addressed, along with small bug fixes like proper `__getattr__` delegation. Overall, these discussions focus on performance tuning, debugging, implementation correctness, and documentation clarity."
2017-03-29,pytorch/pytorch,"The discussions highlight ongoing challenges and user requests related to Windows support for PyTorch, including CUDA GPU compatibility and pre-built binaries, with suggestions for community and industry involvement. There are technical issues concerning MAGMA installation and compatibility with specific GPU architectures (e.g., sm_30, sm_61), where proper MAGMA build configurations are essential. Discussion about gradient manipulation emphasizes the preference for using hooks over custom functions for reversing gradients due to autograd's flexibility and correctness concerns. Some conversations focus on improving data sampling strategies, such as handling non-batch-divisible data in samplers, and refining benchmarking and testing workflows to address bugs and maintain code clarity. Lastly, there are questions about optimizing CUDA kernel execution, especially regarding the persistent flag's use, and minor clarifications on terminology in backpropagation discussions."
2017-03-30,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's CUDA support on Windows, with community suggestions for external resources and porting assistance, but no definitive resolution. Several issues revolve around CUDA and cuDNN compatibility, including slow performance with cuDNN v5 that was mitigated by upgrading to v6 and building from source, suggesting ongoing challenges with GPU acceleration and specific library versions. There are technical tasks proposed such as implementing `nn.Bilinear` in core PyTorch with autograd support, along with the need for proper unit tests and adherence to the development guidelines. Some comments address environment setup problems, like conflicting folder names or directory issues causing import errors, emphasizing the importance of correct environment configurations. Unresolved questions remain around optimal handling of non-batch-divisible data in samplers and ensuring compatibility across hardware and software versions, indicating areas for further development and testing."
2017-03-31,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch's GPU compatibility, including slow `.cuda()` operations, device management in multi-GPU setups, and potential crashes due to device assignment assumptions. There are concerns about the build system's compatibility, especially with older compilers and specific architectures like PPC64, requiring updates and testing across various systems. Users request improved tensor operations support, such as padding for 1D tensors and handling non-batch-divisible cases in data samplers, with suggestions for more intuitive device transfer behavior. Additionally, some issues relate to package installation errors, flaky tests, and limitations in certain functions (e.g., `cuda()` and `torch.dot()`) that need addressing. Overall, these discussions emphasize enhancing robustness, user experience, and broad system compatibility within PyTorch's development."
2017-04-01,pytorch/pytorch,"The discussions highlight several technical concerns including the need for backend flexibility in PyTorch (e.g., using different CUDA backends within a single model), compatibility and build issues across different OS and hardware environments (notably PPC64 and IBM systems), and troubleshooting specific errors such as segmentation faults, attribute errors during loading, and CUDA runtime errors, often related to improper driver or CUDA installations. Additionally, there are concerns about updating and maintaining testing procedures, fixing flaky tests, and correctly implementing tensor operations like remainders to ensure accuracy across signed inputs. Some comments also suggest improvements to documentation and clarification on setup instructions, especially regarding environment configurations like cuDNN paths. Overall, unresolved issues involve ensuring platform compatibility, refining implementation details, and maintaining robust tests for stability."
2017-04-02,pytorch/pytorch,"The discussions highlight several technical concerns: the adjustment of default dampening in optimizers affecting training outcomes and the need to update documentation accordingly; issues with Windows-specific build configurations, particularly related to long data types and libshm, which hinder installation and multiprocessing; discrepancies in data format consistency across modules; performance regressions with dilated convolutions using older cuDNN versions that were later mitigated by updates and building from source with cuDNN v6; and challenges with ensuring compatibility between PyTorch, cuDNN, NVIDIA drivers, and theorems, especially on older GPU/driver setups, which can cause runtime errors like `CUDNN_STATUS_NOT_INITIALIZED`. Additionally, questions about the location of pre-defined models and the flaky nature of certain tests point to ongoing maintenance challenges and the need for clearer documentation and more robust test practices."
2017-04-03,pytorch/pytorch,"The discussions highlight several technical concerns: the need to address the 32-bit `long` issue on Windows, with patches and conversions to `int64_t` postponed for future releases; the potential addition of a `.gpu()` method to support multiple backends and improve clarity over existing `.cuda()` and `.cl()` methods, with nascent suggestions for more descriptive naming conventions; uncertainties about platform-specific behavior of `ptrdiff_t`, especially on PPC64 architectures, impacting pointer arithmetic and segfaults; challenges with data serialization involving lambdas or nested functions, where alternative pickling modules like `dill` or `cloudpickle` are suggested; and compatibility issues when misusing different versions of cuDNN due to manual replacements, which could lead to runtime bugs, alongside the usual environment setup concerns such as `MACOSX_DEPLOYMENT_TARGET`."
2017-04-04,pytorch/pytorch,"The discussions highlight several technical issues: the need for clearer error messages indicating expected tensor types (e.g., specifying that targets should be LongTensors), and handling edge cases like parallel transpose operations on 1D tensors wrapped in Variables, which currently produce unintelligible runtime errors. There are concerns about and reports of segmentation faults and crashes when running PyTorch on certain architectures, such as PowerPC64, suggesting potential compatibility or compilation issues. Users are also addressing environment configuration challenges, such as replacing cuDNN libraries with incompatible versions, which can cause subtle bugs or instability, emphasizing the importance of proper version management. Finally, improvements are being discussed for tensor indexing and gathering mechanisms, with suggestions to adopt more Pythonic and flexible interfaces."
2017-04-05,pytorch/pytorch,"The discussions highlight several technical issues including the need to patch `libshm` with no-op stubs for Windows support due to dependency on `libshm` for installation and testing, and ongoing slow CUDA performance with recent PyTorch versions potentially due to package conflicts or GPU driver incompatibilities. There are concerns about the interface consistency and implementation of vector operations in `VSX.c`, especially regarding the incorrect function prototypes and the need to align them with existing SIMD and non-SIMD versions. Additionally, there are questions about the design and unification of tensor constructors across CPU and GPU, and the handling of data sharing and data types, with suggestions to improve API consistency. Lastly, some discussions address build configuration issues like `LD_LIBRARY_PATH` for MAGMA support and the clarity of tutorials referencing `torch.utils.data`, advocating for clearer documentation and better instructions."
2017-04-06,pytorch/pytorch,"The discussions primarily revolve around the need for implementing GPU-compatible and autograd-integrated versions of CPU-specific functions like im2col/unfolded_copy and unfolded_acc, with contributions and pointers provided for development. There are technical concerns regarding the correctness and consistency of vector operations, specifically correcting a mistaken assumption about function semantics and fixing SIMD optimizations, exemplified by the VSX.c file work. Several issues address model compatibility, such as loading deprecated Torch models into PyTorch and handling legacy issues with model serialization and attribute access. Additionally, inconsistencies between 2D and 3D pooling index conventions, and type mismatches in gradient computations, are highlighted as areas requiring clarity and fixes. Overall, there’s an emphasis on refining low-level tensor operations, serialization robustness, and API consistency for user-facing functionalities."
2017-04-07,pytorch/pytorch,"The discussions highlight efforts to improve PyTorch's loss masking by exploring GPU kernel implementations and masking approaches, with concerns about convenience and efficiency. Several bug reports address test failures and exceptions related to tensor operations, such as invalid indices and tensor resizing, prompting investigations into error handling and debugging strategies. There is consideration of replacing `unfold` with custom `im2col`/`col2im` functions to enable differentiability, alongside questions about where these should be implemented in the backend. Additionally, issues related to nan values in batch normalization parameters due to input data initialization and best practices for model inference mode are discussed. Overall, the threads focus on optimizing functionality, increasing robustness, and clarifying implementation strategies within PyTorch's codebase."
2017-04-08,pytorch/pytorch,"The discussions primarily revolve around technical challenges in PyTorch, including successful compilation and linking of static libraries (issue #643), ensuring proper model evaluation mode to disable dropout (issue #1217), and troubleshooting memory leaks during training, possibly due to improper variable or resource management (issue #1217). There are also questions about proper label formats for loss functions, especially avoiding negative label values (issue #1204), and ensuring hooks are correctly registered for all outputs, especially in dict-based or named output scenarios (issue #1205). Additionally, users seek guidance on package installation specifics (issue #1207), reproducing errors with minimal code snippets (issue #1213), and optimizing performance with BatchNorm versus InstanceNormalization (issue #1215). Overall, suggestions emphasize careful configuration, verification of model states, and proper handling of data and hooks to avoid common pitfalls."
2017-04-09,pytorch/pytorch,"The discussions primarily revolve around troubleshooting training issues, such as increasing losses with Adam and label validity in loss functions, emphasizing the importance of correct data labeling (e.g., avoiding negative labels). Concerns are also raised about appropriate hooking of model outputs, especially with dictionary outputs, and ensuring consistent behavior across interpreter runs, indicating the need for improved autograd tracking. Additionally, there are queries about environment setup and dependencies, such as installing specific CUDA or magma packages, highlighting potential compatibility or firewall issues. Memory management problems, including potential memory leaks during training and plotting, are mentioned, with suggestions to verify allocator behavior and simplify code to isolate causes. Overall, unresolved questions focus on proper handling of output structures, reproducibility, environment setup, and debugging memory leaks."
2017-04-10,pytorch/pytorch,"The discussions highlight several technical concerns, including the incomplete implementation of automatic broadcasting for general tensors, with current solutions relying on expansion methods. Memory management issues are recurrent, particularly memory leaks observed during neural network training and the impact of allocator behavior on memory stability, especially under different OS environments and with plotting code. Other points focus on tensor contiguity and data layout choices, debating the trade-offs between maintaining fixed formats like NCHW versus flexible layouts in NLP tasks, with suggestions to address potential overheads or switch by transposing data. Additionally, shader and GPU-related concerns are raised, notably risks of deadlocks and performance implications associated with CUDA sparse routines and kernel optimizations. Unresolved questions pertain to fixing memory leaks, optimizing tensor operations, and clarifying compatibilities for features like `drop_last` in data loaders."
2017-04-11,pytorch/pytorch,"The comments highlight issues related to hardware and compiler compatibility, such as disabling SSE4 support for legacy processors and compiler selection to avoid specific bugs. Several discussions focus on PyTorch's API robustness and usability, including supporting old-style Python classes, enabling keyword arguments in C++ bindings, and improving data layout flexibility for NLP applications. There are concerns about CUDA-related configurations, like the impact of cudnn.benchmark on dilated convolutions and device management, with suggestions to set CUDA device context explicitly. Some threads address debugging and testing failures, especially in DataParallel implementation, indicating ongoing stability and correctness improvements. Lastly, there are performance considerations in functions like RNNs and backward kernels, emphasizing the importance of optimizing custom operations for speed."
2017-04-12,pytorch/pytorch,"The discussions highlight ongoing challenges with half-precision (fp16) support in BatchNorm layers, including how to truncate parameters for minimal accuracy loss and ensuring fp16 compatibility across models, particularly after recent updates. There are concerns about whether returning dictionaries with named outputs will be reliable, given variable hashing issues across interpreter runs, and suggestions to improve autograd output tracking. Some issues relate to performance implications of data transposing in convolutional operations for NLP tasks, with opinions leaning towards maintaining a single, optimized data layout like NCHW. Additional technical considerations involve handling thread management in engine destruction, resolving MKL-related segfaults and library linking errors post-merge, and general robustness of model initialization and module support across different configurations. Overall, these discussions emphasize improving fp16 support, debugging stability issues, and refining model introspection and data layout strategies."
2017-04-13,pytorch/pytorch,"The discussions highlight several technical challenges in PyTorch, including the unsupported fp16 support in cuDNN batch normalization due to parameter precision requirements, and potential model truncation effects on accuracy. GPU and CUDA device detection issues are also prevalent, with questions about environment variable configurations (e.g., `CUDA_VISIBLE_DEVICES`) and their impact on backward operations. Multiple users encounter memory-related problems and segmentation faults linked to threading, glibc, or heap corruption, especially when running from source or with specific library linkages such as MKL. Some issues involve code correctness, such as a bug in `linalg.py` requiring a fix in the diagonal function, while others concern utility tools for memory management and profiling. Overall, the discussions emphasize ongoing development, debugging, environment setup, and feature roadmap considerations for enhancing PyTorch’s robustness and capabilities."
2017-04-14,pytorch/pytorch,"The discussions highlight challenges with PyTorch's tensor operations, such as handling non-contiguous tensors, especially in sparse contexts, and the need for clearer API semantics (e.g., `contiguous()` versus reordering). There are concerns around the reproducibility and determinism of GPU operations, notably with CUDA's non-deterministic routines and the impact on autograd and optimization procedures. Upgrading and maintaining PyTorch within conda environments is problematic, with issues related to package compatibility and installation errors, particularly on old versions like 0.1.8. Additionally, users seek utility tools for memory management and profiling to optimize resource usage. Overall, key unresolved questions involve making sparse tensor operations more predictable, ensuring deterministic GPU behaviors, and simplifying environment upgrades."
2017-04-15,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for better OpenCL support in PyTorch, particularly linking CUDA functions across modules, and efforts to adapt CUDA kernels for OpenCL compatibility. There are questions about enhancing the `nn.Softmax` API to support higher-dimensional tensors, and suggestions to unify initialization functions like Kaiming to accept a consistent `gain` parameter, with considerations about potential breaking changes. Users report issues related to installation, library linking, and runtime errors, often resolved through reinstallation or system cleanup. Additionally, there is interest in optimizing parameter management by filtering trainable parameters for the optimizer to avoid mistakes. Several unresolved questions remain around improving multi-device support, API consistency, and tooling for contributions and maintenance."
2017-04-16,pytorch/pytorch,"The discussions highlight various technical concerns including hardware compatibility and system-specific bugs, such as SSD recognition issues on Windows and CUDA support in Cygwin, as well as memory management and memory leaks observed during training, potentially linked to the system's memory allocator or cudnn. There are questions about the correct usage and implementation of `.cuda()` and other tensor methods, emphasizing that `.cuda()` should be a function that returns a new object rather than a property, and its expense due to device copying is acknowledged. System and environment dependencies, such as OS choice, installation methods, and library versions, are frequently mentioned, affecting reproducibility and performance. The need for additional checks, especially in CPU-only code and specific libraries like THNN, is noted, alongside suggestions to expand functionality in `torch.nn.functional` and ensure correct optimizer usage in training scripts. Unresolved questions include CUDA support in Windows/Cygwin, handling memory leaks during training, and best practices for device management within PyTorch code."
2017-04-17,pytorch/pytorch,"The discussions primarily revolve around proper module initialization and usage in PyTorch, with concerns about reinitializing layers within the `forward` method, as seen with the Maxout layer example, and suggestions to instantiate layers in the constructor for correct training behavior. There are questions about whether to use the `nn.functional` interface for certain operations, emphasizing best practices for defining model components. Additionally, some issues mention compatibility and build errors on specific platforms like macOS/OSX, indicating ongoing work to address such errors. Contributors are also working on adding tests for new features or fixes, ensuring code robustness. Finally, there are discussions about integrating documentation tools (like Dash) and maintaining external contributions to the PyTorch ecosystem."
2017-04-18,pytorch/pytorch,"The discussions highlight ongoing development goals such as implementing ModelParallel and enhanced DataParallel support, along with performance optimizations for RNN kernel launches and memory management, including tackling out-of-memory errors and memory leaks. There is a focus on improving serialization methods via `state_dict`, ensuring deterministic behaviors in cuDNN and related functions, and aligning shared library dependencies with Torch7 to facilitate distribution. Several technical challenges are raised, such as refining gradient computations (e.g., removing unnecessary `.zero()` calls), handling variable length sequences efficiently with indexing versus masking, and addressing threading deadlocks and reference count inconsistencies in autograd. Overall, the community emphasizes both feature improvements—like better RNN implementations and deterministic cuDNN modes—and stability issues, such as memory leaks and multithreading deadlocks, with ongoing discussions about backend code design and distribution complexities."
2017-04-19,pytorch/pytorch,"The discussions highlight various technical concerns, including ensuring the universality of `CrossEntropyLoss` and merging `nll_loss2d` into `nll_loss`. Key questions involve optimizing model export to production formats like HDF5 and Caffe2, handling build configurations for MAGMA, and resolving platform-specific build failures, notably on OSX and Windows. There are suggestions for performance improvements in sparse gradient operations, such as avoiding expensive coalescing by implementing direct `indexAdd` with in-place or out-of-place options, and addressing API design choices like whether to make `coalesce` in-place or always coalesced. Additional concerns include correctly managing exceptions on different architectures, handling GPU indexing distinctions related to `CUDA_VISIBLE_DEVICES`, and designing user-friendly APIs for sparse weight decay with explicit `flush()` mechanisms to prevent silent errors. Unresolved issues mainly revolve around the proper handling of sparse tensor operations, build configuration nuances, and ensuring correctness across platforms."
2017-04-20,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, including adding features like unnegative sampling, implementing probability distribution functions (e.g., lgamma, beta functions), and making loss functions like CrossEntropyLoss more universal. Technical concerns include ensuring CUDA compatibility (e.g., linking against cusparse, handling exceptions), optimizing sparse and dense gradient updates (e.g., lazy updates, coalescing), and improving performance for specific operations like embedding computations and sparse tensor manipulations. There are also issues related to debugging and porting (e.g., fixes for segmentation faults, compatibility across architectures like PPC64 and aarch64). Additionally, suggestions for API improvements include error raising during quantization, optimizers' API changes for sparse updates, and better handling of tensor shapes and slicing in autograd. Unresolved questions involve implementation timelines, stability of numerical functions (e.g., beta, gamma), and best practices for managing sparse vs. dense data representations."
2017-04-21,pytorch/pytorch,"The discussions highlight challenges with legacy Torch models, particularly compatibility issues when loading old `.t7` files into PyTorch, exemplified by attribute errors such as `'DepthConcat' object has no attribute 'outputSize'`. Several comments address code maintenance, including fixing memory leaks, enhancing unit tests for modules like `Linear`, and refactoring autograd functions, with some fixes awaiting patch inclusion. There are concerns regarding CUDA-specific issues, such as linking errors with `cusparse`, and kernel bugs affecting RNN computations, along with implementation nuances like out-of-place `nn.Tanh` and support for padding operations. Overall, key themes include improving model compatibility, ensuring resource management, fixing CUDA and kernel bugs, and enhancing test coverage and API consistency."
2017-04-22,pytorch/pytorch,"The discussions raise several technical concerns including the default storage location for models (possibly `~/.torch`), and the difficulty in locating specific model definitions. There are issues related to proper usage of functions, notably the `torch.cat()` function, which requires a sequence and a dimension argument rather than individual tensors with an `out` parameter. Memory leaks and checks in the THNN library are also noted, particularly in CPU-only contexts and with certain CUDA/cuDNN versions. Additionally, there are suggestions to enhance padding layers, such as making `ConstantPad2d` a module, and debates on the necessity of a dedicated crop layer, given tensor slicing capabilities in PyTorch. Unresolved questions include the proper model storage paths and clarifications on correct function signatures."
2017-04-23,pytorch/pytorch,"The discussions highlight several technical concerns, including addressing a common memory leak issue observed on specific configurations (e.g., torch7+cudnn 5.1+Ubuntu 14.04 and Python 3.5) and the necessity to upgrade packages like numpy to resolve certain problems. Users report segmentation faults during backpropagation with certain model versions, suggesting compatibility or stability issues that may be version-dependent. There is also a mention that current ffi implementation does not support kernel functions, indicating an area needing development. Additionally, some comments suggest simplifying data sampling by passing index lists directly to DataLoader instead of creating custom subsets or samplers. Lastly, a bug in torch.stack() involving variables is noted as needing correction, with training tests skipped on CUDA-dependent builds."
2017-04-24,pytorch/pytorch,"The discussions address several core technical concerns: implementation and semantics of tensor broadcasting—highlighting differences between in-place and non-in-place operations and numpy-like behavior; the design choice for the `coalesce` operation in sparse tensors, balancing between in-place versus out-of-place, with emphasis on documentation and duplicate index handling; challenges with debugging and avoiding deadlocks in multithreaded autograd on Windows, suggesting potential fixes like custom copy constructors; compatibility and memory leak issues across CUDA, Python versions, and Docker environments, indicating ongoing troubleshooting; and broader API enhancements such as introducing standard cropping layers, handling dilation in convolutions, and managing CUDA IPC constraints, alongside considerations for differentiability of certain functions like `argmax`."
2017-04-25,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's flexibility and robustness, including enabling user-specified paths for cuDNN (Issue #280), supporting advanced tensor indexing (Issue #1080), and implementing standard cropping layers (Issue #1331). There are concerns about code consistency and deprecation, such as the introduction of `ZeroPad2d` versus `ConstantPad2d`, and the deprecation of padding and cropping layers in favor of functional APIs (Issue #1326, #1335). Several issues address technical implementation details, like optimizing tensor broadcasting operations in TH/THC (Issue #1353), fixing shared memory configurations (Issue #1355), and handling specific operator bugs (Issue #1347), with some unresolved questions about correctness and compatibility. Additionally, there are discussions about proper model attribute access within DataParallel wrappers (Issue #1341) and dealing with build environment configurations (Issue #280). Overall, the community is focused on improving usability, API consistency, and internal efficiency, with various proposals and pending issues requiring further clarification or implementation."
2017-04-26,pytorch/pytorch,"The comments highlight ongoing challenges in customizing build environments, such as specifying custom cuDNN paths during compilation, and the need for clearer error messaging, especially around device asserts and memory leaks during backpropagation. There are discussions about enhancing PyTorch’s internal functions, including implementing differentiable `im2col`/`col2im` layers and improving debugging information with developer modes or more informative stack traces. Memory management issues, such as leaks caused by unfetched gradients or improper detachments, are being addressed with practical fixes like manual detach calls. Additionally, concerns about compatibility and build issues—like CMake configurations, handling tensor type inconsistencies, and proper testing on different hardware—remain open, with suggestions for architecture improvements and increased transparency in error reporting."
2017-04-27,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's sparse tensor operations, particularly regarding `coalesce()` performance and consistency, as well as the need for better documentation on duplicate index handling. There are concerns about error message clarity, especially for device assertions and fail states like out-of-bounds errors, with suggestions for improved, user-friendly debugging modes. Compatibility and correctness issues are also discussed, including discrepancies in how advanced indexing is implemented, handling of 1x1 normalization in BatchNorm, and differences in behavior across environments and Python versions, such as the impact of apparent bugs or missing features in certain backends or configurations. Additionally, the need for clearer testing strategies and the potential for optimizing certain core operations (like `coalesce`) for speed are noted, alongside suggestions for modular design improvements to separate tensor types and autograd functionality."
2017-04-28,pytorch/pytorch,"The discussions highlight several key technical concerns including the implementation and support of advanced indexing in PyTorch, with considerations for combining advanced and basic indexing and optimizing linear index computations. There are ongoing debates about tensor and variable handling, specifically whether to automatically convert tensors to variables with `requires_grad=False`, and the separation of tensor operations from autograd to maintain clarity and flexibility. Additionally, issues related to GPU support and specific functionalities such as `lgamma` in CUDA, the correct passing of parameters like dilation in transposed convolutions, and the need for upstream code updates like upgrading CuDNN versions are discussed. Some discussions also address implementation details for loss functions, in-place sparse tensor operations, and ensuring test coverage aligns with new features or fixes. Overall, unresolved questions remain about support for certain features (e.g., NCE, advanced indexing nuances) and requires further clarification or development upstream."
2017-04-29,pytorch/pytorch,"The discussions highlight challenges with implementing GPU support for new functions (e.g., lgamma) in the THC library, as well as managing GPU memory during training, with a focus on preventing memory leaks and optimizing memory usage. There are ongoing efforts to refactor or extend existing modules, such as upsampling operations, to improve code reuse and avoid repetition, alongside addressing test failures caused by inheritance or attribute errors. Additionally, there is consideration for enhancing cropping functions to handle arbitrary dimensions more flexibly, and minor issues related to environment compatibility and binary builds are being addressed. Overall, these threads emphasize improving GPU kernel integration, memory management, code modularity, and robustness of testing and compatibility."
2017-04-30,pytorch/pytorch,"The discussions primarily address performance optimization and usability concerns in PyTorch, such as kernel launch overhead in GPU RNN computations and the potential benefits of in-place operations versus masking/tensor indexing for variable-length sequences. There is interest in supporting variable-length sequences without requiring batch sorting and exploring padded sequence handling via `pack_padded_sequence`. Several questions revolve around extending functionality, such as implementing LRN, backpropagation for certain operations, and adding specific operations (e.g., lgamma on GPU). Some issues highlight practical setup and documentation challenges, including properly building documentation and installing specific CUDA versions. Overall, key themes focus on optimizing RNN performance, enhancing sequence handling flexibility, and ensuring smooth user setup and documentation processes."
2017-05-01,pytorch/pytorch,"The discussions highlight ongoing challenges with API inconsistencies and attribute changes, such as the renaming of `previous_functions` to `next_functions` in autograd, and the need for clearer documentation for Variables versus Tensors. Several comments address test failures linked to specific issues (e.g., #1347), suggesting rebase and merge resolutions, while some issues remain unresolved due to conflicts or environment discrepancies, notably on CI platforms like Travis. Optimization topics are raised, including performance comparisons between BatchNorm and InstanceNormalization, and opportunities for enhancing training efficiency with in-place operations and parameter grouping. There is also mention of improving modularity and parametrization in optimizer design, and suggestions for utility functions to facilitate parameter filtering. Overall, the discussions reflect continuous efforts to stabilize, optimize, and clarify PyTorch's API, with ongoing work needed to resolve environment-specific failures and improve developer workflows."
2017-05-02,pytorch/pytorch,"The discussions highlight concerns about backward-compatibility and API stability when introducing features like broadcasting and gradient computations, with efforts to implement these without disrupting existing code. Technical questions include handling of broadcasting dimensions, sparse tensor operations, and gradient hooks, as well as the challenges of compiling and linking dependencies such as OpenMP libraries, especially on Linux and macOS. There are suggestions for improving autograd's API clarity, managing scalar representations, and refining functions like `renorm` and `cumprod` for better stability. Unresolved issues involve fixing test failures across CI environments, ensuring proper inclusion of dependencies, and improving user interface options such as model-to-GPU assignment in optimizers."
2017-05-03,pytorch/pytorch,"The discussions primarily focus on compatibility and implementation challenges within PyTorch, such as integrating cuDNN API support for OpenCL, resolving CUDA kernel assertion errors, and ensuring correct linking and linking behavior across modules. There are concerns about reproducibility and stability of certain functions, like `cumprod`, especially near boundary cases such as inputs close to zero. Multiple issues involve build processes, including using `python setup.py build develop` and handling dependencies like Pillow and libpython. Additionally, there are troubleshooting efforts around deadlocks during data loading, discrepancies in tensor scalar interpretations, and compatibility of sparse CUDA embeddings with DataParallel, reflecting ongoing efforts to improve robustness, correctness, and cross-platform support."
2017-05-04,pytorch/pytorch,"The discussions highlight concerns about hardware and compiler compatibility, such as the use of multiversioning and bias toward supporting older processors without specialized instruction sets (Issue #535.0), and difficulties with slow CUDA initialization and kernel performance (Issues #537.0, #1466.0). There are questions about the correct installation procedures and package versions, especially when using Conda with CUDA support to ensure proper hardware utilization and performance (Issues #537.0, #1466.0). Bug fixes and enhancements are discussed, including default parameter settings in normalization functions (Issue #1467.0), and implementation considerations for finite difference calculations like `cumprod` and `cumprod` backward stability (Issue #1439.0). Testing, documentation, and code structure are touched upon, emphasizing better testing strategies, the importance of clear loss function design, and code review practices (Issues #1370.0, #1370.0). Unresolved questions remain around performance optimization, compatibility fixes, and improving usability and documentation clarity."
2017-05-05,pytorch/pytorch,"The discussions highlight efforts to optimize tensor operations, such as reversing sequences with non-negative strides, and addressing backward compatibility and testing challenges, including handling exceptions in data loading and ensuring proper function support at the functional level. Several issues concern improving API usability, like clarifying variable views versus leaf variables for gradient calculation, and preventing bugs in equality assertions involving sets. Compatibility issues with cudnn algorithms and ensuring correct support for higher-order differentiation are also discussed, alongside suggestions for better testing practices, such as incorporating CUDA tests. Overall, maintainers focus on fixing bugs, enhancing feature support, and clarifying behaviors for users, while unresolved questions often involve technical implementation details and how best to integrate these improvements without disrupting existing functionality."
2017-05-06,pytorch/pytorch,"The discussions highlight several technical issues: (1) Compatibility and platform-dependent issues with integer types and CUDA/cuDNN, including fixing `RuntimeError: CUDNN_STATUS_BAD_PARAM` by verifying cudnn versions and configurations; (2) Bugs in tests involving `Variable` versus `Tensor` comparisons, and incorrect assumptions about data structures in functions like `pack_padded_sequence`; (3) Workarounds for cuDNN algorithm selection bugs using `torch.backends.cudnn.benchmark=True`; (4) Concerns about correct broadcasting behavior in reduction operations, suggesting explicit dimension squeezing for version 0.2, and (5) GPU memory management in DataParallel, especially how model parameters are stored and the impact of model placement on RAM usage across devices. Unresolved questions include fixing cuDNN errors on certain platforms, proper test assertions, and optimizing device placement for multi-GPU training."
2017-05-07,pytorch/pytorch,"The discussions primarily address the implementation and refactoring of PyTorch modules and functions, such as adding functional counterparts, handling dict inputs, and improving hooks registration to accommodate named variables. Several issues concern correctness and compatibility, including rebase challenges, support for arbitrary dimensions in cropping, and ensuring proper autograd tracking. There are notable technical challenges with CUDA and cuDNN versions, including errors related to API mismatches and driver initialization failures. Suggestions include leveraging `register_buffer` for buffers, simplifying error handling in data loaders, and adding gradient verification tests to prevent nested backprop bugs. Many unresolved questions relate to extending support for 3D upsampling, fixing bugs with data handling, and ensuring stability across different CUDA and software versions."
2017-05-08,pytorch/pytorch,"The discussions highlight ongoing efforts to support higher-order derivatives in autograd, with concerns about the limitations of current autograd functions like ConvBackward being non-differentiable, which impacts gradient penalty implementations in WGANs. There is mention that enabling second-order derivatives requires codebase adaptation and potentially PRs to existing functions. Additional issues cover ensuring proper handling of Variable-to-bool conversions to prevent runtime errors, and maintaining backward compatibility with broadcasting semantics, with debates over default behavior changes and their implications. There are also technical considerations around dataset collation, especially preserving custom list-like structures within batch processing, and device management questions regarding tensor device allocation and memory initialization behaviors. Overall, the focus is on enhancing autograd capabilities, ensuring code stability, and improving user experience through better defaults and handling of edge cases."
2017-05-09,pytorch/pytorch,"The discussions highlight ongoing issues with memory leaks during backpropagation, potentially linked to driver versions or implementation details like tensor subclassing and backward compatibility. Several comments address the complexity of handling device-specific memory management and the nuances of subclassing loss functions (`_Loss` vs. `nn.Module`) for clarity and performance. There are concerns about the behavior and defaults of broadcasting semantics, especially regarding dimension squeezing and backward compatibility, with suggestions to defer certain decisions until more robust support (like broadcasting) is ready. Additionally, some technical bugs, such as unresolved errors in gradient computation and memory management (e.g., unfreed copies, unnecessary free calls), are discussed, with proposals for fixes or ongoing work to resolve them. Overall, the main issues revolve around improving memory management, interface clarity, and adhering to numpy-like semantics while maintaining backward compatibility."
2017-05-10,pytorch/pytorch,"The discussions highlight several technical issues: compatibility and correctness of tensor data types across platforms and operations, particularly the use of fixed-size integer types like `int64_t` versus platform-dependent types; problems with cuDNN backend errors (`CUDNN_STATUS_BAD_PARAM`) and mismatched tensor types during convolutional and batch normalization operations; persistent hanging or slowdowns associated with multi-worker data loading (`num_workers=0`) and system configuration conflicts; challenges with implementing gradient computation and differentiability for functions like `unfold` and `im2col`, and inconsistencies in gradient scaling and loss functions; and bugs or limitations in internal components such as fused RNN wrappers and the need for workarounds or hotfixes. Many seemingly unresolved questions concern ensuring compatibility, stability, and correctness across diverse hardware and software environments, as well as improving or fixing backward pass implementations."
2017-05-11,pytorch/pytorch,"The discussions highlight several technical challenges in PyTorch development, including the need for improved support for higher-order gradients and backward compatibility in autograd functions, especially in custom or fused RNN modules. There are ongoing issues with memory management and efficiency in operations like im2col and unfold, with suggestions to develop dedicated, memory-efficient kernels versus using higher-level tensor functions. Compatibility bugs, such as errors in gradient calculations involving certain tensor operations, are being addressed through patches and fixes, though some remain unresolved or pending integration. Additionally, there are concerns about system-specific bugs, such as CUDA Out-Of-Memory errors linked to Python data structures and the nuances of GPU memory management, which require careful debugging and reproducibility. Overall, the key focus is on enhancing autograd support, memory efficiency, and system robustness while managing the complexities of low-level backend implementations."
2017-05-12,pytorch/pytorch,"The discussions highlight ongoing technical challenges with PyTorch's implementation, including issues with bias addition in cuDNN v6 for input layers, and discrepancies in index consistency between CPU and GPU operations like `max`. There are concerns about the performance impact of fp16 support on RNNs, particularly related to cuBLAS's SGemmEx calls, and questions about enabling or optimizing fp16 computations for LSTMs. Additionally, there are troubleshooting efforts and suggestions for code improvements, such as renaming functions and fixing bugs related to tensor views and in-place operations. Overall, unresolved questions involve ensuring consistent results across hardware and improving performance and robustness of the library's core functions."
2017-05-13,pytorch/pytorch,"The discussions highlight ongoing challenges with the management and preservation of stateless modules, particularly RNNs, and the need for potential caching mechanisms to improve performance without compromising memory safety or user expectations. Several comments address CUDA and multiprocessing issues, emphasizing the limitations of CUDA's initialization in multiprocessing contexts and investigating potential workarounds for runtime errors. Memory management concerns, such as memory leaks, inconsistent behaviors between CPU and GPU, and the timing of bug fixes in upcoming releases, are frequent topics. Proposed solutions include cache reuse strategies, code refactoring (like autograd refactors), and certain code fixes, though some issues remain open or have temporary workarounds. Overall, unresolved questions focus on balancing performance, memory consistency, compatibility, and safe CUDA operations during multi-process execution."
2017-05-14,pytorch/pytorch,"The discussions primarily center around optimizing PyTorch functionalities, such as removing the `persistent` argument for cuDNN algorithms to favor performance-driven automatic selection, while maintaining user control. There is a concern about test failures and correctness, exemplified by issues with the `mask_fill` function and tensor operations like `addmm`, indicating potential bugs or type mismatch problems. Several comments highlight the need for clearer naming conventions and documentation updates, alongside addressing merge conflicts and improving test coverage. Additionally, there are ongoing efforts to review significant PRs, with some delays due to reviewer availability and the intricacy of the changes. Overall, the focus is on enhancing code robustness, clarity, and performance, with unresolved questions about default behaviors and integration timing."
2017-05-15,pytorch/pytorch,"The discussions highlight ongoing improvements and issues in PyTorch, including the implementation of automatic broadcasting semantics aligned with NumPy, with current broadcasts requiring explicit expansion functions. There are concerns about package stability, caching, and compatibility across different Python versions and hardware, such as CUDA driver/toolkit versions and device-specific bugs. Several comments address the need for better testing, particularly for in-place operations, gradient checks, and consistency across CPU and GPU, with tools like `gradcheck` recommended. Implementation of sparse matrix multiplications remains challenging due to the lack of efficient routines that handle sparsity directly. Additionally, there is a desire to improve the API design, such as choosing between subclassing `Module` versus `_Loss`, and ensuring in-place operations and backward functions behave correctly and consistently across different contexts."
2017-05-16,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's support for numpy-like broadcasting semantics, element-wise loss functions with masking capabilities, and the extension of RNN modules with more flexible and performant implementations, including cuDNN integration for LSTMCell. Concerns are raised about the performance implications and internal API design, such as the use of infinite generators in data loading, the management of workspace allocations in CUDA operations, and the mutability of internal structures like `resize_`. Several discussions address debugging and stability issues, including segmentation faults, compatibility with different CUDA versions, and the correct handling of various tensor operations and serialization. Overall, there is a focus on balancing usability, performance, and internal API robustness, with some unresolved questions about the best design patterns for extending and optimizing core functionalities."
2017-05-17,pytorch/pytorch,"The discussions highlight ongoing development efforts such as implementing locally connected layers in PyTorch, with suggestions to create modules based on existing functions; the need for supporting non-broadcasting behavior in library functions to prevent subtle bugs; and adding features like LRN modules or modifications to RNN cells. Several technical issues are addressed, including shared memory limits in Docker, CUDA compatibility, and precise grad accumulation methods, with some contributions awaiting review or merge. There are also concerns about package distribution, notably the challenges with PyTorch and torchvision installation via pip, and the importance of clear, maintainable code practices, such as controlling broadcasting behavior. Unresolved questions include the best approach to disable automatic broadcasting, handling sparse matrix operations, and improving user and library code interface clarity."
2017-05-18,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's functionality, such as implementing locally connected layers via `SpatialConvolutionLocal` and exposing corresponding modules, and developing a stateful NCE loss module for efficient negative sampling. There are concerns about code stability and correctness, including fixing test errors through rebasing and updating documentation, as well as resolving serialization issues in `torch.save` when working with file-like objects that lack `fileno`. Compatibility and environment-related problems are highlighted, notably CUDA driver and libstdc++ version mismatches, requiring driver updates and environment synchronization. Additionally, there are API design considerations like possibly removing the `ravel` function due to its potential for copying and renaming parameters for clarity. Unresolved questions include proper integration of these new modules, ensuring backward compatibility, and the best practices for subclassing loss functions versus modules."
2017-05-19,pytorch/pytorch,"The discussions highlight several core technical concerns, including difficulties with installing PyTorch and torchvision due to cache issues and lack of prebuilt packages on PyPI, suggesting the need for clearer installation instructions or alternative methods. There are ongoing efforts to improve or implement features such as adding support for sparse tensor operations, customizing loss modules like NCELoss, and addressing performance and correctness bugs, with some changes still pending review or integration into official releases. Compatibility issues, particularly with CUDA drivers and environment mismatches like differing libstdc++ versions, are also prominent, necessitating troubleshooting and environment management solutions. Additionally, there are suggestions to enhance API robustness, such as raising exceptions on unsupported tensor operations (e.g., non-contiguous views) and documenting behavior changes across versions. Overall, the discussions reflect active development, bug fixes, feature requests, and the need for better user guidance to handle platform-specific challenges."
2017-05-20,pytorch/pytorch,"The discussions primarily address challenges in model deployment and serialization, with suggestions to convert weights to numpy or export models to Caffe2 for production efficiency, seeking updates on progress post-Caffe2 launch. There are concerns about the management and memory handling of RNN parameters, particularly regarding dynamic memory moves and the impact on user code, with proposals for caching weight buffers and invalidation hooks. Users report issues with training speed, notably when unrolling LSTMs or switching off CuDNN, indicating a need for better performance optimization or workarounds. Clarifications are sought on certain API behaviors, such as the differences between `ravel()` and `view()`, and questions about implementing weight normalization with various layers are raised. Overall, the threads reflect ongoing efforts to improve model deployment, memory management, training efficiency, and API usability within PyTorch."
2017-05-21,pytorch/pytorch,"The discussions primarily revolve around optimizing and extending PyTorch's functionality, including integrating cuDNN API implementations (such as convolutions, pooling, and activations) into cuda-on-cl, and addressing linking and compilation challenges for CUDA and OpenCL support. Several issues pertain to installation and environment setup complexities, such as managing compiler versions (e.g., clang, nvcc), fixing build caches, and resolving module import errors in Jupyter. Additionally, there are technical questions about implementing advanced loss functions like NCE loss and practical suggestions for weight normalization techniques and faster data transfer methods. Unresolved concerns include improving build robustness, handling large data batch limitations, and efficiently integrating custom modules or loss functions into PyTorch workflows."
2017-05-22,pytorch/pytorch,"The discussions highlight ongoing development challenges in PyTorch, including the integration of new functions like `lgamma` and `digamma` with autograd support, and issues with CUDA compatibility on new Apple clang versions, requiring specific downgrades and environment configurations. Several issues pertain to debugging and optimizing performance, such as improving data copying, addressing potential deadlocks in multi-GPU data loading, and ensuring correct tensor shape handling. There are concerns about proper type checks in convolution functions and handling of non-gradient leaf variables to prevent segmentation faults. Additionally, users seek clarification on behavior with multiple backward passes, device management via context managers, and the impact of specific configuration flags like `long_args: True` in cwrap, indicating ongoing efforts to enhance usability, stability, and performance."
2017-05-23,pytorch/pytorch,"The discussions highlight various technical issues in PyTorch, including compatibility challenges with GPU libraries such as cuDNN and libcudnn, especially on systems with musl-based or long-term support distributions like Alpine Linux, and user-reported errors related to CUDA parameters and device management. There's interest in extending PyTorch support for complex tensors, which involves modifying underlying C libraries and creating Python bindings, with guidance offered for integrating these features. Several users reported build failures, especially when compiling from source or utilizing prebuilt binaries, often related to missing dependencies or version mismatches, emphasizing the importance of correct environment setup. Additionally, there is ongoing work and questions around implementing functions like lgamma and digamma, managing autograd behavior with scatter operations, and ensuring proper device management and reproducibility. Overall, these discussions point to active development areas, troubleshooting necessities, and feature enhancements to improve stability, extensibility, and functionality."
2017-05-24,pytorch/pytorch,"The comments address various technical concerns including PyTorch's internal design choices such as dual biases related to cuDNN compatibility, and the complexities of implementing features like inter-method tensor interpolation, which some users see as beneficial for broader GPU support. Compilation and build issues are highlighted, notably GCC version compatibility and errors caused by system configuration or package installation scripts, notably on conda environments with GCC 4.8.5 which encounters numerous header and macro definition errors. There are discussions about improving memory management during serialization, with suggestions to minimize overhead. Additionally, concerns about cuDNN bugs—such as batch normalization epsilon support in cuDNN 5.1.10—and the handling of dilated transpose convolutions, as well as code robustness including test suite errors, are raised as unresolved or ongoing development issues."
2017-05-25,pytorch/pytorch,"The discussions highlight several key technical concerns: one involves the need for enhancing hook mechanisms in autograd to access original input/output during forward passes, addressing limitations for operations like visualization; another centers on installation and cache issues when installing PyTorch and torchvision, with suggestions to clear pip cache or avoid using cached files; performance-related questions include optimizing GPU memory management, addressing slow autograd speeds for RNNs, and the need for CUDA backend development for complex tensors. Additionally, there are compatibility challenges, such as the support restriction of cuDNN for small epsilon values, and dependency issues with libraries like libcudnn on Alpine Linux. Unresolved questions include improving error messaging for CUDA/CuDNN incompatibilities and balancing strictness versus flexibility in tensor operations like scatter. Overall, the discussions emphasize improving debugging, performance, compatibility, and usability within the PyTorch ecosystem."
2017-05-26,pytorch/pytorch,"The discussions highlight several technical concerns, including compatibility issues with Windows builds and cuDNN, as well as potential bugs in autograd variable traversal and gradient explosion in large class scenarios. There are requests for missing features like the Local Response Normalization (LRN) layer and better handling of parameters, buffers, and memory management. Performance disparities between CPU and GPU, especially for small networks, are discussed, with some noting that PyTorch may not always outperform numpy, contradicting the official claims. Additionally, evolving API components such as `masked_copy` and `masked_select` are being debated, along with concerns about reproducibility and proper resource management in data processing and autograd traversal."
2017-05-27,pytorch/pytorch,"The discussions highlight several technical concerns, including a potential bug with `batch_norm` related to cuDNN and tensor type compatibility, especially with FloatTensor, HalfTensor, and DoubleTensor. There is ongoing work on merging conflicting PRs, such as structural fixes and standardization efforts, with some tests failing due to merge conflicts. Users report issues with scalar and 0-dimensional tensor behavior, questioning the consistency of tensor interpretation compared to NumPy's scalars and 0-d arrays. Several users face installation challenges on Windows and Python versions, with suggestions to use specific conda channels or environment configurations. Lastly, questions arise about tensor operations such as `gather` and printing, and the importance of understanding in-place versus assignment behaviors in CUDA operations."
2017-05-28,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch functionalities, such as adding bilinear sampling with GPU support, implementing cosine similarity in `nn.modules.distance`, and improving weight normalization techniques, including potential integration via the optimizer. There are concerns about compatibility and stability, exemplified by a segmentation fault in `Conv2d` on Python 3.4/Ubuntu 14.04, likely due to missing type checks and memory management issues. Serialization problems are also noted, especially with large arrays exceeding 2GB, which are recognized as Python limitations rather than PyTorch bugs. Additionally, issues like the default gradient scaling in `MultiLabelSoftMarginLoss` and the absence of certain features in `torch.nn` reflect areas for further development. Overall, the discussions emphasize feature expansion, robustness, and compatibility challenges within the PyTorch ecosystem."
2017-05-29,pytorch/pytorch,"The discussions highlight ongoing efforts to integrate bilinear sampling functionality into PyTorch's core, with emphasis on GPU implementation and efficient memory usage, particularly for high-channel and batch scenarios. Several issues concern compatibility and setup, such as CUDA version requirements for Pascal GPUs, and challenges with package installation via pip, including cache clearing and repository support for specific versions. There's also a technical concern regarding numerical stability during backpropagation, especially when using `torch.log` with small inputs, with solutions like `torch.clamp` suggested to prevent gradient explosion. Additionally, implementation details like library detection, OS-specific behaviors, and printing issues are discussed, with proposals for fallback strategies to ensure robustness. Overall, the conversations focus on enhancing functionalities, improving stability, and ensuring smooth setup and environment configurations."
2017-05-30,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionality and usability, such as adding element-wise loss functions and dimension-aware distance calculations like `euclidean_distance`. There are concerns about the memory footprint of certain operations, notably in recurrent models and optimizer state management, suggesting possible optimizations like saving intermediate states separately. Compatibility and support issues are raised, including Windows builds (with and without CUDA support), handling device IDs in CUDA, and making features like `pixel_shuffle` invertible. Additionally, there's interest in understanding internal implementations, such as `NLLLoss` for element-wise loss computation, and clarifications on autograd behaviors, such as the non-invariance of `next_functions`. Overall, these discussions reflect active development, testing, and testing gaps, with some features pending integration or further refinement."
2017-05-31,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting within PyTorch, including the implementation of element-wise losses, especially for NLLLoss, and the need for mathematically precise implementations. There are concerns about package compatibility, installation issues with pip and PyPI, and the execution of large-vocabularies with NCE-loss, emphasizing performance optimizations like efficient sampling methods. Technical challenges also include GPU device management intricacies, such as reproducibility and index handling with CUDA_VISIBLE_DEVICES, and compatibility issues with certain PyTorch operations, exemplified by TypeError in torch.addmm. Additionally, there are design discussions around broadcasting behavior, potential toggles for broadcasting compliance, and extending features like WeightNorm to RNN architectures, with some unresolved questions about maintaining usability and backward compatibility."
2017-06-01,pytorch/pytorch,"The discussions highlight several technical concerns, including limited official Windows support, especially for 32-bit builds, and the need for clearer error messages and validation checks (e.g., input tensor dimensions and parameter types) to improve user experience. Contributors suggest enhancements such as adding more informative RuntimeErrors, Python-level input validation, and checks for tensor shape consistency in convolution functions. There is also emphasis on understanding autograd's internal behavior, like the non-invariance of `grad_fn` references, and considerations around shared memory and parameter views with data parallelism. Additionally, suggestions for improving caching, checkpointing, and code clarity—like clarifying function implementations and handling edge cases—are discussed, leaving some questions about the best practices for these enhancements."
2017-06-02,pytorch/pytorch,"The discussions highlight challenges with integrating custom C/CUDA implementations into PyTorch, including proper backpropagation and weight sharing, with questions on tying encoder-decoder weights and ensuring optimizer updates. Several issues address CUDA multiprocessing limitations, such as sharing tensors across processes and the associated runtime errors, emphasizing the need for workarounds and proper device context management. There are suggestions to improve error messages by including more specific type and argument information, especially for functions like `TupleParser` and convolutional layers. Ongoing efforts are focused on performance optimizations, like caching in cuDNN RNNs, and enhancing user diagnostics, such as more informative errors for CUDA restrictions. Some unresolved topics involve fixing specific build errors on macOS, handling small epsilon in batchnorm, and correctly implementing input shape validations across various modules."
2017-06-03,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for improved and more flexible RNN APIs to support custom and bidirectional implementations, as well as the difficulty in extracting layer outputs in PyTorch compared to other frameworks. Issues related to CUDA support and driver compatibility are raised, such as problems with sharing CUDA tensors across processes and potential driver installation errors. There are also questions about environment setup and system conflicts, notably regarding MKL library conflicts and installation steps on different platforms. Additionally, performance optimization topics are addressed, including the impact of autograd on training speed, and GPU utilization troubleshooting. Unresolved questions include best practices for model layer access, environment configuration, and handling system-specific CUDA and MKL issues."
2017-06-04,pytorch/pytorch,"The discussions highlight several key technical issues: the lack of official Windows support due to resource constraints, with a suggestion to accept PRs addressing Windows compatibility rather than full support; an ongoing implementation of a flexible `crop` function (supporting modes like `center-left` and `center-right`) with considerations about efficiency, CUDA compatibility, and recursive implementation; a proposal to move CUDNN shape checks into the `Convolution` constructor, raising questions about handling batched versus non-batched inputs; concerns about missing type checks in Conv2d leading to runtime errors; and various bug fixes, testing concerns, and feature enhancements such as invertible pixel shuffle. Overall, the discussions focus on improving robustness, compatibility, and functionality through targeted code changes and clarifications, with some unresolved questions about best practices and implementation details."
2017-06-05,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA/cuDNN support, particularly regarding shape validation, batch assumptions, and backward compatibility, with suggestions to improve shape checks and error messages in convolution implementations. Multiple comments address debugging GPU utilization issues and verifying proper hardware and driver configuration, including ensuring cuDNN/MKL are correctly integrated. There is interest in supporting sparse tensor operations and optimizing CPU functionality, with proposals to integrate libraries like SuiteSparse or Intel MKL for non-dense QR factorizations. Several comments involve code fixes, PR reviews, or reverting problematic changes—indicating active development and maintenance concerns. Unresolved questions remain about leveraging cuDNN capabilities effectively, verifying environment configurations, and extending functionality to non-batched or sparse tensor scenarios."
2017-06-06,pytorch/pytorch,"The discussions highlight several technical concerns, including difficulties in installing `torchvision` on Windows due to missing Linux/macOS builds, and issues in loading models trained with older PyTorch versions that trigger CUDA initialization errors. There are questions about optimizing performance, such as replacing the `persistent` flag in cuDNN with heuristics, and the potential for more memory-efficient implementations of `im2col`. Other topics involve enhancing error messages and robustness in convolution shape checks, managing large optimizer state dictionaries with options like `map_location`, and strategies to prevent memory leaks or cyclic references in RNNs, possibly through caching or weak references. Lastly, there is ongoing attention to linking issues related to threading libraries (`libgomp` vs. `libiomp`) and ensuring correct sharing of parameters like `gamma` and `beta` across batch normalization modules for parameter sharing."
2017-06-07,pytorch/pytorch,"The discussions primarily focus on ensuring backward-compatibility and seamless integration of new features, such as broadcasting and in-place operations, while minimizing disruption to existing code. Several comments address implementation details, like memory efficiency in variance calculations, handling of specific tensor operations (e.g., median, sparse QR), and library linking choices for sparse and cuSolver support, often weighing trade-offs between performance and complexity. There's ongoing work on refining API design (e.g., interface rewrites, consolidating upsampling methods) and addressing bugs or limitations, such as in dilated transposed convolution and the support for various sparse matrix formats. Additionally, complexities around build issues, package support on different platforms, and dependencies (like MKL, OpenBLAS, OpenMP) are highlighted, reflecting efforts to improve stability, performance, and compatibility. Unresolved questions include finalizing new feature implementations, resolving build or library linking problems, and reviewing pending pull requests for integration into the main repository."
2017-06-08,pytorch/pytorch,"The discussions primarily address enhancements and fixes to PyTorch functionalities, including tensor operations such as minibatch discrimination, matrix multiplications, and support for complex types. Several comments highlight ongoing issues with hardware support on Windows, CUDA-related errors like device-side asserts and kernel launch failures, often suggesting environment variable adjustments or code revisions as solutions. There are also technical concerns regarding API consistency, such as the behavior of `torch.matmul` versus `torch.dot`, and the need for polymorphic and more flexible tensor functions to reduce code duplication and improve usability. Additionally, some threads focus on architectural improvements, including thread management with MKL and IOMP libraries, and the integration of new features like `digamma`, `bernoulli_`, and sparse tensor operations, alongside ongoing merge conflicts and maintenance challenges. Unresolved questions mostly revolve around bug fixes, performance issues, and feature integration pending review and implementation."
2017-06-09,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's autograd refactoring, with specific focus on enabling functions like softmax to accept a user-specified dimension, and ensuring consistency in default dimension choices across tensors. Contributors are exploring safer implementations for sparse tensor operations, considering polymorphic approaches to reduce code duplication, and contemplating API changes for better backward compatibility and usability. There are concerns regarding CUDA-related seed management and ensuring correctness and stability in backward gradients for certain functions, with plans to review and merge related PRs for future releases. Additionally, discussions mention improving the packaging, such as wheel configurations and dependencies, and addressing backend compatibility and code robustness issues. Unresolved questions include default dimension selection for softmax, handling zero or negative inputs in certain functions, and refining API interfaces for sparse tensors."
2017-06-10,pytorch/pytorch,"The discussions highlight several technical issues: (1) In Issue #643.0, a problem with `libgomp.so` causing compilation or runtime errors, with suggested solutions including library removal, symlinking, or using `LD_PRELOAD`; (2) Issue #1408.0 addresses performance degradation during training, with recommendations to profile code and switch optimizers like to Adam; (3) Issue #1616.0 clarifies that `long_args: True` affects keyword argument support, prompting questions on argument passing syntax; (4) Issue #1643.0 discusses a runtime error in backward propagation, with suggestions to rebase and fix it in subsequent releases; (5) Additional comments consider code robustness, such as adding comments directly in source code for maintainability and implementing efficient versions of activation functions like SELU, alongside dealing with test failures due to missing `legacy` support."
2017-06-11,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, such as implementing more stable and integrated loss functions like BCELoss with combined SoftMax, addressing complex number support through custom TH extensions, and improving complex tensor operations, including their testing and integration. Key technical challenges include resolving symbol linking issues with custom forks of TH, managing undefined symbols like `THCudaZDoubleTensor_equal`, and ensuring correct CUDA device handling, especially when moving modules and tensors with `.cuda()`. There is also interest in extending core features, such as adding peephole connections to LSTMs, refining sparse tensor APIs, and incorporating new loss implementations in the framework, with questions about appropriate code locations and integration points. Additionally, limitations in autograd's current design prevent nested backward passes, raising questions about possible engine modifications to support such use cases. Overall, the discussions reflect efforts to make PyTorch more stable, flexible, and feature-rich through both internal improvements and community contributions."
2017-06-12,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's Windows support, including merging Windows-compatible changes into master and establishing build processes, with some uncertainty about required modifications. There are considerations around adding new tensor types (e.g., ShortTensor) and ensuring proper type conversions (like int16 and int8) from NumPy, alongside updates to support signed char in C code. Work is also focused on optimizing specific layers such as `BCELoss`, with questions about implementing more stable variants and their integration in C/CUDA versus Python. Additional challenges include debugging worker crashes in data loading, managing multi-GPU sparse tensor behavior, and resolving CUDA library compatibility issues, often influenced by driver updates. Overall, unresolved questions concern stable, efficient implementations for new loss functions and tensor types, alongside addressing build, support, and compatibility hurdles."
2017-06-13,pytorch/pytorch,"The discussions highlight several core technical concerns, such as ensuring accessibility to the PyTorch source code after repository publicization, and integrating advanced features like RDMA in process groups for distributed training. There are performance and usability questions, including updating conda packages with new functionalities, optimizing sparse operations on multi-GPU setups, and addressing potential bottlenecks in operations like padding and masking. Compatibility issues are also addressed, notably the need for Windows support through CI integration and handling CUDA driver errors, with suggestions to improve testing and debugging workflows. Lastly, ongoing development areas such as extending support for CSR/CSC sparse tensors, implementing new loss functions, and refining autograd expand behavior are discussed, with considerations for structured refactoring and incremental merging of large code changes."
2017-06-14,pytorch/pytorch,"The discussions highlight several technical concerns, including the challenge of merging Windows-specific code into the main PyTorch repository and ensuring compatibility across platforms, with suggestions to test on Linux/Mac before merging Windows branches. There are ongoing efforts to enhance loss functions with weighted class criteria and support for multi-label imbalanced data, requiring integration into existing APIs without major rewrites. Memory management issues related to shared memory (shm) limitations causing deadlocks are noted, with workarounds involving expanding shm size. Compatibility and correctness issues are raised with GPU tensor indexing, needing updates to support CUDA and variable expansion handling in autograd. Lastly, there are development updates on features like reverse sequence functions, functional API improvements, and handling of shape broadcasting, with some unresolved questions about specific implementations and dependencies like cudnn versions."
2017-06-15,pytorch/pytorch,"The discussions primarily revolve around integrating and improving core PyTorch functionalities, such as adding file I/O methods to tensor and storage classes, and implementing in-core bilinear sampling with GPU support, which is relevant for applications like Mask R-CNN. There are concerns about memory management and deadlocks associated with shared memory (`/dev/shm`) when using DataLoader workers, especially during transitions between training and testing epochs. Implementation suggestions include creating `from_file` class methods similar to `from_buffer` for various storage types, and adding a sequence reversal utility to facilitate bidirectional RNNs without PackedSequences. Questions also arise around the best approach to testing `Function` and `Variable` behaviors to avoid redundancy, as well as ensuring proper module importation after installation. Overall, the focus is on enhancing core features, optimizing memory management, and clarifying best practices for extending and testing PyTorch components."
2017-06-16,pytorch/pytorch,"The discussions highlight ongoing efforts to integrate and optimize bilinear sampling and `im2col` functionalities within PyTorch, including considerations for memory efficiency and API design, with suggestions to unify related operations under a single layer like `nn.functional.unfold` or `nn.functional.d2col`. There is concern about proper initialization and training behavior of embedding layers, specifically ensuring padding indices remain zero during training updates. Several issues address handling NaN values consistently across CPU and GPU, along with bugs related to broadcasting and tensor dimensionality, with plans for fixes in backward pass implementations. The community also discusses the development of documentation and extension packages to improve usability and extension like custom storage docs and external modules. Lastly, various hardware-related problems, especially CUDA memory management and platform-specific bugs, are recognized, with some solutions involving system reboots or build adjustments, and open questions about monitoring GPU memory on MacOS."
2017-06-17,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch with complex number support, including dealing with symbol linking issues and ensuring proper testing, especially for complex tensor math and functions. There are concerns about compatibility and completeness in the autograd system, notably the lack of double backpropagation for certain layers like pooling and batch norm, with warnings about potential runtime errors during backward passes. Several issues relate to GPU memory management, including detecting memory usage and handling out-of-memory errors, suggesting the need for better tools or practices for memory diagnostics. Additionally, improvements are proposed for API ergonomics, such as implementing chaining for model initialization to enhance code clarity. Overall, the focus is on resolving technical integration challenges, expanding functionality, and improving usability and debugging support in PyTorch."
2017-06-18,pytorch/pytorch,"The discussions highlight several key technical concerns: the need to add the `PoissonNLLLoss` to the `torch.legacy.nn` module due to testing failures and questions about the separation of legacy modules from `torch.nn`. There are suggestions to improve API usability and flexibility, such as introducing an optional exact calculation for `log(target!)` using element-wise operations but facing issues with autograd variables lacking certain methods. Additionally, there is debate over implementing parameter initialization methods, with proposals including chaining initializers via method calls or passing dictionaries, and considerations about the most Pythonic and flexible approach. The discussions also involve whether to enhance `nn.Module` directly with a `reset_parameters` method or to modify the existing `reset_parameters` function, aiming for more comprehensive and user-friendly initialization routines."
2017-06-19,pytorch/pytorch,"The discussions primarily revolve around ensuring proper tensor contiguity in cuDNN batch normalization functions, suggesting the addition of assertions and `.contiguous()` calls to prevent errors. Multiple comments address issues with undefined symbols and library linking for complex number support, indicating the need for proper symbol declarations and build configurations. There are concerns about CUDA indexing bugs, with solutions proposed involving copying index tensors onto the GPU, and potential expansion of autograd to handle broadcasting more seamlessly. Several reports highlight bugs and regressions, including crashes with `dot` operations, missing features like median on CUDA, and memory leaks, emphasizing ongoing debugging and testing. Additionally, maintenance questions are raised about legacy module separation, documentation discrepancies between versions, and environment setup for correct package installation."
2017-06-20,pytorch/pytorch,"The discussions highlight significant challenges with GPU memory management, notably the enormous memory requirements for large linear layers (up to 40GB), and questions about optimizing resource usage and backpropagation performance. There are issues with segfaults during backward passes and autograd engine stability, especially related to embedding layers, CUDA reinitialization, and the effects of code changes on reproducibility across different hardware and software environments. Several feature enhancements are suggested, such as integrating functionalities like `ConcatDataset` into core PyTorch, and improving support for packed sequences and distributed training failure diagnostics. Additionally, there are concerns about correctness and efficiency of certain operations, including seed setting in multiprocessing and the accuracy of documentation concerning statistical functions. Overall, unresolved questions revolve around debugging lower-level engine issues, memory optimization, and improving utility features for more robust and efficient training workflows."
2017-06-21,pytorch/pytorch,"The discussions highlight ongoing work and issues related to extending PyTorch functionalities, such as implementing common functions like cosine similarity, complex tensor support, and FFT operations, with contributions and troubleshooting from the community. There are technical challenges in ensuring compatibility across Python versions and operating systems, especially regarding shared library dependencies and BLAS routines, with suggestions to use cblas API or workarounds for unavailable functions. Concerns about autograd's behavior causing segmentation faults and the proper handling of gradient propagation (e.g., in Threshold operations) are noted, alongside considerations for private API exposure, like `_in_bad_fork`, to manage CUDA seed settings in forked processes. Additionally, actions like moving code to appropriate locations, adding tests, and improving code comments are discussed to improve maintainability and robustness. Many unresolved issues involve low-level library dependencies, autograd stability, and ensuring correct operation across diverse environments."
2017-06-22,pytorch/pytorch,"The discussions reveal ongoing efforts to integrate bilinear sampling into core PyTorch, with performance and memory efficiency considerations, especially for high-channel applications like Mask R-CNN. Several issues concern GPU compatibility, including indexing and handling of nan values, as well as the need for more stable and clear APIs for custom weight initialization, RNN implementation without PackedSequences, and shared memory challenges in multiprocessing. There are also technical challenges related to CUDA and OpenCV interactions, clang compatibility, and ensuring robust autograd support, particularly for in-place operations and complex modules. Overall, key concerns involve improving core functionalities, GPU and CPU consistency, API clarity, and debugging stability issues while addressing unresolved questions about optimal API design and backward compatibility."
2017-06-23,pytorch/pytorch,"The discussions highlight several technical challenges including the handling of undefined behavior and unspecified semantics in certain tensor operations like index_copy, which can lead to unpredictable results when indices are repeated. There are ongoing compatibility issues with compiler versions and target architectures, notably concerning thread-local storage support and CUDA linkage errors on macOS, which are often addressed by environment variables or flags like NO_DISTRIBUTED=1. Users express concerns about the support for autograd over integer tensors, clarifying that derivatives for integer operations are not defined, but gradients over floating-point approximations might still be possible with custom implementations. Additionally, there are issues related to NaN handling, performance optimizations with Jupyter kernels, and the availability of APIs such as `grad()`, which is only present from source in certain versions. Overall, unresolved questions revolve around ensuring cross-platform compatibility, correct semantics of advanced tensor operations, and clarifications on autograd’s capabilities with non-floating-point data types."
2017-06-24,pytorch/pytorch,"The discussions highlight several technical concerns including the compatibility and availability of prebuilt PyTorch packages across different Python versions, particularly issues with conda and pip installations, as well as challenges in building from source due to compilation errors on certain systems. There are questions about CUDA runtime errors, such as the CUDA ""invalid device function"" error, which suggest potential issues with device-specific builds or environment configurations. Additionally, some discussions address the desire for higher-level abstractions, like a more expressive container system or parallelizing criteria, as alternatives or supplements to existing `nn.Sequential`. Lastly, users express frustrations with the current implementation and support documentation, pointing to the need for clearer guidance and more flexible model composition tools."
2017-06-25,pytorch/pytorch,"The discussions highlight various technical issues and questions related to PyTorch functionalities, including compatibility and installation problems with CUDA and different Python versions, particularly the challenges with building and running PyTorch on certain systems and the need for proper CUDA configuration. There are concerns about the correct usage and limitations of `F.pad()` for different tensor dimensions and how to handle sequence padding and causal convolutions effectively. Additionally, there are suggestions for improving core PyTorch features, such as porting dataset concatenation utilities from `torchnet` and optimizing tensor operations with better broadcasting and pooling implementations. Unresolved questions include the proper setup of CUDA device management, handling version-specific installation issues, and refining functionalities like padding and dataset concatenation for wider support and usability."
2017-06-26,pytorch/pytorch,"The discussions highlight challenges related to CUDA compatibility and GPU support, emphasizing the need for proper architecture specification during compilation and potential Windows support improvements. Several issues pertain to extending PyTorch functionalities, such as integrating median functions into the appropriate modules, adding support for EmbeddingBag in older versions, and improving high-level interface modules like RNN stacking, with suggestions for more explicit input/output descriptions or base classes for RNNs. There's also concern about handling edge cases like zero-element tensors, ensuring compatibility with various frameworks (e.g., Keras vs. PyTorch), and resolving regressions in pooling operations, indicating ongoing stability and feature completeness efforts. Additionally, questions about the functional interface for embedding and the organization of tensor reduction functions suggest a focus on API usability and internal code structure improvements."
2017-06-27,pytorch/pytorch,"The discussions highlight several key concerns: the need for official Windows support in PyTorch, with suggestions to split large PRs into reviewable commits; handling of reference counting and graph traversal in the autograd system to prevent memory leaks and cycles; fixing tensor operation errors such as mismatched device types and memory management bugs; improving API consistency and support for functionalities like embedding in functional style and bias handling in layers; and resolving platform-specific issues such as CUDA compatibility on macOS and ensuring correct tensor type recognition (e.g., HalfTensors). Proposed solutions include modularizing large code changes, modifying reference counting strategies, extending API support for complex operations, and adjusting low-level memory management, with some questions about best practices for adding features or fixing inconsistencies. Unresolved issues remain around the implementation details for reference management, sparse gradients, and cross-platform support challenges."
2017-06-28,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's autograd and module functionalities. Key concerns include the limitations of module hooks registration due to autograd's internal structure, leading to partial gradient inputs, and plans for refactoring to address this. There are requests for missing layers like Local Response Normalization (LRN) and challenges with implementation differences across CPU and CUDA, particularly for indexing and resizing operations. Suggestions for facilitating sequential and recurrent operations include developing flexible container layers like `TimeDistributed` or `Bottle`, and improving RNN API generality to support various activation functions. Additionally, handling resource management and tensor size stability in operations, as well as performance optimizations, are recurring technical considerations."
2017-06-29,pytorch/pytorch,"The discussions highlight concerns about large-scale, reviewability of code PRs, with a specific issue related to splitting large commits into smaller, manageable parts. Several issues address compatibility and correctness concerns, such as inconsistent behavior on CPU versus CUDA (e.g., for `CrossMapLRN2d`), potential bugs in autograd functions with duplicated indices, and differences in numerical results across hardware or versions (e.g., in `LRN` and tensor operations). There are also questions regarding the proper implementation of certain functionalities, like the necessity of `dropout` as an autograd function versus a regular function, sparse gradient support for `Embedding`, and handling of half-precision tensors, especially CPU HalfTensor limitations. Additionally, synchronization constructs like `critical` and `parallel` regions in multithreaded contexts are debated due to potential performance and correctness issues, alongside challenges in building and testing in environments with different dependencies and hardware (e.g., MKL conflicts). Unresolved questions remain about the best approaches for code refactoring, ensuring numerical stability, and enabling features like sparse gradients, with some issues pending further review and testing."
2017-06-30,pytorch/pytorch,"The discussions highlight several key areas: firstly, the need for improved tensor printing options in PyTorch to match numpy's full tensor display; secondly, the challenges and considerations in implementing and using weight normalization, particularly with RNNs, including potential stability and performance issues; thirdly, API design proposals for applying layers over sequences or axes, such as `TimeDistributed`, with suggestions for more flexible, user-friendly solutions like `Bottle` or `scan`-like utilities. Additionally, there are concerns about inconsistencies and bugs in functions like `torch.max` across PyTorch versions, and practical issues such as ensuring correct CUDA installation, handling device-specific errors, and managing shared parameters in optimizers. Some suggestions involve simplifying certain operations (e.g., dropout, validation shims) and enhancing usability, but unresolved questions remain regarding the most efficient, flexible abstractions for recurrent or time-distributed processing and compatibility across hardware/software configurations."
2017-07-01,pytorch/pytorch,"The discussions highlight significant challenges with the computational expense of Hessian matrix calculation, with current methods being too slow and limited to scalar derivatives, and suggest potential batching strategies for improvement. There are concerns about the design and implementation of `reset_parameters` in `nn.Module`, including serialization issues like pickling, and questions about refining interface consistency and checks in loss functions like `BCELoss` to ensure size matching and predictable behavior across different input formats. Performance regressions are observed in matrix multiplication operations after recent PRs, attributed to unnecessary memory copies in autograd functions, with ongoing investigations into minimizing data transfers to optimize speed. Additionally, there is a confusion regarding the `LSTM` documentation, which incorrectly describes input size expectations, prompting a clarification that the documented behavior does not match the actual implementation. Overall, these discussions point toward enhancing API consistency, computational efficiency, and code reliability across various components."
2017-07-02,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) the need to split large pull requests into smaller, reviewable commits for better manageability; (2) limitations in autograd for second-order derivatives, particularly the computational expense of full Hessian matrices, with some interest in future implementation plans; (3) serialization challenges such as pickling `lambda` functions and class instances, especially across different Python versions; (4) questions about implementing a no-GPU version of PyTorch and specific behaviors of tensor types like `HalfTensor` on CPU; and (5) issues related to memory leaks caused by `expand_as`, as well as suggestions for extending functionality with customized CUDA extensions. Overall, the discussions focus on improving code maintainability, enhancing automatic differentiation capabilities, resolving serialization issues, and expanding usage flexibility."
2017-07-03,pytorch/pytorch,"The discussions highlight several technical issues, including a bug fix in source code modification for a non-documented module, and plans to extend autograd support for advanced indexing to enable full differentiation. Performance concerns are raised about changes in matrix multiplication routines affecting GPU timings, with suggestions to optimize memory copy operations during GEMM operations. Memory leaks are identified when using unpacked tensor sizes (e.g., `*tensor.size()`) due to Python list unpacking, with proposed fixes involving converting to explicit lists to prevent leaks. Additionally, there is discussion around consistency in naming conventions for loss functions and activation functions like `log_softmax` vs. `logsigmoid`, and potential renaming to improve clarity and maintainability. Unresolved questions include how to best document and support non-standard modules, optimize BLAS usage, and standardize function naming across the library."
2017-07-04,pytorch/pytorch,"The discussions primarily focus on improving the stability and usability of loss functions like `BCELoss`, with suggestions to create more stable versions combining `SoftMax` + `BCELoss` and considerations for renaming or restructuring related classes (e.g., `BCEWithLogitsLoss`). There are technical concerns about the behavior of the `out=` interface in tensor operations such as `torch.index_select`, questioning whether to modify implementation details to ensure in-place results or enforce pre-resizing, to prevent bugs arising from unintended tensor resizing. Additional issues include clarifying the correct detection and configuration of BLAS backends like Accelerate and MKL, and how to properly handle configurations across different systems. Questions also address code consistency, backward compatibility, and fixing potential bugs related to autograd wrapper functions, particularly for operations like `addmm`. Unresolved items involve finalizing PRs, improving documentation, and clarifying the usage of experimental modules such as `Conv2dLocal`."
2017-07-05,pytorch/pytorch,"The comments highlight several technical challenges within the PyTorch development and usage, including module import errors due to incorrect working directory assumptions, and runtime library loading issues related to MKL and CUDA architecture compatibility, often requiring environment configuration or reinstallation. There are discussions about code design patterns, such as the C++ and Python object lifecycle management, and potential improvements like adding features for complex tensor operations and better C++ wrapper integration. Network-related problems are also mentioned, particularly with downloading dependencies over unstable connections, leading to unreliable build processes. Additionally, some comments suggest code refactoring or bug fixing, such as proper tensor size handling, ensuring compatibility across CPU and GPU tensors, and fixing backward pass issues in specific modules."
2017-07-06,pytorch/pytorch,"The discussions highlight several technical issues, including the need for better handling and optimization of batch normalization across multiple GPUs, with self-implemented aggregation functions to improve performance. There are concerns about compatibility and build configuration, particularly regarding CUDA architectures and the detection of BLAS libraries on macOS, suggesting improvements like detecting cblas availability via test programs. Support for sparse gradients in embedding layers and the potential to extend this support to other functions is mentioned, along with recommendations for automatic type conversions. Additionally, there are suggestions for modularizing large PRs for easier review, fixing variable creation from non-tensor inputs, and maintaining comprehensive documentation versions. Overall, unresolved questions focus on improving cross-platform compatibility, performance optimizations, and expanding tensor functionality features."
2017-07-07,pytorch/pytorch,"The discussions highlight challenges with synchronizing BatchNorm buffers across GPUs, including potential code solutions for aggregating running mean and variance in multi-GPU setups. Concerns about memory leaks introduced by frequent tensor operations such as `expand_as` and `size()` unpacking are noted, with suggestions to avoid such patterns to prevent leaks. Several issues address the need for improved model introspection, such as implementing a `summary` method for detailed layer information, and handling differences in `.cuda()` semantics between modules and tensors. Other technical concerns include managing shared memory and `shm` space during data loading, fixing GPU detection on macOS, and improving error handling when loading pretrained models. Unresolved questions pertain to better support for partial state_dict loading, extending `LayerNorm` to PyTorch, and ensuring correct behavior for empty `shared_ptr`s in autograd functions."
2017-07-08,pytorch/pytorch,"The discussions highlight several technical concerns: the persistent installation issues with PyTorch due to broken or incompatible precompiled wheels, especially on diverse operating systems and compiler configurations; the potential addition of native APIs such as sharing optimizer parameters across threads, similar to A3C; suggestions to include functions like `einsum` and tensor reshaping utilities directly within core tensor modules; and the possibility of supporting conversions between HalfTensor and NumPy arrays, contingent on compatibility between CUDA and NumPy's half types. There are also ongoing questions regarding specific implementation details, such as extending functionality to `torch/tensor.py` and addressing test failures on certain platforms, to improve usability and compatibility of PyTorch's API."
2017-07-09,pytorch/pytorch,"The discussions highlight ongoing issues with installing and running PyTorch, especially on MacOS and Windows, where installation errors and binary compatibility problems persist, with concerns about installation complexity potentially hindering adoption. There are technical challenges related to the build process, such as dependency management for complex types (notably with Thrust, THPP, THCUNN) and ensuring compatibility across CUDA versions and OS targets, including simplifying support for CPU-only and mixed environments. Additionally, questions are raised about extending functionality, like adding `ignore_index` to `nll_loss2d` and implementing numpy conversion for HalfTensor, with some uncertainty about the standard compliance and codebase inconsistencies for various tensor types. Overall, the community seeks solutions for smoother installation, improved compatibility, and expanded feature support, while some build and platform-specific issues remain unresolved."
2017-07-10,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for support of complex types in modules like THNN and THCUNN, which requires code modifications such as changing complex type definitions and enabling relevant build flags. There are ongoing efforts to ensure compatibility across platforms, particularly macOS and OSX, with issues related to linking CUDA libraries and supporting specific OS deployment targets. Additionally, challenges around managing multiple installations of PyTorch and dependencies like MKL on OSX are noted, emphasizing the importance of clean environment setups. Developers are also exploring improvements in design patterns, such as subclassing schedulers for batch vs. epoch updates and handling duplicate indices in tensor operations, with some proposals for solution trade-offs. Unresolved questions remain regarding support for duplicate indices during operations, cross-platform build consistency, and refining APIs for reinforcement and stochastic nodes."
2017-07-11,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionality, stability, and usability, including optimizing GPU support, correcting tensor operations (e.g., `view` behavior, in-place modifications, and zero-dimensional tensors), and refining C++/Python object wrapper patterns for consistency and memory safety. Several comments focus on installation difficulties, especially pip and conda issues, and the need for more robust, user-friendly instructions or packaging solutions like Docker images. Concerns are raised regarding testing processes, runtime performance, and the handling of in-place operations to prevent undefined behavior, emphasizing the importance of proper memory management and tensor broadcasting. Additionally, there are suggestions for enhancing model summaries, debugging tools, and ensuring code consistency and clarity, with some unresolved questions about the most appropriate wrapper pattern and support for features like zero-sized tensors."
2017-07-12,pytorch/pytorch,"The discussions highlight several technical concerns including the need for official and precompiled Windows binaries for PyTorch, with a hope for future support, and challenges in supporting variable input/output sizes in certain layers and GPU memory management, particularly regarding zero-strided inputs and CuDNN limitations. There are validation and testing issues across different Python versions, notably related to pickling errors in Python 2.7 and flaky test failures, necessitating adjustments such as removing lambdas for serialization and addressing version dependencies. Variability in environment configurations, such as CUDA compatibility and Docker build reproducibility, is also a concern, with some users experiencing difficulties building or running on different hardware or driver setups. Lastly, there is ongoing work to improve core functionalities like affine grid generation, multi-processing data handling optimizations, and ensuring code compatibility and correctness across multiple platform versions."
2017-07-13,pytorch/pytorch,"The discussions reveal ongoing challenges with implementing and deprecating functions like `Softmax` and `inverse`, handling library dependencies such as `libgomp.so`, and managing compatibility issues with system configurations and CUDA versions. Several issues address API behavior inconsistencies, such as the semantics of `.cuda()` for modules versus tensors, and the support for specific functions like `median` and `einsum` on CUDA tensors. Concerns are raised about internal code support, including proper serialization in Python 2.7, handling zero-dimensional tensors, and ensuring correct behavior of modules like `BatchNorm`. Additionally, there are technical adjustments proposed for ATen's function signatures, optimization sharing among threads, and explicit device management, with some unresolved questions about system-specific glitches and the proper way to extend or modify the API to improve flexibility and robustness."
2017-07-14,pytorch/pytorch,"The discussions highlight several technical concerns including the need for official Windows support and improved build configurations for PyTorch, similar to TensorFlow's availability on Windows. There are recurring issues with CUDA compatibility, such as architecture-specific compilation errors and missing or misdetected cuDNN and NCCL dependencies, which affect GPU functionality. Several comments address gaps in tensor operations, notably the absence of certain functions like `median` on CUDA tensors and challenges with zero-dimensional tensors in TH/THC libraries. Additionally, there are ongoing efforts to enhance autograd capabilities, like enabling nested backward calls, and to improve the robustness and clarity of documentation and testing practices. Unresolved questions involve proper handling of specific tensor types, dependency management, and future-proofing features like backward in backward computations."
2017-07-15,pytorch/pytorch,"The discussions highlight several technical challenges: in Issue #524, users face CUDA architecture compatibility issues, which can be addressed by modifying build configurations to include specific GPU architectures like 5.0. Issue #566 emphasizes installation difficulties across platforms, with suggestions to improve package distribution and instructions, noting that current methods sometimes lead to unhelpful errors and hinder adoption. Issues #1280 and #1767 concern device indexing and normalization techniques, prompting clarifications on device visibility, normalization methods, and behavior of batch versus layer normalization. In Issue #2028, inconsistent environment setup and hardware/software mismatches, such as MKL version discrepancies, cause runtime errors, emphasizing the need for clearer dependency management. Lastly, issues #2108 and #2110 discuss internal API behaviors, specifically module parameter registration and module management, raising questions about best practices for attribute assignment versus explicit registration to ensure robustness and clarity."
2017-07-16,pytorch/pytorch,"The discussions highlight ongoing challenges in improving error messaging clarity, particularly translating low-level C++ errors into meaningful Python exceptions, which would aid debugging tensor shape mismatches and device issues. There is concern over providing detailed tensor size and type information during assertion failures, suggesting that enhanced diagnostic output could greatly assist users. Several threads address the need for better management and representation of tensor attributes, like strides and data types, especially for new features or bug fixes (e.g., FP64 handling in NumPy tensors). Compatibility and performance considerations are raised regarding dynamic shape inference in models and the efficiency of reduction operations over multi-dimensional tensors, emphasizing that solutions should balance speed and correctness. Lastly, some discussions focus on foundational library issues, such as ensuring proper linkage of C++ standard libraries across environments and refining the integration of features like normalization layers and variable handling, to maintain robustness and usability in diverse deployment scenarios."
2017-07-17,pytorch/pytorch,"The discussions highlight various technical issues in PyTorch, including code modifications (e.g., changing `long` to `int` in torchvision utils), discrepancies in advanced indexing behavior (especially with duplicates and shape manipulations), and potential bugs related to tensor normalization and softmax dimension handling. There are concerns about the correctness and numerical stability of functions like `log_sigmoid`, with suggestions for alternative implementations. Discussions also address backend implementation strategies for normalization layers, emphasizing avoiding unnecessary overhead and ensuring compatibility and performance, particularly for LayerNorm and InstanceNorm. Lastly, there is debate over linking strategies involving `libstdc++` and `libgcc`, with consensus leaning toward dynamic linking for stability, while other issues pertain to debugging runtime symbol resolution and ensuring consistent behavior across tensor types, especially in reductions and comparisons."
2017-07-18,pytorch/pytorch,"The discussions highlight a need for more robust and user-friendly error handling, particularly in catch-all C-level exceptions, to improve debugging by providing tensor sizes, shapes, and clearer messages, especially during tensor operations like transpose and size mismatches. There is concern over inconsistent behaviors and documentation regarding normalization layers such as BatchNorm, InstanceNorm, and LayerNorm, with suggestions to clarify their distinctions and optimize implementation approaches—potentially moving some to backend modules for efficiency. Several issues address API design, such as enabling modules to accept multiple inputs seamlessly, improving the clarity of `.cuda()` semantics across modules and tensors, and enhancing test coverage to verify correctness and performance gains of new features, including custom function implementations and normalization strategies. Additionally, performance benchmarking inconsistencies and build errors in custom or experimental code indicate interest in both optimization and stability. Overall, the thread underscores ongoing efforts to improve error diagnostics, standardize normalization conventions, and refine both usability and efficiency in PyTorch’s core and extension modules."
2017-07-19,pytorch/pytorch,"The discussions predominantly revolve around addressing backward compatibility and usability concerns, such as the default behavior of functions like `keepdim` in reductions, and the need for automatic broadcasting fixes to align with NumPy semantics while minimizing user disruption. Several issues highlight ongoing efforts to improve automation, correctness, and performance—such as enabling double backward support for pooling functions, optimizing LayerNorm implementation, and refining the C++/Python object management pattern to ensure wrapper uniqueness. Additional technical challenges include resolving serialization errors in Python 2.7, handling CUDA tensor sharing across subprocesses, and verifying build dependencies like `libgcc`. Overall, these discussions reflect efforts to enhance API consistency, maintainability, and performance, while carefully managing backward compatibility and platform-specific issues. Unresolved questions include optimal C++ wrapper patterns and ensuring reliable multi-process CUDA operations."
2017-07-20,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's Windows support, including the need for a CMake configuration for easier dependency management and clarification on header inclusion for compilation. Several issues address autograd and serialization limitations in Python 2.7, such as problems with pickling methods and the implementation of double backward functions for certain operations like ELU and pooling. Questions also arise regarding the behavior of wrapper objects (Variable vs. function pattern), GPU parallelism with DataParallel, and performance optimizations like transposing matrices without overhead. Additionally, there are concerns about packaging, including missing MKL .so files in wheels and compatibility issues across platforms like OSX and Linux. Overall, the conversations reflect active development, bug fixes, and feature improvements aimed at robustness, usability, and performance."
2017-07-21,pytorch/pytorch,"The discussions highlight the challenge of accessing and tracking per-timestep gradients for RNNs in PyTorch, with suggestions ranging from using hooks on weights to leveraging `autograd.grad`, though the latter may be computationally expensive or conceptually ambiguous. There is concern about API complexity and usability, prompting considerations for API enhancements like gradient accumulation options in `backward()` but with acknowledgment of potential semantics issues. Compatibility and consistency issues are recurring themes, such as differing behaviors between CPU and CUDA tensors, version discrepancies, and module versus tensor `.cuda()` semantics, which affect reproducibility and debugging. Optimization requests are made for specialized kernels like depthwise separable convolutions, with realization that current implementations may be suboptimal. Lastly, various operational concerns include version management, build consistency, and ensuring robust testing across environments."
2017-07-22,pytorch/pytorch,"The discussions highlight ongoing development and refinement in PyTorch features, including implementing Zoneout or Variational Dropout and ensuring consistent behavior between Functions and Variables for storing metadata, with plans for future revisiting. There are concerns about correctly handling padding indices in functional embedding operations, alongside questions about how to properly test and verify outputs against modules, including running doctests. Memory utilization discrepancies are also discussed, with advice to verify CUDNN support when diagnosing out-of-memory issues. Additionally, a fix has been implemented for an out-of-memory bug, which will be included in the next release. Overall, the issues focus on improving feature implementation consistency, testing procedures, and performance diagnostics."
2017-07-23,pytorch/pytorch,"The discussions highlight concerns about GPU memory management and utilization, especially on Tesla K80s, with observations that memory usage can appear doubled, potentially indicating allocation issues. There are ongoing efforts to improve tensor broadcasting and avoid redundant memory views in loss functions like BCE loss, with suggestions to leverage internal functions such as `_infer_size` for better implementation. Some issues involve debugging specific numerical problems, such as incorrect outputs when logits are zero, requiring minimal reproducible examples for diagnosis. Additionally, questions about running PyTorch headlessly on servers are addressed, emphasizing typical use cases. Overall, the key challenges revolve around optimizing memory usage, refining broadcasting logic, and troubleshooting numerical inaccuracies."
2017-07-24,pytorch/pytorch,"The discussions highlight ongoing support limitations for Windows, with community contributions needed for implementation. Memory leak concerns persist in PyTorch, particularly related to graph reference cycles and autograd refactoring, alongside a specific issue with memory growth during model inference. Several issues involve bug fixes and improvements to autograd functionalities, including support for double backward, correct handling of gradient undefined points like abs(0), and ensuring consistent behavior across different functions. There are also technical considerations about DataParallel's parameter ordering, buffer sharing, and compatibility, emphasizing the need for deeper API design around parameter locking. Unresolved questions remain about the timing of fixes in upcoming releases, platform-specific bugs, and the behavior of certain operations under various backpropagation conditions."
2017-07-25,pytorch/pytorch,"The discussions highlight several core issues: the design choice of tensors' contiguity and their impact on the `view` function, with suggestions to either modify `view` to include `.contiguous()` or provide a separate function for non-contiguous views; handling edge cases such as zero-sized storage in memory allocators to prevent segfaults; ensuring backward compatibility and safety in module parameter registration and attribute assignment; improving user control over custom autograd functions to return per-element loss outputs for more flexible operations; and addressing device-specific and performance concerns, including ensuring operations support double backward, optimizing RNN modules without heavy Python unrolling, and understanding nondeterministic CUDA operations' numerical discrepancies. Many unresolved questions pertain to balancing efficiency, code safety, backward compatibility, and user flexibility, often discussed in the context of fixing bugs, optimizing performance, or clarifying API behaviors."
2017-07-26,pytorch/pytorch,"The discussions mainly revolve around bug fixes, feature enhancements, and API clarifications in PyTorch, such as addressing tensor size inconsistencies, enhancing custom convolution modules, and resolving issues with autograd and gradient tracking, especially in recurrent models. Several comments highlight the need for better control over memory usage and graph manipulation, including detaching and reattaching parts of the computational graph, and improving the efficiency of backward computations in RNNs. There are ongoing concerns with compatibility, particularly regarding installation issues on macOS, Python version support on Travis, and behavior changes due to broadcasting or gradient requirements. Some comments suggest potential improvements, like adding parameters for shared weight sizes in convolutions or simplifying mathematical operations for efficiency. Overall, unresolved questions include refining autograd hooks, managing gradient computation for recurrent structures, and fixing build or environment compatibility issues."
2017-07-27,pytorch/pytorch,"The discussions primarily address the behavior and nuances of tensor reshaping and view operations in PyTorch, emphasizing how `torch.view` maintains content order and the importance of tensor contiguity and axis permutations, especially when reshaping multi-dimensional data like videos. There are concerns about floating point determinism and the impact of multi-threading on numerical consistency during CUDA operations, raising questions about reproducibility. One thread highlights potential data type mismatches, such as incorrect tensor types in `binary_cross_entropy_with_logits`, leading to errors, and suggests clarifying target value constraints. Additionally, broadcasting behavior in tensor operations is noted, illustrating how dimension alignment can produce unexpectedly large output shapes, prompting careful consideration of shape compatibility. Overall, the conversation covers tensor manipulation techniques, reproducibility issues, data type correctness, and broadcasting effects within PyTorch."
2017-07-28,pytorch/pytorch,"The discussions highlight several technical challenges including issues with tensor inference and runtime errors like `std::bad_cast` when running models on GPUs, possibly due to mismatched or conflicting library versions and improper linking against CUDA or Torch libraries. There are concerns about outdated or missing Python binaries in specific Linux distributions, affecting compatibility and build processes. Questions also revolve around the implementation details of core modules like `NLLLoss` and how certain loss functions such as `CrossEntropyCriterion` internally combine operations like LogSoftmax and NLLLoss. Additionally, troubleshooting steps involve verifying library links with `ldd`, clearing conflicting library files, and understanding the underlying C implementations. Unresolved issues include ensuring compatibility between different library versions and clarifying the internal workings of loss functions."
2017-07-29,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch package support and installation, particularly related to issues with conda package file corruption and system memory constraints during training. Technical concerns include fixing compatibility issues such as undefined symbols during build from source, stability and correctness of gradient and output comparisons in testing, and addressing specific failures like `test_noncontig` and `test_Conv2d_backward_twice`. There are also procedural questions about update workflows, such as changing target branches in pull requests and improving community support via forums. Overall, unresolved issues mainly focus on fixing build stability, memory management, and testing consistency across different setups."
2017-07-30,pytorch/pytorch,"The discussions highlight several technical concerns: the impact of setting `CUDA_VISIBLE_DEVICES=1` on GPU utilization and DataParallel efficiency, and potential issues with tensor movement caused by other programs; clarification that CrossEntropyCriterion uses LogSoftmax + NLLLoss; challenges in building PyTorch in Docker due to shared memory limits, which was resolved by increasing `--shm-size`; the risks of using the default 'fork' start method in multiprocessing, especially with multi-threaded code and OpenMP, and a workaround involving setting the start method to 'spawn'; and troubleshooting build and runtime errors, such as missing dependencies like `python2-yaml` and potential tensor construction errors, with suggested solutions including environment adjustments and model conversion from Caffe to PyTorch."
2017-07-31,pytorch/pytorch,"The discussions highlight persistent issues with multithreading and memory management in PyTorch, including segmentation faults and memory leaks during concurrent training, suggesting underlying challenges with thread safety and resource handling. Several comments address type mismatches and shape assumptions, such as input tensor types and size constraints in modules like EmbeddingBag, indicating a need for clearer error messages and more robust input validation. There is mention of potential improvements in variable handling and weight normalization to optimize space complexity and computational efficiency, though concerns remain about the impact on runtime and memory usage. Unresolved questions include the stability of these issues across different versions and environments, as well as best practices for correctly managing data types and dependencies. Overall, these threads underscore ongoing efforts to improve PyTorch’s robustness, correctness, and usability in multi-threaded and resource-constrained contexts."
2017-08-01,pytorch/pytorch,"The discussions highlight ongoing stability and compatibility concerns in PyTorch, including persistent memory leaks during RNN training when not detaching states, issues with auto-generated functions requiring explicit arguments like bias, and incompatibilities or errors introduced by recent autograd updates, such as the non-reentrant BatchNorm backward or missing shared memory segments. There are also questions about extending convolution functionalities, such as adding local region control and neighborhood options, which may require significant kernel rewrites. Compatibility issues with NaN handling across CPU and GPU, as well as differences in tensor sorting behaviors with NaNs, are also noted. Finally, there is interest in developing flexible training management tools and visualization extensions, as well as benchmarking new operations, indicating a focus on improving usability, performance, and debugging in the evolving framework."
2017-08-02,pytorch/pytorch,"The discussions highlight several technical concerns, including ongoing issues with DataLoader's shared memory handling on Windows, and the need for flexible dataset splitting beyond built-in methods, such as using custom samplers. There is interest in optimizing and supporting depthwise separable convolutions, with considerations for implementing or porting efficient cuDNN kernels. Compatibility and bug fixes are also discussed, such as fixing embedding support for 2D tensors, handling size overflow in cuDNN, and ensuring consistent type support across different loss functions. Additionally, there are proposals to enhance training automation and visualization through more comprehensive training loops, callback systems, and logging plugins (e.g., Visdom, TensorBoard), aiming to improve usability and extensibility of the framework."
2017-08-03,pytorch/pytorch,"The comments primarily address technical challenges involving PyTorch's performance and compatibility. Issues include optimizing sequence reversal operations on CPU and GPU using gather, with a focus on minimizing reversal overhead and understanding the impact of reversed index tensors on performance; handling memory overflow and segmentation faults in cuDNN RNNs when processing very long sequences, indicating hardware limits and memory management concerns; and dependencies related to building PyTorch from source, such as ensuring proper MKL activation, cleaning build directories, and Python/ OpenSSL certificate issues affecting model URL loading. Additionally, there are discussions on containerization via Docker, version-specific compatibility (e.g., CUDA versions), and workflow optimizations like gradient accumulation over multiple mini-batches. Unresolved questions revolve around the precise impact of gradient accumulation on backward(), GPU memory handling of long sequences, and reproducing certain issues across different GitHub commits."
2017-08-04,pytorch/pytorch,"The discussions highlight various technical challenges in PyTorch, including memory management and optimization for sequence reversals, where more efficient GPU-based implementations are sought. Several issues pertain to compatibility and build errors across different CUDA versions, hardware configurations, and operating systems, with troubleshooting often involving environment setup or code modifications. There are concerns about the efficiency and correctness of GPU memory allocations, especially with the cuDNN backend and long sequences, leading to segmentation faults and instability issues. Additionally, users seek enhancements such as adding features like peephole LSTMs, improved OpenCL support, and more robust, integrated deformable convolution modules. Unresolved questions remain around optimal memory handling, batch normalization behavior, and reproducing specific errors across different versions and systems."
2017-08-05,pytorch/pytorch,"The discussions highlight issues with DataParallel, where users face problems that may involve underlying behavior expectations. There are technical challenges with compiling THCUNN, particularly with different CUDA versions, and solutions such as cleaning builds are suggested. Users are exploring ways to incorporate deformable convolutions and workaround solutions for model loading, including directly modifying URLs or porting code into PyTorch. Concerns about BatchNorm behavior persist, with suggestions to set momentum to 0 to stabilize running statistics even in eval mode, and attention to proper model evaluation protocols. Additionally, some issues stem from environment or version mismatches, addressed by recommended updates or fixes from recent repository commits."
2017-08-06,pytorch/pytorch,"The discussions primarily revolve around technical challenges in PyTorch development, including the implementation and support of tensor operations such as `index_select`, handling of scatter methods, and issues with variable and tensor memory management, especially in relation to backward compatibility and recent updates. There are also user-reported bugs, such as initialization errors, device assertions, and issues with specific functions like `scatter_`, as well as build and environment configuration problems, notably with OpenSSL certificates and CUDA device states. Several comments address runtime errors and stability concerns, like CUDA errors and memory usage inefficiencies, with suggested workarounds and troubleshooting steps. Additionally, there are questions about proper API usage, shape restrictions for certain layers, and version-specific behaviors, reflecting ongoing development and the need for clearer documentation and stability improvements."
2017-08-07,pytorch/pytorch,"The discussions highlight challenges with PyTorch's support and compatibility across different Python versions, operating systems, and hardware configurations, including build issues on Python 2.7 and CUDA-related problems on various GPU setups. Users frequently encounter installation difficulties, memory management concerns (notably with RNNs and long sequences), and bugs related to specific functions such as max_unpool2d, convolution, and advanced indexing. Several suggestions involve improving build scripts for better environment detection, fixing bugs related to tensor operations and broadcasting, and enhancing documentation for custom function implementation and usage patterns. Open questions remain about optimizing memory usage during training, ensuring correct GPU device handling in multi-GPU environments, and maintaining compatibility with evolving compilers and toolchains."
2017-08-08,pytorch/pytorch,"The discussions primarily revolve around the development and support of PyTorch features, including porting TensorFlow's layer normalization to PyTorch, implementing Tree-LSTM and TreeRNN models, and handling performance optimizations like efficient LayerNorm. Several issues concern compatibility and installation challenges, such as dependency mismatches, broken pipe errors during data loading, and platform-specific bugs, especially on Windows and in specific environments like Jupyter notebooks. There are also questions about improving the robustness and correctness of API behaviors, including advanced indexing, sum operations on ByteTensor, and the backward passes in custom modules. Additionally, users seek guidance on reproducing bugs, ensuring support for CUDA/cuDNN, and automating Docker builds for testing, highlighting ongoing efforts to stabilize and improve PyTorch's usability and performance."
2017-08-09,pytorch/pytorch,"The discussions reveal ongoing issues with certain PyTorch features, including the persistence of `std::bad_cast` errors in the master branch, and incomplete support for advanced tensor indexing, particularly with mixed or boolean index tensors, leading to confusing error messages. There are concerns about build consistency, especially relating to Docker images, platform-specific bugs, and ensuring reproducible training results across versions, as seen in entropy-SGD experiments. Additionally, some performance benchmarking and correctness verification are discussed, along with the need for better automation and testing, such as random seed management in CI. Overall, key areas include improving tensor indexing semantics, fixing environment-related bugs, and enhancing the robustness of build and test pipelines."
2017-08-10,pytorch/pytorch,"The discussions highlight ongoing challenges with cross-platform support, particularly for Windows and macOS, and the need for incremental review of large code changes, such as splitting extensive commits for easier review. There are technical concerns regarding the correct handling of tensor operations, broadcasting optimization, and memory management, especially in relation to autograd and GPU memory issues, with suggestions to improve user diagnostics (e.g., clarifying `requires_grad` behavior). Compatibility issues are evident with CUDA and NCCL configurations, requiring clearer documentation and dependency management, including handling specific hardware and driver versions. Additionally, questions arise about maintaining consistency in API behaviors, documentation aesthetics, and support for features like `matmul`, as well as addressing platform-specific bugs and build failures. Overall, the conversations reflect a mixture of bug fixes, performance enhancements, and usability improvements, with unresolved questions about error handling, API design, and build environment robustness."
2017-08-11,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch ecosystem, including difficulties in merging patches across versions (e.g., Windows support patches), and the complexity of advanced tensor indexing behaviors that deviate from intuitive logic. Users report installation issues, especially with wheel compatibility for different platforms and CUDA versions, and complications arising from mixed environments or incomplete uninstalls. There are ongoing challenges with CUDA-related runtime errors, such as device-side asserts and shared memory deadlocks, often addressed by workarounds like restarting processes or adjusting environment variables. Additionally, questions about proper weight initialization, integration of external libraries like NCCL, and API design for modifying core components like RNNs suggest areas where clearer documentation, better tooling, and more flexible APIs could improve user experience."
2017-08-12,pytorch/pytorch,"The discussions highlight concerns about type consistency and error handling when constructing tensors from numpy arrays, suggesting that failures should be consistent between CPU and CUDA tensors, potentially via explicit type checks or conversions. There is a debate over conditionally enabling NCCL based on platform specifics, with a preference for minimal, macro-guarded HIP support rather than direct code inclusion. Additionally, issues related to build configurations are raised, advocating for compile-time guards and minimal code changes to ensure compatibility across CUDA and HIP environments without imposing hard dependencies. Some comments address code clarity and user experience, such as the preservation of module item ordering and naming conventions in print representations, as well as potential pitfalls in autograd support for sparse variables. Overall, the discussions reflect a focus on improving robustness, compatibility, and user-centric design in PyTorch's development."
2017-08-13,pytorch/pytorch,"The discussions highlight several technical concerns: improving the modularity and clarity of torch/torchvision installation processes, especially with pip versus pip3 on Ubuntu; refining the design and implementation of advanced indexing in PyTorch to ensure consistency, logical coherence, and alignment with NumPy behaviors; addressing runtime errors such as `std::bad_cast` on GPU, potentially caused by version incompatibilities or bugs; and managing codebase changes like replacing symlinks with actual files on Windows systems to facilitate seamless compilation and integration. There is also an emphasis on breaking down large PRs into smaller, reviewable chunks for better code review and maintaining consistent code behavior across platforms. Unresolved questions include the detailed handling of multi-point vs. slice indexing semantics and resolving build issues caused by filesystem differences."
2017-08-14,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch's support and implementation of certain functionalities such as tensor inversion on CUDA, the stability and correctness of the Local Response Normalization (LRN) layer, and discrepancies in results between CPU and CUDA implementations. There are concerns about software version consistency, with users experiencing bugs that are fixed in source but not available via package managers like conda, especially on Windows and in Docker environments. Compatibility and interface inconsistencies, such as the argument handling in functions like `sort()` and differences between Variable and Tensor behaviors, are also notable. Additionally, users seek guidance on best practices for model serialization and API updates, with some contemplating the addition of new features versus bug fixes in future releases."
2017-08-15,pytorch/pytorch,"The discussions primarily address installation challenges, especially with pip versus conda, including platform-specific issues and package support for different Python versions and CUDA configurations. There are concerns about optimizing GPU operations, such as implementing efficient depthwise convolutions and managing memory leaks linked to driver versions and hardware, with suggestions for source-level improvements and external kernel implementations. Several comments highlight the need for clearer documentation and better handling of tensor operations, indexing semantics, and data transfer practices to prevent subtle bugs and performance degradation. Additionally, there are ongoing questions about multi-GPU distributed training, correct API behavior, and dependency management, often seeking simplified or more explicit approaches. Overall, unresolved issues involve compatibility, performance optimization, and usability improvements across diverse environments."
2017-08-16,pytorch/pytorch,"The discussions highlight several technical concerns with PyTorch, including dependency management issues like conflicting CUDA libraries (e.g., libgomp.so), and challenges supporting specific Linux distributions such as Alpine due to library incompatibilities. There are questions about extending autograd support to sparse variables and higher-order derivatives, as well as performance and API design considerations around device management and data transfer between CPU and GPU, suggesting potential improvements like `to_current_device()`. Compatibility issues are noted with Python versions (notably Python 3.4 support), and there are suggestions for improving test coverage, error messaging, and model support, particularly in distributed training setups and JIT compilation workflows. Additionally, package installation and environment reproducibility pose recurring challenges, with recommendations to use Docker and handle dependency submodules carefully."
2017-08-17,pytorch/pytorch,"The discussions highlight several technical concerns including the efficiency of sequence indexing methods, with suggestions to utilize existing sequence length information within `PackedSequence` objects to improve performance. Multiple issues relate to GPU and CUDA management, such as the unintended memory usage from repeated weight recomputation, the effect of device memory initialization, and the need to switch CUDA device contexts appropriately during operations like RNN parameter flattening. There are problems with environment and package compatibility, notably crashes when importing PyTorch in Python 2.7 or in certain Python 3 setups, potentially due to conflicting installations or dependencies. Additionally, heavy memory usage and deadlocks in multi-processing data loading point to resource management challenges, including the need to address file descriptor limits and the proper termination of worker processes. Overall, unresolved questions involve optimizing internal implementations for stability, compatibility, and performance, and improving user workflows for code updates and environment configurations."
2017-08-18,pytorch/pytorch,"Key discussions highlight issues related to GPU memory management, with suggestions to optimize memory usage in `im2col` implementations and the importance of understanding data type conversions and their build implications across platforms. There are ongoing concerns regarding the correct handling of device-specific settings in RNN modules to prevent unintended CUDA device initialization and discrepancies between single- and multi-GPU training behaviors. Several comments emphasize the need for comprehensive testing, including unit tests for functions like `pack_padded_sequence` and `last_step_tensor`, and considerations for proper import practices to avoid conflicts with local modules. Additionally, there's discussion on packaging and distribution, particularly ensuring MKL libraries include the correct dependencies, especially for different OS environments, and considerations over the support and documentation of tensor operations like `any` and `all` on Torch ByteTensors. Overall, the conversations underscore the importance of clarity in API design, build compatibility, and thorough testing to enhance robustness and usability."
2017-08-19,pytorch/pytorch,"The discussions primarily focus on the need for proper unit tests, particularly for functions like `pack_padded_sequence`, with recommendations to develop dedicated tests rather than using scripts alone. There are concerns about platform-specific issues, such as ensuring types like `int64_t` and `uint64_t` correctly maintain behavior across architectures and operating systems, especially regarding differences between `long long` and `long`. Troubleshooting steps, like installing `libtcmalloc-minimal4` on certain Linux distributions, are discussed to resolve runtime errors related to memory management. Additionally, there's a clarification that some bugs are in documentation rather than code functionality, emphasizing the importance of accurate docstrings. Overall, several technical and procedural questions remain about testing practices, type definitions, and cross-platform compatibility."
2017-08-20,pytorch/pytorch,"The discussions primarily revolve around dependency and build issues with different libraries (e.g., im2col, col2im, and their autograd support), which can lead to build failures in various environments like Linux, Windows, and CI systems. There is also concern over compatibility and stability, particularly with CUDA versions, driver updates, and package management via conda versus pip, affecting runtime behavior and kernel crashes. Some comments suggest replacing deprecated or unsupported functions (e.g., sum with `keepdim` parameter), or proposing new APIs (e.g., unified `im2col`/`col2im` layers) for memory efficiency and usability. Several issues highlight the need for better reproducibility and troubleshooting, such as reproducing bugs in Docker or verifying environment details. Overall, the conversations point to ongoing maintenance challenges, API improvements, and stability concerns in the PyTorch ecosystem."
2017-08-21,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) API inconsistencies and usability issues, such as the mutability of `Sequential` modules and the desire for an `add_module`-like method, which are debated with various proposals and backward compatibility considerations; (2) challenges with platform-specific build errors, including CUDA version conflicts, compiler issues, and linking problems related to THC/THCUNN, suggesting a need for improved build system robustness; (3) difficulties with multiprocessing in PyTorch, especially runtime errors like `received 0 items of ancdata`, resource limits (e.g., file descriptor exhaustion), and deadlocks, indicating potential areas for better multiprocessing management and user options; (4) support for advanced operations like `im2col`/`col2im` with autograd, including design choices for unified vs. separate layers, and issues with variable support; (5) uncertainty about documentations, particularly around normalization layers and API consistency, along with specific bugs such as in-place indexing behaviors and compiler errors. Overall, the discussions suggest ongoing efforts to improve API usability, platform compatibility, and internal stability, with some unresolved issues awaiting further refinement or user input."
2017-08-22,pytorch/pytorch,"The discussions highlight ongoing difficulties with CUDA and PyTorch's interaction on Windows and Linux, including linking against THC libraries, runtime segmentation faults during multithreading, and variability in random number generation leading to crashes. Multiple comments suggest potential workarounds such as preloading `libtcmalloc_minimal.so.4`, installing specific dependencies, or modifying environment variables, though these solutions have side effects or are considered hacky. Serialization issues with lambda functions and methods, particularly affecting Python 2.7, are also noted, especially concerning object pickling failures during model checkpointing. There are concerns about the stability and accuracy of variance and standard deviation computations across CPU and GPU tensors, with suggestions for optimized combined mean/std implementations using custom kernels. Overall, unresolved questions involve improving multi-threaded GPU stability, clarifying differences between CUDA and CPU behavior, and ensuring serialization compatibility."
2017-08-23,pytorch/pytorch,"The discussions primarily revolve around improving and troubleshooting the PyTorch library, including issues with distributed training backends like MPI support, and bugs related to the `EmbeddingBag` module supporting 2D tensors which were addressed in PR#2429. There are concerns about memory leaks during gradient computations with `autograd.grad` and `backward`, prompting suggestions for better memory management techniques or changes to internal types like `long` vs `int64_t`. Users report crashes linked to import order dependencies, especially in conjunction with libraries like OpenCV and scikit-learn, leading to workarounds such as modifying import sequences or installing memory allocators like `libtcmalloc`. Several issues involve internal API behaviors, such as handling `Variable` objects and `detach()`, with proposals for fixes or PR submissions. Lastly, there are questions about compatibility and support for specific hardware architectures and backends, as well as suggestions for integrating MKL or transitioning to newer backends like NCCL, with some issues planned for resolution in upcoming binary releases."
2017-08-24,pytorch/pytorch,"The discussions highlight concerns around extending tensor operations, such as defining `abs` and `neg` in ByteTensors, and implementing missing functions for comprehensive tensor support. There is an emphasis on improving the `ReduceLROnPlateau` scheduler for better model checkpointing, suggesting subclassing or direct state management, while noting issues with `optimizer.state_dict()` not including parameters. Memory leaks during autograd operations, particularly when using `torch.autograd.grad()` with `backward()`, pose significant challenges, with potential fixes involving proper detachment of tensors or more careful graph management. Additionally, users report performance and compatibility issues, including long execution times for `.cuda()` calls, leading to recommendations for building from source or awaiting binary updates. Overall, unresolved questions revolve around correct model state management, tensor operation completeness, and efficient, leak-free autograd workflows."
2017-08-25,pytorch/pytorch,"The discussions primarily address technical challenges in PyTorch development, including the disconnection of forks after making the repository public, and early-stage features like process group modes and MPI backend support requiring compilation from source. Several issues highlight memory leaks or memory management problems when using autograd with GPU tensors, especially during gradient computations and multi-GPU configurations. There are concerns about CUDA initialization and reproducibility in multiprocessing contexts, particularly related to seed setting and device management. Additionally, questions arise about clear documentation standards, particularly for the API shape representations, and build system improvements, such as transitioning to CMake for better OS support. Unresolved questions include fixing memory leaks, handling device-specific CUDA handles correctly, and streamlining installation from source or binary distributions."
2017-08-26,pytorch/pytorch,"The discussions highlight several key technical issues: (1) the complexity of implementing and optimizing sparse gradient updates in PyTorch, particularly for optimizers like Adam that traditionally require dense updates—proposed solutions include deferred or lazy updates and the introduction of a ""flush"" API, with consideration of how these strategies align with autograd mechanisms. (2) The need for enhancements in memory management during backward passes, especially when using high-order gradients, as memory leaks are observed primarily in models with batch normalization layers, suggesting potential issues with detachment in autograd graphs. (3) The optimization of large-vocabulary sampling methods in NCE loss, such as implementing more efficient sampling algorithms, as well as clarification on weight tying in models, including whether shared weights imply identical input/output spaces. Lastly, miscellaneous issues include bug fixes, API adjustments, and ensuring proper parameter updates in custom loss functions, with discussions emphasizing careful management of gradient flows, memory efficiency, and API design for sparse parameter updates."
2017-08-27,pytorch/pytorch,"The discussions highlight several technical issues, including memory leaks and multi-threaded memory management challenges when using DataLoader with multiple workers, emphasizing the importance of proper model and optimizer device placement. There are ongoing efforts to improve build processes, such as streamlining cross-platform compilation with CMake and addressing platform-specific binary bugs, with some fixes dependent on installing updated binaries or external libraries like libtcmalloc. Performance benchmarking indicates observed speedups on certain GPU architectures, prompting questions about underlying hardware utilization and optimization strategies. Users report recurring crashes in Jupyter notebooks and conflicts arising from binary incompatibilities, prompting the community and maintainers to focus on providing reliable, tested pre-built binaries and addressing platform-specific issues. Overall, unresolved concerns include refining multi-GPU and multi-threaded memory handling, ensuring binary stability across environments, and optimizing GPU kernel performance."
2017-08-28,pytorch/pytorch,"The discussions encompass several technical concerns including a bug fix for a reported issue, potential incompatibilities with CUDA and cuDNN causing hangs during `ConvND` calls, and a solution to avoid distributed training errors by disabling Gloo with `NO_DISTRIBUTED=1`. There are questions about dataset source recognition, specifically whether `TensorDataset` originates from `tnt/torchnet` or `torch.utils.data`. A suggested code modification addresses an off-by-one logic error in `ReduceLROnPlateau`'s patience comparison, with an example demonstrating its effect. Additionally, guidance is provided for using double-precision inputs by calling the `.double()` method on models to match weight precision. Unresolved questions remain regarding stability issues with certain CUDA configurations and handling input data types."
2017-08-29,pytorch/pytorch,"The discussions highlight ongoing challenges with optimizing depthwise convolution performance in PyTorch, including the need for more flexible implementations beyond current slow workarounds. There are concerns about efficiency in sequence processing workflows, particularly regarding packing/unpacking overhead and autograd overhead during backpropagation, with suggestions to optimize tensor operations order. Issues also address improve error handling when mixing tensors and Variables, and small bugs like warnings in cuDNN routines due to tensor type handling. Additionally, there's a focus on correctness in underlying functions, such as the random number generator’s use of `curand_uniform()` in CUDA, and ensuring environment dependencies, like numpy versions, are up-to-date for compatibility. Overall, many issues are in progress, with fixes being developed, but some performance regressions and implementation limitations remain unresolved."
2017-08-30,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting across multiple aspects of PyTorch, including build system improvements (e.g., migrating to CMake for OS agnosticism), and ensuring compatibility with various hardware and software configurations (e.g., Windows support, CUDA, MKL, and NCCL). Several issues concern memory management, including potential leaks with multi-threaded data loading and GPU memory handling, alongside challenges with distributed training and data parallelism. There are questions about the implementation and support of specific functionalities such as depthwise and separable convolutions, autograd, custom modules like `nn.View`, and compatibility of different tensor and variable behaviors across CPU and GPU. Many discussions involve resolving specific errors (e.g., broken pipes, CUDA synchronization issues, ABI compatibility) through code fixes, code refactoring, or environment configuration adjustments. The need for more comprehensive testing, improved documentation, and better support for diverse workflows and hardware remains a consistent theme."
2017-08-31,pytorch/pytorch,"The comments highlight several technical concerns within the PyTorch ecosystem, including potential memory leaks and crashes during multi-threaded or multi-GPU operations, particularly related to torch.autograd and NCCL backends. There are issues with data handling and indexing, especially involving Variables and tensors, where support for advanced indexing with Variables is inconsistent or problematic. Discussions also address the need for better multi-GPU batch normalization support, with suggestions for optional fixed statistics or replacement with alternative normalization techniques. Lastly, there are ongoing efforts to improve error messaging, support for sparse parameters, and ensuring compatibility across various system configurations, with some fixes already integrated into recent master commits."
2017-09-01,pytorch/pytorch,"The discussions highlight several technical concerns: potential memory leaks associated with multi-threaded DataLoader operations, particularly with `num_workers` causing data to spill to disk; challenges with multi-GPU training and proper device synchronization in `DataParallel`, especially regarding NCCL communication and GPU device handling; discrepancies in batch normalization performance across GPUs and the need for optional multi-GPU batch norm implementations; issues with sparse tensor operations and their impact on optimizer behavior, notably with weight decay, and the necessity for consistent handling of sparse and dense tensors; and concerns about the integrity of wheel package hashes after modification, indicating possible post-creation alterations affecting package verification. Proposed solutions include updating and recompiling packages, implementing more robust multi-GPU and batch norm synchronization strategies, and fixing package hash records; unresolved questions include the exact cause of memory leaks, the best way to support multi-GPU batch norm optionally, and ensuring correct tensor and device management across distributed training setups."
2017-09-02,pytorch/pytorch,"The discussions highlight significant breaking changes introduced in PyTorch 0.2.0, particularly regarding the implementation of broadcasting and the handling of dimensions during reduction operations, which may affect existing codebases. Users are advised to review the release notes and test their code for compatibility issues. Additionally, there is clarification around advanced indexing behavior: it always returns tensors with independent storage, except during assignment operations which are atomic. These clarifications aim to address user concerns about indexing semantics and ensure proper usage. Unresolved questions primarily involve confirming the impact of these changes on complex existing code and understanding the precise behavior during combined indexing and assignment operations."
2017-09-03,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for clarity on configuring and verifying cuDNN usage in PyTorch, and troubleshooting build issues on macOS related to thread-local storage support and CUDA linking errors. Users seek better documentation for function signatures, especially for in-place operations like `bernoulli_` and their expected behaviors. There are performance and stability concerns, such as machine crashes when using DataParallel, unexpected results from tensor operations, and difficulties in reproducing or diagnosing random errors and system reboots. Some users also request guidance on setup configurations, including compiler compatibility and environment variables critical for successful builds and CUDA integration. Unresolved questions remain around proper setup for multi-GPU environments and clarifying function semantics."
2017-09-04,pytorch/pytorch,"The discussions highlight several technical concerns: (1) limitations of hooks in accessing original inputs/outputs during forward passes and possible workarounds; (2) bugs and stability issues related to Windows conda packages, backward debugging, and memory management, with ongoing efforts to improve build times and stability; (3) the absence or instability of the Local Response Normalization (LRN) layer within PyTorch, including custom implementation snippets; (4) issues with label values (such as -1) in loss criteria and ensuring labels satisfy expected ranges; and (5) GPU utilization and device management challenges, including environment variable configurations for multi-GPU setups, and hardware stability concerns like machine crashes under DataParallel, with solutions involving environment variables and proper installation procedures."
2017-09-05,pytorch/pytorch,"The discussions highlight several key technical concerns, including the proper implementation and usage of gradient clipping APIs, with verification of whether gradient clipping is correctly performed in user code. There are recurring questions about the support and development of specific tensor operations like `im2col`/`col2im` with autograd, and potential additions of utility functions such as `square`, alongside implementation details for backend compatibility and performance, especially in relation to cuDNN and NCCL integration. Issues also address correctness in distributed training, such as ensuring proper device identification and rank assignment in `nccl` communications, and controlling device visibility in multi-GPU setups. Several threads mention build and packaging problems, including wheel hash consistency and binary compatibility, emphasizing the importance of robust CI testing and version management. Unresolved questions also include support for double backward differentiation and handling in-place operations, with suggestions for API improvements and internal fixes to enhance stability and usability across various functionalities."
2017-09-06,pytorch/pytorch,"The discussions highlight several technical concerns: first, the need for proper handling and clarification of loss functions like Dice loss, ensuring numerical stability and correct sign conventions; second, addressing concurrency and thread safety issues in PyTorch's multithreaded execution, which can cause segmentation faults, potentially mitigated by seed setting or code restructuring; third, complexities involving RNN weight normalization, weight flattening, and their impact on model correctness and memory efficiency, with suggestions for workarounds and potential deeper solutions; fourth, challenges with multi-GPU NCCL/NCCL_EXTERNAL detection and configuration, emphasizing correct build and integration practices; and fifth, issues in the build process and packaging of binaries, such as hash recalculations for wheel files, along with questions about best practices for applying weight decay to biases and handling in-place tensor operations, which require careful management to preserve correctness and compatibility."
2017-09-07,pytorch/pytorch,"The discussions center on persistent technical challenges within PyTorch, including potential deadlocks related to mutex management in multithreading, and performance regressions primarily from autograd overhead and CPU frequency management. Several issues highlight instability in package installation, such as network-related download failures and hash mismatches in wheel files, with solutions involving reinstallation or record updates. Critical debugging questions address numerical stability, exemplified by exploding gradients in norms and the behavior of gradients at zero, with proposed fixes like adding epsilon before norm calculations. There are also concerns about compatibility and implementation details, such as the impact of weight normalization on RNNs, the effect of broadcasting changes breaking tests, and discrepancies in API naming conventions. Overall, ongoing efforts aim at performance optimization, bug fixes, improved robustness, and clarifying API consistency."
2017-09-08,pytorch/pytorch,"The discussions highlight several technical concerns including runtime errors such as `std::bad_cast` in BatchNorm during source code changes, indexing and tensor shape mismatch errors particularly with `gather` and advanced indexing, and issues related to deadlocks and possible memory leaks when using multiple `num_workers` or large models. There are questions about PyTorch's internal behavior with cloning variables, handling in-place modifications, and the impact of `requires_grad` settings on training accuracy. Additionally, community members inquire about improvements to build processes, official support for Windows, and building/compatibility challenges, alongside suggestions for feature enhancements like integrated padding functions and better memory management tools. Many unresolved issues involve debugging error messages, optimizing multi-threaded data loading, and understanding framework internals for stable, scalable models."
2017-09-09,pytorch/pytorch,"The discussions highlight several technical issues in PyTorch, including a lack of support for tensor inversion on CUDA and inefficiencies in flip operations across dimensions. Power supply stability and hardware-related crashes are also noted, especially when using DataParallel due to GPU power spikes. Code quality concerns are raised regarding deprecated or non-optimized inference paths and proper handling of `requires_grad` for autograd functions, with suggestions to set model to eval mode to ensure consistency. Additionally, there are questions about correct padding behavior in `torch.nn.functional.pad` and input formatting for RNNs, alongside troubleshooting guidance for import errors related to MPI and environment setup. Overall, users seek improvements in operation support, hardware stability, code optimization, and clearer guidelines for model configuration and environment troubleshooting."
2017-09-10,pytorch/pytorch,"The discussions highlight persistent issues with multi-GPU training, particularly with the `DataParallel` implementation, which hangs or malfunctions on certain architectures like K80 GPUs, despite code adjustments and environment changes. There are concerns about numerical stability during training, such as NaN gradients in GAN models, with potential solutions like adding epsilon terms before norm calculations. Import errors and environment-specific crashes are also reported, notably in Jupyter notebooks and conda environments, suggesting possible compatibility or build issues. Additionally, there's a debate on API design choices, specifically whether to add a redundant `square` function or rely on power operations, with a tendency to avoid API bloat unless widely demanded. Some bugs and regressions related to scalar handling and symbol resolution are acknowledged and addressed through patches and PRs."
2017-09-11,pytorch/pytorch,"The discussions highlight ongoing efforts to integrate custom C/CUDA loss criteria into PyTorch’s loss framework, seeking to enable weighted, instance-wise, or raw loss tensor outputs for better handling of imbalanced and multi-label data. There is interest in enhancing capabilities such as multi-GPU batch norm, optimizing performance for small batches, and refining backward computation handling, including support for nested autograd calls. Several topics address build and compatibility issues on various platforms, notably macOS, ARM devices, and compiling CUDA code, with solutions like patching or adjusting build scripts. Problems with ambiguous constructor calls and scalar class definitions in ATen are acknowledged, with plans for fixes. Overall, community feedback drives improvements in flexibility, performance, and portability of PyTorch with unresolved questions on build stability and feature support."
2017-09-12,pytorch/pytorch,"The discussions highlight issues with tensor shape expectations in data parallelism, particularly the need for batch dimensions to be first and consistency in input sizes across inputs. Several users report runtime errors linked to mismatched batch dimensions or tensor sizes, raising questions about correct tensor formatting and implementation bugs. There are also concerns about PyTorch's compatibility with various hardware and software environments, including CUDA versions, MPI support on macOS, and the absence of zero-dimensional tensors, which complicates certain operations. Additionally, performance-related topics are discussed, such as the support for nested autograd functions, in-place operation compatibility, and the need for custom CUDA kernels for specific layers. Unresolved issues include improving backend flexibility, handling bugs in container packaging, and clarifying API behaviors to prevent misuse or confusion."
2017-09-13,pytorch/pytorch,"The discussions predominantly address implementation bugs, feature support, and performance issues in PyTorch, including fixes related to autograd cycles, ONNX export support for RNNs, and CUDA/cuDNN compatibility, with some discussions on memory leaks and threading deadlocks. Several users inquire about experimental or workaround methods, such as custom backward functions, and compatibility concerns, notably around CPU/GPU memory leaks, driver versions, and hardware constraints like processor instruction support. Efforts to improve user experience include modifying APIs for padding and enhancing support for volumetric data, alongside requests for clearer documentation. Unresolved questions include support for dynamic RNN export in ONNX, handling of multi-output functions in autograd cycling prevention, and support for older hardware or specific configurations. Overall, ongoing bug fixes, feature expansions, and compatibility adjustments remain central to the community's discussions."
2017-09-14,pytorch/pytorch,"The discussions highlight several technical issues and inquiries, including verification of cuDNN usage in PyTorch, implementation of features like Alpha Dropout and `np.where`, and support for non-contiguous tensors in functions like `grid_sample` with potential performance implications. Users report challenges with saving optimizer states, understanding Python import errors, and memory management during tensor expansion, with suggestions such as ensuring proper environment setup or adhering to API requirements. There are ongoing efforts to add features like orthogonal initialization, support for in-place random sampling, and improvements to neural network modules, with some unresolved questions about functionality and performance comparisons between cuDNN and THCUNN, especially for specific input sizes. Overall, the community seeks clarification, bug fixes, and enhancements to ensure better usability and performance."
2017-09-15,pytorch/pytorch,"The discussions reveal several key technical concerns: the status and implementation of the Local Response Normalization (LRN) layer, with questions about its availability and testing; the configuration and datatypes of Embedding layers, including size choices and datatype specifications; the handling of scalars versus 0-dimensional tensors in autograd, with a proposal to unify support for 0D variables for consistency and simplicity; performance considerations and compatibility issues related to GPU operations (e.g., cudnn flags, padding behavior), and the need for clear documentation on tensor operations like padding; and difficulties with exporting models to ONNX, highlighting the necessity for better error diagnostics, support for additional activation functions like Softmax, and the general approach of programmatic, rather than graph-based, model manipulation."
2017-09-16,pytorch/pytorch,"The discussions primarily revolve around improving and understanding core functionalities within PyTorch, such as the addition of an official Local Response Normalization (LRN) layer, with community-supplied implementations being used in the meantime. There are ongoing inquiries about the necessity and implementation details of BatchNorm's internal states, specifically regarding the `training=False` parameter and the optional `momentum`. Contributors seek guidance on maintaining consistency in optimizer state dictionaries, proposing the use of `defaultdict` to avoid discrepancies. Performance-related claims about PyTorch's speed, especially in GPU vs CPU contexts, are being validated through benchmarks, with community members expressing the desire for further optimization and clearer documentation. Additionally, questions about compilation options, such as disabling magma support and switching BLAS libraries (e.g., MKL to OpenBLAS), indicate interest in customizing build configurations for better compatibility and performance."
2017-09-17,pytorch/pytorch,"The discussions cover several technical issues including potential memory errors and leaks when using multiple workers in DataLoader, especially on Windows, with some users experiencing persistent crashes or runtime errors potentially due to system resource limits (e.g., file descriptors). There are concerns regarding the addition of weighted loss criteria for imbalanced and multi-label datasets, with suggestions for implementing tensor-based loss outputs similar to TensorFlow, and challenges in integrating these features without extensive rewriting. Users also face difficulties installing PyTorch via pip on certain platforms, prompting suggestions to improve distribution methods and clarify installation instructions. Additionally, there are questions about the necessity of specific distance metrics, such as cosine distance vs. similarity, and handling class definitions during model serialization to avoid pickling issues. Overall, the conversations identify ongoing challenges in system compatibility, feature extension, and usability that require further refinement and robust solutions."
2017-09-18,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's API and functionality, such as providing a raw, unreduced loss tensor similar to TensorFlow's cross-entropy functions, and enabling masked log softmax computations to improve numerical stability during training. There are concerns about tensor padding in multiple dimensions, with proposals for more flexible, well-documented methods and examples for multidimensional padding. Several issues address implementation details, such as the appropriate use of integer types (int vs. int64_t) for CUDA operations, ensuring backward compatibility, and performance considerations in kernel modifications. Additionally, suggestions include updating random number generation to follow standard [0, 1) conventions, handling model parameter states correctly when using `state_dict()`, and addressing non-determinism in CuDNN for reproducible results. Unresolved questions include the best API design for scalar and tensor creation, consistency of type usage across CPU and GPU, and ensuring performance gains without regression."
2017-09-19,pytorch/pytorch,"The discussions primarily focus on improving and troubleshooting various PyTorch features, such as fixing import errors related to numpy, implementing custom data collation functions for complex data types, and enhancing model summaries with output shape information. Several comments address compatibility and build issues, including CUDA and NCCL dependencies, as well as memory usage and performance benchmarks for specific operations. There are suggestions to refine error messaging, support non-contiguous tensors in operations like grid_sample, and develop multi-GPU batch normalization techniques. Additionally, questions regarding proper environment setup, model parameter reloading, and handling tensor strides are raised, indicating ongoing efforts to optimize usage and debugging processes."
2017-09-20,pytorch/pytorch,"The discussions highlight several technical challenges, including CUDA device errors caused by architecture-specific compilation issues, particularly with older GPUs like GTX 950M and Quadro 2000, which require explicit CUDA architecture flags. There are ongoing concerns with multi-GPU training and parallelization, specifically with data loading deadlocks, DataParallel's limitations on certain architectures, and the robustness of multiprocessing, especially regarding CUDA driver initialization and thread deadlocks. Additionally, there are bug reports and suggestions for improving error handling, tensor indexing (especially with variables and non-contiguous tensors), and the consistency of internal parameter state representations across save/load cycles. Some issues involve performance optimizations, such as kernel unrolling, and API enhancements like standardized parameter exporting and support for advanced padding or top-k operations, with various solutions ranging from code restructuring to API refinements and kernel customizations. Unresolved questions remain around improving support for non-contiguous tensors with grid sampling, extending multi-node distributed training, and fixing longstanding CUDA and threading bugs for more stable and performance-optimized PyTorch backends."
2017-09-21,pytorch/pytorch,"The discussions primarily revolve around enhancements to loss functions, notably adding a `reduction` or `sum=False` parameter to enable per-sample loss outputs, replacing the traditional `size_average` approach to better align with frameworks like TensorFlow. Several comments focus on implementing masked softmax or cross-entropy computations efficiently and stably, emphasizing the need for accurate, numerically stable methods that support masking. There is concern over CUDA driver initialization in multi-process or multi-GPU contexts, with suggestions to defer CUDA calls until necessary to prevent crashes. Additionally, questions are raised about integrating new functions (like max, min, ones_like) into the core C codebase and extending API consistency to numpy-like behavior, particularly supporting zero-sized tensors. Unresolved issues include ensuring backward compatibility, efficient implementation of custom kernels, and maintaining correctness across different hardware and input conditions."
2017-09-22,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, robustness, and compatibility. Notably, there is a proposal to introduce a 'reduction' parameter for loss functions to replace the ambiguous 'sum' argument, aiming for clearer semantics when controlling loss aggregation; this includes considerations about gradients and backend C implementation changes. Concerns about GPU compatibility are recurrent, especially regarding multi-GPU seed initialization, CUDA driver initialization optimizations, and ensuring functions like `torch.cuda.is_available()` do not undesirably initialize the driver. There is also emphasis on improving multi-GPU batch normalization and distributed communication support, with suggestions to ensure runtime checks for features like GPUDirect support for better error handling. Lastly, maintaining code correctness, such as proper reference counting, and ensuring cross-platform compatibility (e.g., PPC64 systems) are frequent themes, alongside minor documentation updates and test fixes."
2017-09-23,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's flexibility and usability, such as implementing an unreduced (per-sample) reduction option for loss functions, and enabling gradients to flow through targets in certain losses. There are significant challenges around masking softmax computations for variable-length sequences, with proposed custom implementations and considerations for different loss reductions. Compatibility and installation issues are addressed, including efforts to support PPC64 systems and multi-GPU setups. Additionally, enhancements like support for zero-dimensional tensors and updates to CUDA kernels (e.g., changing stride types) are discussed to ensure correctness and performance. Overall, these threads underscore active development to enhance PyTorch’s functionality, portability, and operational robustness."
2017-09-24,pytorch/pytorch,"The discussions mainly address troubleshooting and development concerns within PyTorch, including guidance for newcomers interested in contributing to the codebase, and suggestions for using `ctypes.memset` as a workaround. Several issues revolve around external library interactions, such as OpenCV threading problems on macOS and Linux, and compatibility or symbol leakage problems related to specific PyTorch versions and CUDA. Users seek clarification on handling gradients in optimizers like Adam, indicating confusion about whether gradient manipulations via hooks influence parameter updates. Additionally, there are responses to specific bugs, such as installation fixes and build issues, with some suggesting potential fixes or troubleshooting steps. Key unresolved questions include how to properly manipulate gradients for non-SGD optimizers and clarifications on symbol leakage and compatibility issues."
2017-09-25,pytorch/pytorch,"The discussions highlight numerous technical issues and feature requests, including the delayed availability of the LRN layer, improvements to convolution-related functions like col2im, and the need for more flexible parameter specifications in modules like Embedding, especially regarding data types. There's a recurring interest in aligning PyTorch’s API with NumPy, such as support for scalar vs. 0-d array semantics and dtype specifications. Several comments address the complexities of multiprocessing with CUDA, emphasizing the importance of correct start methods and proper sharing of CUDA tensors across subprocesses. Additionally, there are suggestions to enhance the usability of hooks, learning rate schedulers, and internal module attributes, along with classifications of ongoing bug fixes and code patches. Overall, these discussions reflect active efforts to improve PyTorch’s API coherence, performance, and debugging clarity."
2017-09-26,pytorch/pytorch,"The discussions primarily address technical challenges related to PyTorch's API and functionality, such as fixing deprecated API elements, enhancing error messages (e.g., for group size mismatches in convolutions), and clarifying the behavior of functions like `get_lr` for learning rate retrieval. Several issues involve import and environment stability problems, notably the static TLS error in Jupyter notebooks, with potential solutions including thread management and buffer registration. There are concerns about correctness and bugs, such as attribute handling with `__getattr__`, and potential bug fixes in code cleanup, assertions compilation handling, and optimizer behaviors like Adam’s invariance to scaling. Some comments suggest verifying package support on Windows and improving user-facing error messages for better clarity. Unresolved questions include improving the robustness of certain functions, better error reporting, and future plans for API simplifications or enhancements."
2017-09-27,pytorch/pytorch,"The discussions highlight several key technical concerns, including enabling deterministic algorithms in CuDNN via `torch.backends.cudnn.deterministic = True`, and addressing memory leaks by ensuring proper unmapping and closing of shared memory in multi-process data loading. There are questions about handling optional and non-optional parameters in cwrap, and ensuring backward compatibility with attributes like `num_inputs` and `needs_input_grad` in documentation, with suggestions to potentially include attributes such as `dirty_tensors` and `shared_pairs`. Several issues revolve around CUDA random number generation, particularly ensuring that `torch.rand()` and `uniform_()` produce values within the expected ranges, considering floating-point rounding and hardware implementation details. Additionally, build and compatibility concerns are discussed, such as fixing detection of NCCL during installation, handling different NCCL library paths, and dealing with linker symbol conflicts that cause segmentation faults or compilation errors. Overall, unresolved questions include how to best handle optional parameters in cwrap, fixing build detection and compatibility issues with NCCL, and ensuring correctness and efficiency in GPU random number generation."
2017-09-28,pytorch/pytorch,"The discussions highlight various technical issues and enhancements within PyTorch, including fixing a bug in `torch.multinomial` when handling zero or small weights, and improving multicore and multi-GPU batch normalization for small batch sizes. There is a focus on optimizing performance through C++ implementation of autograd operations, pre-allocating workspace buffers in NNPACK, and refining NCCL detection and configuration for distributed GPU training. Concerns are also raised about the overhead and implementation specifics of file operations like renaming checkpoints, and providing better extensibility for data handling via attributes and accessors. Several unresolved questions revolve around ensuring compatibility with different hardware and software environments, and verifying fixes through community contributions and testing."
2017-09-29,pytorch/pytorch,"The discussions highlight technical concerns related to proper resource management, such as ensuring `UnmapViewOfFile` and `CloseHandle` are called in shared memory leak fixes, and the correct propagation of `deterministic` flags in cuDNN configurations. There are questions about the correct integration points for cleanup routines in the memory allocator and ensuring consistent behavior across different system environments. Several comments address performance and compatibility issues, such as the impact of thread settings, the efficiency of NNPack versus MKLDNN, and the handling of negative probabilities in sampling functions. Unclear or incomplete features are also discussed, like the design and usage of profiling tools, as well as API convenience utilities for CPU/GPU switching, with some suggestions for default parameter choices and error handling improvements. Overall, unresolved questions primarily focus on correct implementation practices, compatibility, and usability enhancements."
2017-09-30,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA and cuDNN support, including compatibility issues, non-contiguous input errors, deterministic algorithm selection, and potential slowdowns with certain configurations like NNPack and NNPACK. There are concerns about internal implementation details, such as the need for better profiling, improvement of error messages, and handling of sparse autograd support. Many questions pertain to API design choices, such as the default behavior of dropout scaling, adding runtime flags for features like NNPack, and potential memory leaks in earlier versions. Community feedback also calls for clearer documentation and better error handling to assist users in debugging complex issues."
2017-10-01,pytorch/pytorch,"The discussions highlight ongoing efforts to improve JIT graph accuracy and stability, particularly addressing discrepancies in graph representations and ensuring reliable backward operations, with several test failures indicating issues with expected graph output formatting or op representations. There is concern about optimizing the PyTorch profiler's performance, especially regarding name demangling speed, and a need for clear documentation on its usage and output. Additionally, suggestions include adding a Python API for CUDA availability checks to support robust testing, and refining the tensor creation API (e.g., `zeros_like`) for consistency and convenience. Discussions about NNPACK emphasize the importance of developing heuristics for its runtime selection rather than user-specified configurations, aiming for better performance prediction across varying input sizes and hardware. Finally, maintaining compatibility and fixing bugs in ONNX export, such as attribute errors and version mismatches, remain relevant unresolved issues."
2017-10-02,pytorch/pytorch,"The discussions highlight concerns about platform-specific test failures, particularly in Windows due to differences in `type_info` name outputs, with recent fixes addressing this by updating the test assertions. There are ongoing performance issues with depthwise convolutions and the desire for more flexible, efficient separable convolution implementations beyond the original design, with community interest expressed regarding their development. Users encounter compatibility and installation challenges, such as resolving shared library symbol errors and ensuring correct linking of `libTH` and `libTHC` files, often addressed by reinstalling or cleaning dependencies. Additional questions focus on ensuring deterministic behavior for cuDNN algorithms, especially for dilated convolutions, and modifying internal APIs like `Variable` and tensor conversions for better usability. Finally, there are OpenMP-related build issues on specific architectures and discussions about proper testing practices for new features, including backward compatibility and correctness."
2017-10-03,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch development, including memory management and performance optimization for GPU/CPU operations, such as the need for better handling of packed sequences to improve autograd efficiency, and the necessity of aligning torch.nn modules with C++ implementations for speed. Concerns about the strictness of tensor index operations, especially for THC versus TH, suggest a need for balanced flexibility without sacrificing correctness. There is also debate on improving model introspection tools like `model.summary()` with hooks, and clarifying API behaviors such as `load_state_dict()` and learning rate scheduler positioning, to enhance usability and debugging. Additionally, unresolved technical issues involve managing DLPack memory safety, ensuring consistent dependency tracking in build systems, and accommodating platform-specific quirks such as signedness on Power architectures."
2017-10-04,pytorch/pytorch,"The discussions highlight challenges with installing and importing PyTorch, especially regarding environment setups, module loading, and compatibility issues such as missing `_C` modules or device-specific problems (e.g., half-precision CUDA errors). Several comments address error handling and messaging improvements, including clearer exceptions for tensor operations, index out-of-bounds, and size mismatches. There are ongoing efforts to extend PyTorch functionalities, such as customizing RNN modules, implementing new loss behaviors, adding support for 3D upsampling, and handling tensor types like half-precision or different backends, with some workarounds and planned improvements discussed. Compatibility and performance considerations are emphasized, especially regarding integration with external libraries (like NCCL, DLPack) and maintaining backward compatibility while evolving APIs (e.g., default dimensions, implicit behavior). Overall, unresolved questions revolve around better error messaging, cross-platform build issues, and enhancing extensibility and robustness of core tensor operations."
2017-10-05,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's compatibility and performance, including Out-Of-Memory issues on small networks, which are addressed by providing precompiled binaries and environment management strategies. Several issues concern memory management, such as ensuring tensor contiguity for BLAS operations and accurately measuring GPU memory usage, with some fixes and workarounds implemented. Multi-GPU training, especially with K80 architectures, raises concerns about deadlocks and suboptimal scalability, prompting considerations of threading models and shared resources. Compatibility problems also arise with older models and conversions from Lua Torch, emphasizing the need for better serialization and migration tools. Overall, the discussions focus on improving robustness, usability, and performance through bug fixes, clearer APIs, and environment configurations, with several unresolved questions about memory handling, multi-GPU stability, and feature support."
2017-10-06,pytorch/pytorch,"The discussions highlight several key issues, including compatibility problems with thread-local storage support on macOS/Clang, which affects building PyTorch from source, especially when enabling distributed computing features; the need for batched tensor operations like `triu` and `tril` to improve performance in batch scenarios such as Gaussian Processes; and ongoing challenges with GPU kernel launch overheads limiting performance for small tensor operations. There are concerns about proper error messaging and compatibility across platforms (e.g., Windows, macOS), as well as user experience improvements like more intuitive tensor type conversions and handling of tensor strides. Additionally, several discussions focus on code maintenance, such as cleaning up THNN, removing redundancies, and refactoring to C/CUDA for efficiency and clarity. Overall, unresolved questions revolve around implementation strategies for batching, compatibility fixes, and balance between performance and simplicity in the codebase."
2017-10-07,pytorch/pytorch,"The discussions highlight various technical issues with PyTorch, including the incorrect return value of `torch.backends.cudnn.version()` due to indentation errors, especially on Windows, and ongoing challenges in building and installing compatible binaries across different platforms and CUDA versions, with some users experiencing platform-specific incompatibilities (e.g., CentOS, Ubuntu, Windows). There are suggestions to improve the codebase organization, such as relocating DLPack-related functions outside the main `torch` namespace into dedicated submodules for clarity and modularity. Additionally, there are improvements being made to the JIT tracing capabilities, notably the introduction of a `torch.jit.scope` context manager to better annotate graph scopes, alongside discussions about integrating external frameworks like C2 via `from_dlpack`. An unresolved concern pertains to resolving version conflicts and ensuring smooth installation procedures across diverse environments, along with deciding the best way to handle tensor flow bridging through DLPack."
2017-10-08,pytorch/pytorch,"The discussions highlight a range of technical concerns including compatibility and bug fixes, such as resolving the `rnn.cuda()` error caused by a version mismatch in cudnn initialization, and addressing issues with `state_dict` being empty in the Adam optimizer, which might be related to internal PyTorch bugs or improper save/load procedures. There are also questions about the differentiability of custom functions and naming conventions for parameters like `keepdims` versus `keepdim`, with suggestions to improve numerical stability and maintain consistency with NumPy. Additionally, community members seek guidance on environment setup, such as building PyTorch with OpenBLAS on VS2015, and troubleshooting hardware-related instabilities during long training sessions, including system reboots possibly due to hardware incompatibility. Overall, the discussions emphasize fixing bugs, ensuring compatibility across software and hardware, and maintaining API consistency."
2017-10-09,pytorch/pytorch,"The discussions cover several core topics including resolving specific attribute errors ('module' object has no attribute 'get_context'), improving batched operations such as `triu` for tensor efficiency, and addressing issues related to CUDA and GPU performance, such as cache management and benchmarking discrepancies. There are ongoing efforts to refine memory handling, ensure compatibility across different hardware and software configurations (e.g., CUDA versions, driver issues), and improve debugging tools like model summaries and output shape inspection. Several questions remain about fixing existing bugs, such as those in `str(output)` and model output shape detection, along with discussions on preventing deadlocks and ensuring stable long-term training. Lastly, some issues highlight installation and environment conflicts, especially around dependencies like `magma`, CUDA versions, and driver compatibility, with troubleshooting steps and plans for future releases being discussed."
2017-10-10,pytorch/pytorch,"The discussions mainly revolve around ensuring correct and efficient GPU operations in PyTorch, with particular emphasis on stream synchronization and memory copying. A notable concern is fixing asynchronous data transfers by properly associating `cudaMemcpyAsync` and `cudaStreamSynchronize` with the current CUDA stream, to prevent deadlocks and inconsistent behavior. Additionally, there are issues related to determinism in cuDNN, compatibility and build problems on various platforms (Windows, PPC64le, macOS), and hardware-related crashes potentially linked to driver or hardware faults. Some suggestions include modifying the internal memory copy routines to maintain proper synchronization, and addressing edge cases in tensor string representation to avoid numerical errors. Overall, unresolved questions pertain to balancing asynchronous performance with ease of use, and ensuring platform-agnostic stability and reproducibility."
2017-10-11,pytorch/pytorch,"The discussions highlight several technical challenges in the PyTorch ecosystem, including environment conflicts when building with OpenBLAS across different platforms (Issue #494), where path ordering and mixed use of Cygwin and MSYS2 cause DLL conflicts. Incompatibilities with CUDA and GCC versions are also noted (Issue #3050), suggesting a need for clearer compile-time support and documentation. Several issues involve debugging segmentation faults and runtime errors, often related to autograd or tensor operations (Issues #3067, #3081), emphasizing the importance of proper synchronization and error handling. Improvements on model introspection tools like `nn.Module` summaries are discussed, with considerations for supporting arbitrary graph structures (Issue #3044). Overall, unresolved questions persist around stabilizing environment configuration, enhancing error diagnostics, and extending framework capabilities for complex models and integrations, requiring ongoing development and more detailed guidance."
2017-10-12,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch memory management and debugging, including fixing memory leaks often related to driver issues, and efficient tensor and model reinitialization. There are concerns regarding proper way to handle custom index operations, scalar tensors support, and compatibility with ONNX, especially for exporting scalars and ensuring correct graph representations. Some issues relate to improving the usability of model and optimizer device placement, and addressing compatibility and error reporting across different CUDA versions and drivers. There are mentions of the need for more comprehensive testing, especially for convolutional and backward pass functionalities, to ensure robustness across configurations. Finally, resolving segmentation faults, improving documentation, and handling multi-GPU and multi-device models are critical topics requiring further investigation."
2017-10-13,pytorch/pytorch,"The discussions highlight several technical challenges including ensuring proper MKL activation during PyTorch installation, troubleshooting CUDA compatibility issues particularly with driver support and library mismatches, and addressing deadlocks or performance degradation in multi-GPU setups. There are reports of build failures due to incompatible compiler or library versions, as well as issues with specific CUDA operations like ConvND hanging or not returning, possibly caused by mismatched CUDA or cuDNN configurations. Some comments suggest modifying or validating kernel implementations, improving documentation clarity on padding, and enhancing test coverage for certain functionalities like broadcasting. Overall, unresolved questions involve debugging installation-related conflicts, verifying environment compatibility, and ensuring robustness across various hardware and software configurations."
2017-10-14,pytorch/pytorch,"The discussions primarily revolve around troubleshooting and improving PyTorch functionalities, including issues with large tensor operations and environment setup in Visual Studio, such as refreshing Python environment databases. There are concerns over binary size limits on PyPI, affecting distributions, and compatibility issues when building from source, often related to CUDA, cuDNN, and driver mismatches. Troubleshooting segmentation faults and autograd deadlocks indicates underlying concurrency and threading complexities, with suggestions to simplify code and isolate problems via minimal reproductions. Additionally, proposals are made to enhance API usability, like supporting scalar exports through ATen tensors and refining documentation for padding behaviors, while ongoing efforts aim to address these technical challenges and improve usability."
2017-10-15,pytorch/pytorch,"The discussions highlight several technical concerns, such as reproducibility issues with multi-GPU setups and CUDA environments (Issue #2332), which were addressed by reinstalling Anaconda and PyTorch. There are ongoing improvements and feature additions, like converting `EmbeddingBag` to new function style and potential PR submissions (Issue #2979), as well as ensuring batch normalization aligns with original research for accurate training and evaluation behavior (Issue #3122). Deadlock and deadlock-related runtime errors specific to NCCL versions have been fixed through code refactoring and device management (Issue #3028). Additionally, there is a technical challenge with supporting 5D tensors in softmax operations, which will be addressed once related issues are resolved (Issue #3124). Other discussions concern code maintenance, such as handling symbolic links for files or directories in the codebase (Issue #3126)."
2017-10-16,pytorch/pytorch,"The discussions highlight ongoing challenges with debugging segmentation faults and runtime errors, often related to CUDA and library compatibility issues, such as mixed CUDA versions or incomplete source builds, with suggestions to perform clean builds and verify library versions. There are questions about enhancing the user experience, including automating setup processes, providing helper scripts, and improving error messaging, especially for complex operations like neural network serialization and multi-GPU configurations. Several discussions also focus on extending PyTorch functionality, such as adding the `.item()` method for scalar extraction, supporting higher-dimensional tensors in functions like softmax, and exposing internal functions to Python while managing backward compatibility. Debugging tips, like using `perf top` and gdb, are suggested for low-level issues like deadlocks and GPU kernel problems. Overall, these conversations emphasize the need for improved robustness, user-friendly tooling, and clearer guidance on dealing with advanced or subtle internal errors."
2017-10-17,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) Ambiguities and inconsistencies in tensor operations, such as the default behavior of `torch.max` and issues with `gather` and advanced indexing, pointing to potential API or documentation updates; (2) Challenges with multi-GPU training, including GPU utilization inefficiencies, bottlenecks on main GPUs, and communication issues in distributed setups, especially with large-scale data like ImageNet; (3) Bugs and segmentation faults arising from improper input dimensions, deprecated features, or low-level GPU kernel failures, often requiring detailed debugging or code adjustments; (4) The need for enhanced support for half-precision, custom gradient encoding/decoding schemes, and improved backend integration for operations like cudnn, with some features pending implementation or requiring careful API design; (5) Installation and environment setup difficulties, such as CUDA library detection, build configuration, and ensuring compatibility across diverse hardware, which suggest ongoing improvements in build system robustness and user guidance."
2017-10-18,pytorch/pytorch,"The discussions reveal ongoing challenges with multi-GPU training efficiency, specifically regarding GPU utilization, load balancing, and memory management when scaling up workflows like ImageNet training on configurations such as K80s and 1080Ti GPUs. There are technical concerns about NCCL backend bugs, GPU communication hang-ups, and the limitations of multi-GPU topologies, prompting suggestions to optimize NCCL and device interconnects. Additionally, there is a focus on improving PyTorch's usability, including handling large tensors, refining model export in training versus inference modes, and streamlining state_dict loading with features like key ignoring and strictness options. Proposals also include introducing flexible support for sparse matrix formats (CSR/CSC) and expanding functional normalization support, alongside considerations for API stability and breaking changes, such as removing stochastic functions and designing distribution modules. Unresolved questions center on backend library support, compatibility issues, and performance bottlenecks that require further investigation and experimental validation."
2017-10-19,pytorch/pytorch,"The discussions highlight debates over API design choices, particularly the mutable API of `nn.Sequential`—with objections favoring explicit list-based constructions for backward compatibility—and how to handle module modifications, such as adding, removing, or changing layers in pretrained models. There are concerns about memory management and performance issues, especially related to GPU-to-GPU communication, sparse tensor operations, and CUDA driver compatibility, alongside challenges with multi-GPU setups and support for specific matrix formats (CSR/CSC). Several questions address debugging difficulties, such as resolving segfaults, driver or environment-specific bugs, and build errors related to compiler compatibility and code deprecations. Broadly, there is ongoing effort to improve testing, compatibility, and feature support (e.g., distributions, ONNX interop) while balancing API stability, code maintainability, and performance optimization."
2017-10-20,pytorch/pytorch,"The discussions highlight various technical concerns, including the difficulty of modifying pretrained models such as changing layers or hyperparameters while retaining weights, and how to properly reuse pretrained weights in custom model reconstructions. There are also issues related to system compatibility and deployment, such as handling CUDA versions, driver configurations, and multi-GPU setups, including p2p communication and IOMMU-related problems. Some comments address the need for enhanced API consistency and usability, like unifying tensor dispatch methods, and adding new functionalities like better distribution support or activation functions, with considerations around stability, correctness, and community consensus. Unresolved questions involve balancing API design choices, ensuring correctness in sparse and non-contiguous tensor operations, and supporting diverse hardware and software environments effectively."
2017-10-21,pytorch/pytorch,"The discussions highlight several technical challenges, including compatibility issues with Windows support for cffi and related build errors, notably linking errors and static TLS limitations when importing certain libraries like sklearn and nltk in Ubuntu environments. There are ongoing efforts to improve error messaging for differentiable function modifications, alongside updates to the ATen library and related C++ code to handle DLManagedTensors and ensure proper naming conventions. Dependency management concerns are raised regarding the use of external libraries like requests versus standard urllib, emphasizing security and compatibility considerations. Additionally, there are questions about refactoring code paths, such as moving code from C++ to Python for simplicity, and maintaining proper versioning and patching workflows for ATen updates, with some discussions on how to best approach changes to the PyTorch and ONNX export processes."
2017-10-22,pytorch/pytorch,"The discussions highlight several core issues: the complexity of multiprocessing and DLL dependency problems on Windows, particularly with CUDA and external DLLs, and solutions involving environment variable adjustments and DLL management; the desire for more flexible and efficient depthwise and separable convolution implementations, which has been addressed via recent PyTorch updates; device compatibility and multi-GPU communication challenges, with suggestions for enabling peer-to-peer traffic and handling IOMMU/IOMMU-related errors, including kernel parameter modifications; ongoing difficulties with importing PyTorch in certain IDEs or environments due to static TLS limitations, and recommendations to order imports or adjust environments; and broader suggestions for device-agnostic programming, such as utilizing 'new' methods and better device-aware APIs, alongside discussions on Docker configurations and build optimizations for CUDA and cuDNN support."
2017-10-23,pytorch/pytorch,"The discussions primarily revolve around runtime issues with DataLoader’s multiprocessing and CUDA interactions, such as deadlocks caused by shared memory limitations, the placement of `model.cuda()`, and constraints on `num_workers`. There are suggestions for refining API semantics, like explicit device-aware dtypes, context managers for tensor defaults, and improved handling of custom collate functions for nested data structures. Several issues involve supporting specific functionalities, such as asymmetric padding in `nn.functional.pad`, implementing size-based tensor splitting efficiently, and adding new distributions in `torch.distributions`. Compatibility and portability concerns are highlighted, including fixing references to deprecated or missing functions like `SpatialDepthwiseConvolution`, handling sparse tensor concatenation, and ensuring class revisions are modular. Overall, many unresolved questions focus on optimizing performance, API clarity, and extending feature support while maintaining compatibility."
2017-10-24,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch’s functionality, compatibility, and performance, including fixes for shared memory issues in Docker, enhancements to distributed training across GPUs, and the addition of new distribution classes for generative modeling. Several concerns relate to ensuring numerical stability and consistency, such as good implementations of log-sum-exp functions, and maintaining API clarity with consistent naming conventions (e.g., `keepdim` vs. `keepdims`). There are technical challenges around device and memory management, like handling static TLS errors when importing multiple libraries, sparse tensor concatenation, and ensuring correct rpath configurations for CUDA and cuDNN support. Contributors also discuss API ergonomics, advocating for device-aware tensor creation, better support for reparameterized distributions, and improvements to ONNX export and autograd functionalities. Overall, unresolved questions focus on balancing usability, performance, and compatibility, especially in multi-GPU and cross-framework contexts."
2017-10-25,pytorch/pytorch,"The discussions highlight recurring issues with DLL loading and binary incompatibilities, especially on Windows, with suggested solutions including copying CUDA DLLs and cleaning environment conflicts. Other critical concerns involve deadlocks and crashes in multi-threaded autograd engine internals, which have been addressed in recent builds, but still require debugging via detailed stack traces. There are ongoing efforts to enhance PyTorch's distribution capabilities, such as adding reparameterized distributions and support for functions like `scatter_add`, along with the integration of new activation functions like Swish, which require careful API and codebase modifications. Additionally, there's attention to performance optimizations, memory management, and ensuring API consistency, particularly for sparse tensors, in-place operations, and device-specific behaviors, raising questions about best practices and future design directions. Unresolved questions include the proper handling of overlapping tensors, NaN propagation versus performance trade-offs, and potential integration of new features into core PyTorch components."
2017-10-26,pytorch/pytorch,"The discussions primarily center on improvements and troubleshooting within PyTorch, including the integration of Windows-specific changes and the merging of GPU memory release features, with some ongoing review and rebase efforts. Several issues address runtime errors and compatibility concerns, such as Gloo transport errors with ibverbs, discrepancies between CPU and CUDA implementations, and handling of unsupported data types in tensor constructors. There is emphasis on enhancing code structure, notably by moving certain ATen operations into dedicated commits or PRs, and improving testing practices, especially for complex features like indexing and fused operators. Additionally, proposals for clarifying documentation, fixing certain operator behaviors, and refining distributed training workflows highlight ongoing efforts to improve robustness and usability. Unresolved questions mainly involve proper error handling in backend libraries, synchronization of code changes across repositories, and implementation details for optimizing kernel fusion and tensor operations."
2017-10-27,pytorch/pytorch,"The discussions primarily revolve around enhancing loss functions with flexible reduction options (`sum` vs. `reduction`) and addressing inconsistencies or errors in their implementation, especially with `BCEWithLogitsLoss` and related losses. There is a recurring concern about backward compatibility and ensuring new functionalities like per-sample losses integrate smoothly, requiring modifications in autograd functions and gradient handling. Several threads highlight challenges in implementing hardware-level features, such as exposing new CUDA functionalities, or addressing build issues caused by dependencies like cuDNN and IBVerbs, emphasizing careful management of library linking and version compatibility. Additionally, there are discussions on API consistency, such as naming conventions (`reduce` vs. `sum`) and representation formats, alongside proposals for new features like probabilistic distributions, gradient quantization schemes, and indexing behaviors in autograd. Unresolved questions mainly involve precise implementation strategies, backward compatibility, and ensuring correctness across diverse hardware, software, and API changes."
2017-10-28,pytorch/pytorch,"The discussions focus on several technical concerns including the nuanced behavior of advanced indexing in PyTorch versus NumPy, with suggestions to standardize or clarify indexing semantics, especially for multi-point and non-adjacent indexing strategies. There is a recurring concern regarding the implementation and compatibility of torch's backend, such as the potential merging of the last Windows PR, the integration of THNN functions like unfold and col2im, and the handling of sparse versus dense tensors for improved performance or clarity. Additionally, questions are raised about the implementation details of functions like batch_index_select for variable-specific indices, the removal of StochasticFunction in favor of torch.distributions, and the correct handling of tensor type conversions to avoid unexpected results. Overall, these threads highlight ongoing development issues, potential API improvements, and integration challenges in PyTorch's core functionalities."
2017-10-29,pytorch/pytorch,"The discussions largely revolve around enhancing PyTorch's flexibility and robustness, including adopting a `reduction` parameter in loss functions to replace `size_average` for clearer additive/averaging behavior, and providing comprehensive documentation on the file architecture for contributors. There is ongoing work to accelerate performance by porting operators to C++ and optimizing backward passes, with updates showing significant speedups. Several issues highlight platform-specific challenges, such as integrating CuDNN on Windows and managing shared library dependencies, which require careful handling of library loading and environment configurations. Users express concerns about reproducibility, memory management, and ensuring proper GPU device setup, indicating the necessity for clearer debugging practices and more explicit guidance for cross-platform compatibility. Unresolved questions include the precise impact of recent internal changes on stability, performance, and usability, and how to best support complex model constructs like sequential containers while maintaining simplicity and efficiency."
2017-10-30,pytorch/pytorch,"The discussions primarily revolve around the organization and implementation of PyTorch's internal files, including the structure of C and Python modules, and the necessary updates to support new functionalities such as loss functions, ONNX export, and ATen operators. Several issues highlight challenges with multi-GPU and multi-processing environments, including segmentation faults, OS call failures, and dependencies on proper device context management, particularly on Windows and Linux systems. There are suggestions for improving the API design, such as enabling dynamic sampling, efficient batch selection, and handling sparse tensors, alongside considerations for backward compatibility and code consistency. Some concerns also address build and deployment issues, including AVX support detection, linking MPI, and dependency conflicts, especially on macOS and in debug modes. Overall, unresolved questions center on optimizing data loading, ensuring compatibility across hardware and OS variations, and enhancing extensibility for custom operations and advanced sampling strategies."
2017-10-31,pytorch/pytorch,"The discussions highlight several technical concerns related to PyTorch, including issues with CUDA OS calls and device-side assertions, which can be mitigated by parameterizing device placement and using environment variables like CUDA_LAUNCH_BLOCKING. There are questions about ensuring consistency between `.cuda()` and `.cpu()` behaviors, especially regarding tensor device placement and context creation, with suggestions to standardize behavior. Other points involve potential improvements in the API, such as adding assertions in loss functions to prevent user errors, or expanding functionalities like `F.pad` to support all tensor dimensions. Some discussions focus on internal implementation details, such as optimizing `inputs()` and `outputs()` for autograd graphs, and handling numerical differences between CPU and GPU random number generation. Unresolved issues include ensuring backward compatibility, managing dependencies like `requests[security]`, and fixing bugs related to tensor operations and reproducibility."
2017-11-01,pytorch/pytorch,"The discussions cover several technical concerns, including compatibility issues with multiprocessing on Windows and interactive modes, and the importance of safeguarding main code with `if __name__ == '__main__'` to prevent GPU-related errors. There are questions about increasing package size limits on PyPi, with suggestions such as using `virtualenv` or running ""pip install --no-deps torchvision."" Several issues address deeper implementation details, such as fixing the default behavior of `torch.max` to support specifying dimensions, improving the consistency of softmax dimension handling, and addressing NaN or unstable gradients in gradient penalty calculations. Other topics include CUDA kernel performance optimizations, issues with HDF5 thread safety, and ongoing code refactoring for cleaner implementation of convolutional operations—highlighting a focus on stability, correctness, and efficiency across the library. Unresolved questions remain about Python version compatibility, proper handling of autograd in custom functions, and maintaining consistency in API behaviors amidst rapid development."
2017-11-02,pytorch/pytorch,"The discussions primarily center on improving PyTorch's functionalities and performance, including adding features like a `pad_sequence` utility, handling variable-sized inputs with padding, and refactoring implementations such as im2col/col2im for convolution operations. Multiple concerns involve optimizing GPU operations, especially related to memory management, register usage, and interactions with CUDA libraries to prevent hangs or slowdowns, as well as addressing issues related to multiprocessing and dataset loading that may cause deadlocks or inefficiencies. There are also discussions on API consistency and usability, such as making `nonzero()` more numpy-compatible and clarifying loss function behaviors, alongside efforts to fix bugs like segmentation faults or tracking down elusive errors in distributed or tracing contexts. Unresolved questions include best practices for freeing GPU cache, handling in-place operations safely, and ensuring compatibility across different Python versions and environments."
2017-11-03,pytorch/pytorch,"The discussions highlight challenges with handling partial or mismatched `state_dict`s during model loading, suggesting methods like updating dictionaries or slicing to align keys. Tensor manipulations, especially viewing and softmax operations on high-dimensional tensors, raise concerns about dimension compatibility, with workarounds like tensor permutation and dimension-aware softmax implementations. Compatibility issues between `torch`, `ipython`, and debugging environments such as PyCharm are noted, often requiring importing `torch` in multiple files to prevent `dlopen` errors. HDF5 file access in multi-threaded contexts is flagged as unsafe without SWMR mode, with solutions involving process start method adjustments. Device management and CUDA support complicate migration of optimizer states across devices, and specific build and linking configurations are discussed to address compatibility and linking errors, especially on macOS and with different CUDA versions."
2017-11-04,pytorch/pytorch,"The discussions highlight various technical topics, including expanding `F.pad` support for all tensor dimensions and enhancing sequence padding functions to accept lists, numpy arrays, or tensors for flexibility. There are concerns about compatibility with GCC 4.9 due to the implementation of `stdatomic.h`, with suggestions to use GCC 4.8 macros to avoid breaking compatibility. Several issues address the correctness of type casting from floating-point to unsigned integers, emphasizing the need to handle undefined behavior and improve test reliability. Additionally, there are questions about GPU device management, specifically about the creation of contexts on unused GPUs and controlling CUDA device usage via environment variables. Performance optimization is also discussed, notably in reducing execution time with JIT compilation for RNN modules."
2017-11-05,pytorch/pytorch,"The discussions highlight technical challenges with PyTorch's internal function registration mechanisms, particularly regarding the `_all_functions` list and module imports that cause registration errors like ""Trying to register second function under name,"" suggesting a need for better initialization strategies. There are ongoing issues with compatibility and support for C11 atomic operations in GCC 4.9, leading to potential fallbacks or code modifications to ensure proper atomic macro detection. Concerns are raised about GPU memory allocation behaviors, specifically the creation of contexts on unused GPUs even when targeted device IDs are specified, indicating potential improvements in device context management. Furthermore, users encounter multiprocessing and DataLoader deadlocks when using spawn mode, with solutions requiring proper setup within `if __name__ == '__main__':`, and persistent errors when attempting to set start methods multiple times across scripts. Lastly, there is a note on the dependency of new neural network components (e.g., depthwise convolution, Gumbel distributions) on upcoming API changes, advocating for wait until the API is stabilized before merging, alongside considerations for accurately computing entropy via the distribution's probabilities."
2017-11-06,pytorch/pytorch,"The discussions primarily address implementation and compatibility challenges in PyTorch, such as updating loss functions (e.g., PoissonNLLLoss, KLDivLoss), handling sparse tensors in autograd, and ensuring correct behavior of advanced features like DataParallel and device contexts. Several comments highlight issues related to CUDA, cuDNN, and GPU driver versions, emphasizing the importance of proper environment setup and build configurations. There are concerns about performance bottlenecks, such as slow convolutions with large channel groups, and the need for efficient support of modern architectures like MobileNets. Additionally, the conversations include ongoing efforts to improve testing, error handling, and feature modularity, with some unresolved questions about backward compatibility and the integration of new CUDA features."
2017-11-07,pytorch/pytorch,"The discussions highlight several critical issues: the inconsistency and deprecation of attributes like `creator` in different PyTorch versions, and the need to update documentation accordingly; build and installation challenges across systems, especially regarding CUDA, cuDNN, and dependencies like magma, including network instability and version compatibility problems; ongoing work to enhance the autograd and tracing mechanisms, including scope management and code instrumentation, with some technical debates about implementation strategies; performance concerns related to memory management on GPUs, potential memory leaks, and efficient cache usage; and various API and feature updates such as improved tensor indexing, loss function behavior, and support for sparse tensors, which involve both bug fixes and design considerations."
2017-11-08,pytorch/pytorch,"The discussions highlight various technical challenges, including the lack of NCCL support on Windows and issues with multi-threaded NCCL deadlocks, with suggested solutions such as serializing reduction calls. Multiple comments address build and compatibility complexities, such as fixing compilation warnings, supporting newer CUDA/CUDNN versions, and resolving memory leaks or slowdowns during training, especially related to specific layers like InstanceNorm. Concerns are raised about proper handling and clear documentation of functions like `std()` and `mse_loss` parameters, as well as code refactoring for improved API clarity, such as naming conventions and sampler interface modifications. Additionally, questions remain about ensuring reproducible ordering in distributed operations and the need for clearer guidance on building or extending features like CUDNN bindings or saving optimizer states. Overall, these reflect ongoing efforts to enhance stability, usability, and performance across various system configurations and use cases."
2017-11-09,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting efforts within PyTorch, including implementation of mathematical functions like softmax and lgamma, and issues with building from source, especially on Windows and with CUDA versions. There are concerns about API design choices, such as refactoring samplers for inheritance clarity and providing user-friendly utilities like batched tensor concatenation. Several technical challenges are noted, including optimizing GPU operations (topk speed, matrix inversion), ensuring compatibility across platforms, and correctly capturing debugging information without performance penalties. Additionally, discussions emphasize verifying correctness and stability of numerical computations, fixing build issues related to dependencies and environment setup, and improving the maintainability and performance of core functionalities. Many unresolved questions revolve around efficient caching strategies, precise build configurations, and extending features like CTC or custom autograd functions."
2017-11-10,pytorch/pytorch,"The discussions encompass various technical challenges in PyTorch development, including adapting TensorFlow-style layer normalization to PyTorch using functions like `torch.nn.moments` and `torch.nn.batch_normalization`, and optimizing custom implementations like LayerNorm for performance. Concerns are raised about maintaining speed with numerous LayerNorm instances, with proposed more efficient implementations, including a specialized `LayerNorm` class, and questions about their numerical stability and compatibility with different tensor dimensionalities. Other issues involve integrating and supporting sparse tensor operations, such as sparse x dense and sparse x sparse matrix multiplications, which are largely unsupported or limited in current PyTorch, prompting questions about current capabilities and future support. Additionally, there are administrative and build system concerns, such as compatibility of CUDNN versions, dependencies management (e.g., MKL detection), and build reproducibility across environments and GCC versions. Finally, some discussions focus on code infrastructure improvements, like adding debug information, renaming variables for clarity, and ensuring update consistency across submodules and Docker images, with an emphasis on improving stability, performance, and developer/user experience."
2017-11-11,pytorch/pytorch,"The discussions highlight ongoing challenges with building and extending PyTorch, including CUDA compatibility issues, compilation errors due to missing files or unsupported compiler versions, and difficulties with integrating newer CUDA/cuDNN versions. Several comments address the need for better handling of tensor indexing, especially regarding empty slices returning `ValueError` versus `IndexError`, and improvements in DataLoader behavior. There are also concerns about reproducibility and deterministic algorithms in cuDNN, with suggestions to assert algorithm support and handle nondeterministic fallbacks. Additionally, discussions mention architectural improvements like PR contributions, memory leak fixes, and the importance of proper documentation, testing, and version support for broader stability and usability. Unresolved questions remain around best practices for serialization with in-memory buffers, deterministic algorithm selection, and correct indexing semantics."
2017-11-12,pytorch/pytorch,"The discussions reveal several core concerns: the complexity of building PyTorch on Windows due to environment configuration and CUDA/Visual Studio compatibility issues, with suggestions for environment variable setup and rebuild instructions; ambiguities around handling zero-sized tensors and whether indexing should raise `ValueError` or `IndexError`; the need for clearer, more flexible tensor creation methods such as introducing `tensor.new_x()` for device and dtype consistency; challenges in implementing device-agnostic code, debating fallbacks versus explicit handling, and potential API improvements like adding device/dtype parameters; and technical details around low-level operations such as ensuring correct precision in random number generation, proper handling of tensor contiguity in `view()`, and maintaining consistency with numpy behaviors—all highlighting ongoing efforts for robustness, usability, and correctness in PyTorch."
2017-11-13,pytorch/pytorch,"The discussions encompass several technical concerns: compatibility issues with CUDA versions, driver and dependency management, and the need for clear error reporting and handling of various data types and memory formats. There are suggestions for improving API design, such as attaching tracing states to specific objects, enhancing the distribution API, and refining tensor creation interfaces to support device and dtype specifications. Some discussions highlight the importance of deterministic operations, especially for functions like `index_add` and broadcasting, as well as addressing non-contiguous tensor views and their impact on operations like `bmm`. Lastly, there are ongoing efforts to optimize performance, handle edge cases (e.g., variable input sizes, memory leaks with cuDNN), and improve robustness in multi-threaded or distributed environments, with some questions remaining about maintaining compatibility and reducing nondeterminism."
2017-11-14,pytorch/pytorch,"The discussions highlight issues related to CUDA support and compatibility, including slow performance, slow initial CUDA tensor transfer, and differences in behavior between CPU and GPU tensors when invoking `.cuda()`. Multiple contributors report slowdowns, unexpected delays, or bugs when using recent PyTorch versions with specific hardware or software environments, especially with CUDA 8 and certain conda packages. There are concerns about the proper handling of buffer exports in ONNX, the consistency of `.cuda()` behavior across modules and variables, and potential bugs in layered normalization implementations. Additionally, questions about building from source versus pre-built binaries, the impact of software version mismatches, and supporting Windows builds point to ongoing compatibility and performance challenges. Overall, these threads reflect efforts to troubleshoot, optimize, and clarify best practices for CUDA integration, tensor operations, and exporting behaviors."
2017-11-15,pytorch/pytorch,"The discussions highlight several technical concerns including the need for differentiable implementations of operations like `trtrs` and `potrf`, which are currently non-priority due to derivative complexity. Building and compiling issues are prevalent, often related to missing include files (`ATen/NativeFunctions.h`, `TH/TH.h`) and environment configurations, with suggestions to improve build scripts and compatibility checks for older GCC versions. Multiple comments point out intermittent test failures, flaky assertions, and memory leak concerns in models, notably with cuDNN and RNNs, indicating potential bugs or issues in GPU libraries and memory management. There are also questions regarding proper handling of library versions, atomic operations, and the impact of specific code modifications (e.g., reassigning grad_fn), along with suggestions for improving code simplicity, such as patching data loaders or refining build options. Unresolved issues include build failures related to missing files, CPU-GPU communication timeouts in distributed training, and the potential need for better integration or wrapper functions for cuDNN components."
2017-11-16,pytorch/pytorch,"The discussions highlight several technical concerns, including the complexity and low priority of implementing differentiable operations like `trtrs` and `potrf`, with references to existing gradient formulas and implementations in TensorFlow and autograd. There are ongoing issues with build and compilation, notably CMake custom command setups, type-related errors in CUDA/C++ code, and handling of file dependencies during source generation, as well as system-specific memory and environment configuration problems affecting multiprocessing and GPU memory leaks. Several questions focus on enhancing API usability and robustness, such as device-agnostic tensor creation, consistent gradient backpropagation verification, and clarifications on bidirectional LSTM outputs. Some issues also involve external dependencies like OpenCV or pillow-SIMD, and platform-specific behaviors (e.g., macOS GPU support). Overall, unresolved questions include improving build reliability, expanding support for differentiable linear algebra functions, and ensuring clear API documentation."
2017-11-17,pytorch/pytorch,"The discussions highlight several technical concerns, including build issues on Windows related to missing header files like 'TH/TH.h' and the need for proper inclusion of ATen headers via CMake or scripts, which currently appears broken. There's an ongoing challenge with Windows build stability, especially regarding the generation of 'NativeFunctions.h' before compiling CUDA sources, and compatibility problems due to mismatched library symbols such as 'THLongStorage_inferSizeN'. Additionally, there are concerns about ensuring reproducibility and correctness in gradient tests (e.g., flaky gumbel-softmax gradient assertions), verifying CUDA and cuDNN usage properly, and managing backward compatibility while updating APIs and interfaces (like `__setitem__` or handling optional parameters). Some discussions also suggest improvements in build configuration flags, package dependencies (e.g., conda environments), and the importance of CI integration for stability across platforms. Overall, unresolved issues mainly involve build process automation, header file management, library compatibility, and test robustness."
2017-11-18,pytorch/pytorch,"The discussions highlight ongoing efforts to improve and debug PyTorch's core functionalities, including the implementation and stability of various loss functions (e.g., PoissonNLLLoss, KLDivLoss), and ensuring proper state dict loading for optimizers, especially concerning GPU migrations. Several comments address enhancing multiprocessing behavior, with suggestions to avoid external dependencies and better handle interactions with libraries like OpenCV, as well as fixing compatibility issues across Python and compiler versions (e.g., GCC 7). There are also considerations about API consistency, such as deprecating or renaming functions like `.multinomial` to better reflect their semantics, and improving representation methods for modules and parameters. Lastly, unresolved questions include ensuring binary retrievals are up-to-date, fixing build errors with different compilers, and handling warnings and version-specific behaviors in testing frameworks."
2017-11-19,pytorch/pytorch,"The discussions highlight various build and compatibility issues, including failures in building C++ extensions on Windows due to recently added hash functions in hash.h, which cause compiler errors—workarounds involve explicitly defining macros or modifying function signatures. There are concerns about discrepancies in environment setups, such as Python and IPython paths not pointing to the same conda environment, affecting module imports, which can be resolved by correctly activating the environment before launching IPython. Several issues relate to build failures on different systems, often due to missing or incompatible dependencies like MKL, OpenBLAS, or CUDA versions, with suggested fixes involving reinstallation or specific conda/pip commands. Incompatibilities with compiler versions (notably GCC 7 or Clang 5) cause ambiguous function calls or hash template conflicts, prompting fixes with template magic or standard-compliant code modifications. Overall, recurring themes include environment consistency, build system quirks, and compatibility fixes for compilers and dependencies."
2017-11-20,pytorch/pytorch,"The discussions highlight several key technical concerns, including build and compatibility issues across different compilers and environments, such as MSVC, GCC, and CUDA, which affect successful compilation of layers like im2col/col2im and extensions. There are questions about enhancing model introspection features like the `summary` function, with suggestions for expanding support to arbitrary graphs and incorporating output shape estimation without forward passes, possibly using symbolic math libraries like sympy. Several questions address the integration and correctness of `pack_padded_sequence` with `DataParallel`, including handling of the `lengths` argument across multiple GPUs. Build failures related to missing or incompatible CUDA components and shared libraries are recurring concerns, alongside discussions on testing strategies, including switching to pytest for improved reporting. Unresolved issues include ensuring compatibility and correctness across differing environments, and improving model visualization and analysis tools."
2017-11-21,pytorch/pytorch,"The discussions highlight ongoing challenges with implementing CUDA tensor operations, particularly tensor flipping with negative strides on GPUs, and the need for efficient, CUDA-compatible methods. There are concerns about licensing issues related to external libraries like Eigen and their impact on PyTorch, alongside efforts to clarify and enhance distributed training workflows, including proper backend configurations and error handling. Questions also arise around model export and visualization, such as ONNX support and scope management for layer-by-layer inspection. Furthermore, there are technical debates on tensor factory design, scalar types, and safety mechanisms in multi-threaded and distributed contexts, including proper handling of signals, memory management, and parameter naming. Overall, key unresolved areas involve improving GPU support for tensor operations, license compliance, robust distributed training, and tooling for model introspection and debugging."
2017-11-22,pytorch/pytorch,"The discussions highlight ongoing challenges in implementing and supporting differentiable operations like `trtrs` and `potrf`, with suggestions for community contributions and reference implementations across frameworks such as TensorFlow, Autograd, and Stan. Compatibility and build issues are prevalent, with users experiencing errors related to system dependencies, header versions, and system configurations, alongside concerns about stability and support for specific PyTorch versions or features (e.g., in-place operations, variable support, and CUDA compatibility). Notably, memory leaks and resource management problems—especially related to file descriptors, GPU memory during backpropagation, and MPI communication—are reported, with workarounds focusing on environment limits and code restructuring. Several issues stem from build system intricacies, header mismatches, and platform-specific (Windows, Linux, macOS) differences, often requiring manual fixes or branching from experimental PRs. An overarching consensus points to the need for improved error messaging, testing, documentation, and utility tools (e.g., performance profilers), as well as addressing low-level backend and header compatibility concerns to ensure stability and efficiency."
2017-11-23,pytorch/pytorch,"The discussions primarily address technical challenges related to PyTorch's behavior on Windows, particularly the necessity of wrapping main code with `if __name__ == '__main__'` to avoid multiprocessing errors. Several issues concern compatibility and setup, such as potential repository corruption, build flags on different Linux distributions, and CUDA/CUDNN version adjustments. There are ongoing enhancements proposed for learning rate scheduling, especially implementing cosine annealing with warm restarts, and improvements to autograd and tensor operations, including gamma functions and in-place updates. Additionally, questions persist about optimizer state dict structures, parameter naming, and test failures highlighting the need for further validation and debugging."
2017-11-24,pytorch/pytorch,"The discussions highlight ongoing challenges with advanced tensor indexing, particularly the non-adjacent multi-point selection logic in PyTorch, where conflicting conventions and usability issues are proposed for a more systematic design, such as always bringing multi-point dimensions to the front or using distinct data structures for slices versus points. Several threads address multi-GPU and distributed training complexities, including device-agnostic parameter management, efficient index selection across batches, and debugging tools for CUDA operations. There are also concerns regarding build and environment configurations, such as resolving library dependencies, CUDA version compatibility, and platform-specific serialization issues. Other technical topics include potential API extensions like adding dimensions support to `torch.dist`, integrating common nonlinear functions into the native operator set, and improving warning mechanisms and test coverage for new features. Unresolved questions involve how to best unify and simplify complex indexing semantics, ensure cross-GPU consistency, and improve developer tools for debugging and serialization."
2017-11-25,pytorch/pytorch,"The discussions highlight several technical points: the need for improved tensor printing options in PyTorch analogous to NumPy, with control over truncation points; the importance of proper build configurations and driver compatibility when compiling with CUDA, including environment variable settings like `TORCH_CUDA_ARCH_LIST`; handling and debugging specific errors such as the `RuntimeError` in autograd closures, CUDA device busy states, and DLL load failures with Pillow versions; consideration of extending PyTorch's special functions via wrapping Cephes functions or integrating scipy-like approaches; and suggestions for API enhancements like unifying split functions and clarifications around distributed model parallelism, alongside addressing build and runtime issues raised by users."
2017-11-26,pytorch/pytorch,"The discussions highlight concerns about performance variability on Windows due to compiler differences and potential memory management pitfalls, with suggestions to optimize compilation and usage patterns. There are ongoing efforts to enhance the functionality and robustness of sequence pooling utilities, such as implementing `pad_sequence` and `pack_sequence`, alongside considerations for maintaining correct ordering and indexing behaviors. Multiple threads address accuracy and correctness issues in core operations, including handling of in-place tensor modifications, bug fixes in convolution routines, and ensuring proper error handling and warnings for backend support (e.g., cuDNN availability). Contributions to feature additions, such as gamma distribution sampling and advanced indexing improvements, emphasize compatibility, autograd support, and API clarity, while also considering potential breaking changes and performance impacts. Unresolved questions include how to best handle serialization, symbolic conversion, and user-facing API design to balance flexibility, correctness, and maintainability."
2017-11-27,pytorch/pytorch,"The discussions highlight ongoing development and debugging efforts within the PyTorch core, including fixes and improvements to loss functions (e.g., `MultiLabelMarginLoss`, `BCELoss`) and performance optimization, especially in Windows CUDA contexts. There are concerns about proper handling of optional function parameters (such as in `torch.norm`), code refactoring considerations (like include orders and header management), and ensuring API consistency, like merging split functions for better usability. Several threads address build and dependency issues, including header includes, version management, and CUDA/cuDNN compatibility, with suggestions for better error handling and tool support. Additionally, model truncation and extending functionality (e.g., in `tiramisu`) and ensuring support for various tensor types like `HalfTensor` are also discussed, emphasizing the importance of robustness and flexibility in the library's architecture."
2017-11-28,pytorch/pytorch,"The discussions highlight ongoing efforts to optimize PyTorch’s performance, particularly regarding cuDNN LSTM benchmarks, multi-GPU training efficiency, and memory allocation bottlenecks. Several comments indicate issues with Windows compatibility, including challenges with multiprocessing, package building, and runtime errors, underscoring the need for better Windows support and troubleshooting. There are technical suggestions for improving PyTorch's internal functionality, such as refining reduction operations with shfl_xor, implementing log-sum-exp in C for numerical stability, and enhancing tensor view semantics to support zero-stride tensors and numpy-like reshaping. Integration and consistency issues are noted with PyTorch's ONNX export and operator attributes, along with concerns about interface clarity, especially around tensor contiguity and batch handling in RNNs. Overall, unresolved questions remain about balancing performance optimizations with correctness, platform compatibility, and API usability."
2017-11-29,pytorch/pytorch,"The discussions highlight several technical concerns, including the phased implementation and availability of functions like `torch.potrf`, which was added after release 0.2.0, and compatibility issues with certain versions and builds, particularly on Windows and in distributed settings. Several issues address memory management and performance bottlenecks, especially with RNN modules, batchnorm on GPUs, and multiprocessing interactions with CUDA, sometimes resulting in out-of-memory errors or deadlocks. There are questions about enforcing tensor contiguity, improving the `view` and `reshape` semantics, and extending model summaries with output shape predictions for arbitrary graphs. Additionally, discussions involve improving error messages, refactoring code to leverage ATen directly, and enhancing the robustness and clarity of new features like distributed NCCL communication."
2017-11-30,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for clearer documentation and API consistency, such as clarifying the behavior of bidirectional RNN outputs and improving error messages for data type mismatches. There are ongoing efforts to optimize implementations like LayerNorm, with suggestions for more efficient CUDA kernels and alternatives. Compatibility issues are addressed, such as conflicts arising from setting `num_workers` in DataLoader and ensuring proper handling of GPU tensors and variables. Several features like the cuDNN-based CTC loss and enhancements to distributed training, NCCL communication, and RNG state management are being developed or refined. Unresolved questions remain regarding the integration of certain operations into ATen, efficient implementations of normalization layers, and system-specific configurations affecting stability and performance."
2017-12-01,pytorch/pytorch,"The discussions reveal several technical challenges and suggestions, including issues with multiprocessing on Windows, particularly around deadlocks and broken pipes, and the need for clearer documentation on support for inplace operations. There are concerns about performance bottlenecks, notably slow LayerNorm implementations that could benefit from kernel optimization, and memory leaks in specific architectures like ResNet50. Compatibility problems with CUDA and cuDNN versions, especially during compilation and runtime errors involving std::bad_cast and linking issues, are also prominent. Additionally, there are ongoing proposals for API enhancements, such as implementing ""same"" padding in convolutions, and improvements in code maintainability, including documentation updates, code refactoring, and better testing practices."
2017-12-02,pytorch/pytorch,"The discussions highlight several technical concerns with PyTorch on Windows and CUDA compatibility, including issues with multiprocessing, particularly with setting `num_workers=0` leading to crashes and broken pipes, as well as errors related to CUDA runtime failures and driver/library detection. There are suggestions to modify code for better multiprocessing stability, such as setting the multiprocessing start method to `""forkserver""` and ensuring code runs within the `if __name__ == '__main__':` block. Compatibility problems are also discussed regarding compiler and library versions, especially with CUDA and GCC, along with build environment considerations like missing shared libraries or driver issues. Additionally, some feature gaps and API ambiguities are noted, such as the behavior of `topk` with the `sorted` parameter and plans for future API improvements like `padding=""same""`. Overall, unresolved issues remain around stable CUDA support on Windows, build environment robustness, and API consistency."
2017-12-03,pytorch/pytorch,"The discussions primarily revolve around CUDA-related runtime errors and hardware compatibility issues, such as ""OS call failed"" on Windows, ""invalid device ordinal"" on multi-GPU setups, and ""invalid device function"" errors, often stemming from mismatched or unsupported GPU architectures, driver issues, or CUDA toolkit mismatches. There's concern about module compilation failures, e.g., missing or skipped convolution kernels, which may be due to build configuration or environment inconsistencies. Some suggestions include disabling certain features like IPC, sharing CPU tensors instead of CUDA tensors, or building PyTorch from source to resolve platform-specific bugs. Additionally, there are discussions on improving API behaviors, like the sorting behavior of `topk`, and addressing miscellaneous issues related to system configuration, such as library conflicts, environment setup, or browser/network issues affecting user experience. Unresolved questions include how to properly handle device-specific code, ensure compatibility across various hardware, and fix build errors systematically."
2017-12-04,pytorch/pytorch,"The discussions highlight several key issues including CUDA driver detection errors during model loading, which can be addressed by ensuring compatible PyTorch and CUDA versions or retraining models with updated PyTorch versions. There are concerns about the performance impact of pinning memory in data loaders, with proposals to introduce a customizable `get_tensors_to_pin` function to optimize this process. Compatibility and API consistency are discussed around operations like `torch.norm`, handling optional parameters, and introducing features like `padding='same'` in convolutional layers, emphasizing the need for efficient implementations and clear behavior alignment with frameworks like TensorFlow. Additionally, multiple issues revolve around ensuring reproducibility in multiprocessing contexts, handling memory layout changes, and updating build dependencies or configurations, with suggestions for interface improvements and better handling of edge cases, such as nested or specialized tensor types. Unresolved questions include how to best implement or support new features like automatic padding computation, differentiable `pin_memory()`, and the impact of memory layout changes on performance and compatibility."
2017-12-05,pytorch/pytorch,"The discussions highlight several technical concerns including the need for better handling of build scripts across different Windows environments, particularly with batch scripts and Visual Studio configurations. There is emphasis on optimizing performance and correctness in core functions, such as batching normalization, RNN support with fast CuDNN kernels, and padding calculations, where proposals involve utility functions and avoiding overhead during each forward pass. Issues related to the stability and consistency of numerical operations are also raised, like handling infinite values in autograd and ensuring deterministic GPU operations, with suggestions to improve error checking and compatibility. Additionally, dependency management and versioning problems are evident, especially with package installation via conda and ensuring proper build dependencies like ninja. Unresolved questions include how to implement more flexible batch partitioning in DataParallel and addressing threading safety in core libraries to prevent deadlocks."
2017-12-06,pytorch/pytorch,"The discussions highlight several technical concerns: the complexity and platform-dependent issues of Windows batch scripts used for building PyTorch, with a need for modifications to enhance compatibility; persistent bugs related to autograd input buffer assertions that seem amplified with multi-GPU setups and custom autograd functions, indicating potential underlying memory management issues; discrepancies in versioning and feature availability, such as missing attributes like `lr_scheduler` in certain PyTorch versions, and clarity on default arguments in documentation; challenges with efficient handling and batching of sparse tensors in the DataLoader, including implementing proper tensor operations in aten; and general maintenance and refactoring considerations, such as eliminating cwrap in favor of YAML declarations, exposing shape compatibility checks in the API, and ensuring robust memory management and error handling in core operations like `repeat`. These points suggest ongoing efforts to improve stability, usability, and internal consistency of PyTorch, alongside unresolved issues with multi-GPU training, backward assertion errors, and build platform support."
2017-12-07,pytorch/pytorch,"The discussions highlight several technical challenges and enhancements in PyTorch, including implementing efficient tensor operations like flipping CUDA tensors, handling negative strides, and supporting operations such as `reshape`, `stack`, `unsqueeze`, and `cat` for sparse tensors. Issues with CUDA initialization and first-invocation delays are addressed by proper installation of CUDA versions (notably cuda80) and environment management, indicating hardware or configuration dependencies. There are areas for improvement in the API design, such as adding a `reduction` parameter to loss functions, refining `grad_fn` handling to prevent misuse, and enabling proper build-time dependency management via PEP 518. Additionally, support for multi-GPU training, Windows-specific build issues, and ensuring compatibility of broadcasting and tensor shape semantics are recurring concerns. Unresolved questions remain around optimal implementation strategies for tensor inversion, gradient validation for custom functions, and best practices for integrating sparse tensor operations."
2017-12-08,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's sparse tensor support, noting limited operation support such as sparse x dense working but sparse x sparse unsupported, and questions about the creation of sparse scalars. There are issues related to multi-GPU training, including potential bugs in DataParallel, memory management, and the need for improved performance profiling and bug reproduction strategies. Compatibility concerns are raised with dependencies like MKL and OpenBLAS versions, affecting numerical correctness and stability across platforms. Several conversations also address build process complexities, such as ensuring proper linkage, handling submodules like Gloo, and fixing in-place function derivative formulas to use output tensors instead of inputs. Unresolved questions remain about extending sparse tensor capabilities, fixing in-place operation bugs, and improving build robustness on different systems."
2017-12-09,pytorch/pytorch,"The discussions highlight challenges with PyTorch's multiprocessing and data loading, particularly deadlocks caused by internal interactions with external libraries like OpenCV, and improvements achieved by porting code to Python 3.0. There are issues related to CUDA operations, specifically errors with `grid_sample` and `affine_grid` in cuDNN, possibly due to recent changes in cuDNN integration, and questions about underlying dependency compatibility, such as shared object dependencies and environment configurations. Participants suggest potential fixes, including avoiding external dependencies, updating the codebase, or submitting PRs to resolve issues. Lastly, unresolved questions remain about debugging segmentation faults with GDB and ensuring compatibility across different environments and Python versions."
2017-12-10,pytorch/pytorch,"The main concerns across these discussions include compatibility and installation issues on Windows, especially with CUDA and dependencies like gloo and opencv, suggesting updates to build configurations and dependency management are needed. There is emphasis on improving user experience through better Tensor construction APIs, such as adding a `dtype` argument or caching CUDA availability checks to reduce repetitive conditionals. Several bug reports focus on runtime and integration errors, such as missing DLLs, broken grid sampling functions with cuDNN, and the need for proper dependency declarations in codegen files. The community also discusses maintaining backward compatibility while evolving API features, exemplified by adjustments to the `sample()` method in distributions. Overall, unresolved questions relate to debugging specific build errors, ensuring feature additions are user-friendly, and maintaining stability amid ongoing development."
2017-12-11,pytorch/pytorch,"The discussions highlight several key technical concerns in the PyTorch repository, including compatibility issues with older and newer package versions, particularly DLL load failures on Windows and CUDA-related segmentation faults in multi-GPU setups. There is an ongoing effort to enhance the functionality and flexibility of tensor operations, such as implementing `stack`, `unsqueeze`, and `cat` for sparse tensors, and improving the `grid_sample` and `affine_grid` functions' support with cuDNN. Additionally, users express need for better container abstractions like a `nn.ModuleDict` to facilitate dynamic model architectures, especially for multi-task learning with runtime module keys. Several discussions also address backward compatibility, API design, and the integration of new features like `sample(sample_shape=())` in distributions, emphasizing minimal disruption to existing usage. Unresolved questions include fixing bugs related to dependency conflicts, managing gradient computation and variable handling, and expanding API consistency across tensor operations."
2017-12-12,pytorch/pytorch,"The discussions highlight several core issues: difficulties in building lightweight PyTorch wheels without dependencies like libcudnn; ongoing challenges with CUDA and GPU support across different hardware (e.g., AMD, Intel, multiple GPUs), often linked to cuDNN bugs and driver compatibility; complexities in the autograd system, particularly hook registration and differentiability of native functions, with suggestions to improve the design via derivatives.yaml and internal implementation adjustments; problems related to memory leaks during gradient computations, with some troubleshooting pointing to CUDA bugs; and the need for more flexible, dynamic module containers such as `ModuleDict` to support runtime module organization, alongside various code review and maintenance challenges. Many unresolved questions pertain to fixing underlying bugs (CUDA, cuDNN), improving modularity and dynamic graph support, and ensuring compatibility across environments."
2017-12-13,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's stability, usability, and correctness, such as fixing bugs related to in-place operations, tensor indexing, and derivative calculations. Several issues highlight the importance of proper handling of default arguments, backward compatibility, and ensuring consistent behavior across different environments (e.g., CUDA versions, CPU/GPU). There are ongoing efforts to refine the autograd system—specifically, the derivative formulas, autograd code generation, and handling of in-place functions—along with clarifications on API changes and ensuring that features like DataParallel work correctly. Additionally, discussions touch on the deployment simplicity, such as avoiding the need for specific CUDA installations, and improving error reporting, stack traces, and test robustness. Overall, the focus is on making PyTorch more reliable, intuitive, and efficient for development and research workflows."
2017-12-14,pytorch/pytorch,"The discussions highlight several technical concerns, including the absence of a direct equivalent to TensorFlow's `tf.where` in PyTorch, and the slow implementation of layer normalization suggestive of the need for CUDA kernel optimization. Questions about the correct usage of extension modules post-build, handling of varying label sequence lengths in CTC loss, and diagnosing segmentation faults and CUDA compatibility issues with different driver versions are recurrent. Several issues pertain to improving API consistency and functionality, such as making `nonzero()` numpy-compatible, implementing adaptive pooling, and streamlining the autograd differentiability framework—particularly around primitive and abstract types. Several proposals involve code refactoring, performance optimizations, and clarifications on behaviors indispensable for stable, efficient deployment across diverse environments. Many unresolved questions focus on ensuring backward compatibility, correct configuration (CUDA/cudnn paths), and broader API coherence."
2017-12-15,pytorch/pytorch,"The discussions highlight concerns around implementing functionality efficiently and accurately, such as replicating `tf.where` in PyTorch (e.g., using `weights[weights == float('inf')] = 0`), and the intention to incorporate such features directly into core functions like `torch.split`. Several questions address debugging and build issues, including CUDA compatibility, error handling, and issues with multi-GPU setups, often suggesting upgrading the codebase or modifying build scripts. Performance bottlenecks are scrutinized, especially related to data loading and print statements, with recommendations to optimize data transformation pipelines and avoid print operations during benchmarking. Some discussions focus on improving autograd and scope tracking, like exposing scope names in Python and refining backward pass generation, alongside bug fixes for shape checks and in-place operation semantics. Overall, the threads underscore ongoing efforts to enhance PyTorch's robustness, usability, and performance in various computational contexts."
2017-12-16,pytorch/pytorch,"The discussions highlight several technical concerns: compatibility issues between CUDA versions and Nvidia drivers, especially regarding backward compatibility and driver requirements; challenges in serializing and exporting PyTorch models, specifically related to including class definitions and compatibility with formats like ONNX; performance considerations and bugs related to tensor operations such as transpositions and in-place updates, which can significantly impact computation speed; the need for improved support and testing across different hardware architectures, including ppc64le; and correctness issues in gradient computations for certain activation functions like Leaky ReLU, emphasizing careful handling of in-place operations and autograd behavior. Additionally, there are ongoing questions about implementation details, such as adjusting kernel flags for hardware builds and ensuring proper memory layout preservation with MKLDNN."
2017-12-17,pytorch/pytorch,"The discussions highlight the need for more user-friendly, numpy-like APIs in PyTorch, with suggestions for a `torch.np` package that matches numpy's API and unifies Tensor and Variable concepts, though this poses design challenges. There's interest in implementing various mathematical functions (e.g., Cephes functions like digamma, trigamma, Gamma, Beta) to enhance statistical support, with considerations for GPU implementations and optional dependencies. Several technical issues are raised regarding internal code stability and safety, particularly memory corruption risks from unsafe variable handling and the use of internal functions like `*Impl`. Additionally, questions about CUDA support versions, environment setup, and compatibility are discussed, emphasizing the importance of clear upgrade paths. Overall, the themes focus on API usability improvements, mathematical capabilities expansion, and ensuring internal code safety and compatibility."
2017-12-18,pytorch/pytorch,"The discussions highlight ongoing efforts to improve numerical stability and efficiency in core functions such as `log_sum_exp`, with suggestions for native C implementations and GPU-optimized sparse matrix operations. Several issues concern API enhancements, such as exposing scope names to Python, handling variable indexing semantics, and supporting sparse tensor operations like zeros and `narrow`. Thread safety and memory safety are addressed, notably in the handling of undefined variables and potential memory corruption from direct member access. Troubleshooting installation and environment issues—such as CUDA path configuration, binary compatibility, and library dependencies—are also prominent. Overall, the conversations reflect an active process of refining PyTorch's core features, interface clarity, and system robustness, with many tasks requiring further testing, review, or environmental adjustments."
2017-12-19,pytorch/pytorch,"The discussions highlight ongoing efforts to optimize custom CUDA kernel execution within PyTorch, with considerations for supporting execution in non-default streams via pycuda, and plans for helper libraries to simplify kernel jitting. Several issues involve handling edge cases, such as unsupported operations on multi-GPU setups, memory leaks, and bugs related to tensor expansion, undefined tensors, and gradients, emphasizing the need for thorough testing and bug fixes. There are concerns about API consistency and safety, particularly around in-place operations, variable undefined states, and data type exposure, prompting suggestions for dedicated overloads and clearer attribute conventions. Additionally, practical challenges like installation difficulties across platforms, managing backward compatibility, and ensuring reliable CI testing remain prominent. Overall, these discussions reflect a mix of performance improvements, robustness, and usability considerations critical to advancing PyTorch's functionality."
2017-12-20,pytorch/pytorch,"The discussions mainly revolve around handling CUDA device-side asserts and their informative reporting, with suggestions to add assertions in loss functions to catch label mismatches (e.g., label range and class count issues). Several comments emphasize ensuring proper import sequences of `torch` and dependencies like `cv2` to prevent runtime errors, highlighting platform-specific and build-related challenges. There are ongoing efforts to improve compatibility and performance, such as fixing BLAS/LAPACK support, optimizing FP16 training, and enhancing multi-GPU/distributed setups. Questions also focus on API design choices, like exposing tensor data types, and streamlining autograd kernels with zero-filled tensors for efficiency. Unresolved concerns include stability across different hardware configurations, CUDA versions, and ensuring CI checks catch regressions effectively."
2017-12-21,pytorch/pytorch,"The discussions highlight several technical concerns including: potential runtime errors and crashes in Python 2.7 due to package and environment configurations, with suggestions to use Python 3 as a workaround; the need for better handling of tensor state changes and parameter broadcasting, especially during inference and multi-GPU setups; challenges in porting custom C++/CUDA functions to ATen, particularly related to proper function signatures and linking with THCUNN; compatibility and stability issues with different CUDA/cuDNN versions, especially for operations like BatchNorm in eval mode and float Half types; and code maintenance questions regarding code refactoring, such as replacing functions with decorators or adjusting for problems in test configurations, CI pipelines, and build processes. Many discussions also express concern over reproducibility and correctness of numerical operations across different environments or library versions."
2017-12-22,pytorch/pytorch,"The discussions highlight several key technical concerns, including the lack of CUDA support for `torch.histc()` in `THC` and the need for its implementation, as well as performance issues related to `narrow` tensors and their backward pass. There are questions about improving user experience, such as handling `lengths` input types more flexibly in functions, and clarifications on the specific behavior of certain linear algebra functions like `scipy.linalg.lapack` variants and their compatibility with PyTorch. Several issues also address internal errors, such as crashes on specific Python/CUDA versions, and the importance of properly testing distributed modules and API changes (e.g., `btrifact`) to prevent regressions. Additionally, questions around code build configurations (dispatch templates, lambda usage in CUDA kernels) and environment setups (CUDA version compatibility) indicate ongoing efforts to stabilize and enhance the framework's robustness."
2017-12-23,pytorch/pytorch,"The discussions highlight issues related to concurrency and file access in HDF5, with suggestions to enable SWMR mode and set the multiprocess start method to avoid unsafe concurrent reads. Several comments address bugs involving gradient computation, specifically the improper handling of masked indexing operations leading to NaNs, with proposed code modifications to filter zeros before operations like `log`. There are reports of runtime errors during distributed training and autograd checks, with solutions involving code fixes, environment variables, or regression tests. Additional concerns include potential breaking changes from library updates, GPU-related errors such as out-of-bounds kernels and device detection issues, and general troubleshooting steps such as running with `CUDA_LAUNCH_BLOCKING=1`. Unresolved questions center around best practices for multiprocessing, error localization, and compatibility across platforms."
2017-12-24,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch's CUDA and dependency configurations, such as DLL load failures, memory management bugs, and the need for proper dependency paths. There are questions about installing specific PyTorch versions (CPU vs. GPU, conda commands) and troubleshooting import errors, often suggesting verifying environment setup or building from source. Certain comments address potential bugs in gradient computations involving indexing operations, indicating that index-based tensor operations may improperly influence the computational graph and produce NaNs, representing a bug in gradient tracking. Additionally, there are inquiries about integrating new functionalities like `grad()` or `KLDiv` into PyTorch, as well as best practices for configuring cuDNN, like whether to enable `benchmark` mode or disable cuDNN entirely under specific circumstances. Overall, unresolved issues pertain to configuration, bug fixes, feature additions, and performance optimizations, with some questions awaiting further investigation or community input."
2017-12-25,pytorch/pytorch,"The discussions highlight significant resource and compatibility challenges, including memory limitations when training large models (e.g., 40GB for certain fully connected layers), dependency conflicts and installation issues particularly on Windows, and the need for offline package solutions to bypass dependency checks. Several reports address troubleshooting technical errors, such as CUDA kernel assertions, inconsistent results across CPU and GPU, and issues stemming from environment configuration, like line ending conversions and pickling errors with multiprocessing. Some commenters suggest specific fixes, such as setting start methods for multiprocessing, adjusting environment variables, or modifying code behavior, but many issues remain unresolved or require further investigation. Overall, the concerns emphasize robustness in installation, environment setup, and handling large-scale models, alongside ensuring reproducibility and compatibility across platforms."
2017-12-26,pytorch/pytorch,"The discussions highlight several technical concerns, including dependency conflicts when installing PyTorch and the benefits of offline installation, DLL loading issues and circular dependencies on Windows, and build failures related to missing or incompatible libraries like NNPACK, MKL, and CUDA. There are questions about the correctness of specific API behaviors, such as the interpretation of tuples in tensor creation, and stability of training results across different environments or hardware (e.g., multi-node, multi-GPU setups). Suggestions include refactoring code for better maintainability, adding proper testing and seed control for stochastic components, and clearer documentation for complex features like KL divergence implementation and randomized testing strategies. Unresolved issues involve fixing build errors, addressing DLL dependencies, and defining standardized behaviors for features like model evaluation mode and randomness sources."
2017-12-27,pytorch/pytorch,"The discussions highlight several technical concerns, including performance issues with dilated convolutions in PyTorch, where users seek optimization strategies, and compatibility challenges with different CUDA and cuDNN versions, particularly when dealing with varying input sizes and benchmarking. There are questions around enhancing support for OpenCL via coriander, requiring address-space management and kernel compilation, as well as ongoing efforts to improve build system compatibility across compilers (GCC 7) and platforms (Windows, different CUDA versions). Bug fixes and feature support, such as color tagging in NVTX, support for specific operator types, and maintaining synchronization with upstream Python distutils, are also prominent topics. Additionally, there are concerns about ensuring correct gradient implementations, managing nondeterminism in tests, and handling data serialization behaviors. Overall, the discussions reflect a focus on performance optimization, cross-platform compatibility, API support improvements, and rigorous testing practices."
2017-12-28,pytorch/pytorch,"The discussions highlight ongoing challenges and considerations in extending PyTorch's compatibility and functionality across diverse hardware and environments, including support for non-CUDA GPUs (like AMD HIP and OpenCL via PlaidML), and ensuring proper integration with MKL and Windows systems. Several comments address API design and usability improvements, such as better handling of bidirectional RNNs, exporting model classes with `torch.save`, and clarifying behaviors in `load_state_dict` and distributed training. There are concerns about correctness and precision in gradient checks, as well as managing dependencies and build configurations, notably with CMake and external packages like PyYAML. Additionally, questions about future features like KL divergence support in distributions and model export enhancements remain open, with some proposals for API adjustments and code restructuring. Unresolved issues include ensuring stable multi-node distributed training, refining API consistency, and improving cross-platform support."
2017-12-29,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's extensibility and usability, such as adding native C++/CUDA functions via ATen extensions, and ensuring compatibility with diverse hardware like ARM and AMD GPUs. Several issues address performance and correctness concerns, including precision discrepancies in tensor computations, variability in experimental results due to RNG states, and challenges with distributed training across multiple nodes or different communication backends like Gloo and MPI. There are also environment and build-related concerns, such as resolving linking errors with NNPACK, handling dependencies like MKL, and platform support limitations, notably on Windows. Additionally, questions are raised regarding proper model serialization, JIT compilation behavior, and the addition of new operations (e.g., `matmul`) to the JIT graph, aiming for more consistent, efficient, and flexible machine learning workflows within PyTorch."
2017-12-30,pytorch/pytorch,"The discussions encompass a range of technical concerns including installation and module import errors, particularly with torchvision, suggesting proper installation commands and troubleshooting steps. Several issues address code quality and maintenance, such as the need to sync distutils with upstream, monkey-patching spawn for easier maintenance, and ensuring changes like the `div` function are correctly implemented. Performance and correctness concerns are raised regarding gradient propagation verification, memory safety (e.g., `index_add_` errors), and hardware/software compatibility, especially with distributed training, MPI, and environment-specific timeouts. Additionally, there are proposals for API enhancements and bug fixes, such as adding support for `matmul` and `bmm` in the JIT, refining in-place operations, and improving documentation and installation guidance. Unresolved questions about hardware-specific behavior, configuration options, and infrastructure issues like domain restrictions also feature prominently."
2017-12-31,pytorch/pytorch,"The discussions primarily revolve around the limited support and functionality for sparse tensors in PyTorch's autograd system, with sparse-sparse matrix multiplication not yet implemented, and current support mainly for dense-sparse operations. Multiple comments address issues related to gradient behavior at boundary cases, such as in the square root function where gradients are undefined or NaN at zero. There are ongoing optimizations and API improvements, including the performance of specific operations like `addcdiv`, and updates to derivative definitions for in-place functions, aiming to enhance computational efficiency and correctness. Additionally, some issues with environment setup, such as compatibility with OpenCV and hardware configurations affecting GPU traffic, are noted, with solutions involving external system adjustments. Unresolved questions include whether sparse-sparse matrix operations will be supported in the future and clarifications on derivative implementations for various tensor operations."
2018-01-01,pytorch/pytorch,"The discussions highlight ongoing limitations in PyTorch, such as limited support for sparse tensor operations and issues with autograd and multiprocessing, particularly concerning thread re-creation post-fork. Multiple comments address GPU compatibility problems, especially with different CUDA and cuDNN versions, suggesting potential encoding, compatibility, and performance pitfalls. There are concerns about the integration and optimization of JIT compilation, especially when dealing with dynamic networks and conditionals, as well as questions about improving documentation and user guidance. Some discussions involve fixing bugs, updating dependencies, and optimizing core functions like random number generation and mathematical operations for speed and precision. Overall, the issues reflect efforts to enhance robustness, performance, and usability, with unresolved questions about support for specific tensor types and platform compatibility."
2018-01-02,pytorch/pytorch,"The discussions highlight several core technical concerns: compatibility and stability issues related to DLL dependencies and Windows support, especially with CUDA and cuDNN, and the need to improve build processes and dependency management. There are questions regarding the correct handling of autograd threads after forking, particularly in multiprocessing contexts, and issues with reproducibility and floating-point precision when solving linear algebra problems with LAPACK and BLAS libraries. Additionally, there are ongoing discussions about refining the PyTorch API, such as treating `train(False)` as `eval()`, and enhancing user experience through documentation, installation instructions, and flexible API features like dynamic network construction and padding mechanisms. Finally, some feature requests, such as adding inverse STFT, support for distribution constraints, and better communication with data loaders, are being considered, with considerations about maintaining library simplicity versus flexibility."
2018-01-03,pytorch/pytorch,"The discussions highlight several technical concerns including GPU compatibility and support on Windows, especially for newer architectures like Volta (Titan V), which require compiling from source due to prebuilt package limitations. There's ongoing work to ensure numerical precision and correctness in LAPACK-related operations and tensor solving functions (e.g., trtrs, potrs), with attention to flag settings and eigenvalue issues affecting stability. The potential for memory leaks associated with pybind11's implicit conversions and refcounting is repeatedly noted, emphasizing the need for better leak detection and resource management. Additionally, there are challenges in implementing and debugging JIT compilation for complex models, especially around dropout and custom modules, and issues with flaky or unstable CI testing conditions on Windows and other architectures. Overall, many improvements are targeted at robustness, compatibility, and performance, but some unresolved issues remain in configuration, testing, and low-level function correctness."
2018-01-04,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's functionality and infrastructure, including the need for native support of (log)beta functions, efficient computation of Hessian diagonals, and improvements in distributed data handling. Several issues concern memory leaks, stability of GPU operations, and cross-framework compatibility, such as with TensorFlow. There is a focus on optimizing build processes (e.g., CUDA, Windows CI) and ensuring robustness of the API, especially regarding parameter referencing in optimizers and handling batches in sparse matrices. Unresolved questions include how to implement certain mathematical operations efficiently, how to standardize testing across different environments, and how to improve performance and stability in both CPU and GPU contexts. Overall, these discussions emphasize enhancing PyTorch’s scalability, usability, and integration with other tools and frameworks."
2018-01-05,pytorch/pytorch,"The discussions highlight several core technical concerns, including efforts to reduce the size of PyTorch wheel files by excluding CUDA/cuDNN dependencies, raising questions about the build and dependency management. There are ongoing challenges with autograd behavior at non-differentiable points (e.g., sqrt(0)), and inconsistencies between tensor operations like `x.norm()` and `torch.sqrt(x*x)`, particularly regarding gradient behavior at boundary points. Issues in model serialization are addressed, such as the benefits of saving `state_dict()` versus full model objects, and the need for class definitions during load, with suggestions for better ONNX export handling and model graph analysis. Additionally, there are performance optimization discussions relating to indexing operations, profiling, and internal memory layouts (e.g., MKLDNN), as well as build configuration concerns, notably the support for debug builds in ATen and compatibility across versions."
2018-01-06,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance batching and graph optimization features in PyTorch, with plans to introduce TF-Fold-like automatic batching and to improve support for lazy execution and JIT compilation. Several comments address the need for clearer documentation, especially regarding padding behavior and tensor operations, with suggestions to include examples for better clarity. Issues related to build processes, such as host hosting for nightly binaries, multi-platform wheel creation, and debugging build configurations, are also discussed. Some technical concerns revolve around ONNX export correctness, particularly scope and aliasing issues caused by in-place operations and node reuse, and the need for precise debugging tools to track node and tensor identities. Lastly, there are questions about API stability, build performance, and error handling strategies in the context of multi-GPU execution and Python version compatibility."
2018-01-07,pytorch/pytorch,"The discussions highlight several technical concerns regarding PyTorch's development, including challenges in package management and environment setup on Windows, particularly with CUDA compatibility and dependencies like NNPACK. There are suggestions for improving batching and sequence processing through lazy evaluation frameworks inspired by Dynet, aiming to simplify sequence batching and reduce manual effort. Compatibility and determinism issues with cuDNN, especially concerning randomness in convolutional and LSTM operations, are also discussed, with proposed solutions such as setting `deterministic = True`. Additionally, multiple conversations address code maintenance, including proper import statements, build issues on macOS, and test reliability on CI pipelines. Overall, the discussions reflect ongoing efforts to improve usability, reproducibility, and robustness of PyTorch."
2018-01-08,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for environment isolation to resolve DLL load failures and ensuring compatibility when calling CUDA or cuDNN functions, particularly with non-contiguous tensors. There are questions about maintaining consistency between model and tensor device placements, addressing potential memory leaks during training loops, and improving support for batch normalization with small batch sizes. Additionally, issues around build configurations—such as correctly handling CUDA dependencies without a CUDA-supporting system—and the integration and testing of new features like `polygamma` and `bmm` suggest a focus on robustness and API clarity. Suggestions for better documentation, regression testing, and conditional compilation based on build type or system capabilities are recurring themes, alongside considerations for clean, maintainable code updates and addressing platform-specific build problems."
2018-01-09,pytorch/pytorch,"The discussions highlight a strong interest in incorporating advanced interpolation methods, akin to NumPy/Scipy, directly into PyTorch for more straightforward and GPU-accelerated tensor interpolation. Several issues address performance bottlenecks and correctness in features like normalization (e.g., InstanceNorm, LayerNorm), and efficient handling or support for specific tensor types such as HalfTensor or BoolTensor, suggesting a need for improved backend generalization and type management. There are concerns regarding CUDA and hardware support, including GPU capability detection, driver compatibility, and the proper use of native CUDA functionalities like shfl_xor, indicating ongoing efforts to enhance portability and hardware abstraction. Additionally, stability and correctness issues are evident in areas such as the autograd view requirements, CUDA memory management, and new operator integrations, with proposals for better testing and refactoring to avoid regressions. A recurring theme is the desire for more native, efficient, and robust support for features that enhance PyTorch's usability both as a neural network framework and a general-purpose tensor computation library."
2018-01-10,pytorch/pytorch,"The discussions largely revolve around improving PyTorch’s functionality and usability, including support for operations like dynamic output sizing and ONNX export limitations, such as handling `InstanceNorm`. Key concerns include memory management issues with DataParallel, particularly GPU memory leaks, and the need for clearer semantics around `requires_grad`, view relationships, and autodiff support. There is also discussion about extending features like `weight_norm` for RNN modules, refining tensor printing and data type defaults for better debugging, and increasing support for operations like `bincount` on GPUs. Several unresolved questions involve implementing missing operators in ONNX, improving CUDA support (particularly for deterministic algorithms), and handling specific user configuration issues for installation and compatibility."
2018-01-11,pytorch/pytorch,"The discussions highlight ongoing challenges with Windows DLL modifications for PyTorch, where replacing system DLLs like `MSVCP140.DLL` and `VCRUNTIME140.DLL` did not resolve issues despite user attempts, suggesting deeper compatibility or installation problems. There are concerns regarding the support for distributed computing, specifically the default build support and proper initialization of process groups, with some noting that the current master build lacks distributed support unless explicitly enabled. Performance variability in tensor operations, such as `index_fill_`, is noted, with some cases experiencing significant slowdowns due to kernel execution conflicts, prompting debates about best practices and expected behavior. Additional topics include integrating ONNX support features like output shape calculation, the importance of ensuring reproducible and deterministic algorithms with cuDNN, and the handling of scalars in ATen to support both Python numbers and tensors. Overall, unresolved issues revolve around system compatibility, build configurations, performance optimizations, and API design considerations."
2018-01-12,pytorch/pytorch,"The discussions primarily revolve around compatibility and build issues, such as ensuring proper support for CUDA, C++11, and headers across different platforms, with specific concerns about headers, compiler support, and environment configurations. Several comments highlight efforts to fix bugs related to PyTorch's build process, including resolving header conflicts, build system hacks, and supporting distributed training. There are ongoing considerations about API design, such as checkpointing mechanisms, module collections, and data structures, emphasizing flexibility and correctness. Debates also touch on multiprocessing, memory access errors, and proper testing protocols, with suggestions for improved error handling and more comprehensive testing. Overall, unresolved issues include platform-specific build bugs, API consistency, and ensuring compatibility with various hardware and software environments."
2018-01-13,pytorch/pytorch,"The discussions primarily focus on improving the handling of `state_dict` in `DataParallel` modules, with suggestions to modify `state_dict` to export only relevant parameters, and to automate key adjustments for multi-GPU training. There's concern about the behavior of `load_state_dict` across different configurations, advocating for a more flexible implementation that handles `DataParallel`-specific key prefixes. Additionally, debates emerge regarding checkpointing methodologies, with suggestions favoring explicit input passing over context managers for greater control. Some discussions address bug fixes and compatibility issues, such as solving GPU-related errors, build failures on Windows, and ensuring tests work across Python versions, often highlighting the need for better documentation and utility functions. Overall, unresolved questions remain about the best practices for model serialization, gradient management in sliced Variables, and the infrastructure to support these enhancements securely and efficiently."
2018-01-14,pytorch/pytorch,"The discussions highlight several key technical concerns, including memory management issues during training and validation, such as out-of-memory errors and potential memory leaks, especially with large models or long validation runs. There are questions about implementing specific features, such as a stable Local Response Normalization (LRN) layer in early versions of PyTorch, and replacing custom RNNs with optimized cuDNN kernels. Some discussions address compatibility and stability problems, like socket timeouts and distributed training failures, often mitigated by configuration adjustments or code modifications. Additionally, there are requests for enhancements to optimizer implementations, learning rate schedulers, and fixing bugs related to in-place tensor operations to prevent unintended side effects. Many unresolved questions pertain to improving performance, stability, and usability in distributed or resource-constrained environments."
2018-01-15,pytorch/pytorch,"The discussions highlight several key technical issues: handling data loading in GPU training, specifically setting `num_workers=0` to avoid subprocess-related errors; challenges in saving and restoring the state of learning rate schedulers, suggesting tracking `last_epoch` as a workaround; difficulties in compiling PyTorch with CUDA 8.0 and newer CUDA/cuDNN versions on systems without root access, emphasizing environment configuration; memory leaks observed during large model training, with ongoing investigations into their causes across CPU and GPU; and API consistency and clarity in distribution-related functions, such as clarifying the use of `logits` versus `logprobs`, and resolving discrepancies in normalization layers and sparse tensor support."
2018-01-16,pytorch/pytorch,"The discussions primarily focus on improving PyTorch's functionality, stability, and interface consistency. Notable concerns include implementing a stable Local Response Normalization (LRN) layer and migrating sparse tensor operations to ATen's declarations. There are technical issues related to CUDA compatibility, especially with different GPU backends and versions of CUDA/cuDNN, and the need for clear warnings when functionalities like cuDNN are unsupported. Additionally, questions arise about best practices for incorporating learning rate schedulers with optimizers, handling autograd cycles and Variable lifecycle, and naming conventions in probabilistic distributions (like logits vs. log-probabilities). Overall, ongoing efforts aim to optimize core operations, enhance user experience, and ensure hardware compatibility, with some unresolved questions about API design and backward compatibility."
2018-01-17,pytorch/pytorch,"The discussions encompass various technical topics including best practices for registering buffers in modules and handling device transfers, issues with GPU memory allocation and interference from external libraries like OpenCV, and the development of ATen native functions for custom CUDA operations. Several threads address reliability and correctness concerns, such as avoiding NaNs during tensor operations, ensuring proper gradient flow, and fixing header and compilation issues in the PyTorch codebase. There are also questions about optimizing CPU performance, managing multi-GPU training with NCCL, and supporting higher-order derivatives with `.backward()`. Overall, the discussions reflect ongoing efforts to improve PyTorch's usability, performance, extensibility, and robustness in complex deep learning workflows."
2018-01-18,pytorch/pytorch,"The discussions primarily revolve around managing module sharing and container views in PyTorch, with concerns about returning slices or views that preserve module registration without encouraging sharing modules between containers. Data layout choices for convolutional layers, specifically NHWC versus NCHW, are debated regarding performance impacts and compatibility with different architectures, with transposing data as a potential workaround. Several issues address hardware-specific challenges, such as NCCL multi-GPU support, CUDA architecture compilation, and environment setup, highlighting the need for clearer user guidance and robust error handling. Additionally, there are discussions on optimization and correctness of tensor operations, including atomicity in in-place modifications, sparse tensor support, and backward compatibility, with emphasis on fixing edge cases, improving test coverage, and ensuring clarity in documentation. Overall, unresolved questions include the best practices for module sharing, data format efficiency, hardware support, and API robustness in complex scenarios."
2018-01-19,pytorch/pytorch,"The discussions mainly revolve around implementing and improving low-level tensor operations such as im2col/col2im for CPU and GPU, with some contributors offering help and PRs to integrate these features. Several issues concern threading, segmentation faults, and GPU support, especially with random number generation, CUDA/cuDNN compatibility, and multithreading stability, often attributed to environment-specific misconfigurations or code-related bugs. There's ongoing debate about module sharing, container flexibility, and API support for sparse tensors, with a consensus on enhancing ATen for standard methods. Additionally, some discussions address build system intricacies, header inclusion orders, and pipeline stability, indicating a broader effort to optimize performance, robustness, and usability across hardware and software configurations. Unresolved questions include fixing segmentation faults in multi-threaded contexts and extending support for advanced operations like svd, qr, and new ONNX and TorchScript features."
2018-01-20,pytorch/pytorch,"The discussions highlight several technical concerns, including stability issues and failure modes in distributed training with the Gloo backend, where increasing timeouts and network configurations are suggested as potential mitigations. There are compatibility and build challenges, such as header installation, linking errors with specific BLAS operations, and issues stemming from compiler and library versions, notably differences in glibc and compiler flags across environments. Reproducibility and correctness concerns are raised regarding optimizer modifications, especially related to weight decay behavior, with suggestions to add configuration flags to preserve default behavior. Performance optimization topics are also discussed, including the potential benefits of SIMD vectorization and OpenMP, particularly for functions like `log` and `exp`, and how compiler choices influence these enhancements. Lastly, some questions remain about best practices for debugging, such as interpreting GDB stack traces, and ensuring code modifications do not adversely affect existing functionalities."
2018-01-21,pytorch/pytorch,"The discussions highlight concerns about input validation and error handling in loss functions, such as adding assertions to verify label ranges in `nll_loss` and issues with input value ranges in `BCELoss`, possibly due to floating-point precision or data preprocessing errors. Several threads address build and compilation issues, notably errors with missing functions like `convolve_5x5_sse` and differences in compilation steps across environments, suggesting potential problems with build configurations, compiler flags, or platform-specific optimizations. Others raise questions about API design, such as replacing `volatile` with `torch.no_grad()`, and handling of data types like ByteTensor versus BoolTensor, along with suggestions for expanding functionalities like max pooling. Reproducibility and backward compatibility, especially regarding optimizer behaviors and weight decay, are also key themes, alongside performance optimization considerations on CPUs and GPUs. Overall, the discussions focus on improving robustness, compatibility, and usability of PyTorch's core functionalities."
2018-01-22,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's features and performance, including developing batch-compatible operations like `mean`, addressing memory management issues during evaluation and transfer learning, and expanding support for interpolation algorithms and linear algebra routines with LAPACK/MAGMA. Key technical concerns involve optimizing multi-threaded CPU operations through OpenMP or Intel C++ compiler directives, ensuring efficient handling of non-contiguous tensors to match numpy performance, and enabling distributed training and dynamic control over autograd and variable volatility. Several questions remain open, such as implementing a differentiable `trtrs`, enhancing the numpy-correspondence API, and ensuring compatibility across diverse hardware and software configurations (e.g., GLIBC versions, CUDA/cuDNN setups). There are also recurring discussions about code maintainability, macro improvements, and ensuring correctness in gradient computations and model serialization mechanisms."
2018-01-23,pytorch/pytorch,"The discussions highlight the importance of accurately obtaining input/output shapes within the PyTorch framework, with suggestions for implementing a standardized `output_shape_for` method across modules to facilitate shape inference without forward passes. Several comments underscore the need for robust multi-threaded random generator handling, with debates on the efficacy of thread-local generators and caching approaches, emphasizing consistency and reproducibility. Issues related to kernel compatibility, particularly with glibc versions and NCCL NCCL plan caching, point to platform-specific challenges and the necessity for better abstraction and caching strategies. Additionally, there is ongoing work to improve the codebase's robustness, including handling of deprecated or inconsistent tensor operations, better error messaging for advanced indexing, and ensuring cross-platform compatibility for building and linking shared libraries. Overall, these discussions reflect a focus on enhancing usability, performance, and stability across diverse environments and use cases."
2018-01-24,pytorch/pytorch,"The discussions highlight several key technical concerns: the performance impact of Python multiprocessing on GPU utilization in Windows environments, with suggestions to analyze process activity and data loading bottlenecks; the need for clearer documentation and support for RNN behaviors, especially bidirectional LSTM states and their unpacking; challenges with CUDA support compatibility, particularly with the latest MSVC versions and cuDNN linking; issues related to batch normalization with small batch sizes and the implications for training stability; and the ongoing effort to unify Variable and Tensor classes, with considerations for efficient type checks and the integration of new features such as handling pretrained embeddings, scalars in distributions, and extension building without `setup.py`. Several questions remain open regarding best practices for RNN state handling, the impact of instruction set optimizations like AVX-512, and ensuring compatibility across different platforms and build systems."
2018-01-25,pytorch/pytorch,"The discussions highlight several technical challenges and questions: Data loader performance issues linked to Python package contention and shared memory leaks, with solutions like adjusting threading parameters; difficulties with CUDA support on latest MSVC versions and environment configuration for GPU acceleration; complexities in understanding RNN hidden states and their unpacking, suggesting a need for clearer documentation; and build system issues, such as link errors, symbol management, and compatibility with various platforms and compilers. Several comments call for improved testing, stricter validation, and more transparent API behavior, especially regarding tensor operations (e.g., sorting, broadcasting) and optimizations. Unresolved questions include whether certain distributions (like Logistic) are necessary and how to best handle ABI compatibility across compiler versions, indicating areas for ongoing refinement and clarification."
2018-01-26,pytorch/pytorch,"The discussions highlight technical challenges in transitioning PyTorch code to C++, including handling tensor and random number generation code, with suggestions to move toward full C++ integration rather than quick fixes like try-catch blocks. There are concerns about compatibility and performance regressions, especially regarding GPU driver issues, CUDA/cuDNN version compatibility, and memory management, which require detailed debugging and version-specific adjustments. Several questions focus on improving user experiences with data loading (`num_workers=0` slowdown), memory usage, and reproducibility of bugs, suggesting the need for scripting and profiling tools. In addition, issues around API evolution, including variable/tensor unification, transforming distribution objects, and versioning, point to ongoing design discussions and the importance of interface stability. Overall, unresolved issues remain in optimizing performance, ensuring compatibility across environments, and formalizing new features like JIT compilation and distribution transformations."
2018-01-27,pytorch/pytorch,"The discussions highlight issues related to GPU memory management and optimization, such as the proper use of `volatile` in validation mode to prevent memory overload, and the increasing shared-memory usage during training, with solutions like `cv2.setNumThreads(0)` being incomplete. Several comments address debugging and stability concerns, including build failures possibly caused by CUDA version incompatibilities and the need for clearer documentation and code examples, particularly regarding variable operations like `scatter_`. There is also a focus on correctness and testing, exemplified by the need to verify entropy and KL divergence implementations for exponential family distributions and debates over modularizing KL divergence functions for better API safety. Finally, discussions on distributed training emphasize integrating mixed-precision support into existing DDP implementations for broader backend compatibility, along with benchmarking and platform-specific issues."
2018-01-28,pytorch/pytorch,"The discussions highlight several technical areas: (1) GPU memory management during validation, with suggestions to set models or data parameters appropriately; (2) threading and performance issues in OpenCV, where disabling threads (cv2.setNumThreads(0)) is a common workaround; (3) build and compilation challenges, notably linking errors with CUDA 9, and questions about build configurations like CMAKE_PREFIX_PATH; (4) improvements and clarifications needed in the PyTorch distribution and autograd code, including function signatures, variable handling, and internal method visibility; and (5) optimization suggestions such as reducing graph retention by deleting intermediate variables to improve memory usage. Overall, these issues reflect ongoing efforts to improve stability, performance, and clarity in PyTorch development."
2018-01-29,pytorch/pytorch,"The discussions highlight various technical concerns, including managing model modes during validation in transfer learning, with suggestions to conditionally wrap tensors in Variables with volatile=True; addressing inconsistencies and fixes in PyTorch binaries across different versions and environments, notably for CUDA and numpy compatibility; and the need for utility functions for calculating padding in convolutional layers to emulate TensorFlow's 'SAME' padding behavior, with debates on the most efficient implementation strategy. Additionally, there are recurrent issues with memory management, such as preventing memory leaks by properly deleting intermediate variables in training loops, and performance regressions observed with certain C++ extensions and device capability querying, requiring efficient caching and better integration with CUDA. Some comments also query the correctness of gradient computations, especially second-order derivatives, and propose contributing to the core library with enhancements like data pointer utilities or better device querying. Overall, unresolved questions include improving the abstraction of paddings, optimizing device information retrieval, and ensuring stability and efficiency across different hardware configurations."
2018-01-30,pytorch/pytorch,"The discussions highlight ongoing challenges and proposals related to enhancing PyTorch with new features such as implementing GPU-compatible interpolation algorithms, padding utilities similar to TensorFlow's 'SAME', and improved support for distributed and mixed-precision training. Several issues focus on technical problems like compilation errors (e.g., missing SSE functions, build failures on specific systems), memory management (garbage collection and memory leak concerns), and compatibility or environment setup (e.g., OpenBLAS threading, CUDA initialization). There are suggestions for implementation strategies, including utility functions for dynamic padding calculation, external library support, and code optimizations (e.g., using `itertools.chain`). Unresolved questions pertain to integrating these features into core APIs, managing system-specific build issues, and ensuring compatibility across platforms with minimal performance overhead."
2018-01-31,pytorch/pytorch,"The discussions primarily revolve around replicating TensorFlow's 'SAME' padding in PyTorch, with proposals for utility functions to calculate appropriate padding based on kernel size, stride, and dilation, emphasizing flexibility and user control over padding decisions. Concerns are raised about the complexity and efficiency of implementing such padding strategies, especially for convolutional operations like Conv2d and ConvTranspose2d. Additional technical issues include problems with CUDA index assertions, memory leaks during multi-GPU training, and the compatibility of extension interfaces, highlighting ongoing efforts to improve robustness, ABI compatibility, and debugging tools. There are also discussions about simplifying data loading processes and handling specific numerical operations, reflecting a focus on performance optimization and API usability. Unresolved questions include how to best integrate these padding utilities into core PyTorch, ensuring cross-platform compatibility, and synchronizing development practices for extending functionality like custom kernels or external libraries."
2018-02-01,pytorch/pytorch,"The discussions highlight several technical concerns, notably the challenges of correctly implementing model checkpointing, with debates over whether to utilize imperative `backward()` or `grad()` for reentrant gradient computations, emphasizing issues around side effects and graph integrity. There is a recurring emphasis on enhancing utility functions, such as enabling modules to report output shapes without executing a forward pass, which involves careful consideration to maintain API consistency and correctness. Additionally, multiple threads address compilation issues and build failures associated with specific hardware configurations, compiler versions, and dependencies like CUDA, cuDNN, and ATen, often related to undefined references or incompatibilities. Concerns regarding potential memory leaks due to reference cycles, especially with Python 2.7 internals, and the importance of proper ABI management for extensions, are also discussed. Overall, unresolved issues remain around safe, backward-compatible API enhancements and stable build processes on diverse system configurations."
2018-02-02,pytorch/pytorch,"The discussions highlight several technical issues, including a recurring error with `multinomial()` missing the `num_samples` argument in torch, suggesting version compatibility or code corrections needed. There are concerns about performance discrepancies, such as the slower speed of grouped convolutions (`groups=in_channels`) even after upgrading PyTorch, indicating potential optimization gaps. Compatibility and build issues are also prominent, especially related to CUDA, OpenBLAS, and library dependencies (e.g., LAPACK), with suggestions to support the new C++ extension system and improve build processes. Several questions revolve around debugging or refactoring code, such as enhancing test coverage, handling tensor indexing constraints, and streamlining code formatting tools like clang-format. Unresolved questions include differences in behavior between cuDNN and non-cuDNN RNNs, and how to better measure and optimize operations across hardware."
2018-02-03,pytorch/pytorch,"The discussions highlight ongoing performance issues and optimization opportunities in PyTorch, such as the efficiency of grouped and depthwise-separable convolutions, with suggestions to improve kernel implementations and hardware support. Several comments address compatibility and build challenges, including CUDA version dependencies, library linking errors, and the need for proper handling of tensor data types and memory management, especially on different OS and hardware configurations. There are questions about enhancing tensor operations, such as sorting within autograd and handling zero-dimensional tensors, with proposals for more native support. Some threads focus on refining distributed and multi-GPU training workflows, including mixed precision support and memory leak mitigation related to Python versions and garbage collection. Overall, key concerns revolve around optimizing performance, ensuring robust build and runtime support, and extending tensor functionality."
2018-02-04,pytorch/pytorch,"The discussions highlight issues with user experience, such as unclear error messages and confusion over internal method visibility, exemplified by discussions around private functions like `._natural_params` and related internal mechanisms. Performance variability across different convolution groups and dimensions has been scrutinized, with experiments indicating inconsistent efficiency on CPU and GPU, prompting questions about optimization and kernel selection. Compatibility problems on macOS, especially related to missing shared libraries like `libiomp5.dylib`, have been encountered despite attempts to update dependencies and environment configurations. Additionally, there are ongoing discussions on proper API design, including the return types of hooks and consistent terminology, alongside maintenance concerns like build errors on Windows and code quality improvements such as suffixing internal fields with underscores. Overall, these threads reflect efforts to improve usability, performance, stability, and code clarity within the PyTorch project."
2018-02-05,pytorch/pytorch,"The discussed comments reveal multiple technical concerns, including GPU device handling, notably how device IDs are managed and the impact of training on different hardware configurations; data loading issues linked to multiprocessing start methods and threading compatibility with OpenCV; and CUDA/cuDNN compatibility problems, especially regarding version conflicts and proper linkage for GPU acceleration. Several questions address reproducibility and correctness of functions like `topk` with NaNs, the behavior of normalization layers (LayerNorm vs. InstanceNorm), and support for mixed-precision operations like testing for half-precision. Notable suggestions include adding smoke tests for different data types, fixing reference cycle issues in Python, and improving kernel performance with new algorithms or code refactoring, such as replacing deconvolutions with upsampling. Unresolved issues often involve dependency management, build environment configuration, and ensuring cross-compatibility between different hardware and software versions."
2018-02-06,pytorch/pytorch,"The discussions highlight ongoing issues with CUDA and cuDNN compatibility, particularly with systems having multiple versions installed, affecting PyTorch's CUDA functionality and performance. There are concerns about source build complexities on Windows, including environment setup and obtaining recent wheel files, alongside persistent bugs in specific operations like MaxPool and Deconv layers. Several threads address performance optimizations and correctness, such as the handling of bidirectional LSTMs, efficiency of certain convolution operations, and numerical stability or accuracy. Moreover, there are procedural suggestions for code API changes, build process improvements, and better CI testing practices, along with troubleshooting environment configurations and dependency management. Overall, unresolved questions revolve around ensuring compatibility and stability across diverse hardware/software setups, improving build/documentation clarity, and refining performance-critical components."
2018-02-07,pytorch/pytorch,"The discussions raise several technical concerns including the reproducibility and stability of CUDA and cuDNN operations across different hardware and software environments, such as issues with MKL and GPU driver compatibility, and the impact of build configurations (e.g., num_workers, CUDA versions). There are questions about optimizing specific operations like depthwise convolution, and whether recent PyTorch updates or external dependencies (MKLDNN, IPython, etc.) are correctly integrated or require additional configuration. Compatibility and correctness of low-level implementations, such as the internal memory layout in MKLDNN, potential issues with undefined tensors and variable management, and the correct handling of argument evaluation order in tests, are also discussed. Moreover, there is ongoing troubleshooting regarding build failures, environment setup, and ensuring consistent behavior across multi-threaded data loading and distributed training setups, with some unresolved issues related to Windows build errors and package installation."
2018-02-08,pytorch/pytorch,"The discussions highlight several technical concerns: first, issues related to compatibility and performance optimizations, such as whether MKLDNN is integrated into Anaconda and how layout formats impact CPU efficiency; second, problems with shared memory and GPU memory management, including the need to properly set environment variables and understand `num_workers` behavior in DataLoader; third, correctness and stability issues, such as segmentation faults caused by incorrect tensor shapes or environment configurations, and the importance of proper testing for half-precision tensors; fourth, build errors related to compiler support for C++ features (`isnan`, `to_string`) in different environments, and workarounds involving including specific headers; and fifth, unresolved questions about code design choices, like whether to keep the `Unserializable` class for backward compatibility, and the organization of IR and name management within the compiler's internal representation."
2018-02-09,pytorch/pytorch,"The discussions highlight ongoing challenges with building and installing PyTorch, including difficulties in obtaining proper wheel files for Windows, dependency management issues with PyPI and torchvision, and inconsistencies in build environments across platforms. There are concerns about numerical stability and correctness of functions like `logsumexp` and transformations in distribution modules, with proposed workarounds and the need for native implementations. Performance optimization topics such as MKLDNN integration, CPU tensor operations, and fused kernels are also discussed, emphasizing the importance of memory layout and backend efficiencies. Additional questions pertain to testing consistency across Python versions, handling gradient synchronization on multi-GPU setups, and maintaining type safety in internal data structures, with some issues pending resolution or further investigation."
2018-02-10,pytorch/pytorch,"The discussions highlight challenges in installing PyTorch, with suggestions to use conda as an alternative to pip due to PyPI availability issues. Hardware-related stability problems, such as system reboots caused by power and memory issues during heavy computations, are noted, with resolution often involving hardware upgrades or specific driver configurations. There are ongoing efforts to improve PyTorch features, including release timing, module management (e.g., ModuleDict), and performance optimizations (e.g., replacing division operations or using index_select for efficiency). Compatibility issues with older compilers and standard libraries, particularly on older Linux distributions, are identified, with workarounds involving code changes or environment adjustments. Lastly, compatibility and conformance with ONNX, pybind11, and initialization conventions in neural network layers remain areas of active development and troubleshooting."
2018-02-11,pytorch/pytorch,"The discussions highlight concerns about shared memory limitations, especially in Docker and cloud environments, affecting DataLoader performance; suggestions include increasing `shm-size` and exploring non-Docker solutions. There is ongoing optimization debate for convolution operations, particularly the performance of grouped and depthwise separable convolutions on CPU and GPU, with bandwidth and kernel efficiency being focal points. Efforts are underway to implement in-house mechanisms for auto-determining input/output shapes in modules, to replace workaround code, with considerations for type safety and compatibility. Build and compatibility issues are discussed, particularly related to compiler versions (GCC) and CUDA, where environmental bugs and deprecated features cause compilation failures, necessitating workarounds and improved build checks. Additionally, profiling and API design questions arise concerning indexing performance improvements, type safety in list structures, and backward compatibility with existing API signatures and hooks."
2018-02-12,pytorch/pytorch,"The discussions highlight several technical concerns, including challenges in porting PyTorch to ARM architectures and mobile devices, with suggestions to focus on running training in the cloud and exporting models for inference. There are issues related to the correctness and compatibility of auto-generated functions, particularly with spatial convolution operations and argument handling, indicating ongoing debugging and potential fixes. Performance optimization topics are explored, such as integrating MKLDNN for CPU workloads, improving fused RNN kernels, and enhancing indexing operations for efficiency; some regressions and bugs are acknowledged, with plans for PR fixes. Distributed training raises questions about gradient averaging versus accumulation, especially with MPI backends, and concerns about proper start methods in multiprocessing contexts for stability. Finally, configuration, build environment constraints, and code maintenance (like hooks support, backward compatibility, and code style) are recurrent themes aiming to streamline development, stability, and performance."
2018-02-13,pytorch/pytorch,"The discussions highlight several core issues, including compatibility with different CuDNN versions and the need for supporting older GCC versions due to compilation errors, which involve handling platform-specific bugs and format specifiers. Performance concerns are raised regarding potential slowdowns with a new data loader implementation and overheads introduced by PRs, prompting benchmark assessments and cautious PR merging. Several test failures and bugs are discussed, such as issues with `TransformedDistribution` sign assumptions, distributed training invariants, and specific segfaults or crashes on Windows, indicating areas needing deeper debugging or correctness fixes. There are ongoing efforts to improve code robustness and documentation clarity, including the handling of monotonic transforms, better error messaging for GCC incompatibilities, and clarifying torch Variable usage. Overall, unresolved questions remain around correct distribution computations, cross-platform build stability, and aligning new features with existing performance and correctness expectations."
2018-02-14,pytorch/pytorch,"The discussions highlight several technical concerns, including compatibility issues with different Python versions (notably Python 2.7), C++ compiler limitations (particularly with gcc versions on Linux and macOS), and platform-specific problems such as DLL load failures on Windows. There are recurring challenges related to CUDA and GPU memory management, especially with driver and toolkit version mismatches, as well as compilation errors stemming from outdated or incompatible compiler configurations. Additionally, there are issues with certain PyTorch functionalities, such as in-place tensor views, autograd behavior, and the handling of custom input types, prompting suggestions for improved error messaging and interface design. Unresolved questions include effective workarounds for older environments, strategies for more robust cross-platform compatibility, and plans for refactoring complex features like distributed training and function compilation."
2018-02-15,pytorch/pytorch,"The comments reveal multiple technical challenges and proposals, including the need for improved batching and graph rewriting capabilities in PyTorch, inspired by frameworks like DyNet, to simplify sequence processing and LSTM modifications. There are ongoing efforts to enhance sequence utilities such as `pack_sequence` and `pad_sequence`, including considerations for handling sequence lengths without sorting and managing variable-length sequences in DataParallel. Several discussions focus on build and compatibility issues, such as resolving compiler errors due to SSE/AVX detection, CUDA and cuDNN version mismatches, and compiler version constraints, especially on older systems or when using different environments. The integration of device management, including CUDA and distributed support, is addressed, with suggestions for more intuitive APIs and fallback mechanisms. Lastly, there is interest in refining type and dtype management within ATen, ensuring consistent handling of variables and tensors, and clarifying their interface between Python and C++ components."
2018-02-16,pytorch/pytorch,"The discussions highlight several technical concerns, including the complexity of batching and sequence processing in PyTorch, with proposals for lazy evaluation and autobatching to simplify sequence operations and improve efficiency. There are issues regarding the implementation and optimization of `pack_padded_sequence`, particularly for backward computations, and suggestions to enhance its speed and flexibility, such as handling tensor inputs directly. Users express interest in native CTC loss implementations, citing performance limitations of the current Python-based versions, and suggest integrating cudnn-ctc for efficiency. Threading and data loading performance are also concerns, especially on CPU with multi-core machines, prompting recommendations for more deterministic and faster data pipelines. Lastly, various API usability questions arise, such as handling tensors in `new()`, automatic broadcasting of weights, and error management in reduction functions, alongside discussions on licensing and code stability."
2018-02-17,pytorch/pytorch,"The discussions highlight several technical concerns, including the availability of new functions like `pad_sequence` which requires building PyTorch from source as it is only on master, and recommendations for handling sequence lengths in `pack_padded_sequence` to avoid errors, emphasizing correct usage of `seq_lengths`. Several issues pertain to GPU memory management and potential out-of-memory errors caused by large Python objects (dictionaries, numpy arrays) or certain code patterns, with efforts ongoing to reproduce and resolve these. Documentation clarity is a recurring theme, notably in clarifying bidirectional RNN behaviors, the expected format of `hn` and `c_n` outputs, and the proper usage of loss functions like `KLDivLoss`. Licensing policies and community contributions are also discussed, though less technically focused. Overall, the issues revolve around improving API usability, documentation clarity, error handling, and environment stability."
2018-02-18,pytorch/pytorch,"The discussions highlight issues with handling ignore_index values in loss functions, specifically when labels include values outside the expected range, such as 255. There are concerns regarding the efficiency and implementation of layer normalization in PyTorch, with suggestions to improve performance and clarifications on how mean and std are computed for CNNs based on the paper. Several technical challenges involve debugging segmentation faults, memory corruption, and compatibility issues with CUDA and cuDNN, along with suggestions to update or verify build environments. Additionally, there is debate over licensing policies, particularly the implications of PyTorch’s license and potential benefits of adopting copyleft licenses. Lastly, several bug reports and feature requests request testing, code refactoring, and clarifications to improve stability, documentation, and usability."
2018-02-19,pytorch/pytorch,"The discussions primarily revolve around resolving hardware and kernel configuration issues affecting PyTorch performance and functionality, such as IOMMU settings, ACS enabling, and multi-GPU communication problems. Several users report slow CPU performance and threading issues, with suggestions to adjust OMP_NUM_THREADS and verify hardware configurations. Several feature-related questions are raised, including the implementation of constant export, handling of weight broadcasting in BCELoss, and behavior of torch.split with non-divisible sizes—proposals include automatic broadcasting of per-element weights and inference of remaining split sizes. Fixes for certain bugs (e.g., batch splitting and model CUDA checks) are confirmed, but some questions remain about best practices and handling edge cases. Overall, ongoing improvements and bug fixes are aimed at enhancing robustness, usability, and correctness of PyTorch functionalities."
2018-02-20,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's handling of stochastic nodes, especially supporting subset reinforcement and multiple heads, suggesting potential API redesigns to support more flexible graph representations. Several comments address the challenges of generating accurate model summaries in dynamic computation graphs, with recommendations to leverage external tools like pytorchviz instead of core modifications. Issues concerning tensor and variable support for zero-dimensional or empty tensors highlight the need for consistent behavior and handling of scalars in operations like concatenation. Additionally, performance optimization topics emerge, including integrating MKL-DNN, fused RNN kernels, and addressing GPU-related issues like NCCL groupings and CUDA JIT compilation. Unresolved questions include implementing custom operators, ensuring compatibility across different hardware capabilities, and resolving environment-specific installation and runtime errors."
2018-02-21,pytorch/pytorch,"The discussions highlight several key technical areas: ongoing efforts to add features such as negative sampling and output shape estimation in core PyTorch modules, with debates on their scope and implementation scope; challenges related to installing and using PyTorch on Windows, particularly DLL load failures and environment configuration; performance considerations for data loading, caching, and multi-GPU communication, including the impact of random sleep in data loading and the proper use of NCCL group APIs; issues surrounding CUDA JIT compilation and memory management, with advice on troubleshooting and optimizing GPU resource utilization; and improvements in debugging, code clarity, and documentation, such as formatting issues in error reports and clearer print representations for sparse tensors. Unresolved questions include whether to incorporate output shape functions into core modules, how to balance performance versus debugging or feature extensions, and best practices for environment setup and resource management across varied hardware and software configurations."
2018-02-22,pytorch/pytorch,"The discussions highlight several technical challenges, including improving compatibility and error handling, such as filtering linter errors, handling import errors, and managing build configuration complexities involving conditional compilation and dependencies like CuDNN and NCCL. There are concerns about the correctness and efficiency of implementations, notably in layer normalization, ROI pooling extensions, and gradient computation, with suggestions for better interface designs and benchmarking. Compatibility issues, such as DLL loading failures on different platforms, and the need for clearer documentation or code fixes (e.g., for denormal flags, model saving/loading, and NumPy-like behavior in tensor operations), are frequently raised. Additionally, performance considerations like checkpointing, distributed training setup, and extending PyTorch with custom C++/CUDA extensions are discussed, with some emphasis on maintaining stability and portability across diverse hardware and software environments. Unresolved questions include how best to manage conditional compilation to reduce complexity, and whether new features or fixes have been fully integrated and tested across different scenarios."
2018-02-23,pytorch/pytorch,"The discussions primarily revolve around technical challenges in building and compiling PyTorch from source, including linker errors related to SSE/AVX instructions, and ensuring compatibility with CPU architectures. Several issues highlight the need for improved hardware detection and configuration, such as handling CPU features, differentiating platform-specific behaviors, and managing GPU resources and their properties correctly. Questions also address API usability improvements, like accepting integers in normalization layers and correctly setting device conditions for optimal performance. Additionally, there's concern over build system consistency, including conditional compilation and proper testing via CI pipelines, as well as ensuring extensions and custom modules (e.g., C++/CUDA extensions) are correctly integrated and exposed in Python. Unresolved topics include managing platform-specific bugs, refining detection logic, and enhancing user guidance for device setup and configuration."
2018-02-24,pytorch/pytorch,"The discussions highlight concerns about platform-specific build configurations, such as setting the MACOSX_DEPLOYMENT_TARGET and managing CUDA/cuDNN compatibility issues, emphasizing the need for flexible and reliable build processes. Users seek improvements in performance optimization, including MKL-DNN integration, fused kernels, and GPU utilization, with some addressing benchmarking and profiling tools to analyze hardware bottlenecks. Compatibility and stability issues are a recurring theme, especially regarding data handling (e.g., `volatile=True`, `pin_memory`) and error resilience, including runtime errors like cuDNN version mismatches and kernel crashes. There are suggestions for better managing code maintainability, such as reducing `#ifdef`s, enhancing API design for weight initialization, and simplifying gradient control for custom optimizers. Unresolved questions revolve around streamlining multi-GPU support, enhancing user configurability, and ensuring backward compatibility while integrating new features and performance tools."
2018-02-25,pytorch/pytorch,"The discussions highlight ongoing challenges with Windows build errors related to unresolved external symbols, specifically for CUDA functions like `torch::cuda::lazy_init`, and suggest potential solutions such as refactoring Storage slicing behavior or exposing device properties via pybind. Memory management issues are also addressed, including risks of use-after-free with shared Storage objects and suggestions to improve Storage slicing to prevent these errors, possibly by allocating separate buffers for sliced Storages. There are concerns about CUDA environment detection, including handling old GPU devices and improving build warnings through CMake, as well as optimizing data transfer efficiency with pinned memory during DataLoader operations. Additionally, some issues relate to build system configurations, such as fixing conda-related command errors in specific versions and relying solely on CMake for cuDNN detection. Overall, unresolved questions focus on improving build robustness, memory safety, and exposing hardware properties more conveniently for C++ extensions."
2018-02-26,pytorch/pytorch,"The discussions highlight ongoing challenges with NCCL support, emphasizing the need to use NVIDIA's NCCL2 library for proper multi-GPU communication, as the included version is outdated. There are concerns about handling errors and system stability during distributed training, particularly regarding data transmission failures and the importance of robust error handling or fallbacks such as setting `num_workers=0`. Improvements to core functionalities are suggested, including caching device properties for efficiency, enhancing debugging and profiling tools, refining `autograd` APIs for selective gradient computation, and ensuring compatibility and clarity in tensor data types, such as distinguishing between `uint8`, `bool`, and `int8`. Overall, unresolved questions remain about error handling robustness, performance optimizations, and API usability, indicating areas for future development and refinement."
2018-02-27,pytorch/pytorch,"The discussions address a variety of technical concerns, including package installation challenges, particularly on Windows and with CUDA compatibility, and workflow issues such as GPU device management and multi-GPU training/testing configurations. There are questions about improving API design, including handling gradients, backward functions, and more flexible tracing and model exporting mechanisms. Several issues relate to environment setup, dependency versions, and system-specific problems, such as libc and cuDNN mismatches, which impact reproducibility and stability. Additionally, there are suggestions for API enhancements (e.g., new autograd backward variants), code correctness (implementing missing backward functions), and performance optimizations, alongside unresolved questions about handling large memory allocations and ensuring consistent behavior across different hardware/software configurations."
2018-02-28,pytorch/pytorch,"The discussions highlight several key technical concerns in PyTorch development, including a need to improve API consistency and support for arguments and in-place operations with Variables, addressing complexities around calling conventions for schedulers like CyclicLR, and handling non-Tensor arguments across operations. There are ongoing efforts to enhance the robustness of memory management, especially for CPU inference and convolutional memory usage, with questions about integrating MKLDNN and better reducing memory footprint. Additionally, challenges in extending the tracing mechanisms, specifically managing symbolic overrides and supporting non-Variable inputs for ONNX export, are acknowledged, alongside interface design considerations such as namespace organization for gradient functions. Unresolved questions also concern testing frameworks, code organization (like build system layouts), and maintaining backward compatibility while evolving API features."
2018-03-01,pytorch/pytorch,"The discussions address various technical challenges and proposals within the PyTorch project, such as compatibility issues with older hardware and operating systems, especially on Windows and macOS, and the need for maintaining multiple PyTorch versions for hardware support. They highlight ongoing development efforts, including optimizing kernels with existing libraries like caffe2, integrating C++ extensions, and improving distributed training and random number generation determinism. Several comments focus on build system and CI infrastructure improvements, such as automating tests for CPU-only paths, ensuring stability across different environments, and addressing compilation errors, particularly on Windows and with specific compiler flags. Additionally, there is a strong emphasis on code quality and maintainability, involving code style fixes, refactoring, and enhancing modularity and abstraction layers. Unresolved issues include fixing segmentation faults, build failures, and ensuring comprehensive testing and compatibility, particularly for newer features and platform-specific constraints."
2018-03-02,pytorch/pytorch,"The discussions highlight various technical concerns, including challenges adapting inference code after removal of `volatile=True`, and issues with GPU and multi-GPU deadlocks and hangs potentially caused by I/O, driver, or system configurations. There are performance considerations with convolution operations, kernel grouping, and memory bandwidth, alongside efforts to improve benchmarking, code dispatch, and operator optimization. Several conversations address installation and environment compatibility issues, such as cuDNN version mismatches, TLS object limits, and build system configurations, emphasizing the complexity of ensuring cross-platform stability. Moreover, there are suggestions for extending PyTorch functionality, like precomputing layer outputs, and discussions on code maintenance, tests, and build integration, all indicating ongoing development and stability challenges."
2018-03-03,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's hardware compatibility, such as issues with IOMMU, ACS, and power management on multi-GPU systems, and efforts to improve performance benchmarking and speedups with CUDA and tensor operations. Several comments focus on code robustness and API clarity, including the absence of an `nn.View` layer, renaming or clarifying loss functions like BCELoss versus CrossEntropyLoss, and handling of in-place tensor operations and memory consumption. There's concern over build configuration and cross-platform compatibility, especially regarding extension building on Windows with MSVS and correct linkage, as well as potential bugs in the FFI layer related to file copying on different operating systems. Additionally, some discussions involve documentation accuracy, code quality (e.g., lint errors), and memory usage monitoring, emphasizing the need for better tooling and testing to ensure stability and performance. Unresolved questions remain about fixing specific hardware bugs, proper build settings, and ensuring consistent behavior across different environments and hardware configurations."
2018-03-04,pytorch/pytorch,"The discussions highlight issues with PyTorch's binary compatibility and support for older hardware, such as lack of SSE4 instructions and unsupported processors, suggesting possible source builds as alternative solutions. Several threads address interoperability challenges, including converting pre-trained models to C++ via ONNX and ATen, and the potential development of a comprehensive C++ API akin to the Python interface, which remains a low priority for the team. Problems related to environment setup and DLL loading errors on Windows, often linked to package or dependency mismatches, are also common. Additionally, there is debate over design choices like implementing a `View` layer versus defining reshaping in `forward()` to maintain clean model structures. Finally, concerns about extension build processes, especially on Windows and compatibility of generated headers such as `TensorInfo.cuh`, point to ongoing integration and tooling challenges."
2018-03-05,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's memory management, data parallelism, and compatibility issues, such as memory leaks during training, memory usage leaks with DataLoader, and debugging GPU memory overlaps. There are concerns about the support for older hardware (e.g., compute capability 3.0) and the need for source builds or custom solutions, especially on Linux and macOS setups with specific CUDA and cuDNN versions. Several questions focus on improving the C++ API for deployment, with suggestions for expanding ATen and providing a PyTorch-like C++ interface, though official plans are undecided. Compatibility and stability issues, including version mismatches (cuDNN), and testing benchmarks for performance impacts (e.g., MaxUnpool3d) remain key unresolved areas. Lastly, there’s interest in refining error handling, API interfaces, and ensuring robust testing to prevent regressions."
2018-03-06,pytorch/pytorch,"The discussions highlight several technical challenges, including persistent installation issues on Windows and macOS, often related to CUDA/CuDNN compatibility and environment setup, with suggestions to reinstall drivers or adjust build configurations. Memory leaks and stability problems with DataLoader workers, particularly under multiprocessing and with specific hardware setups, are recurring concerns, with temporary workarounds like setting `num_workers=0`. Error handling and debugging are complicated by ambiguous or outdated error messages, and improvements such as clearer exceptions for device mismatches or tensor creation safety are proposed. There are ongoing efforts to refine API consistency, such as the organization of symbolic functions for ONNX, and to address regression bugs from recent code merges, with some unresolved questions about cross-device indexing support and ensuring safe tensor creation in C++ extensions. Overall, many issues revolve around compatibility, stability, and API clarity, with community suggestions aimed at incremental fixes and better documentation."
2018-03-07,pytorch/pytorch,"The discussions highlight several core technical concerns, including the handling of sparse tensors with zero dimensions, which require API redesigns to support proper size inference. There is a recurring emphasis on improving numerical stability in functions like `arange` by addressing floating-point errors, with suggestions to align behavior with NumPy and implement safer ceiling operations. Workflow and architecture improvements are proposed, such as refactoring `set_grad_enabled` into context managers for better API clarity, and discouraging subclassing internal data loader classes to prevent resource mismanagement. Compatibility and dependency issues are also discussed, notably fixing symbol resolution errors in gloo and ensuring proper versioning of build tools like GCC. Unresolved questions remain around the best way to support variable and tensor integration in extended C++ modules, and how to handle memory overhead and performance regressions in various matrix/tensor operations."
2018-03-08,pytorch/pytorch,"The discussions highlight inconsistencies in indexing conventions between MaxPool2d and MaxPool3d, with suggestions to standardize and document these differences for clarity. Several issues involve deadlocks and memory leaks related to DataLoader's multi-processing and shared memory management, especially on Windows and in earlier PyTorch versions, with recommended solutions including adjusting shared memory settings and ensuring proper process cleanup. There are concerns about the stability and correctness of autograd, particularly with backward computations, memory management, and the impact of new features like autograd::Variable, prompting requests for C++ tests and API enhancements. Compatibility and bug fixes are discussed across CUDA, NCCL, and platform-specific build issues, emphasizing the need for rigorous testing and environment configuration. Lastly, multiple reports address API evolution, documentation updates, and potential architectural reorganizations, indicating ongoing efforts to improve stability, usability, and clarity in the PyTorch codebase."
2018-03-09,pytorch/pytorch,"The discussions mainly revolve around optimizing tensor operations such as flipping along arbitrary dimensions, with suggestions to utilize negative strides or ensure CUDA support. There are concerns about proper stream synchronization in CUDA memory transfers to improve efficiency and correctness. Several issues highlight the need for better memory management, especially regarding memory leaks, Tensor object lifetime, and compatibility with different system libraries or compilation flags like -fPIC. Compatibility and correctness questions also arise around scalar handling, autograd behavior with scalars, and the precise behavior of functions like `arange` and `randperm`. Overall, the key themes focus on improving performance, memory safety, consistent API behavior, and compatibility across hardware and software environments."
2018-03-10,pytorch/pytorch,"The discussions primarily highlight challenges and inquiries related to running PyTorch models on ARM devices and GPUs, focusing on porting models to open-source frameworks like OpenCL for FPGA/No-CUDA applications and challenges in deploying models on mobile devices. There is interest in enabling PyTorch to work efficiently on diverse hardware, including ARM servers and mobile hardware, with suggestions to utilize cloud training and model export formats for deployment. Multiple comments address troubleshooting installation issues, such as setting up CPU-only PyTorch versions without CUDA and handling module-specific problems like BatchNorm behavior and gradient computations. Additionally, some threads involve ongoing code refactoring and testing, emphasizing the need for rebasing and fixing test failures. Overall, the discussions underscore a broad focus on extending PyTorch's hardware compatibility, improving deployment workflows, and resolving environment-specific issues."
2018-03-11,pytorch/pytorch,"The discussions highlight ongoing challenges with compatibility and implementation of sparse tensors in CUDA, especially regarding DataParallel and batching in DataLoader, with proposed solutions involving native aten functions and custom batching strategies. There are concerns about ensuring gradient computations and backward functions work correctly across different PyTorch versions (notably 0.3.1 and master), along with API changes and API compatibility issues, including updating deprecated APIs. Compilation issues related to specific GCC versions and CUDA compatibility, especially on Ubuntu systems, are also prominent, with suggestions to use Docker or specific compiler versions to resolve them. Maintenance tasks such as merging branches, fixing race conditions in DataLoader, and enhancing cross-platform build scripts are ongoing. Overall, unresolved questions revolve around optimizing sparse tensor support, ensuring API consistency, and improving build reproducibility across environments."
2018-03-12,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionality, including adding native support for sparse tensor batching, optimizing summation operations with vectorization and multithreading, and enhancing the API for symbol handling, with considerations for backward compatibility and code organization. There are questions about the best approach to handle optimizer parameter sharing among threads, and considerations for refactoring internal data structures (e.g., separating legacy from current tensor implementations). Concerns are expressed about ensuring compatibility with different hardware environments, such as GPU availability and CPU threading configurations, as well as the proper handling of deprecated features like Variables. Unresolved issues include integrating new features (like shared optimizer parameters, native sparse tensor batching) into stable releases, ensuring environment-variable respect in threading APIs, and maintaining clear, manageable code structure amid expanding functionality."
2018-03-13,pytorch/pytorch,"The discussions highlight ongoing development and refinement of PyTorch's core functionalities, including support for in-place operations on Variables, improved handling of tensor view operations, and expanding support for scalar sources in functions like `index_add_`. There are concerns around memory management, such as multithreaded memcpy and correct handling of reference counting, especially in CUDA and sparse tensors. Additionally, questions about supporting advanced features like named namespaces in ONNX, handling broadcasting/scalar broadcasting in dimension checks, and improvements in gradient computation efficiency are raised. Several issues mention the need for tests, documentation updates, and ensuring backward compatibility, with unresolved questions related to specific API behaviors and optimization strategies."
2018-03-14,pytorch/pytorch,"The discussions highlight several key technical concerns, including the correct usage and compatibility of custom convolution modules like `Conv2dLocal` in PyTorch, with specific issues around function parameter declarations and compatibility with PyTorch's current version. There are questions about improving the API to support more flexible tensor splitting behavior, closer to NumPy's approach, by relaxing size constraints and adding support for index-based splitting, with different proposals under consideration. Multiple issues address framework stability and correctness, such as automating tests for distribution functions, fixing bugs in gradient computations, and ensuring thread management works correctly with environment variables and hardware-specific instruction sets. Concerns also focus on build process robustness, particularly handling compiler-specific flags and missing CUDA headers, especially on Windows, and on improving DataLoader handling to allow non-tensor data, with debates on API design trade-offs. Overall, the discussions reflect ongoing efforts to enhance PyTorch's API correctness, usability, flexibility, and cross-platform stability."
2018-03-15,pytorch/pytorch,"The discussions highlight several key technical concerns in PyTorch development, including the default setting of `torch.backends.cudnn.deterministic` for reproducibility, with some proposing it should be enabled by default due to minimal performance impact. There are ongoing efforts to improve model introspection, such as adding hooks to `str(model)` outputs, and to enhance debugging, especially for sparse tensors and distributed training issues. Licensing and community contribution dynamics are also discussed, with suggestions favoring more permissive licenses like LGPL or AGPL for broader adoption and proper attribution. Other notable topics involve supporting complex object structures in data loading, refining CUDA and CPU backend integrations, and fixing bugs related to tensor operations, multithreading, and environment configurations. Unresolved questions remain about default behavior adjustments, cross-type tensor operations, and ensuring compatibility across different platforms and hardware."
2018-03-16,pytorch/pytorch,"The discussions highlight several key issues in PyTorch development, including the need for a robust, production-ready C++ runtime and API, with plans to improve ATen and possibly develop better C++ interfaces in the future. There is concern over type consistency and proper indexing, especially with newer data structures like `IntList` versus `ArrayRef`, and fixes involve contributions such as QR/LU determinant computations and handling dynamic tensor dimensions for deployment. Debugging GPU memory leaks and ensuring compatibility across systems and installations remains ongoing, with suggestions such as explicit cache clearing and environment management. Accuracy and stability of loss functions like HingeEmbeddingLoss, as well as proper documentation and API usage, are emphasized, alongside unresolved questions about support for heterogeneous total counts and handling of empty tensors in broadcasting. Overall, these threads indicate active efforts to refine core functionalities, improve stability, and extend PyTorch's usability for production scenarios."
2018-03-17,pytorch/pytorch,"The discussions primarily revolve around autograd support for sparse and sparse ops, with some noting that while embedding support is complete, extending autograd to other sparse ops remains a priority. Several issues highlight difficulties with gradient non-differentiability or undefined gradients at zero, especially in root functions and norm calculations, emphasizing the need for warnings or more robust handling. There are technical concerns about GPU memory leaks, process management, and compatibility issues related to system libraries, CUDA versions, and compiler configurations. Additionally, suggestions include implementing batch sampling with varying total counts more efficiently, handling failures and memory management in DataLoader, and ensuring compatibility with different system and library setups. Overall, unresolved questions remain about improving robustness, support for more operations, and system integration challenges."
2018-03-18,pytorch/pytorch,"The discussions primarily address technical challenges related to PyTorch's GPU memory management, specifically the Windows-specific memory leak issues caused by improper handling of shared file mappings and reference counting, with suggestions to simplify the existing implementation. There are ongoing efforts to enhance profiling tools, including the development of custom modules for measuring CPU and GPU execution times, as well as memory usage, though backward compatibility and hook behaviors remain concerns. Several questions revolve around improving the efficiency and correctness of sampling methods, particularly for heterogeneous multinomial distributions, with proposed vectorized solutions to replace slower loop-based approaches. Additionally, there are discussions on expanding available layers and utilities like ROI operations, and ensuring compatibility and correct behavior in distributed and multi-GPU environments. Unresolved questions include handling non-monotonic transformations in distribution functions, establishing best practices for profiling backward computations, and clarifying the implementation of Windows-specific memory handling."
2018-03-19,pytorch/pytorch,"The discussions highlight several core issues, including kernel and memory management bugs such as invalid free errors and shared memory leakages, particularly on Windows and with multi-process data loading, prompting debates on simplifying allocation strategies and refcount handling. There are concerns about numerical precision and compatibility, like the change in ByteTensor behavior, and the need for better type and shape handling to prevent runtime errors, especially when working with empty tensors or deprecated functions. Performance and correctness are recurring themes, with suggestions for more robust testing, automatic assertions, and improved support for heterogeneous distributions, tensor resizing, and schema migration. Additionally, several questions remain unresolved regarding build configurations, dependency management, and platform-specific behavior, indicating ongoing refinement in PyTorch’s functionality and infrastructure."
2018-03-20,pytorch/pytorch,"The discussions reveal ongoing challenges with building PyTorch on PPC64le architectures, particularly related to compiler compatibility and instruction flags (e.g., `-mavx2`, `-mavx`), as well as issues with dependencies like cpuinfo and NNPACK. Multiple comments address build failures and runtime errors, such as infinite compilation stalls on certain CUDA operations, suggesting the need for more robust handling of architecture-specific compiler flags and shared memory management on Windows. There are concerns about performance optimizations, especially regarding memory copying speeds and the handling of tensor operations, with proposals to improve copy functions and support for half-precision tensors. Questions also arise around proper usage of `DataParallel`, ensuring type consistency in tensor operations, and the implications of code changes on existing functionalities. Unresolved issues include compatibility with legacy compilers, ensuring correctness of memory management across processes, and maintaining backward compatibility while introducing new features."
2018-03-21,pytorch/pytorch,"The discussions primarily address technical challenges related to PyTorch's implementation details: the potential need for improved error messages and documentation for DistributedDataParallel issues; handling varied tensor types and data loading errors, especially with misaligned targets or incompatible memory; concerns over the correctness and defaults of the Upsample function with respect to `align_corners`, where some argue current behavior aligns with official specifications and others suggest adding an `align_corners` parameter for flexibility; address potential compilation problems on platforms like ppc64le due to unsupported compiler flags, and ensuring build scripts adapt accordingly; Lastly, there are ongoing efforts to improve robustness and testing, such as adding automatic tests to prevent regressions in kernel behaviors and fixing overflow bugs in tensor operations, with some discussions about build system quirks and code maintainability for expanding functionality."
2018-03-22,pytorch/pytorch,"The discussions highlight ongoing challenges and feature requests within PyTorch, including improving build support on Windows (e.g., including NNPACK, resolving ninja build failures), and ensuring compatibility with diverse platforms like OS X and MacOS versions. There are concerns about the robustness of optimizer parameter management, particularly in relation to parameter ordering and naming, with suggestions to enhance robustness by associating optimizer state via parameter names. Several issues address performance optimizations, such as depthwise convolutions, and functionality enhancements, like making `CycleLR` subclass `_LRScheduler` for batch updates, or upgrading schema migration strategies for performance data. Additionally, unresolved bugs related to specific functions (e.g., `permute`) and compatibility issues with hardware/compilers underscore the need for better test coverage, error handling, and platform support validation."
2018-03-23,pytorch/pytorch,"The discussions highlight several core technical concerns, including the correct implementation of checkpointing and autograd semantics, emphasizing the importance of using imperative `backward()` to ensure proper gradient flow and side-effect safety. There are questions about handling zero-dimensional tensors and their compatibility with operations like `torch.cat`, alongside considerations for consistent support of Tensor structures across NumPy and PyTorch. Additionally, the appropriate use of data types within CUDA and MKL contexts, such as the correct sizing of `long long int` versus `int64_t`, is raised to ensure compatibility and correctness. Finally, suggestions include refactoring complex features like device-bound dtypes into separate APIs or classes for maintainability, and verifying the mathematical correctness of implementations, especially for functions like group normalization and spectral operations."
2018-03-24,pytorch/pytorch,"The discussions primarily revolve around enhancing the flexibility and safety of the PyTorch API, such as implementing batch-wise learning rate schedulers, refining checkpointing mechanisms to prevent side-effects, and adjusting the structure of module representations. There are technical considerations regarding compatibility, such as supporting specific Visual Studio versions, CUDA/CUDNN versions, and building with or without conda, as well as issues with CPU and GPU device configurations. Several threads focus on improving backend implementations by moving functionalities to ATen for efficiency and maintenance, as well as ensuring compatibility and correctness in components like MKL headers and deprecated APIs. Unresolved questions include how to auto-detect variables for effective checkpointing, handling non-trivial subclass representations, and maintaining performance while introducing new features."
2018-03-25,pytorch/pytorch,"The discussions primarily revolve around ongoing bug fixes and feature enhancements in PyTorch, including addressing build errors related to file paths with spaces and parentheses, ensuring deterministic data loading, and implementing new functions like FFT and FFT shift with support for future extensions via cufft and MKL. Several issues highlight build failures, test instability, and system-specific problems such as MPI installation, storage space, and CUDA compatibility, with suggested workarounds like disabling distributed features or updating submodules. There are suggestions to improve usability and functionality, such as adding the `precision_matrix` property to distributions and ensuring backward compatibility with models saved in Python 2. Additionally, unresolved questions include the release timeline for certain features, the presence of functions like `fftshift` in CUDA/cuFFT, and handling model compatibility across Python versions. Overall, the focus is on stabilizing core features, improving system compatibility, and gradually expanding functional capabilities."
2018-03-26,pytorch/pytorch,"The discussions highlight several key technical concerns, including compatibility issues between CUDA 9.1 and newer MSVC versions, with proposed workarounds like using specific toolset commands; the need for better error handling and testing stability, especially regarding floating-point precision and nondeterminism in operations; and ongoing efforts to improve features such as the implementation of batch QR decomposition in MAGMA, bilinear upsampling behavior (with suggestions to add an `align_corners` option), and enhancements to distributed training stability, notably with GPU + Gloo setups. There are also questions about API design choices, such as whether to treat Python floats as doubles or the default scalar type, and device inference based on default dtype, along with comments on build system configurations, like suppressing warnings and fixing CI failures. Several discussions involve code base maintenance, including rebasing PRs, fixing test failures, and understanding build or binary architecture signals. Overall, the community is addressing compatibility, correctness, API usability, and build stability challenges through incremental improvements and ongoing discussions."
2018-03-27,pytorch/pytorch,"The discussions highlight several key technical areas: challenges with compiling PyTorch on Windows, particularly with MSVC versions and CUDA/cuDNN compatibility; performance issues and deadlocks related to multi-threaded data loading (num_workers) and convolution operations, especially with group convolutions and depthwise convolutions using cuDNN versus Thnn; bugs and regressions in tensor operations such as summing and indexing, and the importance of comprehensive testing, including ASAN builds; proposals for API enhancements like better parameter identification in optimizers, more informative __repr__ methods, and improved distribution shape handling; and integration of Caffe2 into the PyTorch repository, requiring clarity on build processes, dependencies, and future plans for ONNX export robustness. Unresolved questions involve optimizing backend implementations, ensuring cross-platform compatibility, and implementing more deterministic and informative error reporting and model introspection."
2018-03-28,pytorch/pytorch,"The discussions highlight several key technical concerns, including troubleshooting installation issues across various Linux distributions, Python version compatibilities, and the importance of accurate documentation for complex features like normal distributions and CUDA support. Several questions address build system challenges, such as resolving errors during compilation, handling library dependencies, and ensuring proper library versioning (e.g., CUDA, MKLDNN). The need for comprehensive testing, particularly for error messages, hardware-specific issues, and regression fixes, is emphasized, with suggestions to introduce more robust test coverage. Additionally, there are ongoing discussions about architecture support, package integration (e.g., Caffe2), and code maintainability, underscoring the importance of clearer error handling, proper dependency management, and compatibility considerations across platforms."
2018-03-29,pytorch/pytorch,"The discussions highlight significant challenges with deadlocks and deadlock mitigation strategies in DataLoader with multiple workers, especially on Ubuntu systems, and suggest adjusting `num_workers`, kernel shared memory settings, or disabling OpenCV threading. Compatibility and type mismatch errors, such as runtime `CPUDoubleType` versus `CPUFloatType`, require clearer error messages or input type handling improvements. Cross-platform build issues involve conditional compilation for AMD GPUs, avoiding fragile patching methods, and ensuring minimal maintenance overhead. Additionally, concerns about GPU memory allocation failures in distributed setups hint at limitations imposed by locked memory or network buffer constraints, especially on Maxwell architectures, and suggest verifying shared directory permissions and proper initializations. Lastly, there are ongoing efforts to improve build configurations, test robustness, and extend support for CPU and CUDA extensions, emphasizing the need for more comprehensive testing and configuration management."
2018-03-30,pytorch/pytorch,"The discussions highlight several technical concerns in PyTorch development, including the need for better support for small batch sizes, device-agnostic tensor operations, and expanding functionality (e.g., support for `nn.LSTM` with `LayerNorm`). Issues with CUDA compatibility, particularly regarding support for older versions like CUDA 7.5 and driver memory errors, are prominent. There are suggestions to improve the API design by separating device and dtype information and to streamline testing processes across different build configurations. Additionally, some discussions focus on fixing low-level build and linking issues, such as dependencies on NNPACK and MKLDNN, and ensuring correct integration and documentation for new modules or functionality. Overall, unresolved questions include compatibility with older CUDA versions, proper dependency management, and API usability improvements."
2018-03-31,pytorch/pytorch,"The discussions highlight several key issues: the unpredictability in argument parsing and shape inference for functions like `torch.randint`, with potential fixes involving overloading or argument validation; challenges with shape inference and shape analysis for operations like `unsqueeze`, especially regarding IR correctness and corner cases; dependency and build configuration concerns around libraries like NNPACK, with proposals to simplify or remove outdated dependencies; serialization and save/load semantics, including compatibility and implementation details for nested objects and data structures; and general maintenance of code correctness, including documentation, testing, and proper handling of corner cases across different functions and modules. Most unresolved questions concern proper shape inference for certain tensor operations, dependency management for performance libraries, and the timeline for feature releases like the upcoming 0.4 version."
2018-04-01,pytorch/pytorch,"The discussions highlight several technical issues and enhancements, including the need for improved GPU support in data preprocessing, with suggestions to leverage CUDA for data augmentation to speed up training. There are ongoing efforts to optimize mathematical operations, such as implementing vectorized `tanh` using AVX2 and integrating it into existing vectorized code paths for performance gains. Several issues involve clarifying error handling and API expectations, such as ensuring consistent argument types for functions like `torch.randint` and resolving build or environment setup problems related to CUDA, compiler ambiguities, and configuration mismatches across different hardware. There is also a focus on maintaining code stability and correctness, exemplified by the development of robust custom autograd functions and the need for detailed tests and documentation coverage. Overall, the discussions emphasize optimizing performance, improving API clarity, and resolving environment-specific build and compatibility issues."
2018-04-02,pytorch/pytorch,"The discussions highlight challenges with static code analysis tools like pylint for dynamic member functions, suggesting configuration solutions such as `generated-members`. Reproducibility issues are addressed, notably the impact of RNG states and dropout randomness on training results, with suggestions to serialize RNG states or seed per epoch. Hardware compatibility and optimization concerns arise, including support for different CUDA architectures, implementation of optimized vectorized functions (e.g., `tanh` AVX2), and merging performance improvements across CPU backends like mkldnn. Several issues relate to bug fixes, version compatibility, and build process challenges, including update management, dependency handling, and feature support for legacy functions or complex tensors. Lastly, there’s ongoing work to enhance Python API usability through functions like `argmax/argmin`, and structural improvements in device/dtype management to facilitate device-agnostic code, alongside testing strategies and release planning questions."
2018-04-03,pytorch/pytorch,"The discussions highlight frequent user concerns regarding import conflicts caused by local directories shadowing installed packages, particularly in the 'test' directory, which can be resolved by changing the working directory prior to import. Thread management and deadlocks, notably when softmax or other operations spawn child threads, are mitigated by setting `torch.set_num_threads(1)`, with suggestions to document this for better user guidance. Compatibility and performance issues are discussed around hardware and software configurations, including CUDA driver versions, GPU types, and the impact of MKLDNN on CPU performance, emphasizing the importance of matching environment setup to hardware and benchmarking conditions. The importance of proper API design is shown by issues around tensor size specifications—clarifying the need for clear documentation on tensor tuple conventions—along with suggestions for API consistency, such as enabling `argmax` and `argmin` with clear semantics. Finally, ongoing development questions include how best to reflect in-place operations convention (adding underscore suffixes), managing sparse tensor modifications, and potential architectural improvements like separating tests into more manageable components for clarity."
2018-04-05,pytorch/pytorch,"The discussions highlight concerns regarding device placement flexibility during model training and evaluation, with some users seeking more dynamic ways to handle models across different GPU configurations. There are technical debates about implementing features like `.max()` with zero-value handling in sparse matrices for graph neural networks, and considerations about integrating adaptive softmax variants and their impact on training/inference consistency. Several issues focus on ensuring correct support and testing of new features, including support for custom samplers, support for various data types in distributed operations, and robustness of model exporting and ONNX compatibility. Other discussions touch on resolving platform-specific compilation issues (e.g., MKL, AVX), memory errors in distributed training, and improving build/test workflows across environments. Overall, the main concerns revolve around enhancing flexibility, correctness, and performance in distributed, sparse, and cross-platform contexts, alongside ensuring robust testing and documentation."
2018-04-06,pytorch/pytorch,"The discussions highlight ongoing development and debugging efforts in PyTorch, including the integration of process group modes, device support, and distributed training challenges. Notable concerns involve CUDA resource management errors, shared memory limitations, and compatibility issues with different hardware (e.g., GPUs, CPUs, Windows builds). Several comments address the need for better documentation, testing infrastructure, and clearer API interfaces in C++ and Python, including support for sparse matrices and new modules. Additionally, there are questions about build system configurations, kernel optimizations, and refactoring of existing functionalities like autograd hooks and ONNX export compatibility. Unresolved issues often relate to runtime errors (e.g., invalid resource handles, shape inference errors), build inconsistencies, and the necessity for enhanced Windows support or specific hardware configurations."
2018-04-07,pytorch/pytorch,"The discussions highlight several technical challenges, including hardware compatibility issues with AMD Threadripper platforms and their kernel configurations, power supply adequacy, and GPU thermal management for multi-GPU setups. There are ongoing improvements and bug fixes related to PyTorch's build process, architecture support, and sparse tensor operations, with particular attention to enabling support for older GPU architectures and enhancing documentation. Some questions concern threading strategies for data loading to optimize performance, as well as maintaining and updating Docker images and pre-built binaries. Additionally, users seek guidance on integrating external libraries like MKL and NCCL, and there's interest in better support for Windows and onnx export workflows. Despite progress, unresolved issues remain around build support for older or unsupported architectures, editing sparse tensor index handling, and ensuring reproducible, bug-free deployment across varied system configurations."
2018-04-08,pytorch/pytorch,"The discussions highlight ongoing challenges with implementing negative stride support in CUDA for tensor flipping, considerations for switching depthwise convolution backend from THNN to cuDNN for performance improvements, and issues related to bandwidth-bound performance despite kernel size changes in Conv3d operations. There are concerns about ensuring deterministic behavior in cuDNN algorithms, particularly when benchmarking is enabled, and questions about the proper use of public APIs and debugging build issues such as multiple linkage errors and compiler warnings. Additionally, there are discussions about code compatibility, especially with Python 3 on Windows, and how to manage file modifications and environment-specific dependencies in the build and setup processes. Overall, unresolved questions pertain to performance tuning, backend optimizations, and ensuring cross-platform compatibility."
2018-04-09,pytorch/pytorch,"The discussions highlight several key technical concerns, including the correctness of causal convolution implementation in PyTorch, especially regarding padding and dilation, and whether padding can achieve true causality for intermediate elements. There are issues with distributed training stability and errors, particularly with Gloo backend and Gloo+GPU configurations, such as socket closures and invalid resource handles, which affect multi-GPU and multi-shard setups. Compatibility problems are raised with CUDA versions, compiler versions (GCC, Clang), and library dependencies like NCCL and OpenMP, impacting build stability and performance. Additionally, questions about the proper handling of deterministic algorithms in cuDNN, the design of the C++ module interface, and the build configurations for NCCL and other extensions are discussed, alongside ongoing efforts to improve code maintainability and documentation clarity. The unresolved questions focus on ensuring correctness and stability in distributed training, verifying proper dependency management, and clarifying intended design patterns for module abstractions."
2018-04-10,pytorch/pytorch,"The comments highlight ongoing issues related to environment setup, particularly with MSVC toolset configurations, and dependencies like CUDA, cuDNN, and NCCL, affecting build stability and correctness. Several discussions concern PyTorch's internal implementation, such as adding support for locally connected layers, improving autograd hooks, and optimizing sparse matrix operations, along with the need for better API consistency and documentation updates. There are also multiple questions about specific bugs (e.g., batch normalization with small batch sizes, backward errors in cuDNN) and build system intricacies, including ninja dependency and conda environment conflicts. Some comments suggest code refactoring and performance optimizations, such as replacing std::vector with SmallVector for size and stride storage, and enhancing build scripts and extension mechanisms. Unresolved questions mainly revolve around environment reproducibility, compatibility, and ensuring correctness in complex scenarios like extending PyTorch's APIs or integrating with ONNX, with several issues pending further validation or patching."
2018-04-11,pytorch/pytorch,"The discussions highlight ongoing challenges with the PyTorch build process, including environment setup issues such as missing MSVC toolsets, incorrect generator configurations, and the need for cleaning previous builds before recompilation. Several comments focus on improving user experience, like enhancing error messages for tensor operations, handling pickled models across Python versions, and addressing CUDA memory errors on specific GPU architectures. There is also interest in expanding functionality—adding support for complex numbers, refactoring data structures for efficiency, and implementing missing features like `fft` and `istft`—as well as maintaining consistency with external libraries like NumPy. Many unresolved questions remain about compatibility, performance optimizations, and proper documentation updates for new features or user workflows."
2018-04-12,pytorch/pytorch,"The discussions highlight several key concerns: implementation and maintenance of complex tensor support in PyTorch, with proposals to incorporate complex numbers as an additional shape dimension rather than a new Tensor type to balance performance and maintainability; challenges related to building and installing the framework, such as environment misconfigurations and proper build procedures; issues with GPU-related dependencies, such as NCCL and MAGMA version mismatches, and their impact on performance; and ongoing efforts to improve API consistency, code structure, and build system integration, including filtering repository components and supporting multi-threaded or cross-device inference. Additionally, there are unresolved questions about compatibility, API naming conventions, and the future integration of C++/Python interoperability."
2018-04-13,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) the process for installing a custom-compiled PyTorch package, particularly from source with MAGMA, and whether to use `python setup.py install` or pip wheels; (2) clarification and documentation updates around `nn.NLLLoss2d` and `nn.Softmax2d`, especially their functional equivalents, to reduce user confusion; (3) proposals for implementing advanced batching and sequence processing in PyTorch inspired by Dynet, including lazy evaluation and autobatching, with ongoing development noted; (4) considerations around integrating Cephes functions for probability distributions, with suggestions to include more functions and possible GPU implementations; and (5) various issues around environment setup, such as CUDA, MKL, and library conflicts, alongside build configuration challenges, including permission errors and compatibility concerns with different MKL versions and MATLAB."
2018-04-14,pytorch/pytorch,"The discussions primarily address challenges related to model serialization and deserialization, emphasizing the importance of including class definitions when loading models saved with `torch.save(model, ...)` versus `torch.save(model.state_dict(), ...)`. Compatibility issues are evident, especially concerning version mismatches of cuDNN and CUDA, which cause runtime errors in GPU computations and neural network modules like `nn.RNN`. There are ongoing efforts to update and improve code quality, including the refactoring of distributed training interfaces, ensuring code conventions, and handling serialization edge cases such as `Undefined` tensors. Additionally, installation and environment setup issues, particularly those involving Caffe2 and system paths, are discussed, highlighting the need for clean, up-to-date builds to prevent runtime errors."
2018-04-15,pytorch/pytorch,"The discussions primarily revolve around enhancing the flexibility of learning rate schedulers, specifically integrating batch-wise scheduling like `CyclicLR` into the `_LRScheduler` framework while addressing API and attribute consistency issues. Several technical challenges involve ensuring proper environment configurations for CUDA and cuDNN compatibility, including managing include paths, versions, and environment variables such as `MKLROOT` and `PYTHONPATH`. Users also face memory management issues with models like ResNet on larger datasets and hardware, with suggestions to try stable branches or reinstall dependencies like TBB. Additionally, there is ongoing development of C++ modules and API refactoring, emphasizing a phased, experimental approach, and identifying existing bugs or version mismatches, particularly concerning glibc requirements and process management on Windows. Unresolved questions include how to streamline API changes for schedulers, manage environment setup intricacies, and ensure compatibility across hardware and software configurations."
2018-04-16,pytorch/pytorch,"The discussions largely revolve around enhancing PyTorch's functionality while managing complexity and maintainability, such as implementing complex tensor support within the existing tensor API to avoid the overhead of a new tensor type, and introducing utility functions like `logsumexp` for numerical stability with various implementation variants. Key concerns include ensuring compatibility and performance, particularly with GPU operations, and handling system-specific issues like CUDA version mismatches, glibc dependencies, and hardware limitations. Several proposals involve API design choices, such as appropriate methods for attaching/detaching gradients or modules, and clarifying behaviors of functions like `to()` with respect to dtype, device, and numpy integration. There are ongoing efforts to improve test coverage, CI infrastructure, and documentation accuracy, especially related to CUDA/ cuDNN compatibility and operator implementations. Unresolved questions include the best approach to support complex number computations, manage backward compatibility for new API styles, and correctly synchronize asynchronous GPU kernels to avoid race conditions."
2018-04-17,pytorch/pytorch,"The comments encompass various technical concerns, including API design decisions such as the shift to using `requires_grad=False` instead of `volatile=True`, and discussions about implementing a C++ production runtime or improving ONNX and Caffe2 integrations. There are ongoing evaluations of GPU performance optimizations, including enhancements to cuDNN and fused RNN kernels, as well as compatibility with different hardware and CUDA versions. Several issues address the need for clearer documentation and testing automation, especially related to graph passes and code examples, as well as robustness in distributed training setups and error handling. Overall, key unresolved questions involve the development of a stable, efficient C++ API foundation, proper support for dynamic shapes, and ensuring reliable, maintainable code and documentation practices."
2018-04-18,pytorch/pytorch,"The discussions address several key technical concerns: (1) Proper handling of class labels and data ranges to prevent errors in training and evaluation; (2) Clarification of DistributedDataParallel's gradient reduction process, emphasizing gradient synchronization through all-reduce operations rather than parameter broadcasts, raising questions about module state consistency across machines; (3) Precision and numerical stability issues in linear algebra operations such as `trtrs`, `potrs`, and their reliance on LAPACK, with suggestions to improve accuracy by using double precision or constructing matrices with larger eigenvalues; (4) Enhancements to tensor API features, including support for named tuples, max operations, and efficient non-contiguous tensor handling, with suggestions for API design and potential layout toggles; (5) Broader infrastructure and compatibility issues, including CUDA and Eigen library errors, GLIBC version constraints, and build system configurations, highlighting the need for clean-up, fixes, or alternative build options to ensure seamless deployment."
2018-04-19,pytorch/pytorch,"The discussions highlight issues related to class label handling in segmentation tasks, emphasizing the necessity of ensuring labels are within the [0, n_classes - 1] range and the correct use of `.long()` for target tensors. There are concerns about CUDA runtime errors, device-side asserts, and potential GPU memory leaks, often linked to invalid label values or power supply limits. Several questions address PyTorch feature development, such as support for sparse autograd, the implementation status of sub-graph optimizations, and plans for C++ interfaces and custom operators. Compatibility issues with Caffe2, NCCL library configurations, and build system adjustments are also discussed, alongside licensing considerations for the project. Overall, the conversations focus on debugging runtime errors, feature enhancements, and improving framework flexibility and reliability."
2018-04-20,pytorch/pytorch,"The discussions highlight several key technical concerns, including handling class label values in training (notably target values starting from 0), and the importance of correctly converting targets to long type to prevent range errors during loss calculation. There are ongoing questions about enhancing the usability of distributed training, such as the necessity of parameter synchronization timing and the support for different backends like MPI, NCCl, and Gloo, along with stability issues in multi-machine setups. Performance optimization concerns also surface, notably recent improvements in tensor indexing and the speed regression associated with certain slice operations. Lastly, issues related to backward and forward compatibility of serialized models, and the proper configuration of environment dependencies like OpenCV and NCCL, are recurrent themes, along with suggestions for code documentation consistency and build process improvements."
2018-04-21,pytorch/pytorch,"The discussions primarily revolve around troubleshooting and optimizing PyTorch usage, including issues with environment variables like CXX, CUDA versions, and hardware compatibility, which impact installation and performance. Several threads highlight runtime errors such as CUDA out-of-memory, deadlocks during data loading, and specific operator bugs (e.g., nn.RNN, SpatialNarrowAs), with solutions ranging from code fixes, environment adjustments, hardware BIOS updates, to building from source or software recompilation. There are concerns about GPU support for certain modules, multi-threading configurations, and support for openMP on macOS, with suggestions to switch compilation methods or use alternative libraries. Indexing performance improvements and the handling of model intermediate layers are also discussed, indicating ongoing efforts to enhance usability and efficiency. Finally, unresolved questions include compatibility of nightly builds with Jupyter, threading behaviors, and operator-specific bugs, with some solutions pending implementation or requiring further validation."
2018-04-22,pytorch/pytorch,"The discussions highlight persistent technical challenges in PyTorch, including the difficulty of properly resolving hooks registration on modules with submodules, and performance issues with fused kernels and Jacobian-vector products, particularly on `vgg19_bn`. There are ongoing efforts to improve kernel support for operations like softmax/backward, and to refactor functions such as logsigmoid and log_softmax for consistency, with questions about naming conventions and legacy support. Memory management concerns are raised around GPU resource allocation, especially on large models or specific hardware configurations, while discussions around AMD support emphasize branch maintenance versus mainline development. Additionally, compatibility issues with CUDA versions and compiler support are noted, alongside suggestions for code refactoring and API consistency to enhance stability and performance."
2018-04-23,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's support for different hardware configurations, including support for Compute Capability 5.0, macOS build stability, and CUDA support, emphasizing the need for better binary distributions and cross-platform compatibility. Performance bottlenecks are noted in complex operations such as Jacobian-vector products with models like VGG, due to the absence of custom kernels for batch normalization’s double backward, and in graph manipulations like subgraph optimization and kernel fusion. Several issues pertain to memory management, including CUDA Out-Of-Memory errors, the impact of checkpointing on memory usage, and the need for more efficient indexing functions and in-place operations. Additionally, concerns about CI stability, environment setup, and build configurations indicate the need for more robust testing and documentation to improve development workflows."
2018-04-24,pytorch/pytorch,"The discussions highlight ongoing improvements and challenges in PyTorch's features, including the addition of a scalar autograd type, device management (`to` method), and support for complex indexing and view operations, with some debates on design choices like the necessity of explicit view layers. Several performance issues are identified, such as slow Jacobian vector products in models like VGG, and the need for optimized kernels (e.g., for depthwise convolutions and negative stride support), with suggestions for leveraging JIT compilation. Compatibility and correctness concerns are also raised, including issues with merging Variable and Tensor types, handling of mixed GPU devices, and ensuring proper propagation of types and shapes through ONNX export and subgraph optimization. Additionally, there are discussions on integrating AMD support, ensuring stable handling of high-precision or edge cases (e.g., infinities), and maintaining codebase consistency amid evolving APIs and hardware-specific patches. Overall, the community is focused on enhancing robustness, performance, usability, and cross-hardware compatibility, while debating best practices for design and implementation."
2018-04-25,pytorch/pytorch,"The comments cover a range of technical concerns and updates, including difficulties in installing and building PyTorch with specific CUDA and cuDNN versions on Windows 7 and other OS, with solutions such as building from source or using specific package versions. Several issues address bugs and feature requests, such as adding support for batched `gesv` and inverse operations, improving autograd support, fixing compile errors, and enhancing tensor operations like `inverse_stft`. Discussions also mention build stability and CI-related failures, the need for implicit type casting, handling of sparse tensors, and improving debugging and logging outputs. Additionally, there are ongoing efforts to streamline complex installations, optimize memory usage, and maintain compatibility across different environments, with some issues pending fixes or further review."
2018-04-26,pytorch/pytorch,"The discussions highlight several technical issues, including GPU memory consumption and efficiency concerns with native local convolutions versus unfold-based implementations, and difficulties with sparse tensor operations, especially for backpropagation and backward derivatives. Several questions focus on proper usage and debugging of PyTorch features, such as ensuring all tensors are CUDA-enabled, handling scalar tensors correctly, and managing library dependencies and compatibility (e.g., compiler ABI issues, NCCL communicator limits, and C++ headers). There is interest in enhancing user experience through improved tensor printing, error messages, and clear documentation about in-place operations, type casting, and model export formats. Some discussions address back-end bugs and platform-specific issues, such as CUDA kernel bugs with GCC versions and ONNX export errors, alongside proposals for API improvements like implicit type casting and differentiability at boundary points. Unresolved questions involve better integration of custom operations, more robust handling of float NaNs, and extending functionality like inverse STFT or native local convolutions."
2018-04-27,pytorch/pytorch,"The comments highlight several technical challenges and considerations in PyTorch development, including the difficulty of properly implementing backward hooks and the need for clearer documentation and behavior consistency for gradient functions like `register_backward_hook`. There are ongoing efforts to enhance features, such as batching for `gesv`, native support for local convolutional layers, and fixes for memory leaks and GPU memory management. Multiple discussions address compatibility issues with different compiler versions, CUDA/cuDNN setups, and extension module imports on various platforms, notably Windows. Questions also arise about default behaviors like `size_average` in loss functions, and whether certain features (e.g., `torch.eval()`) or API design choices (e.g., `requires_grad` on non-float tensors) should be adjusted for clarity and correctness. Overall, the issues reflect active efforts to improve PyTorch's robustness, usability, and consistency."
2018-04-28,pytorch/pytorch,"The discussions highlight several technical concerns, including the unbounded nature of certain loss functions (e.g., negative Dice, combined cross-entropy and Dice losses) and their implications for training stability and interpretation. Memory management on GPUs is another issue, with details on cache clearing, freeing unreferenced memory, and the potential dangers involved. There are ongoing performance and implementation challenges, such as optimizing CPU-based RNNs, thread management with OpenMP, and vectorization bottlenecks in functions like logsoftmax, as well as issues with the JIT compiler, including graph export/import reliability and dropout support. Additional issues focus on improving tensor display/readability, ensuring cross-platform compatibility for extensions (notably on Windows), and handling specific errors related to device support, undefined tensors, and input validation for functions like multinomial. Unresolved questions include how to better handle loss functions' bounds, provide user-friendly tensor introspection, and maintain compatibility across different Python and C++ API versions."
2018-04-29,pytorch/pytorch,"The discussions highlight various technical concerns including GPU device placement issues with JIT-compiled models, where arguments on different GPUs cause runtime errors, suggesting a need for improved tensor device management and testing. There are questions about the eigenvalue computation in `torch.eig`, particularly handling matrices with complex or imaginary eigenvalues, and whether the eigenvector outputs should be pruned or improved for complex cases. Tests related to sparse initialization, specifically the correctness of sparsity patterns, reveal potential bugs in test assertions and implementation logic. Additionally, there are build and environment compatibility issues, such as missing dependencies like MSVC Redis or CUDA, and user concerns about performance and stability on different platforms like macOS and Windows, often related to library versions or build configurations. Overall, efforts are ongoing to fix bugs, improve test robustness, and ensure cross-platform stability, with some unresolved issues requiring further investigation."
2018-04-30,pytorch/pytorch,"The discussions highlight several technical concerns, including the ambiguity and correctness of `register_backward_hook` functionality in PyTorch, with suggestions to remove or improve its implementation and documentation. Issues with hardware and environment setup are noted, such as NCCL communication problems across multiple GPUs, often related to network detection or device configuration, and the impact of BIOS or kernel settings. Various bugs and regressions are addressed, including performance regressions in `max()` operations, build failures on iOS and macOS, and complications in ONNX export related to operator versioning and domain handling. Additional suggestions involve expanding test coverage, particularly for new or complex functionalities like the C++ API, IR export, and collapse dims, as well as improving build system configurations for smoother development workflows. Unresolved questions focus on ensuring compatibility across platforms and devices and preventing regressions in future releases."
2018-05-01,pytorch/pytorch,"The discussions highlight ongoing challenges with implementing and utilizing data parallelism in PyTorch, including user concerns about non-blocking and deterministic operations like `index_add`, as well as the integration of custom parallel criterion modules without prior installation issues. There is a recurring theme of addressing performance bottlenecks and correctness, such as the inefficiencies with atomic operations, numerical stability in softmax implementations, and the handling of non-contiguous tensors. Several technical bugs and regressions are discussed, including issues with CUDA/cuDNN compatibility, distributed NCCL setup errors, and runtime failures related to device management, with suggested solutions including source builds, environment configurations, and code patches. Additionally, organizational and maintenance concerns are raised, such as the standardization of naming conventions, improving testing frameworks, and clarifying non-deterministic function usage. The overall focus is on improving performance, correctness, usability, and code maintainability in PyTorch's evolving codebase."
2018-05-02,pytorch/pytorch,"The discussions cover several technical concerns including the support for weight decay with sparse gradients, with suggestions for explicit alternative optimizers or error handling when used with sparse tensors. There are ongoing efforts to improve multi-GPU system stability, power management, and hardware configurations, especially for recommendation models and large-scale training. Several issues address performance optimizations such as indexing, kernel profiling, and memory efficiency, alongside bug fixes, JIT regressions, and build system improvements—particularly around MKL, ONNX symbolic handling, and static/dynamic extension registration. Questions about compatibility and installation problems frequently arise, notably regarding NVIDIA GPU management, CUDA versions, and package dependencies, with some workarounds involving Docker or manual configuration. Additionally, there is interest in refining error handling, API consistency, and reproducibility, alongside ensuring future-proofing of features like autograd on sparse tensors and tensor export macros."
2018-05-03,pytorch/pytorch,"The discussions mainly focus on technical challenges related to optimizing functions such as `lbeta`, `softmax`, and `logsoftmax`, including considerations for vectorization and backend-specific implementations like MKL-DNN and Sleef. There are concerns about the compatibility and build system issues, notably integrating external libraries such as Sleef on various platforms, and handling unresolved symbol errors, especially on Windows and when using AVX-512 instructions. Several questions address the correctness and stability of gradient computations, e.g., in `BatchNorm`, `LSTM`, and sparse/dense multiplication, as well as issues with randomness, reproducibility, and deterministic behavior in parallel data loading. Build and deployment configurations, including RPATH management, CMake improvements, and issues with static registration, are also discussed. Unresolved questions include how to dynamically optimize thread usage with OpenMP, ensuring forward/backward compatibility during model checkpointing, and the nature of build failures related to external dependencies."
2018-05-04,pytorch/pytorch,"The discussions highlight several technical concerns, including the difficulty of extending the core RNN classes in PyTorch, with suggestions to improve modularity and performance, notably around custom LSTM variants and gradient truncation. Issues related to numerical reproducibility and deterministic behavior, especially across different environments and with different backend libraries (like MKL or numpy), are prominent, with recommendations for better seed management and compatibility. Compatibility and performance problems also emerge with CUDA, cuDNN, and third-party libraries (e.g., sleef), as well as build errors on various platforms, leading to suggestions for source-level fixes, build configuration adjustments, and more robust testing. Additionally, the conversation touches on API design questions, such as device inference in tensor creation, access to internal modules, and ensuring backward compatibility across PyTorch versions. Unresolved questions involve maintaining deterministic behaviors, improving extensibility of embedded components, and managing platform-specific build issues."
2018-05-05,pytorch/pytorch,"The discussions highlight ongoing challenges with building and deploying PyTorch on macOS, particularly concerning CUDA support and binary distributions, due to limited NVIDIA GPU support and compatibility issues with Xcode and system dependencies. There are technical considerations around improving memory efficiency and tensor representations, such as the potential introduction of `BoolTensor` to better support binary neural networks and ONNX compatibility. Some comments address performance and correctness bugs, such as optimizing initialization routines, handling non-contiguous NumPy arrays, and fixing memory leaks, often requiring code refactoring or better abstractions. Additionally, there is concern over build environment configurations, including proper linking of libraries and ensuring consistent RNG seeds in tests. Overall, the issues revolve around improving cross-platform compatibility, runtime efficiency, and code robustness in the PyTorch ecosystem."
2018-05-06,pytorch/pytorch,"The discussions highlight persistent challenges with Windows compatibility for PyTorch, including DLL load failures and build issues, often related to compiler and environment configurations such as MSVC version and CUDA toolkit compatibility. Several users experience import errors stemming from mismatch or missing dependencies, suggesting that proper environment setup and potential workarounds (like renaming certain files or adjusting build flags) are crucial but not fully straightforward. There are concerns about tensor behaviors, such as stride assumptions in `from_numpy`, and how PyTorch handles non-contiguous arrays, indicating a need for explicit checks or improved support for varied memory layouts. Additionally, questions around device inference, optimizer parameter tracking, and model serialization reflect conceptual uncertainties needing clearer documentation or design clarifications. Overall, unresolved technical hurdles emphasize the importance of environment configuration, build correctness, and consistent API behavior across platforms."
2018-05-07,pytorch/pytorch,"The discussions highlight several key technical issues: users encounter CUDA device functions errors (error 8) often due to incompatible PyTorch versions with their GPU's CUDA capability, emphasizing the need to upgrade and proper version compatibility checks; there are concerns about data layout transposition for performance optimization and the impact of user-visible layout options, with suggestions for graph rewriting and JIT-based solutions; batch normalization behavior differs between training and evaluation modes, with questions on how to enforce batch statistics during inference; compilation challenges arise with specific compiler and CUDA versions, especially on Windows with MSVC and older gcc versions, impacting kernel and library builds; additional topics include managing static initializers for plugin registration, ensuring RNG consistency across different CPU features, and proper handling of memory sharing in CPU/GPU parameter workflows."
2018-05-08,pytorch/pytorch,"The discussions highlight several technical challenges, including improving error messages related to tensor shape mismatches and data type expectations, especially when replacing or copying weights in models. There are ongoing efforts to optimize performance for operations like softmax and logsoftmax via vectorization, with considerations for caching MKL-DNN memory layouts on different platforms, particularly Windows support. Users face issues with CUDA memory access errors, potentially due to bugs or driver/infrastructure misconfigurations, and there are questions about proper handling of numpy strides, tensor versioning, and data sharing semantics. Concurrency and resource management concerns, such as the number of NCCL communicators and their impact on distributed training, are also debated. Lastly, there is interest in flexible API designs for parameter updates and optimizations, including allowing per-step parameter modifications and better error handling for model input dimensions."
2018-05-09,pytorch/pytorch,"The discussions involve extending PyTorch's learning rate scheduler API to support batch-wise updates, with proposed subclassing `_LRScheduler` and refactoring cycle parameters like `step_size` for clarity and correctness. There is interest in implementing input-dependent padding utilities similar to TensorFlow's ""SAME"" padding, with considerations around stride and dilation, and potential support for asymmetric padding in convolutional operators. Additional concerns include optimizing C++ autograd operators through C++-implemented ops for speed gains, handling CUDA and cuDNN compatibility issues across different GPU setups, and improving IDE/editor support for better code navigation and documentation, especially in relation to `torch._VariableFunctions`. Several issues are unresolved, notably around CUDA version stability, ONNX export limitations (e.g., `align_corners` attribute in bilinear upsampling), and maintaining a flexible, public API that balances hackability with clarity. Overall, key themes focus on API usability enhancements, backend stability, and performance optimization across CPU, CUDA, and other accelerators."
2018-05-10,pytorch/pytorch,"The discussions highlight several technical challenges and considerations in PyTorch development. Key concerns include thread safety and deadlock issues related to multiprocessing start methods, third-party library incompatibilities (e.g., OpenCV, libstdc++), and the handling of tensors with zero dimensions, especially for operations like `norm`, `cat`, and `dot`, with debates on appropriate gradient behaviors and API support. There are also questions about compatibility and stability across different hardware setups, such as GPU configurations and MKL linkage, as well as suggestions for API improvements like exposing module members and adding new fused operations. Furthermore, maintenance topics such as build system adjustments, test coverage, and documentation clarity are discussed, alongside unresolved bugs and proposed workarounds. Overall, these threads reflect ongoing efforts to improve robustness, performance, and usability, often balancing backward compatibility with new feature proposals."
2018-05-11,pytorch/pytorch,"The discussions highlight various technical challenges, including handling CUDA errors such as CUDNN_STATUS_NOT_SUPPORTED and OOM issues, with suggested solutions like parameter adjustments, workspace size management, and device settings. There are concerns about the stability and efficiency of low-level operations, notably the implementation of softmax/logsoftmax optimizations, with proposals for streaming algorithms, specialized functions, and performance comparisons involving Sleef. Compatibility and correctness issues also recur, such as ensuring reproducible RNG behavior across CPU architectures, managing tensor sparsity types, and addressing potential bugs in autograd and distributed training setups. Additionally, community-driven contributions and licensing considerations are discussed, emphasizing the importance of core framework features like synchronized batch normalization, extensible LSTM implementations, and licensing strategies to promote adoption and collaboration. Unresolved questions remain around optimizing low-level kernels, robust error handling, and ensuring framework stability across diverse hardware and use cases."
2018-05-12,pytorch/pytorch,"The discussions highlight several technical concerns in PyTorch development, including restrictions in advanced indexing support (notably myriads of cases that don't work), and issues related to memory management and CUDA operations, such as RuntimeError: CUDNN_STATUS_EXECUTION_FAILED and out-of-memory errors, for which workarounds like setting the device explicitly are suggested. There are ongoing improvements in core functions like Im2ColNd and Col2ImNd to optimize performance by relocating parameters to host memory, and enhancements proposed for the pairing of pack_padded_sequence and pad_packed_sequence in ONNX export to ensure correct model serialization. Compatibility issues are frequently mentioned, including Python version differences affecting worker shutdowns and dependency conflicts impacting build stability. Overall, unresolved questions remain around performance impacts of certain calls, precise accuracy targets for mathematical functions like tanh, and ensuring correct pairing of operations during model export."
2018-05-13,pytorch/pytorch,"The discussions encompass a range of technical concerns including the renaming of tensor dimension parameters for clarity, and limitations in support for advanced indexing in different PyTorch versions, with full support introduced in 0.4. Several issues address build failures and undefined symbols linked to MKL, Gloo, and CUDA, prompting suggestions for better error handling, testing, and dependency management. Memory leaks in layers like MaxUnpool3d and MultiLabelMarginCriterion were detected and are to be addressed through improved leak detection mechanisms, which impact test durations. There are ongoing conversations about improving tensor operations such as fold/unfold, and considerations for the design of sparse tensor types — whether to treat dense/sparse dimensions as flexible or strict, and how to handle conversions. Finally, questions remain about integrating reparameterization techniques, adding unit tests, and refining model-saving approaches, with attention to ensuring robustness and efficiency across different system configurations."
2018-05-14,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for clearer device management and documentation regarding tensor placement and DataParallel usage, as well as potential improvements to error handling and user warnings in functions like multinomial sampling under extreme logits or probabilities. There are ongoing debates about API design choices, such as whether to add new functions or modify existing ones for better performance or usability, for example, in sparse tensor behavior, RNN cell implementation, or handling of RNG independence from CPU features. Issues with compatibility and support for older CUDA architectures, NCCL versions, and environment configurations are also discussed, emphasizing stability and build reproducibility challenges. Additionally, there are unresolved questions about proper serialization of device objects, expanding test coverage for new features, and whether certain updates should depend on upstream fixes or version bumps, reflecting a focus on robustness, clarity, and performance optimization."
2018-05-15,pytorch/pytorch,"The discussions highlight ongoing challenges with DLL load failures and import errors of PyTorch on Windows, typically due to version mismatches or misconfigurations, emphasizing the need for proper environment setup and compatibility. Several issues involve build and installation problems, particularly around CUDA, ROCm, and support for specific hardware or library versions like MAGMA 1, with suggestions to improve build scripts, dependency management, and error messaging. There are technical queries related to model serialization across devices, tensor operations such as Cartesian products, and enhancing error messages for operations like BatchNorm and convolutions, indicating areas for API and usability improvements. Some discussions focus on refining PyTorch’s internal APIs, operator support (e.g., pow, asin), and extending testing coverage, especially for complex features like sparse tensors and ONNX integration. Overall, the main concerns involve ensuring cross-platform stability, proper environment configuration, and expanding the feature set with clear, helpful diagnostics and comprehensive tests."
2018-05-16,pytorch/pytorch,"The discussions highlight ongoing development and refining of core PyTorch functionalities, with particular focus on implementing flexible neural network components like Maxout layers, handling deterministic behavior, and improving API consistency for dropout and batch normalization layers. Several issues address compatibility, performance, and robustness concerns, such as fixing flaky tests, ensuring compatibility with CUDA/cuDNN versions, and resolving non-determinism in GPU computations. There is also much deliberation on extending PyTorch's utility through new features like meshgrid, cartesian and combination ranges, and improved parameter management (like calculated parameters and reparameterization hooks). Unresolved questions include defining the internal API and semantics for tensor types (dense vs. sparse), enhancing user-friendly APIs for reproducibility, and integrating complex tensor operations natively into ATen for broader application support."
2018-05-17,pytorch/pytorch,"The discussions highlight several key technical concerns: the management of file descriptors and shared memory leaks in data loading and inference workflows; the need to improve documentation for tensor operations like `.view()` and device handling in modules; performance comparisons and optimization strategies for tensor permutation and random permutation operations, emphasizing warm-up and benchmarking best practices; compatibility issues with NCCL, CUDA versions, and compiler dependencies, especially related to GPU communication libraries and system libraries on Linux and Windows; and suggestions for modular and flexible design in parameter initialization, module device management, and internal functions like `index()`, alongside unresolved issues with PyTorch's backward hooks and internal kernel implementations. Overall, the discussions focus on improving framework robustness, performance, and usability through better tooling, documentation, and system compatibility."
2018-05-18,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionalities, including the lack of bicubic interpolation support with autograd, optimization of softmax/logsoftmax operations via streaming algorithms and potential code fusion, and device-specific handling of data types and tensor operations to improve performance and usability. There are technical considerations around transposing matrices, integrating with CuDNN, and managing sparse versus dense tensors, with some debates on API design, such as exposing internal functions like `index` or handling device inference in tensor creation. Performance tuning issues, especially on Windows and specialized hardware like the TX2, involve adjusting compilation flags, thread management, and launch parameters, and ensuring compatibility across backends and architectures. Several unresolved questions pertain to efficient implementation of complex operations (bicubic interpolation, streaming softmax), the proper inference of device contexts, and the correct handling of certain tensor edge cases and serialization concerns."
2018-05-19,pytorch/pytorch,"The discussions highlight concerns about shared memory and file descriptor leaks in dataloader multiprocessing, with Ivan Prado identifying potential shared memory leaks via open shm files and open file descriptors. There are questions about the non-fPIC compilation of OpenMP in PyTorch binaries, which could improve portability and performance if addressed. Several issues involve improving dynamic type support and serialization robustness, such as singleton management of numpy dtypes and handling PyTorch-specific dtype serialization. Timing performance comparisons of unary operations on torch tensors are shared, showing some improvements but also regressions after patches. Lastly, maintenance and feature considerations involve clarifying the necessity of certain exposed functions (like `index`) and managing internal legacy classes, with some discussions about dependencies, build fixes, and future-proofing features like device-bound dtypes."
2018-05-20,pytorch/pytorch,"The discussions highlight concerns about computational bottlenecks in CPU-based linear algebra, especially with MAGMA on AMD Threadripper CPUs, and potential GPU execution improvements, such as running SVD and symeig exclusively on GPUs. There are questions regarding proper device inference when creating tensors, particularly with numpy arrays, and the complications introduced by conflicting pip and conda installations of PyTorch. Several issues relate to ensuring correct behavior and clarity in autograd, including the needed fixes for gradgradcheck and handling second derivatives efficiently, as well as improving error messages for edge cases like batch std with insufficient samples. Additionally, there are implementation considerations for extending functionality, such as integrating weight normalization into RNNs and managing device-specific code paths, and ongoing efforts to optimize kernel launches on specialized hardware like Jetson TX2."
2018-05-21,pytorch/pytorch,"The discussions highlight several core technical concerns: potential file descriptor leaks in multi-process PyTorch inference, particularly involving shared memory and multiprocessing strategies; the need to improve memory error handling and error messaging for CUDA, index operations, and model serialization issues; challenges with the flexibility and safety of in-place operations and in-depth tensor sharing (e.g., deepcopy and gradient behaviors); performance considerations related to threading, GPU kernel register usage, and the impact of optimization passes like loop unrolling; and infrastructure or API design questions, such as module parameter naming conventions, CUDA driver compatibility, and serialization restrictions, that influence usability, efficiency, and robustness of PyTorch's internals and exporting capabilities. Many issues also involve making error messages clearer, updating or fixing deprecated features, or refactoring APIs for better safety and extensibility. Unresolved questions concern how best to handle side-effect annotations for tracing, improve multi-device synchronization, and streamline build configurations amid evolving dependencies."
2018-05-22,pytorch/pytorch,"The collected comments from the GitHub issues highlight several key technical concerns, including the need to improve file handling in torch.save when writing compressed files, as current implementations bypass gzip and cause corruption. There is discussion about implementing or optimizing CTC in PyTorch, either via cuDNN or alternative Python implementations, with considerations of speed and numerical precision. Several issues address software environment and hardware problems, such as CUDA and driver compatibility, GPU memory errors, and multi-threading performance bottlenecks, often seeking workarounds or better error messages. It is also noted that certain API behaviors, like tensor element access and operator overloading (e.g., transpose, permutation), require clearer documentation or more consistent support aligned with numpy, along with suggestions for optimizing kernel compilation and memory management. Unresolved questions include how to extend autograd to generate derivatives from symbolic expressions, handling in-place operations in automation, and structural design choices for modules and internal APIs, indicating ongoing development and design debates."
2018-05-23,pytorch/pytorch,"The discussions highlight ongoing challenges and enhancements in PyTorch, including the need for backward-compatible tensor flipping with negative strides, support for int8 NumPy conversions, and handling non-contiguous tensors efficiently, especially on CUDA. Several questions address the correctness and performance of neural network operations such as batch normalization, tensor transposition, and stochasticity in CuDNN, emphasizing the importance of consistency and speed in implementations. There are technical considerations about API design, notably the behavior of `transpose`, `permute`, and the removal of `__hash__`` methods for tensors used as dictionary keys, aiming for more numpy-like semantics. Discussions also touch on debugging support, build system issues on macOS, and maintaining backward compatibility while improving multi-GPU and low-precision supports. Throughout, the community seeks clarifications and fixes to advance robustness, usability, and performance in PyTorch’s core and extensions."
2018-05-24,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's usability and consistency, such as clarifying and revising the behavior of functions like `permute()`/`transpose()` to align better with NumPy and ONNX standards, and adding a default behavior for `x.transpose()` to reverse all dimensions. Several issues concern the behavior of reduced or empty tensors in indexing, loss functions, and autograd, with suggestions to better handle edge cases, warn users, or change API expectations. There are technical challenges related to CUDA kernel compilation, register limits, and the handling of sparse autograd support, highlighting ongoing development and debugging efforts. Additionally, discussions highlight the need for clearer serialization, threading interactions, and device management, with proposed solutions ranging from code refactoring to API tweaks. Unresolved questions include the support timeline for sparse autograd, best practices for API consistency, and debugging GPU-related synchronization issues."
2018-05-25,pytorch/pytorch,"The discussions highlight the need for more modular, object-oriented design in PyTorch's RNN implementations, advocating for distinct Cell and State classes to improve extensibility and polymorphism. There is concern over the current API inconsistency for tensor manipulation, particularly with the transpose and permute functions, suggesting potential renaming or API adjustments to better match NumPy conventions. Performance bottlenecks are noted in advanced gradient computation techniques—such as per-sample gradients—due to quadratic scaling with tensor size, with proposals to optimize calculation, possibly via sparse or on-the-fly strategies. Various build and compatibility issues are discussed, especially relating to platform-specific compiler and library configurations, emphasizing the importance of robust dependency management. Overall, the focus is on improving API clarity, architectural flexibility, computational efficiency, and build system reliability."
2018-05-26,pytorch/pytorch,"The discussions highlight significant concerns about the architectural design and extensibility of PyTorch's RNN modules, criticizing the reliance on inheritance, ""if-elif-else"" chains, and the tight coupling of parent and child classes, which hinder flexibility for custom RNN cells and states. Some contributors suggest adopting a modular OO approach with abstract base classes like `RNNCell` and `State`, enabling more dynamic and extendable implementations, as exemplified in their custom code. There are also technical issues related to CUDA kernel resource management, such as register overuse and the need for fixes like `__launch_bounds__`, as well as challenges with Windows DLL dependencies and installation complexities. Additionally, performance bottlenecks when computing per-sample gradients via auto-diff techniques are discussed, with suggestions for more efficient strategies and potential overhead in large models. The overall consensus emphasizes the need for cleaner, more flexible API designs and improved tooling to support diverse RNN architectures and efficient computation."
2018-05-27,pytorch/pytorch,"The discussions primarily revolve around numerical stability and debugging challenges in training neural networks with PyTorch, such as issues with BatchNorm hyperparameters, gradient clipping, and the need for proper input ranges (e.g., using `BCEWithLogitsLoss` instead of `BCELoss` to handle logits correctly). Several threads highlight hardware and platform compatibility problems, including CUDA support on macOS, linker errors during build processes, and driver or system configuration issues affecting CUDA functionality. There are also questions about extending PyTorch features, such as implementing `sparse * dense` tensor multiplication and providing examples for transformed distributions. Additionally, some discussions address bugs related to data loader randomness and engine internal crashes, emphasizing the importance of correct setup and initialization. Unresolved issues include platform-specific build errors, missing documentation entries, and potential internal engine inconsistencies."
2018-05-28,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA and GPU support on macOS, including build failures due to incorrect framework links and linker errors, with suggestions to adjust environment variables and reconfigure build parameters. There are concerns about compatibility and behavior divergence between CPU and GPU implementations, particularly in error messaging, input size handling, and kernel optimizations, with suggestions to unify dimension checks and leverage tensor stride manipulations. Several issues involve integrating third-party libraries such as cuDNN, CUDA frameworks, and ONNX/Caffe2 tools, often complicated by environment setup, library paths, and system configurations, especially on macOS and Windows platforms. There’s a call for more precise error messages, performance improvements, and better testing, including the addition of explicit dtype arguments, validation for flipped tensor operations, and minimal repro cases. Still unresolved are compatibility fixes for CUDA versions, linker errors related to library paths, and ensuring environment-specific configurations are handled gracefully for builds and runtime execution."
2018-05-29,pytorch/pytorch,"The discussions highlight several key concerns, including the proper timing and state management for learning rate schedulers like ReduceLROnPlateau, as well as issues with building and debugging CUDA support on macOS due to incompatible compiler and library configurations. Several queries address improving error messages for CUDA runtime errors, especially when handling infinities or invalid tensor inputs, with suggestions to add input validation or more descriptive exceptions. There are ongoing efforts to enhance PyTorch’s interoperability with ONNX, requiring version bumping and shape transformations, alongside concerns about ensuring backward compatibility and proper testing for new features. Additionally, multiple discussions focus on implementation details such as the integration of distributed training, performance regressions potentially caused by threading libraries, and the design of reparameterization or custom parameter handling within modules. Overall, unresolved issues revolve around build stability, better error diagnostics, feature implementation strategies, and maintaining API consistency amid evolving internal and external standards."
2018-05-30,pytorch/pytorch,"The discussions highlight several key technical concerns including challenges with building and installing PyTorch and Caffe2 across different platforms (e.g., Windows, macOS, Linux), particularly issues with dependencies, environment configurations, and build options such as static vs. shared libraries. There are recurring questions about optimizing performance, especially regarding CPU benchmarks, the integration of MKLDNN, and efficient tensor operations like flipping, which involve considerations of memory layout and hardware acceleration. Compatibility issues are also raised, such as differences in `DataParallel` handling for RNNs and the need for ordering in module parameters, alongside concerns about legacy code maintenance, including deprecation paths for `torch.Tensor` and build system changes. Lastly, there is interest in improving the robustness of testing, especially parallelization and scalability of test suites, and addressing unresolved bugs and build failures related to dependencies and environment inconsistencies."
2018-05-31,pytorch/pytorch,"The discussions primarily concern troubleshooting installation issues with legacy Python versions and environment inconsistencies, highlighting the importance of correct module placement and build environments. There is ongoing work to improve numerical stability and correctness, such as fixing negative values in `nn.BCELoss()` and enhancing shape inference and shape analysis in the JIT compiler, especially for `unsqueeze` and `chunk`. Performance benchmarking and optimization are also emphasized, with efforts to address CPU and GPU throughput, threading, and kernel launch overhead, including kernel flattening and tensor flipping methods. Additionally, issues related to build system configurations, external dependencies (like TBB and MKL-DNN), and compatibility across different platforms and architectures are discussed, alongside proposals for better error messaging, documentation, and API consistency. Unresolved questions remain about supporting dynamic input shapes in certain modules, proper handling of tensor size dependencies during tracing, and specifications for features like per-input `dim` in `DataParallel`."
2018-06-01,pytorch/pytorch,"The discussions primarily center around resolving ongoing technical challenges and enhancements in PyTorch, including fixing dependency conflicts and build issues (e.g., linking against multiple OpenMP libraries and CUDA framework linking on macOS). There are efforts to improve performance, notably reducing tensor allocation overhead and managing threading behavior to prevent oversubscription, with detailed benchmarking and system tweaks. Several feature requests and potential integrations are examined, such as incorporating advanced mathematical functions (incomplete gamma, Meijer G, hypergeometric functions) and supporting more flexible or device-agnostic prediction mechanisms. Issues with backward compatibility, API clarity (e.g., loss reduction flags), and detailed error messaging are also discussed to improve usability. Many unresolved questions involve ensuring build stability across platforms, optimizing performance with thread and memory management, and formalizing support for specialized mathematical functions and new API behaviors."
2018-06-02,pytorch/pytorch,"The discussions highlight persistent challenges with the `register_backward_hook` functionality, with the consensus that it should be temporarily disabled or removed due to correctness and complexity issues, and whether this change should affect all modules or specifically container types. There are technical difficulties in accurately capturing or manipulating input gradients, especially in nested or sequential layers, and potential enhancements in documentation or API design are considered. Several issues concern build environmental dependencies, particularly on macOS and CUDA, emphasizing the need for proper compiler and library configurations. Other topics include code style improvements, including replacing `globals()` with more Pythonic patterns, and addressing flaky tests and backward compatibility in features like DataParallel and ONNX integration. Unresolved questions remain around proper API behavior, documentation clarity, and environment setup for varied platforms."
2018-06-03,pytorch/pytorch,"The discussions highlight several technical concerns including the lack of CUDA support for `torch.histc()` in the `THC` backend, with requests for updates, workarounds, and explanations of blocking issues. Support for autograd with sparse tensors remains incomplete, especially for sparse matrix operations like `mat-vec` and `mat-mat`, which is critical for applications involving sparse data such as point clouds. Users also encounter issues with CUDA on Mac OS X, possibly due to dynamic library dependencies, and reporting problems like runtime errors when initializing CUDA with GPU. Additionally, there are suggestions to improve code practices, such as using `type()` for tensors, and proposals for ecosystem enhancements, like a lightweight model repository within PyTorch to facilitate easier model sharing and testing. Overall, unresolved questions revolve around improving backend support, compatibility, and ecosystem tooling to better accommodate advanced use cases."
2018-06-04,pytorch/pytorch,"The discussions highlight multiple technical concerns including optimizing custom correlation operations in PyTorch by leveraging GEMM or transposition techniques, with considerations on backward calculation performance. There are questions about improving sparse tensor layouts and formats, as well as handling non-deterministic cuDNN behaviors and their documentation. Several issues relate to compatibility, build processes, and runtime stability, such as CUDA framework linking, library dependencies, and ensuring correct reproducibility and numerical precision. Suggestions for enhancing API consistency, like `torch.stack` broadcasting and tensor shape handling in DataParallel, are also presented. Unresolved questions focus on best practices for memory-efficient tensor formats, CUDA kernel optimizations, and ensuring robust build and runtime environments across platforms."
2018-06-05,pytorch/pytorch,"The discussions highlight ongoing challenges with GPU support for certain functions like `torch.histc`, prompting requests for status updates and workarounds. There are technical considerations around supporting operations involving parameter pre-calculation, such as spectral or weight normalization, and how to efficiently enable differentiability and joint parameter updates. Several comments address debugging build issues, particularly related to dependencies, environment configurations, and compiler/linker errors across platforms like macOS and Windows, with proposed troubleshooting steps. The thread also covers refining interfaces for tensor printing, reducing confusion around shape behaviors in DataParallel, and extending functionality with additional mathematical functions like hypergeometric and incomplete gamma, emphasizing the importance of clean APIs and robust support for CPU/GPU operations. Unresolved questions remain regarding support for specialized math functions, the correct handling of in-place operations and inferences in complex modules, and improving build diagnostics and environment compatibility."
2018-06-06,pytorch/pytorch,"The discussions highlight several key issues: (1) the implementation and support of gradient clipping APIs like `clip_grad_norm`, with questions about proper parameter choices for beginners; (2) incomplete GPU support for `torch.histc()` and debate on the implementation difficulties and potential strategies; (3) compatibility and integration challenges surrounding ONNX export, including versioning, operator support, and cross-framework operation support; (4) build and deployment complications on various platforms, including macOS, Windows, and various CUDA or protobuf configurations, often tied to environment and dependency management; and (5) enhancements to the API and internal diagnostics, such as handling tensor shape variations in DataParallel, details of autograd (like `grad_fn`), and the support for sparse tensor formats and device contexts, with ongoing discussions about balancing transparency, user-friendliness, and internal complexity."
2018-06-07,pytorch/pytorch,"The discussions highlight several technical concerns, including incomplete implementation of specific functionalities such as GPU-supported histograms, CDF-related functions, and true convolution operations, with ongoing efforts and proposals to support these features. There are recurring issues related to build system configurations, particularly with MKL, protobuf, and CUDA dependencies, often requiring reconfiguration, recompilation, or workarounds like changing import order or environment variables. Performance optimization topics are also prominent, such as efficient bincount implementations, parallel tensor operations for time-distributed computations, and in-place hook management for parameters. Several discussions address build failures, compatibility across different platforms, and nuanced behavior of internal modules, along with suggestions for code refactoring and API improvements. Unresolved questions include ensuring seamless support in specific environments (e.g., macOS, Windows, conda) and refining API behaviors like representation and parameter updates."
2018-06-08,pytorch/pytorch,"The discussions highlight several key technical issues: the integration of third-party implementations like YellowFin into PyTorch requires review and cleanup; installation challenges—such as resolving pip/failures, CUDA compatibility, and protobuf dependencies—suggest the need for clearer documentation and build instructions; build system modifications, including CMake fixes, submodule management, and dependency handling, are critical for smoother compilation; internal API transitions from TH/THC to ATen involve complex refactoring, necessitating careful planning for backward compatibility and extension support; finally, stable and reproducible behavior—addressing issues like deterministic cuDNN settings and proper device tensor usage—remains essential for reliable deep learning workflows."
2018-06-09,pytorch/pytorch,"The discussions highlight ongoing technical challenges such as ensuring reproducibility and determinism in CUDA/cuDNN operations, with suggestions like enabling `torch.backends.cudnn.deterministic = True`. Compatibility and build issues are frequently addressed, including dealing with compiler flags, environment configurations, and dependency management (e.g., MKL, NCCL, Anaconda, CMake versions). Several reports pertain to DLL loading errors and installation difficulties on Windows environments, with proposed workarounds involving DLL replacements or environment modifications. There are structural considerations regarding threading behavior during backpropagation, addressing concurrency concerns in the autograd engine. Lastly, questions about build configurations and the maintenance of code contracts, especially around backward passes and their thread safety, indicate ongoing efforts to improve stability, clarity, and reproducibility of PyTorch's core system."
2018-06-10,pytorch/pytorch,"The discussions highlight several key issues: (1) confusion and lack of clarity regarding the in-place `bernoulli_()` function semantics and documentation, with suggestions to clarify default parameters and in-place behavior; (2) debates on whether gradient averaging across distributed ranks should be averaged or accumulated, and concerns about convergence slowdown potentially caused by internal state communication; (3) platform-specific build failures and compatibility issues on macOS, specifically related to CUDA support and linking errors, with suggestions for better CI coverage and build configuration fixes; (4) questions about the automatic differentiation mechanism in PyTorch, specifically the calculation of gradients via reverse mode AD and its relation to Jacobians; and (5) some issues with external dependencies such as protobuf and onnx versions affecting build stability, with proposed temporary workarounds. Overall, unresolved questions include documentation improvements, the necessity of internal state synchronization in distributed training, and platform support boundaries."
2018-06-11,pytorch/pytorch,"The discussions highlight ongoing issues related to thread and library conflicts, notably the `libgomp` and `libiomp` linkage problem, which was claimed to be addressed after version 0.3.1, yet performance still degrades with environment variables like `OMP_NUM_THREADS`. Several feature requests and enhancements are debated, such as implementing output shape inference methods for modules, support for sparse-dense matrix multiplication, and better APIs for descriptor classes, though some are deferred due to complexity or demand. Compatibility and build environment issues arise, including CUDA version upgrades, compiler configurations, and dependencies management across platforms (e.g., Windows conda setups). Additionally, some bugs and stability concerns, like `potrf` failures on non-symmetric matrices and multithreading backward contracts, are being diagnosed or fixed, with suggestions for improved testing and API stability. Overall, these discussions reflect efforts in bug fixing, performance optimization, API enhancement, and environment configuration to improve PyTorch's robustness and usability."
2018-06-12,pytorch/pytorch,"The discussions highlight several technical concerns including the need for improved error handling and overflow detection in tensor operations, as exemplified by the request for NaN and overflow warnings similar to NumPy's error settings. There are ongoing efforts to enhance PyTorch's robustness, such as better memory management, deterministic behavior, and more informative error messages, particularly in multi-GPU contexts and memory allocation. Multiple issues concern the integration and compatibility of C++ extensions, build system configurations (e.g., CMake, MSVC runtime, environment variables), and ensuring proper support for features like weight normalization, scripting, and JIT compilation. Some discussions focus on improving API consistency, user ergonomics (e.g., default tensor types, deprecated features), and validation via regression tests. Unresolved issues include addressing certain build failures, implementing new features like broadcasting or in-place hooks, and ensuring backward compatibility and reproducibility in multi-GPU and distributed training setups."
2018-06-13,pytorch/pytorch,"The discussions highlight several core technical concerns, including handling out-of-range labels in loss functions (Issue #1204.0) where ensuring target labels are within class range is crucial, and performance optimizations in autograd and tensor operations (Issues #8409.0, #8410.0) that require careful design to avoid bugs like incorrect zeroing assumptions or handling undefined inputs. There's a recurring emphasis on build and environment setup challenges on diverse platforms (Issues #8364.0, #8368.0, #8369.0), emphasizing the importance of reliable compiler and dependency configuration. Concerns about code clarity and API design are raised, notably regarding the use of macros in C++ modules (Issue #8347.0) and the need for transparent, maintainable APIs. Lastly, multiple issues address bug fixes in existing features, such as correct handling of variable types, build system corrections, and tests for recently introduced functionalities, citing unresolved problems like CUDA errors, segmentation faults, and build inconsistencies that require further investigation or refactoring."
2018-06-14,pytorch/pytorch,"The discussions highlight several key concerns including device and data range errors in training due to out-of-range labels and tensor formats, the need for clearer API conventions especially around learning rate schedulers and tensor device queries, and performance regressions observed after updates to PyTorch on various platforms. There are recurring issues with build environment configurations, notably related to CUDA, protobuf, and compiler compatibility, which affect both CPU and GPU operations and often result in crashes or incorrect behavior. Several discussions focus on improving error messages for device-side assertions to facilitate debugging, and optimizing specific operations like flip and bincount for different memory hierarchies. Lastly, there are ongoing efforts to refine code structure, such as the removal of deprecated functions, simplifying tensor options, and improving serialization and autograd features, alongside addressing build stability and compatibility concerns across different environments."
2018-06-15,pytorch/pytorch,"The discussions largely revolve around improving PyTorch's usability and robustness, including simplifying device management by defaulting to GPU execution, and addressing threading and compatibility issues with third-party libraries like OpenCV and Caffe2. Concerns are also raised about pipeline efficiency, such as data loader initialization and memory consumption, especially for large datasets or edge cases, with proposed solutions including early iterator preparation and streamlining autograd handling. Several technical bugs are highlighted, notably related to CUDA, compiler compatibility on macOS, and distribution distributions, with workarounds like BIOS updates or manual library linking suggested. Ongoing issues include ensuring backward compatibility, handling scalar and tensor shape edge cases, and avoiding build failures due to environment mismatches or compilation flags. Unresolved questions focus on refining schema deprecation, optimizing autograd and stream management, and ensuring stable, performant builds across platforms."
2018-06-16,pytorch/pytorch,"The discussions primarily revolve around enhancing and troubleshooting PyTorch's functionality, including efforts to support OpenCL for FPGA/No-CUDA applications and debates on the viability of OpenCL versus proprietary solutions for GPU acceleration. There are concerns about implementing specific operations like `Conv2d` in OpenCL, as well as issues related to distribution functions such as `log_gamma()` and `MultivariateNormal`, with suggestions to improve numerical stability and shape handling, and questions about potential errors or crashes. Additionally, discussions explore optimizing data loading and training efficiency, including preloading data iterators to reduce TTFB in large-scale training. Some issues relate to build failures, dependency management, and ensuring code compatibility across different Python versions and compiler setups, with ongoing work to fix and improve these aspects."
2018-06-17,pytorch/pytorch,"The discussions primarily focus on enhancing the PyTorch learning rate scheduler API, particularly by introducing a batch-wise `batch_step()` method to unify scheduling approaches across different implementations, and clarifying the `step_size` parameter to improve user understanding. There is also debate about how to implement per-batch learning rate updates effectively and whether to subclass or modify existing classes for such functionality. Additionally, some conversations address NCCL-related issues with multi-GPU setups, where environment variables and configuration files impact GPU communication behaviors, with users reporting failures and seeking troubleshooting guidance. Other threads discuss consulting the ATen library's internal mechanisms, such as obtaining the correct `THCState`, but face technical hurdles related to API access and proper context handling. Overall, key unresolved questions include optimizing scheduler APIs for batch updates, understanding and configuring NCCL for multi-GPU communication, and integrating low-level ATen functionalities for advanced GPU stream management."
2018-06-18,pytorch/pytorch,"The discussions highlight ongoing development challenges and feature considerations in PyTorch, including the delayed support for complex numbers in `torch.fft` and the potential benefits of supporting `torch.as_tensor` to avoid unnecessary data copies and deprecate `torch.Tensor`. There are concerns about API consistency, especially regarding the naming and behavior of learning rate schedulers compared to frameworks like Keras, and questions about the best way to implement dynamic argument handling and op schemas, such as the use of `*` in function signatures. Performance and compatibility issues are addressed, notably TBB and OpenMP conflicts, CMake build warning flags, and multithreading optimization. Additionally, discussions cover API ergonomics, backward compatibility, and build system stability, with some topics marked for further investigation or awaiting PR reviews."
2018-06-19,pytorch/pytorch,"The discussions cover various technical issues including the implementation and testing of new tensor functions (e.g., `mean` support for ByteTensors), ensuring correctness and performance in autograd and distribution modules, and resolving bugs in distributed training setups and memory management. Several questions arise about the integration of third-party libraries like TBB and NCCL, including performance regressions and static initialization bugs, with plans to reintroduce TBB after addressing conflicts. Concerns are also raised about API design choices, such as the use of macros in C++ modules, and the need for clearer, more robust error messages and test coverage for edge cases like shape inference and GPU device management. Some unresolved issues involve debugging segmentation faults, addressing gradient mismatch failures, and ensuring platform compatibility—especially on Windows—through CI testing. Overall, the discussions aim to improve function implementation, optimize performance, and enhance API usability and stability."
2018-06-20,pytorch/pytorch,"The discussions highlight several key technical issues: the need for clearer handling of model state prefixes in DataParallel (e.g., removing 'module.' prefix), with scripts provided for manual correction; performance and memory implications of data loader prefetching and early iteration preparation; compatibility and error handling in various hardware and software environments, such as CUDA NVCC version, GCC version discrepancies, and multi-GPU setups; concerns about ensuring correctness and stability in autograd operations, especially regarding zero-sized tensors, undefined inputs, and gradient calculations with expand/contiguous/clone semantics; and considerations for maintaining backward compatibility and simplifying API design, such as the use of tensor constructors versus specific tensor types, and the management of global versus thread-local context. Unresolved questions include how to best handle numerical accuracy issues, improve error messaging and testing automation, and standardize certain features across backends and APIs."
2018-06-21,pytorch/pytorch,"The discussions highlight several key concerns: issues with loss function configurations, particularly how negative or scaled dice loss is computed and whether to divide by the number of classes; challenges with PyTorch build and environment setup, especially compiler mismatches and ensuring the correct compiler is used during compilation; the need for improved testing and validation of CUDA stream synchronization and device consistency in autograd, with suggestions for better stream management and explicit synchronization; uncertainties regarding schema matching and function support in TorchScript, especially for operators like min, max, and split with various argument types; and questions about using newer functions like `logsumexp` in earlier PyTorch versions, prompting workarounds or updates to the API."
2018-06-22,pytorch/pytorch,"The discussions highlight ongoing challenges with supporting complex numbers in PyTorch, with plans to utilize C10 to facilitate this support. Several issues pertain to build system complexities, especially relating to CUDA version compatibility, compiler (gcc) errors, and dependencies like Eigen, protobuf, and ONNX, often requiring patches, environment adjustments, or workarounds. There are concerns about thread safety and global state management, with proposals for a context abstraction that allows flexible, atomic global and thread-local configurations. The development process also involves addressing build failures, improving test coverage (notably on CUDA and distributed components), and ensuring consistency with upstream tools like CMake. Unresolved questions include optimal strategies for dispatch table immutability, handling library reloads, and balancing backward compatibility with newer environment requirements."
2018-06-23,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch's implementation and usability, including updating network parameter names when modifying model structures (Issue #872), advancing the LBFGS optimizer with line search methods (Issues #938 and #7066), and streamlining installation processes across different environments (Issues #7318, #8787). Critical technical concerns involve CUDA build configurations, particularly with specific GPU architectures and CUDA versions (Issues #8734, #8785, #8787, #8822), as well as ensuring correct behavior of functions like `output_padding` in transposed convolutions (Issues #8816, #8826). Several issues also address debugging and stability, such as build errors, GPU memory errors, and inconsistent model behavior during save/load cycles (Issues #8781, #8820, #8818, #8820). Unresolved questions center on proper handling of environment variables during build, documentation clarity, and ensuring consistent, reliable model training and inference workflows."
2018-06-24,pytorch/pytorch,"The comments highlight ongoing technical challenges related to GPU memory management, particularly out-of-memory errors during training and validation steps, with suggestions to ensure proper variable containment and context management (e.g., torch.no_grad). There are concerns about the incomplete support for sparse autograd, which is critical for specialized domains like point cloud processing, indicating a need for better sparse tensor autograd support. Build and compatibility issues are frequently discussed, such as dependencies on specific CUDA versions, issues with cuDNN, Eigen headers, and CMake requirements, emphasizing the importance of maintaining compatible build environments and backward compatibility. Additional points include bug fixes for specific hardware and software combinations, improvements in API consistency (like the `.to()` method), and proposals for testing and error message handling to improve robustness and developer experience. Overall, unresolved questions revolve around improving memory efficiency, expanding sparse support, ensuring build stability across platforms, and refining API consistency."
2018-06-25,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionalities, including implementing advanced line search algorithms, handling memory leaks, and enhancing API parity with frameworks like TensorFlow. Key technical concerns involve refining the global and thread-local context management for ensuring thread safety, and the need for consistent, reliable behavior in tensor operations (e.g., split, load/save, and tensor device handling). Several issues address build system robustness, dependency management, integration of C++ extensions, and ensuring compatible, efficient, and portable implementations across different platforms and CUDA versions. Unresolved questions pertain to design choices such as the synchronization of the dispatch table, the handling of dynamic vs. static context updates, and deprecation strategies for certain APIs. Overall, these dialogues reflect active development to enhance stability, performance, and API consistency within PyTorch."
2018-06-26,pytorch/pytorch,"The discussions highlight several technical concerns including clarifications around tensor copying and the semantics of the `copy` parameter in `torch.tensor`, with requests for explicit documentation. There are ongoing efforts to improve convolution padding behaviors, such as supporting ""same"" padding and asymmetric paddings, along with questions about consistency and API design, especially when accommodating stride and dilation complexities. Compatibility issues related to CUDA versions, compiler toolchains, and GPU backend bugs (notably NVCC and cuDNN bugs fixed in CUDA 9.2) are also prominent, with solutions involving environment configuration and precise error messaging. Additionally, there are discussions about code organization, such as how to properly expose functions in the Python API via C extensions and ensuring proper build processes on various platforms. Finally, some comments address broader API usability and correctness, including naming conventions for variables, handling zero-sized tensors, and the correct interpretation of device arguments."
2018-06-27,pytorch/pytorch,"The discussions highlight ongoing technical challenges such as handling library dependencies and build configurations, notably with MKL and CUDA versions, which impact compilation and runtime behavior. There are concerns about code readability and consistency, such as variable naming conventions within mathematical expressions and the printing of tensor modules, with debates on standardization. Other key issues involve debugging complex behaviors like the inheritance of RNG states across subprocesses, the accuracy of timing GPU operations, and the correctness of specific function implementations (e.g., `log1p`, broadcasting strategies). Several discussions revolve around improving development workflows, such as rebasing best practices, managing submodule updates, and ensuring tests are robust against environmental inconsistencies. Unresolved questions remain about handling sparse tensor operations, improving distributed process management, and standardizing API behaviors across different backend configurations."
2018-06-28,pytorch/pytorch,"The discussions highlight ongoing efforts to extend PyTorch with advanced recurrent architectures like convolutional and peephole LSTMs, with questions about official support and implementation details. There is concern about various bugs, such as memory leaks, runtime errors, and segmentation faults, particularly when handling sparse tensors, CUDA compatibility issues, and memory management problems across different GPU architectures and driver versions. Several issues relate to testing robustness, including precise gradient comparisons, floating-point tolerances, and CI environment stability, especially on platforms with limited memory or specific driver/hardware configurations. There are questions about API design choices, such as exposing private parameters, dynamic module type checking, and the proper way to override or bundle loading functions. Overall, unresolved challenges involve ensuring compatibility with CUDA/ROCm versions, improving test reliability, optimizing memory handling, and refining user APIs and documentation for complex tensor operations and extensions."
2018-06-29,pytorch/pytorch,"The discussions highlight concerns about the efficiency of numerical linear algebra operations, especially on CPU architectures like Threadripper and ROCm environments, with issues such as parallelization, threading bugs, and memory errors arising during compilation. There are recurring questions about the correctness and precision of floating-point computations, particularly with float16, and whether test tolerances are appropriately set given numerical limitations. Several bug reports involve system-specific failures, including memory exhaustion, process crashes, and protobuf/serialization errors, often linked to mismatched dependencies or build configurations. The integration and namespace management of symbolic identifiers (e.g., Symbol, onnx::, aten::) are debated, with worries about correctness and clarity in the new design. Additionally, there are requests for proper testing, API design improvements, and build system fixes to ensure stability and compatibility across diverse environments."
2018-06-30,pytorch/pytorch,"The discussions address multiple technical concerns including handling cache invalidation issues in PyCharm on macOS, particularly with Python stubs, as well as improving the usability of `load_state_from_file` by emphasizing the `map_location='cpu'` parameter. There is a proposal to expose all functions currently in `torch.nn.functional` to the main `torch` namespace, such as `tanh` and `sigmoid`. Other issues involve compatibility challenges, such as CUDA support limitations with LLVM 9.1 on macOS, and build errors due to incompatible gcc versions or specific serialization bugs in pretrained models. Additionally, there are performance optimization suggestions for reducing the number of backward operations, and questions around proper tensor operations, such as obtaining diagonal matrices and handling batch processing in probabilistic sampling."
2018-07-01,pytorch/pytorch,"The discussions raise several technical concerns including the need for consistent and error-free implementation of features such as setting minimum learning rates in schedulers, metaclass-based module cloning, and the handling of namespace semantics for Symbols to improve clarity and functionality. There are questions about the correctness and stability of specific features such as the behavior of random number generation during subprocesses, the accuracy of certain CUDA operations like SVD, and the proper way to initialize modules using `nn.init` methods. Proposed solutions include fixing test failures, enhancing symbol namespace enforcement, and possibly refactoring to support more robust cloning and initialization mechanisms. Unresolved issues involve ensuring backward compatibility, improving support for complex operations like multivariate sampling across batches, and addressing known limitations in CUDA implementations."
2018-07-02,pytorch/pytorch,"The discussions highlight several technical issues: (1) multi-processing data loading errors with h5py, solvable via start method adjustments; (2) the deprecation or availability of certain PyTorch functions like `torch.btrifact`; (3) challenges in distributed training setup, including backend choices (`nccl` vs. `gloo`) and sampler configurations; (4) compatibility and build problems related to CUDA versions, ROCm, and library linking, especially on macOS and with different compiler toolchains; and (5) issues with memory management, such as out-of-memory errors during ROCm builds and symbol limits affecting exports. Proposed solutions include patching environment variables, reworking initializations, and improving build scripts, though some issues remain unresolved or in diagnostic stages."
2018-07-03,pytorch/pytorch,"The discussions highlight several technical concerns, including the necessity and behavior of functions like `ravel` versus `view(-1)`, and the potential for more generalized batch operations (such as `bincount(dim=X)` or extending `inverse` to batch tensors). There is emphasis on optimizing performance issues, particularly for GPU and CPU implementations, with suggestions for benchmarking and refining kernel operations, especially for layers like BatchNorm. Clarifications are sought on API consistency, documentation accuracy, and build environment configurations, such as symbol exports and device management (e.g., `to()` methods for optimizers/schedulers). Unresolved questions include how best to handle certain tensor behaviors, maintain backward compatibility, and streamline API design for composite operations across hardware and software layers."
2018-07-04,pytorch/pytorch,"The discussions highlight several technical concerns, notably the problematic state of `register_backward_hook`, which is suggested to be deprecated or removed due to its inconsistent behavior and complexity in implementation, with proposals for alternative methods like autograd functions. Documentation discrepancies are also a concern, such as argument names in functions like `matmul`, prompting discussions on aligning docs with actual API signatures. Compatibility and installation issues are prominent, particularly relating to CUDA, NCCL, and handling dependencies across different platforms (Windows, Linux, conda environments), as well as problems related to importing torch due to missing DLLs or environment configuration. Performance improvements through serialization optimizations, such as using numpy in `__reduce__` methods, are under consideration, alongside discussions on enhancing features like `bincount`, and extending support for batch operations in functions like normalization layers. Finally, there are ongoing conversations about developing new ATen-based implementations for better efficiency, updating APIs for batch processing, and ensuring reproducible, portable code across various hardware and software configurations."
2018-07-05,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) inconsistencies and bugs in tensor slicing and indexing, such as unexpected behaviors when slicing tensors with out-of-range indices, which have been addressed in recent fixes; (2) memory management issues during training, especially with validation steps and GPU memory leaks in complex models like GANs, prompting suggestions like proper variable tracking and usage of `torch.no_grad`; (3) challenges in distributed training setup, including optimal backend choices, process configurations, and performance optimization when scaling across GPUs; (4) compatibility and integration issues with external libraries like ONNX and Dlib, often involving crashes or segmentation faults, which require careful environment management; and (5) differences between Python scalars and tensors, dtype handling, and serialization performance, with proposed improvements such as numpy integration for efficiency and better documentation for clarity. Overall, these comments reflect ongoing efforts to fix bugs, improve performance, and clarify best practices in PyTorch development."
2018-07-06,pytorch/pytorch,"The discussions highlight ongoing efforts to optimize GPU operations in PyTorch, including replacing standard RNNs with CuDNN kernels and supporting ROCm/HIP for AMD GPUs, with questions about ensuring compatibility and performance. Several issues relate to PyTorch's internal functionality, such as bugs in tensor operations (like `.view()` and tensor creation), memory management, and interface design for distributed training (e.g., DDP and master-worker patterns). Compatibility and stability concerns are evident in version bumps for ONNX opset, serialization formats, and supporting different compiler/toolchain configurations (e.g., GCC, CUDA, NCCL, and libcudart). Specific challenges include fixing assertions on uninitialized variables, handling data types like `Half`, ensuring proper plugin and build procedures on various platforms, and maintaining backward compatibility with serialized data. Overall, unresolved questions involve version updates, build system configurations, and interoperability with AMD/ROCm hardware."
2018-07-07,pytorch/pytorch,"The discussions highlight challenges in ensuring proper environment configuration for PyTorch on Windows and with multiple conda environments, emphasizing the importance of consistent package installations. There are ongoing efforts to improve PyTorch's internal functionalities, such as setting device limits, deterministic hashing, and eigenvector backward failure handling, often requiring careful implementation and testing. Compilation issues arise when building from source on older Linux distributions or incompatible GCC versions, with suggestions to update submodules or adjust compiler settings. Compatibility and dependency management are critical, especially concerning ONNX integration, ROCm support, and operator overloading for Half types, which currently face technical barriers. Overall, key unresolved questions involve environment setup, backward-compatible enhancements, and tackling build failures across diverse hardware and software configurations."
2018-07-08,pytorch/pytorch,"The discussions highlight several technical concerns, including debugging out-of-range label errors in segmentation tasks, where label values must be within valid class indices to prevent CUDA assertion failures. There are questions about PyTorch's support for specific functionalities, such as batch operations on diagonal matrices and behavior of `torch.diagonal`, with some features implemented via PRs or suggested as desirable additions. Issues related to non-determinism in device hashing, compatibility with external libraries like pybind11, and build system challenges, especially on Windows and ROCm platforms, are also prominent. Additionally, some discussions address handling of tensor indexing and the appropriateness of including certain tests, while questions about version support and environment setup point to ongoing development and deployment complexities."
2018-07-09,pytorch/pytorch,"The discussions highlight several technical issues and suggestions within the PyTorch repository. There is a significant focus on improving or removing backward hooks due to their incorrect implementation, with some advocating for documentation fixes and alternative approaches like `grid_sample` and `interpolate` for resizing and interpolation needs. Memory management concerns are raised, including potential leaks, especially with data loaders and CuPy's DLPack handling, prompting fixes and PRs. Build-related issues are present, especially regarding compatibility with compiler flags, third-party dependencies, and environment setup, notably on Windows and with CUDA versions. Additionally, there are ongoing debates about API consistency, such as argument naming, support for higher-dimensional `eye_like`, and precise control over backend features like MKL and cuBLAS, alongside suggestions for enhancing testing, error handling, and code structure for clarity and stability."
2018-07-10,pytorch/pytorch,"The discussions highlight several key technical concerns, including modifying `clock_gettime` for more efficient timing in CUDA, and adding support for `pos_weight` in BCELoss/BCEWithLogitsLoss to improve loss functions. There are also questions about correct API design for optimizers and schedulers, such as the potential for unifying their interfaces and making `grad_fn` non-callable, as well as issues with re-implementing `hash()` for `at::Device`. Several discussions concern build and installation problems on different environments, especially related to CMake configurations, CUDA version compatibility, and library dependencies like glog and Eigen. Unresolved issues include ensuring proper stream synchronization behavior, handling tensor size variability, and integrating new features like block sparsity or the `BatchTensor` concept into core PyTorch, with some PRs and features being deprecated or requiring further rebase and fixes."
2018-07-11,pytorch/pytorch,"The discussions highlight several key issues: the need for proper random seed control in DataLoader workers, with solutions involving seed resets and worker_init_fn adjustments; build and installation challenges due to CUDA, Eigen library compatibility, and dependency management, particularly when using cmake, setup.py, and third-party modules; and performance regressions in PyTorch, prompting ongoing profiling and optimization efforts. Compatibility concerns are evident regarding Python versions, CUDA toolkit mismatches, and system configurations across various platforms and environments like Docker and Singularity. Several specific bugs and feature requests remain unresolved, such as hashing for custom objects, increasing exported symbols, and clarifying API semantics, especially around parameter handling and operations' auto-differentiation. Overall, the threads reflect active troubleshooting, feature development, and system integration efforts to enhance stability, performance, and usability in diverse deployment scenarios."
2018-07-12,pytorch/pytorch,"The discussions highlight several technical challenges including compatibility issues between CUDA 9 and GCC 7, leading to build failures due to deprecated CUDA macros, suggesting the need for code adjustments or environment modifications. There are concerns about building PyTorch from source within virtual environments, emphasizing the importance of proper setup, dependency management, and environment variables, especially when installing or compiling with CMake or setup.py. Other significant issues involve debugging build errors related to specific functions and code dependencies (e.g., improper argument handling in Functions, missing libraries, or deprecated code parts), with suggestions to improve modularity and compatibility. Additionally, questions around API consistency, such as implementing multivariate gama functions, handling tensor splitting semantics, and output formatting for print options, indicate ongoing efforts to refine usability and correctness. Overall, key unresolved questions include environment configuration for successful builds, adapting code for compatibility across compilers and CUDA versions, and ensuring precise, reliable computations align with mathematical expectations."
2018-07-13,pytorch/pytorch,"The discussions highlight several core issues: the need to support custom backward gradients and define functions that access data across multiple steps; ambiguity around the design and benefits of extending functions with new style approaches; challenges in implementing true circular convolutions and their CUDA support; ongoing refinements in GPU and device management, including stream and device guards; and concerns about code consistency, such as naming conventions (e.g., `eye_like`), handling of thread-local variables across shared libraries, and the importance of correct subclassing and behavior for tensor factory functions. Additionally, there are architectural considerations related to integrating new features with existing modules, ensuring compatibility across platforms, and clarifying the semantics and API behaviors for tensor operations."
2018-07-14,pytorch/pytorch,"The discussions cover a range of technical concerns, including clarifying tensor copying semantics in issues like #339, proposing the addition of an `extend()` method to `nn.Sequential` for improved code readability (#358), and implementing a custom `ClusterRandomSampler` to handle multi-cluster datasets efficiently (#1512). Other topics involve optimizing convolution operations, resolving tensor dimension mismatches, and addressing implementation details such as thread-local variable management and symbol export issues. There are also suggestions for API enhancements, like adding functions `expand_at` or `new_eye`, and discussions on code standardization, especially about property definitions and documentation practices. Many unresolved questions relate to compatibility, performance improvements, and specific feature implementations, with some issues pending further development or review."
2018-07-15,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's package distribution, with recent updates confirming built wheels are now available, thereby removing prior installation difficulties. There are technical considerations around optimizing convolution templates, particularly in TensorFlow, and porting efficient algorithms like cuDNN7's direct kernel for 3D convolutions. Several issues address the correct implementation and behavior of gradient and backward passes, especially for operations like embedding and normalization, emphasizing the need for fused or custom double backward functions to improve efficiency and accuracy. Users express concerns about runtime errors related to multiprocessing on Windows, and API design questions are raised regarding device-agnostic functions, such as `randn`, and maintaining consistency across different device and dtype options. Overall, the discussions reflect continuous refinement of PyTorch's core functionalities, debugging, and user experience improvements, with some unresolved issues around error handling, backward compatibility, and performance optimizations."
2018-07-16,pytorch/pytorch,"The discussions highlight several key areas of concern, including the potential addition of an `extend()` method to `nn.Sequential` for improved code readability, and considerations about API consistency for tensor options, with debates on whether to introduce `like` or `as` parameters versus existing `new_*` methods. Issues with implementation specifics, such as handling of `hash()` for tensors, are raised, alongside questions about debugging and build processes, particularly on Windows, CUDA, NCCL, and Caffe2 installation challenges. There are also technical discussions on optimizing convolution operations (depthwise conv), kernel symbol export problems, and improving auto-vectorization detection. Unresolved questions remain about proper memory handling, API design decisions, and ensuring reproducibility and correctness in various environments."
2018-07-17,pytorch/pytorch,"The discussions highlight several key technical concerns: the proper management of CUDA device environments, especially in multi-GPU or multi-env setups; the need for consistent and user-friendly API designs (e.g., `options()` vs `new_*` functions, and handling tensor creation with `like` or `as` parameters); the challenges in supporting specific functionalities such as parameter grouping in optimizers, in-place or sparse gradient operations, and expanding tensors with variable dimensions. There are also recurring issues related to build system configurations and library dependencies, particularly with CUDA versions and shared library handling, which affect reproducibility and compatibility. Several proposals suggest improving documentation clarity, API discoverability, and code maintainability, with some unresolved questions about best practices for certain features (e.g., thread local variables, symbol exporting, and interoperability). Overall, the discussions reflect ongoing efforts to enhance usability, stability, and extensibility of PyTorch’s core features."
2018-07-18,pytorch/pytorch,"The discussions highlight several core technical concerns, including challenges in cross-platform and cross-compiler compatibility (notably with pybind11 and CUDA-related builds), leading to build failures and linking issues (e.g., missing protobuf, gflags, or onnx dependencies). There are recurring problems related to environment setup, such as CUDA and cuDNN version mismatches and libtorch/pytorch installation complexities, particularly on Windows and with GPU support. Several tickets address exception handling, API inconsistencies (like sparse tensor support and division by zero), and the need for better error checking and testing. Additionally, proposals for improving design patterns (e.g., stream/ device guards) and feature support (like ONNX export and constant folding) are discussed, with a focus on stability and maintainability. Unresolved questions involve proper build configurations, dependency management, and ensuring compatibility across different development environments."
2018-07-19,pytorch/pytorch,"The discussions primarily revolve around compatibility and feature support in PyTorch, such as the availability of functions like `pad_sequence` which require building from source beyond version 0.3.1.post2, and the support for packing 4D tensors in multi-turn dialog scenarios. Several comments address performance optimizations and bug fixes, such as unrolling RNNs more efficiently, handling memory leaks, and improving CPU/GPU data transfer times. Issues related to build system challenges are frequently mentioned, including CMake version requirements, NVIDIA/CUDA compatibility, and proper environment setup, with particular focus on issues in building Caffe2 and detecting CUDA capabilities on old hardware. Ongoing efforts include code refactoring for consistency (e.g., CUDA context handling, macro renaming), adding tests for new features, and synchronizing with external repositories like ONNX and Detectron. Unresolved questions often concern ensuring cross-platform support, backward compatibility, and the correctness of gradient and serialization behaviors across different versions and configurations."
2018-07-20,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch development, such as compatibility issues with CUDA versions, compiler backend mismatches, and complex build errors, particularly around GPU support (Issue #8831, #8834, #9640). Several users express concerns about model serialization and deserialization, especially when saving and loading models with components like Batch Norm layers, and issues with protobuf serialization (Issue #9539). There is interest in improving debugging and profiling tools for GPU memory management, with suggestions for memory monitoring solutions and handling out-of-memory errors (Issue #2198). Compatibility and support for different Python versions, including Python 3.7, and clarifications on build configurations and environment setup are also prominent. Additionally, unresolved questions include converting models built with other frameworks like nngraph to PyTorch and resolving build failures due to compiler or environment incompatibilities."
2018-07-21,pytorch/pytorch,"The discussions primarily revolve around installation challenges, such as the unavailability of specific PyTorch versions for Windows prior to 0.3.0, and difficulties in building or importing binaries, often related to environment setup or protobuf serialization errors. Several issues involve improving or fixing functionalities, including performance enhancements in argmax/argmin operations, support for sparse tensor types and hybrid representations, and ensuring compatibility with various hardware and software configurations (e.g., CUDA, cuDNN, GCC versions). There are also technical debates on design patterns, such as the use of init methods in modules, the proper way to save/load models with batch normalization layers, and managing warnings like uninitialized variables. Additionally, some threads address tooling and community support questions, such as cache invalidation in IDEs, account restrictions, and build process optimizations, highlighting ongoing efforts to stabilize and enhance usability."
2018-07-22,pytorch/pytorch,"The discussions highlight ongoing technical challenges such as improving error messages for advanced tensor indexing, handling non-contiguous tensors and their impact on operations like `view`, and enabling efficient custom CUDA kernel development through ATen extensions. There are concerns about API usability, exemplified by changes in `TensorDataset` constructor signatures that may confuse users, and questions about the internal implementation of functions like `_matrices_` in `mm` to better match tensor dimensions. Several issues revolve around sparse and hybrid tensors, emphasizing the need for supporting 0-dim tensors, autograd on sparse values, and robust operations on sparse matrices, including potential lazy representations. Additionally, there are maintenance considerations, such as testing, build process improvements, and integrating C++ operations directly without relying on Python wrappers, all aimed at making PyTorch more flexible, efficient, and user-friendly."
2018-07-23,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch’s functionalities, including implementing support for mean over multiple dimensions and better dataset splitting, as well as improving ATen extensions, JIT compilation, and distributed training. Key concerns include ensuring compatibility and performance across CPU and GPU, such as optimizing MKLDNN and cuDNN integrations, and resolving build issues with dependencies like MKL-DNN, gflags, and Caffe2. Several threads address code stability and correctness, including handling warnings, compiler flags, and potential segfaults. There are suggestions for more flexible and user-friendly APIs, especially in sparse tensor operations, along with considerations for backward compatibility and implementation consistency. Unresolved questions focus on improving abstract tensor operations, managing internal representations, and ensuring environment setup reproducibility."
2018-07-24,pytorch/pytorch,"The discussions reveal concerns about maintaining API consistency, such as aligning `torch.meshgrid` with NumPy semantics and standardizing sampler interfaces. Several issues address build failures, especially related to ROCm support, MKL-DNN integration, and environment configurations, highlighting the need for better caching and dependency management. Debugging and compatibility challenges—particularly with compiler versions, binary incompatibilities, and external dependencies like LMDB or CUDA—are prevalent. There is ongoing debate about code maintainability and refactoring, including macro consistency (`JIT_*` vs `AT_*`), virtual function design (`dim()`), and internal representations of sparse tensors. Overall, unresolved questions focus on build robustness, library dependencies, and API behavior clarity, especially in edge cases and platform-specific contexts."
2018-07-25,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's functionality and compatibility, including implementing 'same' padding in convolution layers with clear, input-dependent parameters, and supporting more flexible sparse tensor operations and autograd for `_values`. Several comments highlight challenges with device management, especially GPU device assignment, driver compatibility, and CUDA version issues, prompting suggestions for API improvements and better error handling. There are concerns regarding the stability and correctness of parallel and distributed training, such as the impact of data distribution on loss computation, and the necessity of maintaining consistent API behavior across versions. Additionally, build and deployment issues are frequently mentioned, including compiler compatibility, build system robustness, and packaging details. Unresolved questions largely involve ensuring backward compatibility, optimizing performance (e.g., in convolutions and tensor operations), and clarifying design choices for device inference and error reporting."
2018-07-26,pytorch/pytorch,"The discussions highlight ongoing challenges with multi-GPU training, particularly regarding synchronization errors, invalid resource handles, and nondeterministic behavior in distributed training using NCCL and GLOO backends. There are concerns about supporting various sparse matrix formats, extending tensor layout options, and managing modifications to internal representations like sparse tensors and types for clarity and consistency. Compatibility issues are also raised, including build failures across platforms (Windows, Linux, Android), dependency management, and ensuring proper build workflows with caching. Additionally, questions about handling scalar outputs in DataParallel and the impact of nondeterminism on performance and reproducibility remain unresolved."
2018-07-27,pytorch/pytorch,"The discussions reveal ongoing development and troubleshooting in the PyTorch ecosystem, including feature gaps such as support for complex numbers, sparse matrices with autograd, and multiple sparse matrix formats. Several issues relate to memory management, including memory leaks and GPU memory monitoring, and correctness issues like floating-point exceptions, nan values, and handling zero-length sequences. Compatibility and build issues also recur, with concerns about CUDA versions, environment setup, and compiler differences affecting functionality. Suggestions include adding new features (e.g., batched inverse, BatchNorm variants), improving API consistency and usability, and refining internal code organization, while some topics remain unresolved or pending review, such as performance improvements, breaking backward compatibility, and ensuring robust, reproducible testing practices."
2018-07-28,pytorch/pytorch,"The discussions cover several technical topics: the need to support negative strides or advanced tensor access patterns, with ongoing efforts and suggestions like device-specific kernel accessors; performance improvements through building from source with MKL-DNN and addressing CPU bottlenecks; improving code organization, particularly in dependency and build system structure; and ensuring proper default argument handling in function signatures, especially for native functions with overloading or default values. There are also concerns about debugging and verifying DLL loading on Windows, along with verifying consistency across different versions and configurations. Unresolved questions include the integration of batch support for matrix inverse, the implementation and utility of device accessor classes, and better diagnostic tools for environment issues."
2018-07-29,pytorch/pytorch,"The discussions primarily revolve around improving code clarity and standardization, such as replacing custom property classes with Python’s built-in `@property` decorators, and handling tensor operations' backward compatibility and scalar broadcasting behaviors. Several technical issues concern debugging and fixing errors related to CUDA, cuDNN versions, and MPI concurrency bugs, highlighting compatibility and environment-specific challenges. There are also proposals to optimize backward operations, such as replacing double backward implementations with fused index addition and multiplication for efficiency. Additionally, debates on deprecated or inconsistent behaviors—like scalar tensor handling in DataParallel—point to the need for clearer API expectations and potential deprecation plans. Unresolved questions include technical feasibility of certain backward operation replacements and the impact of environment-specific bugs on PyTorch’s stability."
2018-07-30,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionality, compatibility, and performance, such as refining the `load_state_dict` interface with a proposed `diff_state_dict` API to enhance consistency and manage collisions more reliably. Several issues concern build and integration challenges, notably around complex dependencies like Eigen, CUDA, and Android NDK, with specific attention to build errors, environment configurations, and submodule synchronization. There is a focus on performance optimizations, including shape propagation, masking operations, and sparse tensor representations, with discussions on handling complex API paths, such as variable argument functions and sparse tensor formats like CSR and COO. Unresolved questions include proper handling of sparse tensor internal states, ensuring consistency across functions that are sensitive to gradient flow, and improving user-facing documentation and support for diverse platforms. Overall, the team emphasizes maintaining API stability, robust testing, and simplifying deployment across various hardware and software environments."
2018-07-31,pytorch/pytorch,"The discussions highlight ongoing challenges and developments in PyTorch, including the need for more versatile and user-friendly interpolation functions akin to NumPy’s `interp1d` and TensorFlow’s broadcasting utilities, with specific interest in general-purpose grid sampling and interpolation methods beyond existing `grid_sample` and `interpolate`. Concerns are raised about ensuring deterministic results, proper handling of NaN propagation, and efficient sparse tensor operations, including support for various formats like COO and CSR. Several technical questions involve the integration and compatibility of external libraries such as pybind11, Eigen, and Caffe2, emphasizing correct build configurations, version consistency, and header management. There is also discussion around refining API consistency (e.g., for `nn.Sequential`, `broadcast`, and `einsum`), maintaining cross-framework compatibility, and ensuring robust testing, especially for regressions and platform-specific issues. Unresolved questions include best practices for tensor broadcasting, handling of unused parameters, and ensuring backward-compatible, efficient implementations of advanced linear algebra operations such as SVD and eigen decompositions."
2018-08-01,pytorch/pytorch,"The discussions predominantly revolve around technical challenges in PyTorch and Caffe2 development, including issues with onnx export failures and shape inference errors, often caused by version mismatches or incompatible operator implementations such as cuDNN activation functions. Several comments highlight build and environment configuration problems, particularly with CUDA, cuDNN, and compiler versions, emphasizing the need for consistent environments, proper build procedures, and potential workarounds like clean builds or environment modifications. There are inquiries about implementing negative sampling and softmax functionalities directly in PyTorch, with some developers exploring porting code from Cython or addressing the absence of certain features. Troubleshooting of segmentation faults during export and serialization issues with pickle suggest ongoing stability and compatibility concerns, alongside discussions on ensuring reproducibility and correctness in numerical operations. Finally, some suggestions involve code and build system refactoring, such as API design considerations, build script improvements, and header management to enhance portability and maintainability."
2018-08-02,pytorch/pytorch,"The discussions highlight several technical concerns, including the incomplete support for histogram functions on GPU and the challenges in implementing them efficiently, with suggestions involving tiling and CUDA-based approaches. There are ongoing issues with floating point inaccuracies in sampling from categorical distributions, especially when probabilities are exactly zero, suggesting potential need for tolerance-based conditions. Several bug fixes and improvements are discussed, such as fixing bugs in specific modules like MaxUnpool3d, updating submodules like Eigen, and addressing issues arising in multi-GPU setups involving NCCL deadlocks. Questions about backward-compatibility, support for Python 2 versus Python 3, and build issues on Windows with CUDA/cuDNN versions are prevalent, along with proposals to improve API consistency like adding batch step support for learning rate schedulers. Unresolved issues include proper handling of zero-probability bins, more robust GPU distribution sampling, and unifying error reporting systems across C++ and ATen."
2018-08-03,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's functionality and stability, including fixing bugs in specific operations like MaxUnpool3d, optimizing pooling implementations, and expanding interpolation and indexing capabilities. There are concerns about the stability and maintainability of features such as synchronized batch normalization, with calls for integrated solutions rather than extensions. Several issues address build and environment compatibility, notably compiler and CUDA-related errors, and dependencies on specific system configurations or compiler versions, emphasizing the need for better build robustness. Additionally, there are technical debates around broadcasting semantics, fusion optimizations, and the order of static initializations, indicating unresolved questions about internal implementation details and performance improvements. Overall, the discussions reflect active development, bug fixing, and feature enhancement, with some unresolved challenges related to build stability, feature support, and integration of advanced functionalities."
2018-08-04,pytorch/pytorch,"The discussions largely address technical challenges related to PyTorch's internal operations, including handling of batch normalization evaluation mode, CUDA and cuDNN compatibility and driver issues, as well as memory management and leaks, particularly in GPU contexts. Several comments involve debugging, performance benchmarking, and integration or porting of algorithms like matrix exponential or batched matrix inversion, emphasizing the importance of correctness and efficiency. Issues also cover API design choices, such as naming conventions and tensor support in functions like `torch.cat`, and implementation details like kernel fusion and backward pass support for custom CUDA operations. Unresolved questions include driver compatibility on specific hardware, memory leak fixes, and the proper way to extend API functionality while maintaining performance. Overall, these discussions focus on improving robustness, performance, and usability of PyTorch in diverse computing environments."
2018-08-05,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's functionality and compatibility, including adding more window functions like Blackman for STFT, addressing implementation details for features like iSTFT, and optimizing the SobolEngine for quasi-Monte-Carlo sampling with considerations for seeding and performance. Several issues concern GPU support, particularly CUDA and cuDNN compatibility, installation challenges, and WSL limitations, alongside troubleshooting specific performance concerns in group convolutions and CUDA timing accuracy. There are also discussions about code organization and documentation improvements, such as refining dataset subset handling and clarifying initialization schemes. Unresolved questions include optimizing Sobol sampling without relying heavily on Python loops, resolving IDE autocomplete issues, and ensuring compatibility across different environments and backends."
2018-08-06,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for improved tensor display formats (e.g., showing tensor dtype and size), and considerations around the use of implicit versus explicit tensor broadcasting and expand operations, especially in the context of graph fusion and tracing. There are ongoing efforts to enhance performance and usability, such as optimizing DistributedSampler, refining the tensor repr, and adding support for sharing kernel weights in depthwise convolution. Several issues point to compatibility problems with compilers (e.g., GCC versions, MSVC link errors) and dependencies (e.g., Caffe2, protobuf), suggesting a focus on build stability and environment consistency. Additionally, multiple threads discuss refining APIs (e.g., tensor indexing, concatenation, batch sampling) and ensuring code compatibility across different hardware, backends, and platforms. Unresolved questions include handling cross-platform build issues, improving runtime performance without sacrificing correctness, and ensuring the stability of core features amid evolving backend implementations."
2018-08-07,pytorch/pytorch,"The discussions highlight several core technical issues, including the slow performance of certain local convolution functions and the need for better extension support for custom CUDA/CPP ops, with suggestions to migrate to newer PyTorch APIs like `at::Tensor`. There is concern about discrepancies in behavior for probability calculations and the desire for more intuitive default behaviors, such as summing over batch dimensions, alongside the need for clearer documentation and potential API changes. Compatibility and build issues are addressed, including resolving unresolved external symbols in Windows, macro and environment setup for cross-platform builds, and performance profiling for convolution kernels on different hardware and backends. Some discussions focus on improving data handling and DataLoader efficiency for large datasets, e.g., through batch sampling strategies or refining serialization processes. Finally, questions about CI infrastructure, cross-compilation, and setup scripts emphasize ongoing efforts to streamline development workflows across diverse hardware environments and operating systems."
2018-08-08,pytorch/pytorch,"The discussions highlight several key technical concerns: users are experiencing hardware-related issues such as system reboots due to power draw from multiple GPUs, which can be mitigated by adjusting power limits via `nvidia-smi`; there are compatibility problems and import errors on Windows and Mac, often related to DLL access permissions or environment setup; discrepancies and bugs in tensor operations (e.g., `topk`, `clamp`, `eigen` functions) affecting correctness and performance, especially in distributed or CUDA contexts; and challenges in integrating custom C++/CUDA extensions with newer PyTorch versions, necessitating migration to the `at::Tensor` API and proper build system updates. Additionally, maintaining cross-platform build and test environments, especially for architectures like ppc64le, is an ongoing effort with community coordination. Overall, these issues underscore both hardware limitations and software compatibility, requiring targeted fixes, better tooling, and documentation updates."
2018-08-09,pytorch/pytorch,"The discussions primarily revolve around issues and questions related to PyTorch's CUDA and cuDNN configurations, such as the impact of `cudnnGet*` returning incorrect algorithms, and the workaround of enabling `torch.backends.cudnn.benchmark=True` to improve stability and performance. Several comments address build and installation challenges, including dependency management, environment conflicts, and platform-specific issues, with suggestions like using pip over conda and fixing OpenMP configurations. There are also technical questions about specific features, like implementing Sobol sequences either in Python or at the ATen level, and the behavior of functions like `topk` and `broadcast_tensors`, with some concerns about performance and correctness, especially regarding targets' value ranges and shape broadcasting. Additionally, users inquire about auto-completion problems in IDEs, differences between CPU and GPU implementations, and test failures due to datatype mismatches or deprecated APIs. Unresolved questions include best practices for certain API designs, handling of specific error cases, and ensuring consistency across platform and hardware configurations."
2018-08-10,pytorch/pytorch,"The discussions highlight ongoing improvements and challenges in PyTorch, including implementing batched linear solvers like `gesv` with support for non-square matrices, and the need for clear API documentation and backward compatibility. Several issues concern CUDA and ROCm backend compatibility, error messaging clarity, and ensuring consistent behavior between CPU and GPU implementations, particularly in sampling and distribution functions. There are technical questions about tensor reshaping and memory operations to optimize performance without unnecessary copying, as well as build environment setup, cross-compilation, and CI infrastructure for diverse architectures like PPC. Additionally, data loading, module integration, and ensuring test coverage for edge cases are emphasized, with unresolved queries about specific error messages and API stability."
2018-08-11,pytorch/pytorch,"The discussions highlight several technical concerns, including the limitation of `gesv` supporting only square matrices with questions about generalizing to non-square cases and related workarounds like adding zero-rows or using `torch.gels`. There are ongoing questions about enhancing tensor operations, such as enabling `var` over multiple axes, improving debug output with dimension names, and handling tensor reshaping to prevent memory inefficiencies. Issues with multi-GPU RNN parallelization and DataParallel support point to challenges in managing hidden states, with calls for better API design or fixes. Additional questions address build and environment setup, notably dependency management, platform-specific MKL configurations, SOVERSION handling, and missing functionalities like `roll`, as well as debugging and plugin registration uncertainties."
2018-08-12,pytorch/pytorch,"The discussions highlight a recurring issue with error 3 in PyTorch 0.4.1, especially for users downgrading from newer versions, with no definitive workaround identified; some rely on rerunning code as a temporary fix. There is a concern about the impact of SOVERSION specifications on library distribution, specifically how symlinks are handled in pip packages, potentially doubling download sizes, which complicates patching efforts. Additionally, questions arise about the stability and proper installation of specific PyTorch versions via pip, with users suggesting that referencing the package name should suffice for installation. A bug related to tensor printing rather than `expand_as` is also mentioned, indicating ongoing issues with tensor representation. Overall, the discussions emphasize stability, packaging, and usability challenges in the PyTorch ecosystem."
2018-08-13,pytorch/pytorch,"The discussions primarily revolve around compatibility, robustness, and feature support in PyTorch, including issues with installation mirrors and network-dependent builds in China, and compatibility with various hardware configurations such as Arm64, CUDA, and rocm. Several issues address tensor operations and autograd, such as in-place modifications, sparse tensor support, and correct backward implementations for new functions like copy_. There is emphasis on expanding core functionalities like hyperbolic functions, matrix exponential, and transformer layers, alongside suggestions for infrastructure improvements like better testing, build configuration, and support for advanced models. Unresolved questions include handling of heterogeneous environments, serialization of backward hooks, and expanding support for operations like roll and inverse hyperbolic functions, indicating ongoing development and refinement needs across the project."
2018-08-14,pytorch/pytorch,"The discussions highlight several key issues: the potential support for negative strides in tensors, which is deemed complex and unlikely; the ongoing development and support for complex numbers with no fixed ETA; performance concerns and benchmarking challenges around tensor sharing, especially in data loading and distributed contexts; floating-point precision issues in random algorithms, suggesting a need for tolerance-based checks; and various bug fixes, API improvements, and internal refactorings—often with attention to compatibility, code cleanliness, and correctness, such as ensuring proper error handling, schema consistency, and performance optimization. Many discussions also involve review coordination, testing, and documentation updates for features like ONNX opset bump, tensor initialization, and JIT. Unresolved questions include the support for negative strides, precise performance impacts of certain internal changes, and the correctness of numerical operations in floating-point computations."
2018-08-15,pytorch/pytorch,"The discussions highlight ongoing efforts in extending PyTorch's support for complex numbers, optimizing GPU operations such as sequence reversal, and ensuring compatibility with new ONNX/ONNX Opset standards, with particular attention to the impact of code refactoring and build system changes. Several threads address performance concerns, especially related to CPU/GPU transfer bottlenecks, comparison with Cython implementations, and the efficiency of new algorithms like Sobol sequence generation. Reproducibility issues are discussed regarding randomness seeding in multi-worker DataLoaders, with solutions involving seed resetting and worker-specific seed initialization. Compatibility and maintainability challenges are evident, particularly with legacy code migrations (e.g., TH/Caffe2), API changes (such as tensor data access and type conversions), and the complexity of supporting both legacy and new features across versions. Unresolved questions include the best approach to handle mutability in distributed and JIT contexts, and how to best manage build system updates without sacrificing existing extensions or causing compatibility issues."
2018-08-16,pytorch/pytorch,"The discussions highlight technical concerns related to hardware stability and power management, with users experiencing system shutdowns likely due to insufficient PSU capacity when using multiple GPUs and BatchNorm layers, and proposals for load ramp-up strategies. There are ongoing improvements in PyTorch's convolution implementation, including potential true convolution support and CUDA optimizations, along with suggestions to enhance scientific computing usability. Performance regressions are noted in modules like SobolEngine, prompting investigation into operator overheads. Additionally, discussions cover GPU compatibility issues under Windows Subsystem for Linux, the need for standardized attention modules and scaling strategies like Transformer models, and enhancements to indexing and workspace management to improve flexibility and efficiency. Unresolved questions include best practices for handling zero-sized tensors and whether certain device type distinctions (kCPU vs kCUDA) should be unified for better code clarity."
2018-08-17,pytorch/pytorch,"The discussions highlight several key technical concerns: the handling of data types and tensor constructors, such as ensuring proper support and error handling for int8, uint8, and bool tensors; and the complexities around naming and semantics of ByteTensor versus BoolTensor. There are ongoing debates about best practices for specifying tensor types, or designing APIs that clarify their semantics, especially for byte and boolean data. Support for distributed training and multi-GPU synchronization faces issues like timeouts and configuration errors, necessitating improved robustness and clarity in setup. Additionally, there are considerations for developing standardized, efficient model components (e.g., attention modules, transformers) and improving low-level build and environment reproducibility, as well as addressing specific bug reports related to shape inference, version compatibility, and framework internals. These discussions reflect both functional and architectural challenges in PyTorch’s development trajectory."
2018-08-18,pytorch/pytorch,"The discussions highlight concerns about memory management and device allocation in DataParallel, especially regarding model parameters being stored on a single device and device-specific backward memory usage. Several issues pertain to CUDA compatibility, such as runtime errors, driver and library mismatches, and driver access permissions, often complicated by system-specific policies or outdated hardware/software setups. There is ongoing interest in improving PyTorch's support for complex tensors, masking in transformer models, and code autocompletion, with suggestions to expand modular, standard implementations within the community and optimize for named dimensions. Compatibility and build issues, particularly with HIP/ROCm and specific hardware configurations, have prompted fixes like header filtering in build scripts. Overall, unresolved questions involve system-specific errors, performance regressions, and feature enhancements for user-friendly and efficient model development."
2018-08-19,pytorch/pytorch,"The discussions primarily revolve around adding a `torch.set_default_device` function to facilitate device management with new semantics, and integrating support for complex tensors, including API implications for FFT and spectral functions—raising questions about breaking changes and backward compatibility. There are ongoing efforts to improve low-level correlation operations and optimize CUDA implementations, with suggestions for simplified approaches using GEMM primitives or explicit correlation kernels. Several issues address build and compatibility challenges, notably relating to CUDA versions, Xcode requirements, and compiler support (e.g., gcc7), alongside problems with Python bindings and serialization in different environments. Additionally, performance discrepancies on multi-core CPUs and bugs in ONNX export and operator tests are reported, highlighting unresolved questions concerning build configurations, test data robustness, and compatibility with various toolchains."
2018-08-20,pytorch/pytorch,"The discussions highlight ongoing development of complex tensor support, including API considerations for `torch.fft` and spectral operations, balancing backward compatibility with potential breaking API changes. There are technical issues reported with eigenvalue computations in `torch.eig`, especially regarding complex eigenvalues, eigenvector representations, and differences from NumPy results. Problems related to building and importing PyTorch on Windows and Linux environments, often due to DLL access restrictions or build configuration errors, are frequently discussed, alongside potential solutions like adjusting process start methods or updating submodules. Several issues involve optimizing or fixing multi-GPU/NCCL functionalities, including addressing memory access errors and build failures. Additionally, questions about sparse tensor training, autograd support, and ensuring consistency across various configurations and versions are also raised."
2018-08-21,pytorch/pytorch,"The discussions highlight several technical concerns, including the need to clarify and improve the API consistency between `permute`, `transpose`, and `swapaxes` to align more closely with NumPy semantics, with a cautious stance against making disruptive changes. There are recurrent issues with build environment compatibility, especially related to linking errors on iOS with static libraries, and dependency management for extensions like LMDB and ROCm, which require proper configuration flags. Several problems involve version discrepancies and backward compatibility, such as loading models trained on earlier PyTorch versions or differences in numerical stability and gradient computation—particularly with RNNs, variable expansion, and small matrix operations. Unresolved questions include how to handle breaking API changes gracefully, improve error messages for user clarity, and address platform-specific build and dependency issues effectively. There is also an ongoing need for more robust testing and review of experimental features and PRs to ensure stability and correctness."
2018-08-22,pytorch/pytorch,"The discussions reveal concerns about GPU optimizer compatibility, specifically the need for optimizers like Adagrad to support `.cuda()` and proper device handling to prevent type errors. There are ongoing updates and fixes related to batch linear algebra operations, such as batch `symeig()` on GPU, and the integration of advanced linear algebra routines in PyTorch 1.0. Issues regarding build compatibility on older systems are evident, notably with CUDA and compiler support, highlighting challenges in compiling from source on legacy Linux distributions and Windows environments with DLL dependencies. Several suggestions include improving documentation for CUDA and device management, enhancing error handling for tensor aliasing and pointwise operations, and streamlining build processes with clearer dependency management. Unresolved questions focus on the stability and performance implications of GPU reductions, the compatibility of PyTorch with specific hardware and software configurations, and improving test infrastructure and code tracking (e.g., file move tracking)."
2018-08-23,pytorch/pytorch,"The discussions predominantly revolve around clarifying the correct usage of the learning rate scheduler (`scheduler.step()` vs `optimizer.step()`), the proper handling of optimizer and scheduler in training loops, and addressing specific errors such as `AttributeError: 'int' object has no attribute 'step'` caused by passing incorrect arguments. Several issues mention environment setup and compilation challenges across platforms (Ubuntu, CentOS), particularly regarding CUDA, GCC versions, and dependencies like glibc, with suggested solutions including building from source with specific configurations. There is also concern about backward compatibility and the distinction between tensor constructors like `torch.LongTensor()` and `torch.tensor()`, as well as ensuring updates and regressions are properly tested and documented. Additionally, some discussions address implementation details, such as the behavior of out-of-place operations, naming conventions, and code generation for neural network modules, alongside troubleshooting runtime errors and segmentation faults related to threading and environment misconfigurations."
2018-08-24,pytorch/pytorch,"The discussions cover various technical challenges with PyTorch, including issues with DataLoader multiprocessing that may produce zero-dimension tensors or hang, and the impact of settings like `num_workers=0` and `pin_memory` on runtime stability. There are concerns about the precision and distribution properties of `torch.rand()`, especially regarding the rare occurrence of zeros in large samples and the implications for floating-point uniformity on CPU and CUDA. Several hardware and compatibility issues are highlighted, such as segmentation faults related to CUDA/ROCm, large memory allocations exceeding system limits, and failures in build environments due to compiler or internal CI errors. Additionally, questions arise around API behavior, such as conformance with previous versions, support for functions like `roll`, and understanding internal implementations and backward compatibility nuances. Unresolved questions include fixing surface issues like the `CXX` compiler setting for builds, clarifying the status of new features, and ensuring CI stability amidst complex system and hardware interactions."
2018-08-25,pytorch/pytorch,"The discussions highlight several technical concerns including the lack of backward compatibility with Python 2.7, which causes issues with `inspect.signature` in hook registration, suggesting a need for conditional handling using `getargspec`. There is a debate on whether certain tensor properties like contiguity should be tracked, with arguments favoring simplicity over detail due to the tight coupling with strides and potential added complexity. The need for better API consistency is noted, especially regarding hook registration and copying behaviors; for instance, the `Tensor` class's `retain_grad()` method may require signature adjustments based on Python version. Additionally, some discussions focus on fixing or clarifying tensor copying semantics, especially when creating detached tensors or copies that shouldn't track gradients. Lastly, the issues encompass ongoing maintenance tasks such as linting or integrating experimental benchmarks, with some issues marked as beginner-friendly or unresolved."
2018-08-26,pytorch/pytorch,"The discussions highlight several technical concerns: (1) inconsistent behavior of `.to(device)` for modules versus tensors, with the consensus that `nn.Module` applies `to()` recursively, contrasting individual tensor behavior; (2) issues experienced during model export, especially with tensorboardX/onnx that cause models to break upon re-invocation; (3) performance bottlenecks in `bmm` with small batch sizes, suggesting potential inefficiencies or root causes needing further investigation; (4) support limitations in MIOpen, where dropout is not supported for certain hardware (e.g., AMD ROCm), and (5) the necessity to update/test code and associated CI pipelines following recent changes, such as merged commits and API updates."
2018-08-27,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch’s functionality, including support for depthwise convolutions, type promotion, and masking in attention modules, with emphasis on integrating common attention blocks into core while keeping architectures flexible. There are concerns about compatibility with older hardware and libraries (e.g., SSE, MIOpen, NCCL), requiring environment configuration, driver updates, or build adjustments. Several issues involve CUDA-related errors like out-of-memory, timeouts, and runtime failures, often mitigated by rebuilding, environment setup, or hardware considerations; some of these are acknowledged as fixed or complex to debug. The community suggests enhancing onnx export support, refining the API for tensor expansion, and expanding test coverage for new features like `expand`. Overall, unresolved questions concern CUDA hardware compatibility, build environment correctness, and how to verify and extend operator support for different architectures and distributed setups."
2018-08-28,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch documentation discoverability and SEO, particularly around functions like LU factorization and `torch.btrifact`. Several issues pertain to runtime errors such as segmentation faults, CUDA out-of-memory errors, and segmentation faults due to memory limitations or driver mismatches, often linked to specific environment setups or hardware constraints. There are also technical concerns about build processes failing due to operator overloading conflicts (e.g., with `at::Half` in CUDA), and the need to fix bugs related to distributed training, serialization errors, and internal kernel dispatch issues. Some conversations address potential feature enhancements like stream synchronization, stream-aware backward calls, and exposing file/line info in C++ modules for debugging, alongside requests for test improvements and code rebase requests. Overall, unresolved questions remain around build robustness across platforms, efficient handling of small matrices, and correctness of tensor operations, with many issues pending fixes or further review."
2018-08-29,pytorch/pytorch,"The discussions center around enhancing PyTorch's functionality and compatibility, including support for different data types such as `float` and `long`, and ensuring consistent behavior across CPUs and GPUs, especially in tensor operations like `logsumexp` and `bmm`. Several comments address build and installation issues, notably dependencies conflicts, binary support for older GPU architectures, and C++ build system stability, with suggestions for code modifications and build procedures. There are also questions about API usability, such as adding source code annotations (`__file__`, `__line__`) for debugging, and extending functionalities like `pad_sequence` to support pre-padding. Unresolved topics include fixing memory issues during training, handling of tensor shape mismatches in models, and refining performance for batched operations on CPU and GPU, with some proposals awaiting PRs or further review. Overall, the conversations reflect ongoing efforts to improve PyTorch’s robustness, usability, and performance, with specific focus on compatibility, debugging, and optimized tensor operations."
2018-08-30,pytorch/pytorch,"The discussions encompass issues related to environment and compatibility challenges, such as conflicts with GPU drivers, cuDNN versions, and system libraries (e.g., libc++, leveldb), which affect building and runtime stability. Several bug reports highlight failures in module imports, missing symbols, or incorrect behavior of PyTorch features like `pdist`, `split`, and shape inference, often requiring code adjustments or environment fixes. There are ongoing efforts to improve user experience with tooling, including data loader randomness, shape static export, and API simplifications, alongside plans for internal API refactoring and documentation clarifications. Compatibility and build system issues, such as compiler version constraints (e.g., gcc 5.5), and configuration of optional components (e.g., leveldb, full-caffe2), are also prominent concerns needing systematic resolution."
2018-08-31,pytorch/pytorch,"The discussions primarily revolve around PyTorch's compatibility with various hardware and software configurations, including GPU support with older CUDA versions, and issues related to build errors caused by missing or incompatible libraries such as leveldb, libcaffe2, and dependencies like NCCL and OpenMPI. Several issues address API and functionality concerns, such as the behavior of normalization layers (InstanceNorm vs LayerNorm), the implementation of tensor splitting and chunking, and the proper handling of packed sequences with variable lengths. There are technical questions about the internal structure and attributes of PyTorch components, like the support for `__file__` and `__line__`, and the stability of operations like `torch.load()` with different device specifications. Additionally, issues highlight the need for better documentation on instruction set support, and improvements in debug information, such as including source file and line metadata in errors, to aid development and troubleshooting. Unresolved concerns include ensuring correct export semantics for ONNX, handling of dynamic sequence lengths, and refining the JIT support for complex operations like `einsum`."
2018-09-01,pytorch/pytorch,"The discussions highlight several technical concerns: the appropriate use of Layer Normalization versus Instance Normalization for image data and video classification; challenges in building Caffe2 with GPU support due to environment and library issues, including CUDA and cuDNN compatibility; difficulties in integrating and testing specific code updates, such as handling state keys or implementing Gloo-based distributed training; and troubleshooting runtime errors like CUDA kernel generation failures, openblas linkage issues, and potential bugs in cuDNN or OpenBLAS. Additionally, there are considerations around code modifications, such as parameter.grad set to None, which impact optimizer behavior, and the impact of introducing scalar types on existing tensor code. The discussions suggest various workarounds, but unresolved questions remain regarding optimal configuration and compatibility for GPU support, as well as stability and correctness of recent code changes."
2018-09-02,pytorch/pytorch,"The discussions highlight ongoing concerns with CUDA/cuDNN compatibility and build-related errors, with suggestions to manage multiple CUDA versions via environment variables and to switch to tested, compatible binaries. There are issues around API stability and proper documentation for functions with internal or underscore-prefixed methods, especially in sparse tensor operations. Build failures linked to compiler versions (e.g., GCC 5.5) and linking errors involving libraries like openblas are noted, with recommendations to adjust environment settings and package configurations. Several discussions focus on precision and performance trade-offs, including the necessity of maintaining accuracy during tensor operations and the implications of floating-point representation limits. Additionally, some topics touch on API design challenges, such as encapsulation, test robustness, and the need for added test cases to prevent regressions."
2018-09-03,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch repository, including incomplete implementation of functions like `potrs` and `potrf`, and documentation inaccuracies regarding input shapes for `CrossEntropyLoss`. There is an emphasis on improving batch operations such as matrix inverse and cholesky decomposition for arbitrary batch shapes. Users frequently report build issues on Windows, Linux, and dependencies, often tied to environment mismatches, incompatible library versions, or compiler bugs, with suggested solutions involving environment reconfiguration or code adjustments. Additionally, there are ongoing efforts to optimize numerical routines, address code generation errors in CUDA, and enhance testing coverage, with some unresolved issues related to versioning, reproducibility, and platform-specific bugs."
2018-09-04,pytorch/pytorch,"The discussions encompass various technical concerns including compatibility issues with different PyTorch versions and dependencies (e.g., GOM, cuDNN, OpenBLAS), as well as build and integration challenges involving CMake, GCC, and environment configurations. Several comments highlight the importance of rigorous testing practices, debating the efficacy of checking in serialized expected outputs versus more scalable or tolerant testing methods, especially in light of platform and external library differences. There are ongoing efforts to improve code portability, visibility of internal APIs, and the correctness of functionalities such as SVD, autograd, and tensor serialization, with particular emphasis on handling edge cases, performance benchmarks, and proper documentation. Multiple threads address build failures, linker errors, and the need for clearer error reporting, indicating unresolved issues in environment setup and code compatibility. Overall, the discussions reflect a mix of bug fixes, infrastructure improvements, and methodological debates aimed at enhancing robustness, portability, and testing reliability within the PyTorch framework."
2018-09-05,pytorch/pytorch,"The discussions highlight concerns about default configurations and performance trade-offs, such as the non-default setting of `torch.backends.cudnn.deterministic = True` due to potential slowdowns, and the suggestion to automate or document this better. Several issues address error handling and diagnostics, like providing clearer error messages for ill-conditioned matrices or more informative exceptions during matrix decompositions. There are ongoing efforts to improve API usability, exemplified by suggestions to extend `torch.pdist` with additional metrics or to normalize more flexibly with `nn.LayerNorm`, alongside questions about applying normalization to RNNs. Build and compatibility challenges are evident, notably with CUDA, cuDNN, and CMake configurations, including platform-specific issues on Windows and macOS. Lastly, questions persist about code correctness and performance optimizations, such as optimizing `bmm` for small batches, refining the storage and serialization mechanisms, and enhancing testing coverage for edge cases."
2018-09-06,pytorch/pytorch,"The discussions highlight concerns about distinguishing functions affected by cuDNN deterministic flags and emphasizing the need for comprehensive documentation. There are technical challenges related to the implementation and optimization of sparse tensor operations, including autograd support and kernel specialization for various sparse/dense combinations. Several issues involve environment setup complexities, such as CUDA/CUDNN version compatibility, build system configurations, and dependencies like Eigen and OpenBLAS, often requiring clearer guidance and better integration with tools like CMake and setup.py. Reproducibility in distributed training remains problematic due to hardware heterogeneity and atomic operations, underscoring the need for clearer documentation and consistent behavior across architectures. Finally, ongoing performance regressions, build failures, and test discrepancies suggest a need for more systematic testing, multi-platform validation, and code review to ensure robustness and usability."
2018-09-07,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for better documentation on the deterministic flags (`torch.backends.cudnn.enabled`, `deterministic`), and considerations for expanding their references across relevant functions for reproducibility. There is debate over supporting multiple data types in loss functions like BCELoss and CrossEntropyLoss, and how best to handle RNN weight flattening to avoid memory issues—proposed solutions include disabling `flatten_parameters` or creating more robust fixes. Several issues pertain to build and environment setup, such as resolving DLL load errors on Windows, compatibility with CUDA versions, and handling platform-specific compilation nuances like protobuf and protobuf symbols. Additionally, questions about code fuseability, graph optimization, and proper handling of constants in JIT traces reflect ongoing efforts to improve performance and correctness, while the need for clearer documentation organization for tensor methods and linear algebra operations is also noted. Unresolved questions include optimal configuration for building from source, the impact of GPU capabilities on operation support, and streamlining error handling during module import failures."
2018-09-08,pytorch/pytorch,"The discussions highlight concerns about ensuring reproducibility in cuDNN dispatch functions, suggesting adding references and documentation for deterministic modes, especially for convolution and loss functions. There are ongoing challenges with building and compatibility on legacy systems like CentOS6, involving compiler and glibc issues, and requests for prebuilt wheel files to support older environments. Several technical issues relate to build system intricacies, such as symbol linkage in THC, accurate environment variable detection for GPU architecture, and dependency management in third-party components. Some discussions focus on testing and validation, including ensuring tests cover unhashable tensors and verifying fixes for build failures caused by library conflicts or deprecated components. Overall, unresolved questions include proper handling of compile-time dependencies, precise documentation for reproducibility configurations, and stability of build processes across diverse system configurations."
2018-09-09,pytorch/pytorch,"These discussions highlight several key issues in PyTorch development, including the importance of input validation and error messaging for loss functions (e.g., ensuring label classes match model outputs and asserting valid input ranges for `nll_loss` and `BCELoss`), and challenges with exporting models wrapped in `DataParallel` for ONNX compatibility. There are concerns about initialization schemes, with historical bugs in weight initialization affecting training performance, and suggestions to document and possibly revise these practices. Additionally, dependencies related to CUDA libraries, such as runtime compatibility and device-specific code issues, pose ongoing setup and portability challenges, especially across different GPU architectures and CUDA versions. Finally, discussions also touch on implementation details like replacing internal helpers for device code and managing build/test workflows, with unresolved questions about best practices for handling these technical nuances."
2018-09-10,pytorch/pytorch,"The discussions highlight several technical concerns, including the necessity of proper device management (e.g., ensuring tensors are on the same device during operations), and the need for better shape inference and shape handling, especially with indexing and batching nuances. There are ongoing efforts to optimize matrix operations using MKL and OpenMP, with considerations on performance impacts based on matrix sizes and hardware (e.g., Xeon vs. desktop CPUs). Several issues address build system and dependency management, such as fixing submodule updates, CMake configuration, and CUDA compatibility, often requiring clean builds or environment adjustments. Additionally, issues related to JIT characteristics—like handling constants, schema registration, and graph fusion—are discussed, with suggestions on how to improve or modify current behavior, including dealing with unsupported operators, trace vs. scripting discrepancies, and integration with ONNX and Caffe2. Overall, unresolved questions persist around performance optimizations, compatibility across hardware/software versions, and correctness of graph transformations, underscoring active development and complex maintenance challenges."
2018-09-11,pytorch/pytorch,"The discussions highlight several key issues: (1) The complexity and fragility of building NCCL support, especially regarding system detection and build configurations in the PyTorch/Caffe2 integration, with suggestions to rely more explicitly on user-provided paths. (2) Compatibility and build problems on macOS with specific Xcode versions and CUDA versions, emphasizing the need for appropriate command line tools and environment variables. (3) The ongoing transition from TH/THC to ATen, including plans for refactoring APIs, porting utility functions, and managing internal API stability, with concerns about exposing internal headers in extensions and ensuring ABI stability. (4) Performance and correctness considerations, such as controlling recursion limits for graph compilation, cache management in distribution algorithms, and ensuring proper GIL handling in Python-C++ API boundaries. (5) Various unresolved build and linking issues, notably unresolved external symbols, symbol visibility, and compatibility across different platforms and compiler versions, with suggestions for incremental fixes and the importance of clear API boundaries."
2018-09-12,pytorch/pytorch,"The discussions highlight several technical concerns: inconsistencies and ambiguities in device and device index management, suggesting the need for clearer semantics, possibly via explicit functions like `ensure_has_index()`, or the introduction of separate types such as `FullDevice`. There are issues with the correctness and stability of JIT tracing, serialization, and graph optimization, with specific instances of nondeterminism, high memory usage, and flaky tests, prompting suggestions for better error handling, test order management, and environment-specific fixes. Build-related problems, such as missing protobuf files, compilation errors on Windows, and dependency mismatches, have prompted discussion of code fixes, patches, and the importance of detailed environment setup instructions. Additionally, questions about API deprecations, clean API design (e.g., avoiding moving files for diff ease), and better documentation for reproducibility and deterministic behavior are raised. Overall, unresolved questions remain around device semantics, reproducibility in distributed setups, and ensuring build stability across environments."
2018-09-13,pytorch/pytorch,"The discussions highlight several key technical areas: enhancements to PyTorch's handling of sparse tensors, including support for autograd and varied input combinations; the need for better management and removal of hooks in modules; and challenges related to building and integrating Caffe2 and ONNX components, particularly on different platforms and configurations. There are ongoing efforts to improve performance, such as caching in interpolations, and to fix issues related to memory leaks, build failures, and compatibility concerns with specific compiler versions and dependencies. Several unresolved questions include proper implementation of sparse operations with autograd support, better testing for complex features, and resolving build errors stemming from missing dependencies or incompatible environments. Overall, the discussions reflect active development, bug fixing, and feature enhancement efforts, with attention to platform differences and dependency management."
2018-09-14,pytorch/pytorch,"The discussions highlight several core issues: the design of complex tensor support and its API implications for fft operations, with debates on backward compatibility versus API-breaking changes; device management improvements through global flags or context-aware methods for seamless CPU/GPU switching; and efforts to stabilize behavior around requires_grad, retain_grad, and tensor creation semantics. Additionally, there are concerns about refactoring internal structures like RNG states for CUDA, and ensuring stability and performance of distributed training, including compatibility of process groups and signals. Several discussions propose modifications to API consistency, such as adding `to()` method, refining tensor factory functions, and better documentation of tensor methods. Unresolved questions involve handling backward compatibility, optimizing memory handling, and addressing build failures across different platforms and dependencies."
2018-09-15,pytorch/pytorch,"The discussions highlight several key points: (1) The need for a more flexible RNN API in PyTorch to accommodate custom cells, especially around stacking and bidirectional configurations, with some preference for specialized functions like PeepholeLSTMCell rather than cluttering the API. (2) Ongoing efforts to extend floating-point info in PyTorch to better align with numpy's finfo, including defining constants like eps, tiny, and max, and ensuring consistency across C++ and Python. (3) A persistent issue with leaked semaphores in DataLoader when used with DistributedDataParallel and tqdm, indicating interaction bugs that are being investigated. (4) Troubleshooting distributed training failures and errors such as connection resets and process kills, with suggestions to test against master builds and handle signals properly. (5) General maintenance and compatibility concerns, such as ensuring backward compatibility with `c10d` in Apex and resolving issues with deprecated APIs or threading, alongside continuous improvements to test infrastructure and API design."
2018-09-16,pytorch/pytorch,"The discussions highlight concerns about compatibility and documentation related to PyTorch's instruction set support, particularly SSE4.3, with some users seeking unofficial builds or guidance on compiling from source for older CPUs. Performance optimization is also a focus, with suggestions to implement vector caching and unit tests for Vec256 specializations to improve efficiency and reliability. Additionally, there's a notable issue with the proper and safe use of `multiprocessing.set_start_method('spawn')`, where repeated calls inside scripts can lead to unintended multiple invocations, suggesting a need for clearer documentation or code adjustments to ensure it's only called once within `__main__`. Finally, some users propose that practical examples and warnings be added to the documentation to clarify distribution behaviors and avoid common pitfalls, such as distribution returning `-inf` or framework crashes during large-scale or low-level operations."
2018-09-17,pytorch/pytorch,"The discussions highlight ongoing challenges with thread management and environment configurations affecting performance and reproducibility in PyTorch, such as inaccurate expectations from `set_num_threads`, oversubscription due to `OMP_NUM_THREADS`, and achieving consistent RNG sequences across CPU and GPU. Several issues concern error messaging clarity, particularly for shape validation failures and linking/linker errors on Windows, suggesting a need for more informative errors and proper dependency management. There are questions regarding the implementation of new features like multiple modes in `EmbeddingBag`, handling of `torch.tensor()` copying behavior with `requires_grad`, and the API design for state diffing. Additional concerns involve ensuring compatibility during distributed training, addressing flaky tests, and considering API design choices such as returning dictionaries versus multiple values. Unresolved questions include fixing build failures, aligning API semantics (e.g., hash functions, rank-based errors), and clarifying device and memory management practices."
2018-09-18,pytorch/pytorch,"The discussions highlight ongoing concerns about PyTorch's API consistency, particularly around `requires_grad`, `retain_grad`, and the impact of input `requires_grad` on autograd behavior, which causes confusion and potential bugs. Multiple issues involve improving error messages, handling edge cases, and ensuring backwards compatibility while evolving APIs, such as replacing `requires_grad_()` with `retain_grad()`. There’s interest in optimizing and porting core functionalities like batched linear algebra operators and extending support for multi-GPU RNN training, with noted challenges in handling hidden states across devices. Also, several threads address build and deployment concerns, including support for older CPU instruction sets, extension reloading stability, and REPL compatibility. Unresolved questions remain about the internal upgrade path for serialization formats, optimization of new operators, and ensuring consistent, informative error reporting for both users and developers."
2018-09-19,pytorch/pytorch,"The discussions cover several core technical concerns including the handling of device-specific bugs and build issues (notably on Windows and with CUDA), the need for clarity and standardization in numerical info functions (like finfo and iinfo) aligned with NumPy conventions, and performance optimizations for element-wise operations and softmax implementations, especially regarding numerical stability with large values. There are ongoing efforts to fix bugs related to specific ops (e.g., embedding bag, batch normalization, softmax), improve compatibility and correctness of model exports and DataParallel usage, and address build system inconsistencies across environments. Additionally, some conversations highlight the importance of adding thorough testing, code review, and handling edge cases like empty tensors and scalar operations, alongside platform-specific fixes. Unresolved issues include ensuring compatibility of certain functionalities across CPU and CUDA, managing build dependencies (like MKL, Numpy), and fixing runtime errors in distributed and multi-device scenarios."
2018-09-20,pytorch/pytorch,"The discussions reveal ongoing challenges with multi-GPU device management and device-agnostic coding, emphasizing the need for a global device setting or simplified device handling via `.to(device)`. There are multiple reports of build and runtime errors related to CUDA, NCCL versions, and underlying infrastructure (e.g., protobuf, NCCL, compiler settings), indicating compatibility and configuration issues. Several discussions address bugs and performance concerns in specific functionalities such as RNN Cell support, backward pass streaming, and custom operator overloads, with suggestions including code refactoring, better error messaging, and more robust testing or rebase strategies. There are concerns about test flakiness and handling of low-level details like stream pooling, which affect reproducibility and performance optimization. Lastly, some issues relate to build system intricacies, including dependency management, build caching, and environment setup, highlighting the need for clearer documentation and tooling improvements."
2018-09-21,pytorch/pytorch,"The discussions highlight several technical concerns, including the proper setup for running PyTorch with multiprocessing on Windows and Linux, with particular focus on DataLoader worker configurations, kernel compilation issues, and device compatibility, such as GPU support and hardware stability. There are questions about the correct handling of dependencies like MKL-DNN and OpenCV versions, as well as specific errors related to H5PY file access in multiprocessing contexts and device-side CUDA assertions. Additionally, modifications to the API and internal code optimizations—such as refactoring load_state_dict behavior, improving CPU kernel performance, and managing thread limits in CUDA kernels—are proposed, with some solutions awaiting upstream review or additional testing. Unresolved issues include debugging specific runtime errors (e.g., CUDA errors, file header versions, and backend mismatches), as well as coordinating changes across different environments and ensuring compatibility and correctness of new features."
2018-09-22,pytorch/pytorch,"The discussions highlight several key technical issues: difficulties with transitioning between CPU and GPU in PyTorch, including the need for a global device flag or simplified device management; challenges in implementing and correctly sampling within the SVRG optimizer, especially regarding snapshot gradients and model pairing; issues with handling variable sequence lengths correctly in RNNs and using `pad_packed_sequence`'s `total_length` to avoid errors; complications in multi-GPU setups, particularly with `batch_first=True` and sequence length handling; and build or API visibility problems related to C++ template instantiations and macro configurations. Several suggestions involve improving device handling, optimizing optimizer sampling procedures, ensuring sequence length consistency, and fixing build visibility issues, with some unresolved questions about best practices for multi-device data interchange and code modularity."
2018-09-23,pytorch/pytorch,"The discussions primarily revolve around improving error messaging for data type mismatches in PyTorch, with suggestions to enhance user guidance via DataLoader and tutorial updates. There is ongoing development and discussion about adding `Storage.new_pinned` functionality to support pinned memory, with collaborative input on implementation approaches. Multiple issues highlight test failures and stability concerns, especially related to distributed data parallel (DDP) and backend support, including PyPy compatibility and device-specific kernel paths such as THNN versus cuDNN for normalization layers. Additionally, questions are raised regarding documentation clarity, the configuration and enablement of ideep/MKL-DNN support, and dependency management issues like the inclusion of the `future` package. Overall, community members suggest incremental improvements, implementation clarifications, and bug fixes to enhance PyTorch’s stability, usability, and feature completeness."
2018-09-24,pytorch/pytorch,"The discussions primarily revolve around segmentation faults and CUDA-related runtime errors in PyTorch, often caused by improper GPU resource allocation, kernel launch bounds, or driver/hardware incompatibilities. Several users report issues with segmentation faults when importing torch, which are sometimes mitigated by reordering imports, reinstalling dependencies, or modifying CUDA kernel launch bounds (e.g., adding `__launch_bounds__(1024)`), followed by recompilation. Other concerns include build failures due to unsupported features like MPI, compilation errors related to __declspec macros, and precision or API compatibility issues involving custom operators or tensor types. Some discussions suggest modifying build scripts or environment variables (e.g., specifying the correct compiler, disabling MPI) to resolve building and runtime issues. Unresolved questions persist regarding kernel launch resource limits, the correct handling of mixed data types in CUDA kernels, and certain test failures in the JIT and autograd components, indicating ongoing challenges in stability and compatibility."
2018-09-25,pytorch/pytorch,"The discussions highlight various technical issues within the PyTorch ecosystem, including segmentation faults caused by library conflicts or hardware resource limitations, such as CUDA resource exhaustion or device-specific compilation optimizations. There are concerns about backward compatibility and build failures stemming from environmental mismatches, compiler flags, and support for specific features like sparse tensors or complex number promotion. Several threads focus on improving test robustness, code refactoring, and API consistency, such as handling non-tensor inputs or refining gradient propagation. Notably, some problems are identified as hardware or platform-specific (e.g., on Jetson TX2 or Windows), requiring targeted fixes or workarounds, while others relate to code design choices like the need for C++ implementations over Python for performance-critical routines. Unresolved questions remain around the correct handling of type dispatch in CUDA kernels, serialization of new modules, and ensuring feature support across diverse hardware and software environments."
2018-09-26,pytorch/pytorch,"The discussions highlight multiple core issues: (1) implementation details and limitations of specific PyTorch features, such as sparse optimizers, batch normalization behavior, and support for multi-dimensional batch semantics, with suggestions including modifying momentum and moving batch norm to native code; (2) compatibility and build problems, especially on Windows with CUDA/cuDNN, requiring adjustments to CMake flags, environment setup, and handling of system-specific configurations; (3) concerns about API design and stability, like argument ordering in functions such as `unique()` and the need for backward-compatible interfaces; (4) testing and debugging challenges, including dealing with compiler intrinsics, valgrind warnings, and serialization errors, with proposed fixes like patching serialization functions or adjusting build flags; and (5) integration of external tools like Numba, CUDA, and third-party dependencies, along with process improvements such as code reorganization and better environment management."
2018-09-27,pytorch/pytorch,"The discussions highlight ongoing concerns regarding the implementation and support of various PyTorch operators and functionalities, such as adding new math functions (e.g., max, min, arrange) and supporting mean for ByteTensors, with considerations about whether to implement in C or Python. There are issues related to runtime errors and device-side asserts during training, often tied to data consistency, activation functions, or the need for proper synchronization and bug fixes. Several discussions involve build and compilation challenges, especially on Windows platforms, including dependency management, compiler flag issues, and ensuring correct detection of CPU features like AVX. Additionally, there are questions about the structure of the codebase, including directory organization, operator registration, and build system improvements, as well as specific concerns about backward compatibility, correctness of gradient computations, and serialization formats. Unresolved issues include ensuring new functions are correctly integrated at the C/C++ level, fixing build failures on certain architectures, and clarifying API behaviors like `torch.pdist`."
2018-09-28,pytorch/pytorch,"The discussions primarily address technical challenges in PyTorch related to tensor reshaping and batching, especially in computer vision tasks; support for batch operations across various tensor dimensions; and issues with exporting models to ONNX, notably missing tensor shape information after dynamic operations like interpolation. There are concerns about build and compatibility issues, such as supporting older GPUs with CUDA, compilation errors on different architectures, and the necessity of reliable testing and documentation updates. Additionally, some threads focus on PyTorch's distributed training stability, serialization of distributed modules, and core improvements like moving batch normalization to native implementations for efficiency. Unresolved questions include handling batch semantics for 1D to 2D transformations, improving build automation for nightly releases, and ensuring proper support across different compilers and system configurations."
2018-09-29,pytorch/pytorch,"The discussions primarily revolve around numerical precision and compatibility issues in PyTorch, such as the difficulty of passing gradient checks with float tensors due to precision loss, particularly in adversarial attack scenarios, and the performance slowdown when using double tensors. There are persistent build and compilation errors linked to specific commits, compiler versions, and environment configurations, notably introduced or exacerbated by recent pull requests like efefd1d7cf37, causing failures in CUDA and other builds. Compatibility and debugging challenges are also evident in issues related to import errors, runtime errors during model serialization, and errors in distributed training, sometimes related to the order of imports or replication issues. Several discussions discuss fixing or avoiding particular bugs through source modifications, environment setup tweaks, or waiting for releases, with some ongoing investigations into the causes of specific build failures and runtime errors. Overall, unresolved questions include how recent code changes influence stability, performance, and correctness, and how best to mitigate environment-specific issues or integrate fixes across different platforms."
2018-09-30,pytorch/pytorch,"The discussions highlight issues with CUDA compatibility and environment configuration, notably the importance of correctly setting environment variables and driver versions to enable `torch.cuda.is_available()` and avoid hangs or errors. Several reports mention runtime errors during model loading and serialization, often linked to potential file corruption, serialization differences, or bugs related to specific CUDA versions and compiler settings. There are concerns about nondeterministic behavior in GPU operations such as atomicAdd, affecting reproducibility and correctness, with suggestions to document deterministic behavior options and handle non-determinism explicitly. Additionally, setup challenges with package installation, environment management, and build processes are discussed, along with proposals for improving type support and code annotations for better IDE compatibility. Unresolved questions include ensuring consistent serialization, handling multi-GPU nondeterminism, and clarifying the impact of environment configurations on runtime stability."
2018-10-01,pytorch/pytorch,"The discussions highlight ongoing development of new features and performance improvements in PyTorch, such as implementing `Storage.new_pinned` and supporting zero batch size inputs, with some technical challenges and design considerations. Several issues concern compatibility, correctness, and serialization, including serialization of hooks, handling of complex number types, and fixing bugs related to tensor shapes and expansion behaviors. There are recurring concerns about build environment configurations, compiler compatibility, and runtime optimizations, especially regarding AVX detection, CUDA support, and multi-architecture builds. Additionally, some discussions focus on debugging and fixing errors encountered during builds, tests, and serializations, often requiring clean builds or environment adjustments. Unresolved questions include clarifications on certain API behaviors (e.g., bool scalar handling) and verifying bug fixes through testing and rebase efforts."
2018-10-02,pytorch/pytorch,"The discussions highlight several key technical concerns, including inconsistencies in behavior when moving tensors and modules to GPU (`.cuda()`), with suggestions to standardize these operations for better usability. There is concern over discrepancies in performance and behavior of BatchNorm implementations across different PyTorch versions, with proposals to modify the initialization to optimize CUDNN performance. Serialization issues are also prominent, with corrupted or inconsistent checkpoint files prompting investigations into potential data corruption or worker-related conflicts. Additionally, compatibility and build issues across compiler versions and platforms, notably related to ATen and CUDA tests, are discussed, emphasizing the need for more robust reproducibility and error handling. Finally, questions about in-place tensor operations, gradient hooks, and potential backward compatibility problems underline ongoing challenges in maintaining stability and correctness in evolving PyTorch codebases."
2018-10-03,pytorch/pytorch,"The discussions highlight several technical challenges, including issues with multiprocessing sharing strategies, especially on Linux, and the importance of verifying environment consistency such as mount points when encountering segmentation faults. There are concerns about the correct setup and compilation procedures, particularly related to building from source, handling dependencies like SLEEF, and ensuring compatibility with system configurations on macOS and Linux. Additionally, some threads address bugs like corrupted model checkpoints, the implications of in-place tensor operations (e.g., expanded tensors and memory overlap), and the need for clearer build and testing workflows, including the impact of different backend configurations such as cudnn and hip/miopen. Overall, the conversations emphasize rigorous debugging, environment validation, and careful code maintenance to address both correctness and usability issues in PyTorch development."
2018-10-04,pytorch/pytorch,"The discussions highlight a need for improved object-oriented design in PyTorch's RNN modules, suggesting a more flexible hierarchy with abstract Cell and State classes to enable extensibility and reduce reliance on cumbersome conditional logic. There's concern about balancing generality with cuDNN optimizations, which complicate inheritance structures. Multiple issues address the correct export of models wrapped in DataParallel, compatibility with different environments (such as Jupyter and CUDA versions), and support for sparse tensor operations, including masking, softmax, and row-wise reductions, emphasizing the importance of flexible, sparse-aware utilities. Additional focus is placed on build and compilation workflows, such as re-building dependencies efficiently and resolving CI failures related to third-party libraries like NCCL and MIOpen. Overall, the core themes involve enhancing modularity, usability, and compatibility of PyTorch's core components and tooling infrastructure."
2018-10-05,pytorch/pytorch,"The discussions highlight several technical concerns, including limitations of current hook mechanisms in PyTorch (Issue #262), which do not provide access to original inputs/outputs during forward passes, prompting suggestions for module output manipulation. Segmentation faults and runtime errors (Issue #926, #12284) often relate to environment setup, dependencies, or hardware compatibility, such as CUDA driver issues or CPU instruction sets. Performance and API limitations are addressed through proposals like adding parameters to functions (e.g., `dim` in `pad_sequence`) or improving in-place operation safety checks (Issue #12230). Several unresolved questions revolve around enhancing the linear algebra support (e.g., non-square `gesv`, `svd` options), improving sparse linear solvers, and ensuring robust code generation and testing practices, especially with new features like JIT and type annotations. Deployment and build system configurations also feature prominently, emphasizing the need for reliable CI workflows and environment management."
2018-10-06,pytorch/pytorch,"The comments highlight persistent issues with GPU compatibility and performance, especially on ARM-based devices like Jetson Xavier, where device hanging and kernel compilation delays are observed. Several discussions address API usability and efficiency improvements, such as enabling non-sorted `pack_padded_sequence` processing and optimizing tensor indexing operations, with some pull requests merged or pending review. Compatibility problems between various installation methods (conda, pip, building from source) and platform-specific builds (x86 vs ARM) are recurring, emphasizing the need for better distribution support, including ARM builds. There are also concerns about software build stability, including linker memory errors on Windows and deprecated or ambiguous operator overloads causing compilation errors. Overall, community feedback focuses on enhancing platform support, API flexibility, computational efficiency, and robustness of the build system."
2018-10-07,pytorch/pytorch,"The discussions primarily revolve around optimizing data preprocessing and model export procedures, such as converting data types for efficient batching and correctly handling DataParallel wrappers when exporting models to ONNX. There are significant concerns about CPU threading and performance tuning via environment variables like OMP_NUM_THREADS, MKL_NUM_THREADS, and hardware specifics, with users reporting varied results and troubleshooting strategies. Compatibility issues are also highlighted, including problems with version mismatches, deprecated functions like Tensor.expand() interacting with in-place operations, and errors in build processes related to MKL and CUDA components. Additionally, there is ongoing development around documentation readability and internal build format improvements, with suggestions for including build metadata in distribution packages. Unresolved questions include the proper handling of deterministic behaviors in CUDA and refining the usability of internal APIs like Declarations.yaml."
2018-10-08,pytorch/pytorch,"The discussions predominantly revolve around installation issues, such as the lack of prebuilt pip packages for certain Python versions and how to properly handle DataParallel models during export to ONNX, with solutions involving stripping the 'module.' prefix from state_dicts. Several comments address performance testing and benchmarking, emphasizing the importance of stable, uncontested metrics like instruction count over instruction per second, especially on benchmarks with hardware variability. There are ongoing API and internal architectural concerns, including the merging of `Variable` and `Tensor`, namespace organization of C++ modules, and the movement towards a shared core `c10` library, highlighting the need for better dependency management and build configuration, especially on Windows and with external libraries like MKL. Additionally, issues related to build system bugs, toolchain configurations, and handling of LAPACK functions (like `pstrf`) suggest an emphasis on improving build robustness and consistency across platforms. Unresolved questions include the timing and merging of various PRs, better version compatibility checks, and clarity around function naming conventions."
2018-10-09,pytorch/pytorch,"The comments highlight ongoing challenges with PyTorch's DataLoader multiprocessing issues, such as hangs and deadlocks, which are reportedly mitigated by recent fixes like #11985, and considerations on how large objects lock the `mp.Queue`. Compatibility and building concerns are prominent, including ensuring proper CMake setup with Torch integration, and handling model export issues involving missing tensor info during ONNX conversion, especially for operations like `F.interpolate`. Multiple discussions focus on supporting APIs (e.g., `torch.finfo`, `F.normalize`) and extending feature support across different versions, along with debugging and error messaging improvements—particularly with CUDA, distributed training, and ONNX export reliability. There are also community questions about code organization, type hinting, and maintaining backward compatibility, especially regarding attributes like `__init__` and version-dependent behaviors. Overall, resolving multiprocessing stability, improving cross-version and export functionalities, and refining API usability are recurring priorities."
2018-10-10,pytorch/pytorch,"The discussions frequently address the behavior and implementation of tensor operations such as `resize_`, `contiguous()`, and `view`, emphasizing the need for safer and more predictable APIs, especially concerning non-contiguous tensors and in-place modifications. Several comments highlight issues with loading and saving model states across different hardware setups, including handling DataParallel models and differences in PyTorch versions, suggesting potential improvements in `state_dict()` handling and version validation. Building and integrating PyTorch with external dependencies, such as CMake configurations, CUDA, and third-party libraries like MAGMA or ONNX, is a recurring topic, with issues related to system compatibility, build failures, and environment setup. There are concerns over ONNX export correctness, particularly regarding how certain operations like `Interpolate` are represented in the graph, and suggestions to fix shape inference and operator registration issues. Finally, several comments suggest improvements in build workflows, testing, and documentation to better handle platform-specific nuances and to prevent regressions, especially around performance regressions and proper support for features like sparse tensors and autograd."
2018-10-11,pytorch/pytorch,"The discussions highlight a significant concern with the class design of PyTorch's RNN module, primarily its reliance on parent classes depending on specific child modes (e.g., LSTM, GRU), leading to poor polymorphism and difficulty extending new cell types. Several contributors suggest refactoring towards a modular object-oriented architecture, introducing abstract base classes such as `RNNCell` and `State`, to enable flexible composition and facilitate custom implementations. There is an acknowledgment that current implementation complexity stems partly from balancing general user-defined RNNs with cuDNN optimizations, which complicates inheritance patterns. Other technical issues discussed include improving sparse tensor operations, addressing build and environment inconsistencies, and ensuring performance optimizations are correctly applied across platforms, with no clear resolution for some unresolved or abandoned issues."
2018-10-12,pytorch/pytorch,"The discussions mainly revolve around improving PyTorch's performance and usability features, including implementing efficient local convolution operations (Issue #499.0) possibly via custom CUDA extensions, and optimizing RNN looping efficiency with JIT (Issue #711.0). Several questions address increasing the speed of data loading, batch processing architecture (Issue #4959.0), and ensuring compatibility and proper linking with third-party dependencies like OpenCV, glibc, and MKL-DNN (Issues #6607.0, #12560.0, #12578.0). There are concerns about internal tensor behavior, particularly around view operations in no-grad contexts and sparse tensor support (Issues #12502.0, #12488.0, #12489.0), aiming to clarify autograd semantics and support for sparse data. Additionally, some issues focus on build-system configurations, testing infrastructure, and code integration challenges, such as handling merge conflicts, file generation strategies, and ensuring test robustness (Issues #12493.0, #12501.0, #12598.0). Overall, the threads highlight ongoing efforts to enhance performance, correctness, and extendability in the PyTorch ecosystem."
2018-10-13,pytorch/pytorch,"The discussions highlight ongoing challenges related to CUDA compatibility, with users seeking guidance on ensuring GPU functionality within containerized environments like Singularity, and requests for backporting fixes to support CUDA 10.0. There are concerns about build system configurations, especially for integrating MKL-DNN, CMake, and dependencies like GLOG, emphasizing the need for aligning `setup.py` and third-party build scripts. Several threads address API stability and usage, such as the correct way to share external pointers in C++, and clarifications on behavior of autograd hooks, particularly regarding backward compatibility and limitations in specific modules. Issues related to performance optimizations and benchmarking are also discussed, including faster implementations for linear algebra operations and considerations for creating smaller, optimized binary wheels. Unresolved questions remain about feature timelines, maintenance of nightly builds, and handling of specific use cases like tensor attribute deductions and custom operators."
2018-10-14,pytorch/pytorch,"The discussions highlight several key technical concerns, including the proper handling of buffers and device-specific method overrides in modules, with suggestions for explicitly wrapping buffers and ensuring consistent device behavior. There are issues related to the correctness and determinism of backward implementations, such as implementing double backward or alternative methods to avoid non-deterministic behaviors in sparse or index-based operations. Compatibility and robustness in multi-process or distributed environments are also raised, particularly regarding process exit codes, custom backends, and tensor serialization. Questions about API design, such as introducing new tensor-like types (e.g., SparseMaskedTensor) or extending existing functions with additional flags, focus on balancing flexibility with simplicity and maintainability. Unresolved issues include ensuring consistent scope and view behavior under `no_grad()` contexts, handling versioning and code structure for functions like `unique`, and managing complex interactions between autograd, views, and device-specific code."
2018-10-15,pytorch/pytorch,"The discussions cover several technical issues and feature requests related to PyTorch, such as the need for manual patching of state dictionaries after model modifications, improving native CTC loss support, and handling parameter sharing between modules for deployment. Several conversations address build and installation challenges on different platforms (Linux, macOS, Windows), including configuration details for CMake and shared libraries. There are ongoing concerns about sparse tensor support, including view operations, sparse matrix multiplication, and autograd compatibility, as well as issues with multi-processing on Windows requiring proper main module guards. Other topics involve API improvements, function naming clarity, and debugging failed tests or warnings, with some suggestions for code refactoring and better documentation. Unresolved questions include how best to handle sparse tensor dispatch and ownership, as well as how to streamline build processes and internal debugging tools."
2018-10-16,pytorch/pytorch,"The discussions highlight ongoing challenges related to performance optimization and compatibility, such as the slow speed of `SpatialConvolutionLocal`, the inefficiencies of im2col-based convolutions, and the need for CUDA extension support for fixed kernels. There are concerns about the correctness and determinism of tensor operations, especially in relation to `indexAdd_` and non-contiguous tensors, with suggestions to implement explicit double backward passes or to improve in-place operation safety. Support for proper GPU memory querying, device management, and distributed training workflows remains complex, especially on Windows environments requiring `if __name__ == ""__main__""` guards. Additionally, compatibility issues such as outdated pybind11 configurations, legacy code, and build environment variations (e.g., CUDA versions, compiler paths) complicate development and deployment, with proposals to streamline build processes and incorporate more comprehensive tests. Unresolved questions include how to safely handle overlapping tensor memory during in-place modifications and how to extend functionality like `pinned_memory` or deprecated features while maintaining backward compatibility."
2018-10-17,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's API consistency, serialization behavior, and support for sparse and complex tensors, with specific concerns about backward compatibility, deserialization of hooks, and API design choices like TensorOptions. There are technical questions about the serialization mechanism, such as removing hooks from serialization, and whether to support non-coalesced sparse tensors more robustly. Several issues address bug fixes and feature updates, including handling deterministic upsampling, integrating NumPy-like properties, supporting different environment setups, and ensuring consistent behavior across CPU, GPU, and various backends. Streamlining the internal infrastructure—like code generation, build dependencies, and kernel optimizations—is also a recurring theme, alongside ensuring documentation and testing keep pace with development. Unresolved questions involve the correct implementation of variable propagation in no-grad contexts, correct handling of tensor views during gradient computation, and improving support for sparse tensor operations, with some discussions pending code review and integration."
2018-10-18,pytorch/pytorch,"The discussions address several core issues in PyTorch development, including the need for more general-purpose interpolation functions beyond existing solutions, with community contributions proposing 1D and extension to unstructured 2D interpolations. There are concerns about the correctness, efficiency, and API consistency of gradient calculations, especially for complex models, sparse tensor operations, and batch normalization behavior when inputs have specific autocorrelation properties. Additionally, performance optimization and compatibility issues are raised, such as memory leaks, build failures across environments (Windows, ROCm, CUDA versions), and the handling of data movement and kernel optimizations. Several discussions also focus on API naming conventions, stability of internal modules, and the implementation of features like spectral normalization, with community questions about future plans and design abstractions. Unresolved questions include whether enhancements like explicit coalescing checks, new abstraction layers, and broader interpolation support will be integrated into future releases."
2018-10-19,pytorch/pytorch,"The discussions highlight several issues related to PyTorch's internals and tooling, including changes in how `state_dict` deep copies behave with `Parameter` objects, and the impact of recent code refactors on existing functionalities such as distributed training and `c10d`. There are ongoing concerns about deadlocks and stallings in DataLoader, especially on Windows with multiprocessing, potentially caused by improper shutdown procedures or threading issues during worker termination. Several build and environment setup challenges are noted, including CUDA and cuDNN version detection, compiler flags for NCCL, and external dependency configurations, which affect reproducibility and performance. Additionally, suggestions for improving code annotations, type hints, and handling deprecated functions aim to enhance maintainability and static analysis, while some questions remain about specific implementation details such as the use of `at::Tensor` vs `torch::Tensor`. Unresolved technical questions include managing in-place updates during model loading, testing distributed components effectively, and ensuring build environment consistency across platforms."
2018-10-20,pytorch/pytorch,"The discussions reveal several technical concerns, including compatibility issues with specific versions of PyTorch (notably 0.4.0, 0.5.0, and 0.4.1) often caused by changes in internal APIs or attribute expectations (e.g., the `_metadata` attribute in state_dicts). There are challenges with building and integrating custom CUDA/Caffe2 modules on Windows, especially related to linking errors and build configurations, as well as concerns about ensuring correct device placement of tensors (e.g., verifying device assignment in CUDA). Some discussions address the need for clearer documentation and testing to prevent regressions, such as verifying `__all__` consistency and avoiding issues with DataLoader usage in multi-process environments. Additionally, questions are raised about extending sparse tensor functionalities, implementing differentiable matrix functions (expm, logm, powm), and ensuring compatibility with various CUDA versions and compiler setups. Overall, unresolved issues include maintaining backward compatibility, correcting build errors on different platforms, and enhancing the robustness of API transitions."
2018-10-21,pytorch/pytorch,"The discussions highlight challenges in version compatibility and model loading, especially regarding PyTorch's evolution from versions like 0.3 to 0.4 and the associated model state_dict structure changes. There are concerns about the reliability of download statistics sources, which impact understanding of user adoption across Python versions. Issues related to sparse tensor support, including implementing `view` and matrix operations like `matrix_power`, are emphasized as important features for efficiency in constraint solvers. Additionally, there are maintenance considerations such as cleaning up API documentation, `__all__` definitions, and handling build system warnings. Overall, unresolved questions include how to maintain backward compatibility without breaking new features and improving sparse tensor operations."
2018-10-22,pytorch/pytorch,"The discussions encompass several key technical areas: first, the challenge of accurately tracking and analyzing CUDA and GPU memory usage, fragmentation, and allocator behavior, with proposals for diagnostics, cache management, and profiling tools; second, issues related to supporting sparse tensors, including their shape transformations, batch processing, and efficient matmul operations, highlighting the need for improved APIs and memory savings; third, difficulties in model export, graph tracing, and conversion to other frameworks such as ONNX, with suggestions for better symbol export and parameter access; fourth, concerns about build and environment inconsistencies—particularly with CMake, compiler versions, and Python environments—that impact reproducibility and stability; and lastly, various bugs and performance regressions in specific modules like BatchNorm, C++ extension handling, and JIT compilation, prompting suggestions for fixes, better testing, and clearer API semantics."
2018-10-23,pytorch/pytorch,"The discussions highlight various technical concerns, including the removal of the `--version-script` in PyTorch's build process and its impact on symbol exports, which can cause runtime symbol conflicts and crashes due to global symbol loading. There are questions about improving build configurations, especially regarding MKL-DNN integration, CMake and dependency management, and ensuring compatibility across platforms and compilers. Several issues relate to performance bottlenecks, such as slow exponentiation operations and small matrix factorizations in cuDNN, alongside suggestions for code refactoring, type safety, and API consistency, including handling sparse tensors and in-place operations like `resize_`. Inconsistencies and challenges with testing, debugging, and CI configurations are also addressed, alongside concerns about code clarity, backward compatibility, and the complexity of refactoring core modules or APIs. Overall, unresolved questions focus on ensuring build stability, runtime correctness, performance optimization, and maintaining API usability across diverse environments."
2018-10-24,pytorch/pytorch,"The discussions highlight several technical concerns, including the potential performance implications of different data layouts (NCHW vs NHWC) in GPU operations, and whether exposing layout options to users could benefit fast prototyping despite added complexity. There are ongoing efforts to improve CUDA memory diagnostics, fragmentation tracking, and to fix bugs caused by recent code merges, such as issues with tensor sharing across processes and ONNX export failures for specific operators like pixel_shuffle. Stability and correctness issues are also discussed, including failures in multicore/multithreaded environments, especially with CUDA tensors, and compatibility challenges with specific hardware like ROCm or AMD platforms. Additionally, there are concerns about build configurations, proper testing, and ensuring code consistency across different environments and versions, as well as discussions on simplifying or standardizing API behaviors (e.g., sparse tensors, batch operations, naming conventions). Unresolved questions remain regarding the best approaches to handle data layout flexibility, CUDA kernel stability, and build system adjustments for comprehensive hardware support."
2018-10-25,pytorch/pytorch,"The discussions highlight various technical concerns including GPU memory management during training, validation, and GAN training processes, with solutions like reducing batch size or using `drop_last`. Several comments address build and compatibility issues, such as CUDA architecture detection, library linking, and supporting multiple platforms (Power, OSX, Windows), often proposing environment variable configurations or build script adjustments. There are questions about PyTorch's C++ API, data conversion between OpenCV and tensors, and ensuring correctness in autograd with sparse gradients. Additional topics include runtime errors related to version mismatches, non-determinism in backward passes, and the importance of proper error handling, with some suggestions for code restructuring, better testing, and documentation improvements. Unresolved issues remain around build environment detection, platform support, and ensuring complete integration of features like ONNX or custom fusion mechanisms."
2018-10-26,pytorch/pytorch,"The discussions highlight various technical challenges involving PyTorch's type hinting, IDE autocompletion, and dynamic introspection limitations, with proposals suggesting the use of `.pyi` stub files and enhancing Jedi support. Compatibility issues are frequently addressed, such as supporting older Linux distros, outdated GCC versions, and integrating new features like the C++ dataset API, which some users feel risks diverging from Python APIs. Several issues focus on debugging and reproducing bugs related to nan propagation, NaN values in CUDA convolution, and kernel correctness, with ongoing efforts to fix or isolate these errors. Platform-specific build issues, particularly relating to ROCm/hip and CUDA libraries, are also common, along with discussions on code organization, build configurations, and benchmarking performance. Unresolved questions include ensuring the consistency of API semantics during refactoring, handling dtype changes properly, and maintaining platform compatibility across hardware and software environments."
2018-10-27,pytorch/pytorch,"The discussions highlight persistent issues with CUDA and library compatibility, notably the `libnvrtc` and `libcaffe2.so` linking errors, often related to mismatched library versions or missing symbols, which can sometimes be resolved by adjusting CUDA or cuDNN versions. Several users report environment-specific problems, especially when mixing conda, pip, or system-installed libraries, leading to undefined symbols or runtime failures, and many emphasize the difficulty of debugging with insufficient backtrace information. There are technical debates around deterministic backward passes for embedding operations, with suggestions to implement explicit double backward functions to handle non-determinism, and questions about the nature of higher-order derivatives for sparse tensors. Some issues concern build errors related to Eigen's CUDA headers and architecture mismatches, as well as runtime inconsistencies such as incorrect sum or mean outputs due to kernel memory staging bugs. Overall, there's a need for clearer error messages, better environment management, and robust debugging tools to resolve these compatibility, implementation, and build challenges."
2018-10-28,pytorch/pytorch,"The discussions highlight several technical challenges in PyTorch, such as proper management of DataLoader iteration to prevent batch repetition, memory leaks when using Pandas DataFrames with DataLoader, and difficulties in compiling the framework on macOS due to MPI-related linking errors. There are performance-related concerns, including the efficiency of inverse matrix operations (with a preference for MAGMA over cublas), and optimizing gradient computations for dilated convolutions. Users also seek improved default batching mechanisms for arbitrary shapes, more efficient algorithm implementations for optimization methods like QHM, and enhanced support for circular convolutions, emphasizing native, less memory-intensive solutions. Unresolved questions include ensuring compatibility and efficiency across different hardware and platforms, and whether recent architectural additions (like DynamicDAG) improve maintainability and speed."
2018-10-29,pytorch/pytorch,"The discussions highlight several technical concerns: (1) the broken state of ROIPooling and related ROI pooling implementations, with questions about their availability in pure PyTorch; (2) issues with MKL and MKLDNN build configurations, including the need for better detection or user control over system versus bundled libraries, and recent changes removing AVX2 dependencies; (3) various build and compatibility problems across platforms (macOS, Windows, Linux) involving library formats, compiler versions, and environment variables, often requiring specific patches or environment adjustments; (4) correctness and stability of PyTorch's API, including breaking interface changes, multigpu synchronization issues, and detailed handling of tensors on different devices, contexts, or in distributed setups; (5) unaddressed or ongoing bugs such as incorrect test failures, memory leaks, and runtime errors under specific configurations, along with suggestions to improve documentation and testing strategies to handle flaky or black-boxed tests."
2018-10-30,pytorch/pytorch,"The discussions predominantly revolve around addressing runtime hangs, deadlocks, and inter-process communication issues in PyTorch, often linked to multiprocessing and GPU synchronization, with various proposed technical fixes such as synchronization mechanisms, PR merges, or system-specific workarounds. Several issues pertain to build failures, especially related to CUDA, NCCL, and compiler-related bugs, highlighting concerns about environment compatibility, system headers, and compiler flags. Incompatibilities and bugs introduced in specific PyTorch versions (notably 0.4.1 and recent nightly builds) raise questions about regression and platform-specific behaviors, including MacOS, Windows, and Linux. Known limitations include support for C++ extensions, batching mechanisms, and ensuring correctness of JIT and distributed training features, with ongoing efforts to improve documentation, testing, and code robustness. Unresolved questions also address the stability of features like tensor operations, API changes, and the correctness of various components under different hardware and system configurations."
2018-10-31,pytorch/pytorch,"The discussions predominantly revolve around improving distributed training interfaces, with suggestions to pass parameters directly through `init_process_group()` and enhancing C++/Python data loader consistency. Several issues highlight bugs or limitations in existing implementations, such as difficulties with `stack` behavior under TorchScript, memory leaks when iterating with multiple workers, and inconsistencies in kernel operations like `maximum`/`minimum`. Notable concerns include build system dependencies, compatibility and performance problems on different hardware or platforms (e.g., Windows, macOS, ROCm), and the need for better differentiation and handling of NaN propagation in `max`/`min` functions. Unresolved questions include the proper way to handle gradient flow through masking functions, reconciling differences between C++ and Python interfaces, and fixing build environment issues that cause CI failures."
2018-11-01,pytorch/pytorch,"The discussions mainly revolve around GPU memory management and performance, with issues such as out-of-memory errors during data loading or model training, and the impact of DataParallel configurations. Several comments highlight environment-specific build and compatibility problems, especially on macOS, Windows, and with different CUDA versions, sometimes requiring environment variable adjustments or code modifications. There are also technical concerns about the correctness and efficiency of operations like tensor copying, shape inference, and kernel fusion, as well as compatibility issues with tools like ONNX and PyTorch's front-end and C++ APIs. Unresolved questions include best practices for capturing distributed training states, handling tensor dimension limits (notably for large or high-dimensional tensors), and improving tooling and workflows for building, debugging, and testing across platforms."
2018-11-02,pytorch/pytorch,"The discussions highlight concerns about the compatibility and correctness of C++ extensions, sparse tensor implementations, and build configurations, with specific attention to ensuring proper API behavior and performance, especially on different hardware and operating systems. Several threads address build process challenges, including CUDA environment setup, Visual Studio configurations, and caching strategies for sparse and CSR representations, often proposing changes or workarounds to improve stability and efficiency. There are questions regarding the correctness of gradient computations, particularly for sparse tensors and indexing operations, as well as suggestions for improving autograd mechanisms and device handling. Issues also involve optimizing deep learning primitives like convolutions and spectral norms, as well as integrating non-traditional hardware like OpenCL GPUs, suggesting a need for more flexible, unified API design and broader hardware support. Unresolved topics include refining autograd behavior in edge cases, enhancing sparse tensor support (including batching and autograd), and resolving build and compatibility problems across platforms."
2018-11-03,pytorch/pytorch,"The discussions highlight challenges with GPU memory management and out-of-memory errors in PyTorch, especially when using DataParallel or large batch sizes, with suggestions to monitor memory and potential workarounds. There are concerns about the unintuitive behavior of excluding invalid elements during backpropagation, notably that NaNs can propagate due to gradient calculations involving NaN values, suggesting the need for hooks or alternative handling. Issues related to compatibility and correctness are raised around MAGMA library bugs affecting batched matrix inversion, emphasizing the importance of matching library versions and alternative testing. Additionally, some comments address various implementation details, such as ensuring proper code refactoring, handling of multiprocessing on Windows, and interface clarifications for autograd functions, with suggestions for code robustness and documentation improvements. Unresolved questions remain about optimizing algorithms for specific hardware like OpenCL-enabled GPUs and managing code modifications without access to certain resources."
2018-11-04,pytorch/pytorch,"The discussions reveal several core issues: compatibility problems with older CUDA and PyTorch versions, particularly CUDA driver mismatches and installation errors; performance and stability concerns related to MAGMA library versions affecting batched matrix operations; serialization and loading state_dict errors in model checkpointing; and build environment challenges, including linking errors for MKL-DNN and platform-specific compilation issues. Solutions suggested include updating or patching MAGMA to newer versions, using hooks to manage gradients when dealing with NaN or non-differentiable operations, ensuring correct environment variables and build configurations for dependencies, and using scripts or environment specifications to facilitate reproducible builds across different systems. Some unresolved questions pertain to whether newer MAGMA versions will fix known bugs, how to handle zero-dimensional tensors during autograd, and platform-specific build instructions, especially on Windows and macOS."
2018-11-05,pytorch/pytorch,"The discussions primarily revolve around challenges in PyTorch related to gradient handling and tensor operations, including the need for hooks to exclude certain tensor elements from backpropagation, and the desire for a more flexible ""exclusion"" mechanism that treats NaNs or zeros differently during differentiation. Several comments highlight performance concerns, such as slow initializations in spectral normalization, inefficiencies in batched matrix operations, or slow convergence of certain functions like the initial power iteration in spectral norm. There are recurring issues related to build and compatibility problems, especially with CUDA versions, MKL libraries, and on Windows, including linkage errors and unconfigured dependencies. Additionally, questions about specific API behaviors—such as the proper handling of `.numpy()` conversions, improvements to distribution and multiprocessing support, and clarity in function interfaces—are frequently raised, with some solutions involving code refactoring, better documentation, or community-driven extensions."
2018-11-06,pytorch/pytorch,"The discussions encompass a range of technical concerns including platform-specific challenges with MAGMA availability on Windows and Linux, issues with batch normalization and input batch size handling, and build configuration complexities such as dependency detection (LMDB, NCCL) and library compatibility (CUDA, cuDNN, MKLDNN). There are suggestions to enhance framework features, such as introducing batched operations like `torch.det` and improving support for sparse tensors, while also addressing performance regressions and correctness, e.g., floating-point precision and device memory management. Notably, questions arise about JIT behavior, including whether to expose runtime flags, and the proper design of APIs for distributed and GPU-enabled training. Several discussions focus on build and installation troubleshooting, including reproducing errors, environment setup, and integration of third-party dependencies. Lastly, there are ongoing efforts to refine usability, documentation, and testing infrastructure to ensure robustness across different platforms."
2018-11-07,pytorch/pytorch,"The discussions primarily revolve around handling compatibility issues in model parameter loading and state dicts, especially related to naming conventions and version changes, which complicate merging models across different PyTorch versions and save formats. There are concerns about the correctness and robustness of CUDA and NCCL support, including build and driver detection issues, as well as memory leaks observed during data loading, which may be influenced by tensor operations and device synchronization. Several discussions address build system and CI testing challenges, including bug fixes in the CI pipeline and platform-specific compilation issues, with suggestions for improving build configurations and test coverage. Additionally, questions are raised about extending introspection (e.g., JIT flags), improving documentation, and ensuring feature completeness (e.g., Half type, ONNX support), while some threads focus on correct implementation of internal APIs and dependencies (e.g., MKL, CUDA, code export). Unresolved questions include ensuring consistent behavior across platforms and backends, and verifying fixes for various bugs in nightly versus stable releases."
2018-11-08,pytorch/pytorch,"The discussions highlight several core technical issues: the confusion surrounding the `-I` parameter in command-line usage, with questions about its meaning and impact, as well as the persistence of environment-related import errors in PyTorch, especially concerning NumPy version incompatibilities and system environment interference mostly on Windows and Linux systems. There are concerns about PyTorch's C++ API stability, breaking changes, and guidelines for maintaining compatibility across versions, alongside performance considerations in features like advanced indexing and batched inverse operations. Additional questions address proper use of device memory management (e.g., `.detach()`, `with torch.no_grad()`), and potential improvements such as enhancing type hinting with stubs and ensuring compatibility with CUDA 10. Overall, unresolved questions focus on environment setups, version stability, performance optimization, and correct API usage."
2018-11-09,pytorch/pytorch,"The discussions primarily revolve around integrating new features and improving infrastructure stability, such as the implementation of YellowFin optimizer, support for dict in ONNX, and the stabilization of the C++ API amidst breaking changes. Significant technical concerns include resolving DLL load errors on Windows, GPU memory spill issues, and platform-specific build challenges like MKLDNN support on ppc64le architectures. There are ongoing efforts to enhance tooling support, such as working with stub files and ensuring compatibility across different PyTorch versions, with some discussions about managing merge conflicts and build system adjustments. Additionally, users express concerns about stability and performance, including numerical inaccuracies and performance discrepancies between LSTM variants and quantized operations. Overall, the maintainers focus on balancing rapid development, breaking changes, and cross-platform compatibility while addressing specific bugs and infrastructural improvements."
2018-11-10,pytorch/pytorch,"The discussions highlight ongoing challenges with integrating and maintaining custom optimizations (e.g., YellowFin), handling deprecated tensor creation APIs, and resolving operator lookup failures in TorchScript due to operator version mismatches or missing implementations. There are concerns about environment-specific issues, such as library conflicts with Matlab, environment variable interference, and variability in memory leaks related to DataLoader's multi-threaded data handling, especially with NumPy arrays. Additionally, compatibility and stability of the C++ API across different PyTorch versions and Python bindings are a recurring theme, with suggestions to batch breaking changes or improve namespace separation. Finally, there are practical considerations about build stability, environment configuration fixes, and ensuring code quality through linting and rebase practices."
2018-11-11,pytorch/pytorch,"The discussions primarily focus on enhancements and issues related to PyTorch's linear algebra functionalities, such as adding batched inverse, `torch.det`, and `torch.logdet`, with ongoing development efforts and code reorganization. There are concerns regarding code review, including merging conflicts, coding style consistency (e.g., clang-format), and ensuring compatibility with different Visual Studio versions, particularly VS2015 versus VS2017. Several threads address performance optimization, such as the speed of dropout on CPU and CUDA/cuDNN performance on Windows, with suggestions to verify configuration and hardware specifics. Documentation improvements are suggested, including better tutorial guides—particularly for OS X—and clearer structure for modules like `functional`. Lastly, some issues involve implementation bugs, such as returning from functions correctly and fixing build/script failures across different environments, with ongoing testing and integration plans to address these."
2018-11-12,pytorch/pytorch,"The discussions highlight several key technical concerns, including the optimization of data layouts (NCHW vs. NHWC) for GPU performance, with suggestions to expose user-configurable layout options to balance complexity and flexibility. There are issues related to system stability and hardware compatibility, such as crashes caused by power supply or BIOS settings when training large models, and the importance of BIOS updates or disabling Turbo mode for stability. Several bug reports and feature requests address deep learning specifics, like ensuring correct handling of tensor shapes, device synchronization, and efficient parallelism in distributed training, as well as runtime errors related to target value ranges in loss functions. Questions about environment dependencies (CUDA, NCCL, compiler versions) and build procedures (from source, container images) reflect ongoing challenges with reproducibility and hardware/software compatibility. Overall, the discussions underscore the need for robust error handling, flexible user options, and environment stability to improve PyTorch’s performance and usability."
2018-11-13,pytorch/pytorch,"The discussions highlight ongoing technical challenges including ensuring compatibility and correctness in GPU operations, such as fixing non-determinism and floating-point inaccuracies in operations like spectral norm calculations and pooling, often due to floating-point rounding issues. There are concerns about build environment dependencies and platform-specific issues, for example, OS X support and CUDA driver compatibility, with suggestions for testing and validating build systems thoroughly. Questions also focus on extending PyTorch's functionality, such as adding new tensor operations (e.g., `max`, `min`, `arrange`, `ones_like`), properly exposing C++ and Python APIs, and ensuring backward compatibility with existing interfaces. Memory-related issues, including CUDA memory leaks, heap corruptions, and proper resource management (e.g., in dataloaders and multiprocessing contexts), are recurring, alongside efforts to improve testing, profiling, and debugging tools. Unresolved questions involve fixing multi-GPU nondeterminism, ensuring correct implementation of tensor shape calculations, and maintaining cross-platform support, with some discussions about incorporating new features like callback management and enhancing API stability."
2018-11-14,pytorch/pytorch,"The issues primarily revolve around build and compatibility challenges, such as problems with MPI backend support, CUDA and compiler version dependencies, and linker errors due to symbol duplication, which have been addressed by rebuilding or updating tools and dependencies. Several discussions highlight difficulties with distributed training, including process synchronization, proper parameter handling in DistributedDataParallel, and process group initialization, prompting recommendations to avoid changing model parameters after wrapping with DDP. Compatibility concerns with compiler versions (notably Visual Studio 2017 updates) and hardware-specific optimizations (MKL, AVX, CUDA) are also prominent, with suggested workarounds like cleaning build caches and setting specific compilation flags. Additionally, some questions relate to documentation gaps and API design choices, such as handling empty tensor indices, custom operator loading, and naming conventions, often seeking clearer guidance or better support for custom extensions. Unresolved questions persist about framework support for MPI in C10d, performance implications of synchronization points, and best practices for building and deploying PyTorch with various configurations."
2018-11-15,pytorch/pytorch,"The discussions highlight inconsistencies in the design of layer operations, particularly compare and contrast between linear and convolution layers, suggesting a need for standardization. Several comments focus on extending or improving the core library with utility methods like `get_output_shape_for`, while some participants argue for keeping such features outside core to avoid maintenance burden. Numerous reports of build, runtime, or compatibility issues—such as segmentation faults, linker errors, and precision discrepancies—point to ongoing challenges in ensuring cross-platform stability, especially on Windows, macOS, and with GPU configurations. There are questions about improving testing frameworks, dealing with floating-point errors, and handling deprecated or conflicting dependencies, indicating a need for more robust, portable testing and error detection. Lastly, multiple suggestions revolve around enhancing user experience, like better error messages, support for sparse and ONNX-related operations, and clarification around multithreading and distributed training concerns."
2018-11-16,pytorch/pytorch,"The discussions highlight several technical concerns, including challenges in loading older state dictionaries with dotted module names due to recent changes in module naming conventions, and the need for more streamlined support for sparse tensor operations, such as reduction functions, transpose, and batch support. There are unresolved issues with NCCL and multi-GPU communication on the same machine, especially regarding barriers and environment variables, as well as stability problems in multi-processing, caused by tensor type sharing and memory handling bugs. Build and compilation hurdles are noted, particularly with Windows, CUDA versions, and CMake configurations, requiring fixes or workarounds to enable successful environment setup. Additionally, some issues involve discrepancies in ONNX export graphs and the necessity to update documentation and tests to reflect recent code and API changes."
2018-11-17,pytorch/pytorch,"The discussions highlight several key technical concerns: issues with multiprocessing and shared memory limitations affecting DataLoader functionality, especially with `num_workers>0`, and solutions like increasing `--shm-size` in Docker; the need for clearer naming conventions and API consistency, such as replacing `detach` with `stop_grad` or `start_grad`; challenges in implementing batched operators (e.g., `gesv`, `cholesky`) and support for sparse tensor operations, including reductions, `coalesce`, transpose, and batching, to better support Graph Neural Networks; and debugging and build/test stability issues related to compiler flags (`-Werror`), runtime errors in CUDA, and ONNX export discrepancies. Several proposed solutions include refactoring APIs, expanding sparse tensor functionalities, adjusting build configurations, and verifying behavior through tests, though some issues remain unresolved or require further experimentation."
2018-11-18,pytorch/pytorch,"The discussions primarily revolve around effective data loading in PyTorch, highlighting the necessity of creating a persistent iterator outside training loops to avoid repeated batches and errors, with some suggestions to manage multiprocessing issues. There is also focus on implementing sophisticated conditional batch normalization modules, with multiple variants proposed to handle domain-specific normalization without shared running statistics. Several issues address technical pitfalls such as memory leaks, unsupported GPU operations (e.g., `Beta.rsample`), and version compatibility problems with PyTorch and Caffe2, often involving debugging and environment configuration. Some discussions suggest improvements in core functions like `eye_like` for better API completeness, and handling of in-place operations in JIT tracing. Unresolved concerns include fixing non-determinism in embedding gradients, addressing build errors on specific system configurations, and maintaining compatibility with different hardware and software environments."
2018-11-19,pytorch/pytorch,"The discussions reveal ongoing efforts to improve PyTorch's build and runtime stability, including fixes for CUDA-related issues, such as making the CUDA stack fork-safe, and addressing interoperability with different GPU architectures and driver versions. Several comments address the challenges of ensuring deterministic results, handling data format and device compatibility (especially for C++ inference and CUDNN support), and managing dependencies like protobuf and nccl to prevent conflicts or unresolved symbols. There are also discussions about API consistency, such as replacing legacy tensor aliases with modern equivalents, and addressing user-reported errors related to imports, runtime crashes, and test failures, often requesting additional debugging information. Overall, the conversations highlight the complexity of maintaining cross-platform compatibility, ensuring robust performance across hardware and software configurations, and refining API and build system coherence."
2018-11-20,pytorch/pytorch,"The discussions raise several key technical concerns including memory management behavior in GPU operations, particularly regarding caching allocators and memory freeing timing, as exemplified by grid_sample memory usage. There are questions about implementing support for operations like `grid_sampler` in ONNX, especially operators like `AffineGridGenerator`, and the need for expanding sparse tensor functionalities (e.g., reductions, transposing, batched support) to improve usability in graph neural networks. Several issues address build and compatibility challenges, such as CUDA, cuDNN, NCCL, and library linking errors on various platforms, with suggestions to use nightly builds or reconfigure dependencies. Additionally, there are concerns about the stability and API consistency of the C++ frontend, especially around in-source builds, the support for deterministic operations, and the development roadmap for features like fused comparison operators and mutability. Unresolved questions mainly focus on enhancing framework robustness, supporting new operators in ONNX, and establishing stable, user-friendly APIs."
2018-11-21,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA memory management, such as runtime out-of-memory errors and the need for more precise synchronization methods (e.g., using CUDA events or device-wide synchronization). Several issues focus on improving multi-GPU training and communication, including correct barrier semantics, NCCL stream synchronization, and ensuring consistent batch normalization across devices. There are concerns about platform compatibility, notably building and deploying PyTorch on various architectures (e.g., PPC64le, CentOS 6, ARM/Android) and the associated dependencies like MKL and protobuf. Some discussions address integration issues with TorchScript, ONNX export, and serialization compatibility, especially across Python versions and system environments. Lastly, there is an emphasis on codebase improvements, such as unifying tensor representations, suppressing redundant device calls, and refactoring build configurations for stability and portability."
2018-11-22,pytorch/pytorch,"The comments primarily address CUDA and GPU-related setup issues, including proper CUDA directory configurations, library paths, and driver compatibility, highlighting that incorrect installations can cause operational failures. Several discussions focus on the integration and testing of deep learning tools like cuDNN, MKL-DNN, and NCCL, emphasizing the need for CI support and proper configuration to ensure reliable builds and runtime performance. There are concerns about model serialization consistency, version compatibility (PyTorch, cuDNN, CUDA), and the effects on inference speed, with some users reporting significant slowdown in C++ exports versus Python. Issues related to ONNX export correctness, including graph shape and operator support, are also prominent. Lastly, there is ongoing work to improve the API's robustness, error handling, and feature exposure, such as barriers in distributed training, advanced indexing semantics, and backing support for sparse tensors."
2018-11-23,pytorch/pytorch,"The discussions highlight recurring issues related to CUDA compatibility and library linking, especially with CUDA versions and MKL dependencies, emphasizing the need for correct environment setup and dependency management. Several users face challenges in compiling or deploying PyTorch/LibTorch with OpenCV, CUDA, and Caffe2, often requiring adjustments to CMake files, environment variables, or reverting specific commits. There are ongoing efforts to improve code stability, such as fixing overflow warnings, refining API consistency, and addressing runtime errors like illegal instructions or missing shared libraries. Some conversations focus on enhancing performance through shared memory settings, proper device handling, and multithreading, alongside troubleshooting specific build errors across platforms. Unresolved questions remain about precise timelines for future releases, support for CPU-only configurations, and resolving dependency/linking errors in various environments."
2018-11-24,pytorch/pytorch,"The discussions highlight challenges in extracting model parameters from traced graphs in PyTorch 0.4.1, specifically issues with accessing weights embedded within nodes, and errors encountered when using deprecated or incompatible methods like `_get_method`. There is a recurring interest in improving support for non-standard neural networks with distributed training, with questions about the comparative performance and accuracy between Multi-GPU (DataParallel) and distributed (DistributedDataParallel) approaches. Users also express difficulties in deploying PyTorch and its libraries within C++ environments without Intel MKL, facing library loading errors such as `libiomp5.dylib`. Additionally, concerns arise regarding resource limitations causing kernel crashes in Jupyter notebooks during model training or inference sessions. Overall, these discussions underscore the need for better tooling, clearer guidance on model inspection, and more robust support for multi-framework and multi-language integrations."
2018-11-25,pytorch/pytorch,"The discussions highlight challenges with managing shared memory and data loading in Docker environments for PyTorch, with attempts to resolve runtime issues through configuration adjustments. There are ongoing efforts to enhance sparse tensor support, including batch matrix operations, serialization, and autograd functionality, along with questions about current development status and implementation plans. Profiling discrepancies between Python and C++ models, particularly slower C++ performance compared to Python, are noted, raising questions about efficiency and acceleration practices. Further, users encounter difficulties with C++ code generation from protobuf files, grappling with namespace issues and build compatibility, especially on macOS. Lastly, there are concerns about ONNX export fidelity, especially regarding unsupported layers like `pixel_shuffle` on certain branches, prompting testing and version management to ensure consistent model deployment."
2018-11-26,pytorch/pytorch,"The discussions highlight several critical technical issues, including potential redundancies and missteps in gradient optimization code, such as an unnecessary `.mul_(momentum)` operation, and challenges in implementing sparse tensor support, including reductions, coalesce options, and batched sparse tensors. Key questions revolve around enhancing distributed training robustness, like interface selection and MPI support, and addressing build and linking issues across platforms (e.g., MKL, CUDA, OpenCV integration). There are concerns about performance discrepancies between Python and C++ models, as well as ensuring reproducibility and deterministic behavior, especially with random number generators and floating-point operations. Several unresolved issues involve error handling, build failures, and ensuring correctness of exporting mechanisms (e.g., ONNX graphs), alongside suggestions for improving testing coverage and clarifying API behaviors."
2018-11-27,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch's functionality and interoperability, including the lack of general-purpose 1D and unstructured multi-dimensional interpolation functions and the need for more advanced indexing, warping, and sparse tensor support. There's emphasis on improving distributed training, with suggestions for better timeout handling, API design, and process management, alongside concerns about deadlocks and environment-specific bugs. Several discussions involve performance bottlenecks, such as slow tensor printing and addmm operations, as well as serialization issues related to undefined tensors and ONNX export stability. Addressing these issues involves both API enhancements—like explicit support for range locking, disacquisition of deprecated APIs, and flexible dataset APIs—and system-level improvements in memory management, environment configuration, and backend optimizations. Unresolved questions focus on ensuring backward compatibility, environment-specific bug fixes, and more robust, user-friendly tooling and documentation."
2018-11-28,pytorch/pytorch,"The discussions highlight several core issues: the proper usage of functions like `torch.cat`, requiring sequence and dimension arguments, and the need for clearer API consistency; challenges in supporting sparse tensor methods (e.g., `max`, `norm`) planned to be integrated into ATen, with user contributions encouraged; difficulties with CUDA and environment configurations causing segmentation faults, driver incompatibilities, or build issues, including specific cases like driver corruption or incompatible compiler flags; performance concerns such as slow tensor printing on GPUs, large tensor print outputs, and potential performance regressions in JIT or distributed training, with fixes like PR14418 addressing some; and ongoing work on feature extensions like batch matrix multiplication for sparse tensors, device unification with HIP, and debugging complex model conversion or tracing issues, with several unresolved environment-specific or implementation planning questions."
2018-11-29,pytorch/pytorch,"The discussions highlight ongoing development challenges in PyTorch, notably the incomplete support for complex tensors, including autograd and GPU acceleration, with contributors expressing strong interest and ongoing efforts. Several issues relate to low-level implementation details such as CUDA and ROCm build errors, memory errors, and linking problems, often resolved through patching or system reconfiguration. In addition, there are feature requests and bugs concerning tensor operations (e.g., `cat`, sparse matrix support, `einsum`, `dist` metrics), as well as usability improvements like process binding, data loader shared memory, and PyTorch integration with Jupyter. Some technical concerns involve ensuring compatibility across hardware and backends, fixing internal bugs (e.g., in `size`, `load_state_dict`), and improving code safety, especially with device and type management. Overall, many issues are targeted toward increasing PyTorch's stability, completeness of features (like sparse ops, complex support), and cross-platform consistency, with some problems addressed by patches and others awaiting further engineering solutions."
2018-11-30,pytorch/pytorch,"The discussions highlight ongoing efforts and challenges in extending PyTorch's support for complex tensors, especially on GPU and with autograd, with some users requesting better support and performance optimizations. Several issues address technical bugs, such as shape inference in ONNX export, memory management during distributed training, and CUDA driver interactions, often requiring workarounds or updates to the underlying libraries. Support for distributed training, including process binding, gradient synchronization, and error handling, remains a concern, with suggestions to improve robustness and user experience. Questions about proper installation, environment configuration, and backend library linking (e.g., MKL vs OpenBLAS) indicate needs for clearer documentation and tooling. Overall, unresolved bugs, performance bottlenecks, and feature limitations in tensor interoperability, distributed training, and compilation support are key challenges being actively discussed and addressed."
2018-12-01,pytorch/pytorch,"The discussions highlight recurring issues related to environment setups, library compatibility, and installation errors, particularly with PyTorch versions (notably 0.4.1) and CUDA drivers, where conflicts such as undefined symbols in shared libraries (e.g., libshm.so, libcaffe2.so) are prominent. Several users experienced crashes and import errors that were resolved through reinstallation of Nvidia drivers, CUDA versions, or cleaning build artifacts, emphasizing the importance of proper environment management. There are suggestions to optimize performance for global pooling operations by implementing specialized GPU kernels, and questions regarding differences in implementation impacts between one-step and multi-step or tensor-specific pooling operations. Additionally, there are concerns about coding practices, such as handling file end-of-line characters for cross-platform compatibility and ensuring reproducible, clean environments for debugging and testing. Unresolved topics include effective debugging with GDB in conda environments, how to avoid memory leaks linked to MKL, and strategies to properly serialize optional or undefined parameters in PyTorch models."
2018-12-02,pytorch/pytorch,"The discussions encompass multiple technical concerns including the limitations of current sparse tensor operations in PyTorch, especially regarding batch matrix multiplication and their integration with autograd, prompting requests for support for sparse batch multiplication and better sparse tensor kernels. There is an ongoing issue with performance variability in pooling operations such as global average and max pooling due to implementation differences and kernel configurations, leading to suggestions for optimized or specialized implementations. Errors related to model serialization, especially with torchscript, and compatibility issues with different versions and build configurations are also prominent, highlighting the need for improved model I/O support and versioning consistency. Concerns about numerical stability and accuracy in mixed-precision (fp16) tensor operations, particularly in mean calculations, are raised, noting regressions caused by recent implementation changes and potential workarounds. Additionally, several discussions focus on optimizing GPU performance, reducing overheads, and circumventing hardware-specific timeout or synchronization issues in multi-GPU setups, alongside requests for clearer APIs and functionality for model parameter management and shape inference."
2018-12-03,pytorch/pytorch,"The discussions highlight several key technical issues: challenges with CUDA and library loading errors, often related to environment configuration and version mismatches; memory management and memory leaks during training, necessitating careful tracking of variables and proper cache clearing; inconsistencies in model serialization/deserialization across PyTorch versions, emphasizing the need for matching export and runtime environments; the implementation and optimization of operations like global pooling and reduction kernels, which may benefit from specialized, faster kernels; and complexities in the JIT compiler, especially regarding constant handling, in-place modifications, and fusion/fusability of constants and scalar values, suggesting potential codegen fixes and API unifications. Several discussions also focus on improving robustness, error handling, and usability, such as warning mechanisms and better process management. Many unresolved questions remain about handling in-place modifications, scalar support, and environment-specific build issues, often requiring further targeted experiments, code fixes, or environment adjustments."
2018-12-04,pytorch/pytorch,"The discussions highlight ongoing challenges with GPU memory management, including reconciling variables that persist references and the effectiveness of `torch.cuda.empty_cache()`. Concerns are raised regarding the export and compatibility of models to ONNX, especially when involving operations like `F.interpolate`, `ConvTranspose`, and multi-GPU setups, with issues stemming from model graph representation and specific operator support. There's debate over the design and implementation of mutable versus immutable datasets within the DataLoader, emphasizing the need for clearer APIs and potentially separate data loading strategies for mutable datasets. Additionally, complications around environment setup, such as library linking (e.g., MKL, OpenBLAS), building from source, and platform-specific bugs, suggest that detailed documentation and improved build instructions are necessary. Unresolved questions remain about version compatibility, environment configuration, and enhanced support for complex models, parallel operations, and deployment scenarios."
2018-12-05,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch installation, especially related to environment management and package dependencies, such as issues with torchvision, CUDA, and libtorch configurations on various platforms. There is concern over ensuring correct type casting, reducing code complexity and build errors (notably in native and extension code), and managing hardware-specific bugs like slower GPU performance or issues with distributed training and tensor sharing. Multiple questions focus on proper setup procedures, such as linking CMake packages, handling version mismatches, and building from source, emphasizing the need for clearer documentation and robust setup practices. Additionally, efforts around refining API behavior (e.g., tensor casting rules, indices functions) and improving testing stability are recurring themes, alongside troubleshooting build failures and system-specific incompatibilities."
2018-12-06,pytorch/pytorch,"The discussions highlight several core concerns including the behavior of data parallelism with PyTorch's DataLoader, especially regarding the use of `num_workers` on Windows and in multi-threaded datasets, with proposals to push prefetching into datasets for efficiency. There are questions about proper implementation of type casting APIs, emphasizing NumPy-like control over operation safety (`casting` parameter) and type promotion, alongside considerations for integrating native and extension functions with improved casting semantics. Debugging and compatibility issues are also frequent, such as build failures on macOS related to compiler identification, linking errors with MKL-DNN, and specific runtime errors like NCCL hangs and distributed training inconsistencies. Several discussions focus on improving code structure and maintainability—such as minimizing use of templates, cleaning up code diffs, and robust testing—while unresolved issues often pertain to platform-specific build challenges and ensuring correct, stable runtime behavior across different environments and hardware configurations."
2018-12-07,pytorch/pytorch,"The discussions largely revolve around issues with PyTorch/Caffe2 build and deployment, including challenges compiling the library on macOS and Linux, especially regarding environment configuration (e.g., CMake setup and environment variables). Several reports address performance discrepancies between Python and C++ inference, notably slower C++ execution possibly due to tracing or graph optimization problems, and specific hardware/driver compatibility concerns (e.g., NCCL, NCCL bugs, GPU interconnect issues). There are also recurring challenges concerning model serialization formats (e.g., "".pt"" vs "".pth""), and problems with loading large models or protobuf serialization limits. Additionally, issues related to ONNX operator support, JNI bindings, and CUDA/NCCL runtime errors indicate ongoing efforts to improve stability, compatibility, and performance, though some questions remain unresolved and require further investigation and testing."
2018-12-08,pytorch/pytorch,"The discussions highlight ongoing challenges in packaging PyTorch for Debian, particularly due to licensing issues with CUDA/cuDNN, and the need for stable official packaging. Several users report environment setup complexities, such as IPython and CUDA compatibility, as well as problems with memory leaks and data loading, especially in multiprocessing contexts, suggesting issues with Python's copy-on-write behavior and shared memory management. There are concerns about the stability and correctness of certain PyTorch features, including the implementation of specific layers like SpatialCrossMapLRN, and problems with gradient computations, such as NaNs during training or issues with individual sample gradient extraction. Additionally, compilation errors related to MKLDNN, conflicting code in third-party repos, and protobuf version discrepancies are discussed, with some problems resolved by downgrading dependencies or adjusting build configurations. Overall, unresolved questions focus on stable packaging, environment reproducibility, and fixing bugs related to deep learning operations."
2018-12-09,pytorch/pytorch,"The discussions highlight core issues regarding PyTorch's evolving APIs and functionalities, including the re-implementation of legacy modules like SpatialCrossMapLRN, which are only available in earlier versions (e.g., v0.4.1). Several threads address compatibility and forward-compatibility concerns, such as troubleshooting model serialization with different PyTorch versions (`_rebuild_tensor_v2`) and support for features planned in later releases (e.g., improved DataLoader, iterative datasets, and new distributed training capabilities in v1.1). There are detailed technical questions about sparse tensor operations, specifically the importance of coalesced indices for accurate `spmm` results, and the handling of shared memory across processes for data loading efficiency. Additionally, multiple threads discuss the stability and reproducibility of features on various platforms, including issues with CUDA versions, MKL dependencies, and environment setups, alongside API design considerations like device guards. Overall, unresolved questions include timeline estimates for upcoming features, best practices for sparse tensor creation, and support for complex models in distributed training scenarios."
2018-12-10,pytorch/pytorch,"The discussions highlight several key technical concerns: a recurring issue with the implementation and behavior of sparse tensors, specifically the importance of coalescing indices to ensure correct matrix multiplication results and performance; challenges with memory leaks and resource management in data loading and multiprocessing, often related to Python’s copy-on-write behavior and object sharing; the need for consistency in functions like `unique()` to match NumPy's default sorting behavior across CPU and GPU; compatibility and stability issues related to the build environment, compiler versions, and dependencies such as MKL-DNN, with some errors stemming from merge conflicts and unsupported compiler features; and the desire for improved API flexibility and debugging support in JIT and CUDA operations, alongside improvements in build workflows and documentation practices. Unresolved questions include best practices for sparse tensor coalescing, handling specific runtime errors, and systematic testing of multilayer and cross-platform compatibility features."
2018-12-11,pytorch/pytorch,"The discussions highlight concerns about inconsistent and confusing error and warning messages within PyTorch, emphasizing the need for clearer, more uniform exceptions, especially around tensor operations like matrix multiplication and tensor gathering. Several threads address unresolved or problematic features, such as NaN gradients in gradient penalties, the behavior of `torch.multinomial` with `replacement=False`, and the correctness of sparse tensor operations, including the importance of coalescing indices for performance. There are ongoing debates about API design choices, such as whether to return tuples or tensors in certain functions, how to support device management and extensions like `stft`, and the best practices for handling MKLDNN and MKL dependencies, especially with versioning issues. Performance issues on specific platforms and dependencies, such as mkldnn updates and CUDA driver compatibility, are recurring concerns, along with compatibility and build-bot problems that hinder CI and user workflows. Overall, the discussions reveal a focus on improving error transparency, API consistency, computational correctness, and build robustness within PyTorch's development cycle."
2018-12-12,pytorch/pytorch,"The comments across these GitHub issues highlight several key concerns: the complexity of CUDA and compiler compatibility on macOS, particularly with specific architectures (e.g., 'compute_70') and Xcode versions; challenges in reproducibility during model checkpointing and restoration, especially with optimizers like Adam; the need for improvements in build environment cleanup, such as removing conflicting system-wide libraries (e.g., MKL-DNN); and ongoing efforts to refine PyTorch features, including device management, type hints, and model serialization formats. Proposed solutions include environment re-setup (e.g., clean conda environments), patching build configurations, and better documentation; unresolved questions involve cross-platform build workflows, handling of specific compilation errors, and improving CI robustness for various system configurations."
2018-12-13,pytorch/pytorch,"The comments reveal ongoing development efforts and feature requests within the PyTorch ecosystem, including proposals for supporting advanced RNN architectures like ConvLSTM, enhancements to interpolate and upsampling functions with mode-specific improvements, and integration of support for AMD GPUs and ROCm. Several discussions focus on improving compile-time configurations, build stability, and compatibility, especially concerning CUDA versions, compiler requirements, and cross-platform issues. There are also technical debates around type safety and API design, particularly for handling enums and keyword arguments, as well as concerns about performance, correctness, and proper error handling in various modules like autograd, tensor operations, and model serialization. Unresolved questions include the best approaches to extend support for new features (e.g., ConvLSTM), fixing existing bugs (e.g., memory leaks, slow operations), and ensuring backward compatibility and robustness across different system configurations."
2018-12-14,pytorch/pytorch,"The discussions highlight several technical issues, including environment management and CUDA-related performance regressions, often linked to specific hardware or software configurations. Notable concerns include the handling of padding strategies (notably 'same' padding in convolutions), with proposals for automatic padding calculation versus user-defined approaches, emphasizing usability versus efficiency tradeoffs. Multiple threads address the correctness and performance of custom derivative implementations, especially for convolution and reduction operations, questioning existing APIs and potential optimizations. Build and compilation issues are common, with suggestions to update compilers, dependencies, and submodules—particularly for C++ extensions and ensuring environment consistency. Finally, there are discussions about testing and stability, including timeouts, error diagnostics, and handling of special types like `optional<>`, indicating ongoing efforts to improve robustness and developer experience."
2018-12-15,pytorch/pytorch,"The discussions highlight several technical challenges: segmentation faults associated with DataLoader's `num_workers > 0`, likely due to multiprocessing and `set_start_method` misusage; variability in GPU versus CPU performance, especially with dilated convolutions and MKLDNN, indicating possible implementation or environment issues; bugs or inconsistencies in specific PyTorch operations such as `tril`, `unique`, and `CTCLoss`, with some potentially caused by differing CPU/GPU implementations or static versus dynamic linking problems; and a general need for clearer documentation and better API design, including more robust support for common utilities like one-hot encoding and enum handling. Many issues remain unresolved or depend on environment-specific factors, emphasizing the importance of correct setup, environment isolation, and further validation of GPU operations."
2018-12-16,pytorch/pytorch,"The discussions highlight several technical concerns related to PyTorch, including limitations and bugs in multiprocessing, such as the improper handling of `set_start_method()` calls and resource leaks when using `new_group`. There are issues with NCCL errors potentially caused by memory leaks or CUDA-related conflicts, often linked to environment misconfigurations or incompatible library versions. Some users report significant performance regressions with newer PyTorch versions, notably with dilated convolutions and MKL-DNN conflicts, which may be influenced by system-wide library installations. Additionally, there are questions about licensing transparency, the behavior of division by zero in autograd, and best practices for setup procedures in distributed training. Several unresolved questions involve debugging environment conflicts, improving documentation for multiprocessing, and handling special cases in autograd computations."
2018-12-17,pytorch/pytorch,"The discussions reflect ongoing challenges with CUDA-related memory management, including overflow issues in reserve size calculations and segmentation faults, particularly in long sequence RNNs and large batch computations. There are concerns about correctness and stability of operations like sparse tensor support, batch linear algebra routines, and backpropagation with sparse weights, indicating potential bugs or native implementation limitations. Several hardware/build environment inconsistencies, especially around MKL, cuDNN, and compiler configurations on macOS and Windows, hinder reproducibility and smooth integration of C++/Python modules. Additionally, conceptual and stylistic questions arise around enforcing code linting, API stability, and best practices for modular serialization, emphasizing the need for clearer guidelines and incremental improvements. Unresolved issues include gradient correctness in CTC Loss, performance discrepancies in gather vs. index operations, and the handling of nested or mixed module serialization, pointing to areas for future robustness and usability enhancements."
2018-12-18,pytorch/pytorch,"The discussions primarily revolve around improving state_dict handling in DataParallel to ensure correct model weight loading, with suggested modifications to export from `self.module` rather than always from `self`. Several issues concern correctness and stability in distributed training, including NCCL errors, stream management, and deterministic behavior, with solutions such as stream pooling and warnings for non-deterministic operations. There are technical questions about backward compatibility, especially regarding the loading of saved weights before and after wrapping with DataParallel, and about enhancing API usability through utility functions or schema changes. Other topics include codebase hygiene: enforcing code formatting with clang-format, suppressing or warnings on, problematic compiler flags, and managing build configurations across different platforms and compiler versions. Unresolved or ongoing discussions focus on establishing global deterministic modes and handling optional types, as well as fixing bugs in load functions and C++ static variable definitions."
2018-12-19,pytorch/pytorch,"The discussions highlight several technical concerns: (1) GPU resource management challenges, such as GPU reset conflicts requiring server restarts; (2) compatibility issues between different PyTorch versions and model save/load operations, especially regarding the `_rebuild_tensor_v2` function and backward compatibility; (3) build and installation hurdles on ARM architectures and CUDA-related errors, often due to misconfigurations of CUDA and cuDNN paths, library links, or environment variables; (4) performance considerations for distance computations and tensor operations, emphasizing the need for optimized implementations like `pdist` to reduce memory footprint and improve speed, and possible API enhancements for user convenience; (5) ongoing development issues, including bug fixes, feature requests (e.g., adding utility functions, controlling determinism), and unclarities around threading, distributed training, and backward support, with some discussions indicating workarounds or incremental improvements rather than final solutions."
2018-12-20,pytorch/pytorch,"The discussions predominantly revolve around enhancing PyTorch's functionality and usability, including issues with sparse tensor support, dataset and sampler API design, and distributed training synchronization. Key concerns include ensuring efficient GPU utilization and reducing overhead with custom operators, as well as maintaining compatibility across different hardware and compiler versions (notably support for MSVC v140 and v141). There are also technical challenges regarding autograd behavior with sparse gradients, accurate implementation of learning rate schedules, and proper testing methodologies, especially for components like ONNX export and MKL-DNN integration. Several comments suggest code refactoring or API improvements to simplify user experience, but some unresolved questions remain about backward compatibility, build stability on older systems, and performance regressions that need further investigation."
2018-12-21,pytorch/pytorch,"The discussion highlights concerns about optimizing internal functions like ""pack_padded_sequence"" to handle unsorted sequences more efficiently, and questions regarding whether torch.nn modules (e.g., BatchNorm) and operations (e.g., torch.ne, distributed functions) behave correctly under specific edge cases such as small batch sizes or distributed contexts. There are also recurring issues related to build failures, compatibility with hardware features like AVX and MKLDNN, and discrepancies in ONNX export and model loading across CPU and GPU, including potential bugs in ""torch.jit.load"" in C++. Several threads address install/configuration problems, notably around MKL-DNN dependencies, and the need for better testing, documentation, and consistency in behavior across backends. Additionally, some discussions focus on API stability, backward compatibility, and the design choices for torch scripting/named tuples, with unresolved questions about performance, correctness, and future features like matrix functions and optimized reduce implementations."
2018-12-22,pytorch/pytorch,"The discussions raise multiple technical concerns including compatibility issues with different hardware setups and software versions, such as building PyTorch on various platforms (Windows, macOS, Linux) and with different CUDA/cuDNN versions, which can lead to compilation errors, missing libraries, or runtime bugs like slow training with dilated convolutions. Several issues concern the stability and correctness of functionalities, e.g., handling of dtype and device during tracing, proper integration of MKL-DNN and MKL libraries, and ensuring that custom modifications (like extending symbolic functions or handling optional types) do not break backend or API consistency. There are specific queries about fixing compilation errors related to iterator tags, static linking of libraries, and kernel behaviors that could affect performance or correctness, including memory access violations and floating-point calculation inaccuracies. Some discussions also focus on improving build processes and maintenance strategies, such as managing dependencies, reorganizing code infrastructure, or revising APIs for clarity and backward compatibility, while unresolved questions include the proper handling of external library linking and compatibility with different compiler versions."
2018-12-23,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's capabilities, such as implementing double backward operators for NLP applications, ensuring deterministic behavior in DataLoader with multi-worker setups, and optimizing pairwise distance computations to reduce memory usage compared to full matrix expansions. There are concerns about compatibility and build issues, notably with MKL-DNN integration and cross-compiler ABI compatibility, especially on platforms like Jetson TX2 and Android. Debugging challenges are highlighted, including failures in specific functions like `test_stft` and the need for benchmarks to evaluate performance impacts of certain operations. Overall, unresolved questions include improving user convenience with tensor formats, providing stable and memory-efficient distance functions, and addressing platform-specific build and runtime issues."
2018-12-24,pytorch/pytorch,"The discussions highlight several technical concerns, including the default sharing strategy in multiprocessing, which can be checked with `torch.multiprocessing.get_sharing_strategy`, and issues related to memory allocation errors when working with large tensors or models, sometimes mitigated by setting sharing strategies or environment adjustments. There are recurring problems with non-deterministic behaviors influenced by worker initialization functions, requiring careful seed management and configuration, particularly in multi-worker data loaders. Users also report runtime errors with ONNX export, tracing, and operator compatibility, emphasizing the need for updates and workarounds. Additional questions involve compatibility of libraries, environment setup, and proper tensor-to-OpenCV conversions, alongside suggestions for improving error messages and handling specific features like cudnn benchmarking. Overall, these discussions point to ongoing challenges in ensuring reproducibility, efficient memory use, and compatibility across different PyTorch versions, environments, and deployment modes."
2018-12-25,pytorch/pytorch,"The discussions primarily focus on performance discrepancies between Python and C++ inference, citing significant timing differences and exploring potential causes such as threading, model retraining, or environment configurations like cudnn.benchmark. Several issues pertain to tensor compatibility and autograd-related errors, with suggestions to improve code portability by moving certain functionalities from THC to ATen. Hardware-related concerns also appear, including GPU resource failures and potential hardware faults. Additionally, challenges related to multi-threaded performance scaling, JIT scripting errors, and proper module replication are highlighted, with some requests for code reviews and fixes. Overall, unresolved questions center on optimizing inference speed, ensuring compatibility and stability across environments, and refining internal APIs for better performance and maintainability."
2018-12-26,pytorch/pytorch,"The discussions highlight several core issues: Firstly, the challenge of returning module views or slices from containers like `ModuleList` without violating registration rules, with suggestions to explicitly create views rather than return new containers. Secondly, the difficulty of properly enabling gradient tracking for multiple network outputs, especially when integrating with DataParallel, due to restrictions on changing `requires_grad` flags and the need for robust mechanisms for cloning or copying modules, particularly ScriptModules. Thirdly, various build and compatibility problems—such as undefined references, mismatched dependencies, and platform-specific issues—are discussed, often with workarounds or alternative approaches, including environment management and code modifications. Additionally, there are concerns about proper testing, clear documentation, and versioning of external dependencies like torch and torchvision, alongside questions on API design choices like nesting in `state_dict` or handling tuple outputs in C++. Unresolved questions include best practices for module sharing, the impact of recent code changes on performance, and ensuring stable, reproducible build environments across different systems."
2018-12-27,pytorch/pytorch,"The discussions encompass various technical concerns, including the rationale behind design choices such as explicitly defining `forward` methods instead of embedding layers into `Sequential`, and the limitations of certain PyTorch operations like `flatten` and `bucketize`, with some suggestions for implementing missing functionalities. Environment-specific issues are prominent, such as CUDA resource limitations on older GPUs, CUDA driver crashes during multi-GPU training, and the impact of different cuDNN algorithms on performance, highlighting the need for better hardware compatibility and configuration management. Several discussions address bug reports and potential leaks related to Eigen, MAGMA, and memory management in LAPACK routines, with some diagnosing that these are due to external libraries’ bugs rather than PyTorch code. On the high-level, questions about ONNX exportability, handling tuple outputs in C++, and the implications of backend settings like `cudnn.benchmark` show ongoing challenges in model serialization, interoperability, and performance tuning. Overall, unresolved questions pertain to feature support (e.g., `autograd` for cell states), performance regressions, and compatibility, with suggestions calling for clearer documentation, better environment handling, and external collaboration."
2018-12-28,pytorch/pytorch,"The discussions encompass various technical challenges related to PyTorch, including issues with multiprocessing on Windows and Linux, notably with DataLoader worker limits and sharing strategies; problems with specific operator crashes and debugging difficult segmentation faults; and performance inconsistencies or bugs with certain modules like ReflectionPad2d and module replication. Several participants seek guidance on profiling bottlenecks, managing memory leaks in MAGMA, and optimizing distributed training with DistributedDataParallel. Issues with build configurations, environment setups, and runtime errors such as cuDNN failures are also prominent. Many threads involve troubleshooting, potential workarounds, and requests for more robust, complete solutions to multi-process and distributed training challenges."
2018-12-29,pytorch/pytorch,"The discussions primarily address debugging and improving user experience in PyTorch, including clarifying error messages related to tensor dimensionality and shape expectations in convolutional layers, with proposed source code and documentation updates. Several issues concern compatibility and bug fixes, such as GPU hardware driver mismatches, CUDA/cuDNN version errors, and kernel launch limitations, with suggestions to revert or modify recent code changes to resolve silent failures or inaccuracies. There's also focus on maintaining determinism in operations, handling floating point precision regressions, and ensuring proper ONNX export support, alongside infrastructural issues like dependency management and environment information collection. Overall, the discussions emphasize enhancing robustness, clarity, and correctness of PyTorch features amidst hardware/software variations."
2018-12-30,pytorch/pytorch,"The discussions highlight several key technical concerns, including the absence of a native 'same' padding option in PyTorch's Conv2d, with some proposing a custom implementation to improve developer usability despite potential efficiency tradeoffs. There are also issues related to memory management and performance for pairwise distance computations, with suggestions to enhance built-in functions like `pdist` and `cdist` to handle large datasets more efficiently without excessive memory use. Additional topics include difficulties with in-place operations affecting gradient computations, the need for improved API support for mutable or iterable datasets, and challenges in cross-compiling PyTorch on ARM or Allwinner platforms. Some threads address technical bugs and build system issues, while others suggest improvements for automatic handling of tensor properties and streamlining multi-process training workflows. Overall, these discussions emphasize balancing usability, efficiency, and generality in PyTorch's ongoing development."
2018-12-31,pytorch/pytorch,"The discussions highlight common challenges in PyTorch, including correctly implementing gradient clipping and handling deprecations such as `clip_grad_norm`, with recommendations to use the updated `clip_grad_norm_`. Users frequently encounter environment setup issues, especially related to CUDA versions, driver compatibility, and library dependencies, often resolved by matching CUDA and PyTorch versions and setting appropriate environment variables. There are recurring difficulties with distributed training, such as errors with `pack_padded_sequence` in multi-GPU settings, which can be mitigated by using `DistributedDataParallel` and ensuring correct synchronization and tensor distribution. Performance optimization and benchmarking are discussed, emphasizing the importance of environment variables like `OMP_NUM_THREADS` and hardware-specific optimizations (e.g., AVX2), with suggestions for better tools and practices. Many issues stem from environment mismatches or incomplete minimal reproductions, underscoring the need for clear issue templates and precise diagnostics to facilitate effective troubleshooting."
2019-01-01,pytorch/pytorch,"The discussions primarily revolve around clarifying the behavior of `torch.tensor` regarding copying and memory sharing, with emphasis on understanding whether creating tensors from lists or NumPy arrays entails copying data. There are ongoing considerations about implementing multiple similar optimization algorithms efficiently within PyTorch, debating whether to consolidate them into a single interface or maintain separate implementations, with some advocacy for integration into existing classes like SGD or Adam. Several issues concern multi-GPU training and data handling, especially regarding the correct use of `torch.nn.utils.rnn.pack_padded_sequence()` with distributed data parallel models, and managing the correct distribution and size of `lengths` tensors in multi-GPU contexts. Troubleshooting GPU memory errors, such as CUDNN failures and runtime errors related to memory constraints, are also discussed, including adjustments like `cudnn.benchmark` and ensuring proper memory monitoring. Overall, unresolved questions include best practices for code compatibility across PyTorch versions, efficient model parallelism, and correct handling of input data in distributed training setups."
2019-01-02,pytorch/pytorch,"The discussions highlight several key technical concerns, including persistent bugs in cuDNN versions affecting training stability and memory management, particularly with large models or datasets, and issues with distributed training, such as deadlocks and synchronization challenges in `DistributedDataParallel`. Several comments address implementation details like proper usage of `#pragma` within macros or lambdas, ensuring correct kernel launch configurations, and handling large tensor operations safely, especially for pairwise distance computations. There are also ongoing efforts to improve build processes, static linking, and package support across CUDA versions, alongside modifications to enhance environment compatibility and maintainability, such as adding new APIs (`as_tensor`) and fixing mismatches in target ranges like `NLLLoss`. Unresolved questions remain about optimal configurations to prevent `CUDNN_STATUS_EXECUTION_FAILED` errors, correct kernel launch strategies for large-scale computations, and handling synchronization issues in GPU memory operations during distributed training."
2019-01-03,pytorch/pytorch,"The discussions primarily revolve around the implementation and behavior of various loss functions, notably the dice loss, and their impact on training in imbalanced 3D datasets; concerns include the handling of patches with all background (zero target sum) and the network's learning efficacy. Several issues address CUDA compatibility, GPU utilization, and system configuration challenges, including CUDA version conflicts, driver support, and static linking difficulties. Multiple threads highlight APIs, such as `torch.nn.functional` and `torch.nn.loss`, where features like dice loss are absent or require custom implementation. Questions about optimizing parallelism, stream synchronization, and device memory management—especially in multi-GPU or distributed contexts—are ongoing, with suggestions for better abstractions and debugging support. Unresolved topics include fixing runtime errors in distributed training, improving static linking, and refining documentation for broadcasting and API behaviors."
2019-01-04,pytorch/pytorch,"The discussions highlight several key technical issues with PyTorch, including compatibility problems with compilers (notably MSVC versions and openmp support on MacOS), and the need for better error handling in kernel launches (e.g., launch bounds and grid size limits). There's concern over the stability and reproducibility of in-place random operations across platforms and versions, as well as the performance and correctness of certain functions like `torch._standard_gamma` on expanded tensors. Several topics address extending PyTorch's capabilities, such as adding backend support for new tensor formats (SParseCUDA, layout options), implementing or improving distributed training (e.g., DDP across multiple GPUs), and integrating new features like Sobol sequences or spectral normalization, with questions about the complexity and best practices for these extensions. Unresolved questions include the best way to implement autograd operations using existing primitives, ensuring reproducibility across hardware and software platforms, and fixing underlying bugs in tensor operations or code generation."
2019-01-05,pytorch/pytorch,"The comments highlight ongoing efforts and challenges in implementing GPU-compatible histograms in PyTorch, with considerations around data tiling, CUDA-based algorithms, and recent PR developments such as the bincount support. Several issues address installation difficulties, especially on Windows, often related to dependencies like MKL, Visual Studio, or compiler compatibility, and solutions involve environment adjustments and package management. Additional discussions focus on probabilistic distributions such as Beta and standard gamma, spotlighting numerical stability, GPU biases, and potential underflow issues in gamma sampling implementations. Some issues pertain to codebase modifications, including memory leak fixes, API consistency, and build configurations across different compilers and platforms, with emphasis on proper in-place tensor operations and build modes. Lastly, there are concerns about specific feature requests like kernel pre-compilation and enhancement to support new data types, indicating an active development environment with ongoing problem resolution and feature expansion."
2019-01-06,pytorch/pytorch,"The discussions primarily revolve around limitations and bugs in exporting PyTorch models to ONNX, particularly with dynamic shapes and the `align_corners` parameter in `F.interpolate`, highlighting ongoing work to support these features and workarounds involving static sizes. Several issues address the challenges of ensuring compatibility between PyTorch, ONNX, and hardware drivers, including specific errors in model conversion, runtime warnings, and validation errors, with suggestions to use the latest master branches or modify model parameters. There are also concerns about implementing and registering new tensor scalar types like `bfloat16` in PyTorch, requiring careful interface design and the possibility of unregistering types, as well as discussions about CUDA stream synchronization, model parallelism, and potential performance improvements in sampling algorithms. Unresolved questions include fixing errors related to specific operators (e.g., `einsum`, `Dirichlet`), handling version conflicts after source builds, and improving error reporting and documentation for affected features. Overall, community efforts focus on resolving compatibility, stability, and functionality issues while proposing enhancements for model exporting, type extension, and device management."
2019-01-07,pytorch/pytorch,"The comments encompass various core technical issues, including behaviors of loss functions like Dice Loss in imbalanced datasets, and the lack of built-in support in torch.nn.loss. There are persistent CUDA initialization errors, often related to driver or library mismatches, particularly on different hardware or driver versions, and issues with CUDA driver initialization when using DataLoader in multi-processing contexts. Several discussions relate to improving support for custom tensor types, extending ONNX operator compatibility, and enhancing distributed training and stream management features. Additionally, environment setup and dependency management complexities are highlighted, emphasizing the need for clearer documentation and robust error handling in build and run configurations."
2019-01-08,pytorch/pytorch,"The discussions highlight persistent issues with PyTorch's compatibility and stability across various platforms, including DLL load failures on Windows, CUDA initialization errors, and shape broadcasting bugs, often resolved by environment adjustments or code modifications. Several users report difficulties exporting models to ONNX, frequently due to operators like `Upsample` or shape-dependent operations that challenge TorchScript or ONNX export processes, with suggested workarounds involving hardcoding shapes or switching ONNX versions. Concerns about reproducibility and deterministic behavior, especially with in-place random functions like `bernoulli_()`, are mentioned, alongside suggestions for better serialization and debugging tools. Common recommendations include ensuring correct CUDA and library versions, using environment variables, or switching to nightly builds to address runtime errors. Overall, the community seeks improvements in model export stability, runtime consistency, and clearer guidance for platform-specific issues."
2019-01-09,pytorch/pytorch,"The discussions highlight several technical concerns, such as clarifying tensor creation semantics and copy behavior, addressing performance regressions in cuDNN and OpenMP configurations, and handling operator support and ONNX compatibility, especially with shape-related nodes. There are suggestions for improving usability features like supporting 'same' padding natively, adding batch affine transformations, and enhancing documentation structure for clarity. Several issues involve ensuring compatibility with different hardware, CUDA versions, and build environments, as well as maintaining backward compatibility (e.g., deprecated features like load_lua). Unresolved questions include the best approach to extending tensor data types (e.g., bfloat16, int4), managing in-code registration/unregistration, handling specific bugs like sampling underflow or Python function calls in JIT, and configuring build systems for specific architectures. Overall, the discussions balance extending functionality, improving robustness and performance, and ensuring developer and user ergonomics."
2019-01-10,pytorch/pytorch,"The discussions cover various issues and enhancements in PyTorch, including debugging CUDA and cuDNN errors, particularly related to non-contiguous inputs, OOM conditions, and static linking challenges. Several issues pertain to improving API usability and performance, such as introducing batch support in grid_sample, refining tensor type promotion logic, and enabling batch affine transformations. There are concerns about PyTorch's development workflow, including Python IDE support (e.g., PyCharm autocompletion), testing infrastructure, and C++ extension compilation on Windows. Some discussions involve fixing crashes or unexpected behaviors in specific operations (e.g., unique, sort, serialization), often requiring code reorganization or patches. Overall, the conversations highlight ongoing efforts to improve stability, usability, and compatibility, while also addressing specific bugs and feature enhancements in the framework."
2019-01-11,pytorch/pytorch,"The discussions encompass various technical challenges, including difficulties in installing and building PyTorch with CUDA or with specific dependencies like cuDNN, NCCL, and OpenMP, often due to build environment or linker issues. Several reports highlight runtime errors such as segmentation faults, data loader crashes, or numerical precision problems (e.g., in `torch.lgamma`), sometimes linked to hardware compatibility (e.g., GPUs with different compute capabilities) or data types mismatches. There is ongoing development around extending PyTorch's tensor type system with custom scalar types and refining internal APIs and dispatch mechanisms, with questions about registration, unregistration, and future plans for c10 operator management. Additionally, some comments address limitations of current parallelism APIs, suggestions for improving the user experience, and questions about testing and reproducibility of bug fixes. Overall, unresolved issues often relate to build stability, hardware compatibility, and API extensibility, with community-driven suggestions and planned refactoring to improve flexibility and support for advanced use cases."
2019-01-12,pytorch/pytorch,"The discussions primarily address memory efficiency and object copying in custom convolution implementations, suggesting a modification to avoid additional memory overhead by using `F.pad` directly in the forward pass. There is concern over the correctness of object reference comparisons when slicing or padding tensors, highlighting that `id()` may not reliably indicate memory sharing, especially with `F.pad`. Several issues involve compatibility and build errors, such as CUDA support on Windows, compiler internal errors during Caffe2 build, and CUDA kernel execution failures due to device mismatch, with suggestions to test different configurations or environments. Additionally, questions arise about interoperability between PyTorch and libraries like CuPy via DLPack versus the `__cuda_array_interface__`, and considerations about supporting non-integer powers in matrix functions, as well as dependency and build system challenges affecting performance, particularly related to OpenMP and MKLDNN on different platforms."
2019-01-13,pytorch/pytorch,"The discussions primarily address performance and memory efficiency in implementing causal convolutions in PyTorch, with suggestions to avoid unnecessary object creation and memory allocation through alternative padding strategies. There is a concern about the accuracy and interpretation of object identities used to infer memory sharing, emphasizing that `id()` does not reflect shared data, and highlighting that functions like `F.pad()` do allocate additional memory. Several issues involve compatibility and build errors, including compiler internal errors, and dependencies like LMDB and NNPack, where solutions include code modifications and environment adjustments. Other discussions focus on extending PyTorch features such as support for `enumerate`, `zip`, and integrating new linear algebra routines, along with considerations for hardware-specific optimizations like OpenMP and MKL-DNN. Unresolved questions include optimal strategies for interoperability protocols like `__cuda_array_interface__` versus DLPack, and handling specific runtime errors or bug reports related to GPU and C++ extensions."
2019-01-14,pytorch/pytorch,"The discussions highlight significant challenges with supporting MPI backend in `DistributedDataParallelCPU`, with errors related to MPI support and build issues across environments. Several issues concern CUDA and GPU support, including linker bloat possibly caused by architectures, and mysterious internal compiler errors with certain GCC versions requiring patching. There are ongoing efforts to improve operator support, including defining new operators, handling list inputs, and supporting zero-sized tensors for better developer experience. Compatibility and correctness of CUDA kernel operations like gather and scatter, especially with different data types and device configurations, remain points of investigation. Additionally, maintenance of build dependencies, documentation, and support for features like model parallelism show active development and unresolved hurdles."
2019-01-15,pytorch/pytorch,"The discussions primarily address compatibility and stability issues in PyTorch, such as potential bugs with CUDA memory sharing, tensor contiguity, and serialization, often seeking reproduction scripts and detailed logs. There are concerns about framework build configurations, especially concerning architecture-specific compilation and linking errors on Windows and Linux. Several issues involve API deprecations, proper handling of distribution sampling (notably Dirichlet and Gamma functions), and the integration of scientific functions or mathematical features like special functions and autograd support. Additionally, multiple threads highlight the need for better testing, code merging, and clear documentation for complex features, including distributed training and custom memory layouts. Overall, unresolved questions involve persistent memory leaks during multiprocessing, build-related runtime errors, and ensuring feature compatibility across platforms and versions."
2019-01-16,pytorch/pytorch,"The discussions highlight several key technical concerns: compatibility and reproducibility issues with different hardware and software configurations, such as compiler and CUDA versions, which can cause crashes or inconsistent behavior; implementation details like the need for proper type handling in GPU operations (e.g., gather kernel type templating) and the subtle effects of numerical differences in matrix decompositions (eigen and SVD) across libraries; and usability improvements, including better error/warning messages for common pitfalls in loss function usage, and ensuring build setups are robust across platforms. There are questions about legacy support, build optimization, and API design, such as handling dynamic member inspection and the granularity of gradient control. Some proposed solutions involve code refactoring, stricter validation, or documentation clarifications, but certain issues remain unresolved or require further investigation. Overall, these discussions underscore ongoing efforts to improve stability, clarity, and cross-platform compatibility in PyTorch's ecosystem."
2019-01-17,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, including expanding interpolation capabilities, support for unstructured 2D interpolation, and integrating SSIM for GANs, with concerns about implementation scope and correctness. Multiple issues address CUDA and GPU-related stability, performance, and linking problems, emphasizing proper error checking, device synchronization, and precision consistency, alongside questions about best practices for device management and multi-GPU operations. There are also discussions on software infrastructure improvements such as build optimizations, C++ API coverage, documentation accuracy, and workarounds for known bugs, notably those affecting sparse tensor operations and model serialization. Additionally, several technical questions concern the appropriate design patterns for optional tensor initialization, kernel launch configurations, and compatibility across different compiler environments and hardware architectures. Unresolved issues mostly involve correctness, performance, and maintainability of GPU kernels, serialization, and integration with internal and external tools."
2019-01-18,pytorch/pytorch,"The discussions highlight ongoing efforts to extend and refine PyTorch's functionality, such as implementing missing tensor operations (`type`, `type_as`, `potrs`), enhancing data loading strategies for varied dataset structures, and improving performance through vectorization and process spawning optimizations. Several issues address compatibility and debugging challenges, including errors related to mkldnn, protobuf integration, and model serialization formats, as well as performance regressions on different hardware platforms and environments. There is also concern about ensuring correctness and stability of core algorithms like `svd`, `pseudoinverse`, and `flatten`, especially for ill-conditioned matrices or large-scale data. Proposals involve adding fallback mechanisms, better tooling, or more extensive testing to handle edge cases and regressions. The unresolved questions focus on completed implementation status, correctness validation, and ensuring robust, backward-compatible features across diverse systems."
2019-01-19,pytorch/pytorch,"The discussions highlight several technical challenges, including CUDA initialization errors during data loading and tensor operations, often addressed by adjusting multiprocessing settings, although some users report persistent issues. There is concern over the correctness and maintainability of code generation, especially regarding support for unnamed fields, macro macros, and compatibility across Windows/Unix systems. Discussions also involve enhancing code analysis and documentation generation with tools like mypy and xdoctest, with debates on dependencies and implementation details. Additionally, several bug reports, such as segmentation faults in loss functions and performance regressions after refactoring, indicate ongoing debugging and validation efforts. Overall, unresolved questions about file generation, cross-platform issues, and testing infrastructure remain central to ongoing development."
2019-01-20,pytorch/pytorch,"The discussions highlight ongoing issues with GPU stability and compatibility in PyTorch, such as system crashes due to hardware problems or BIOS settings, and specific operator-related bugs like issues with `Int8Conv` and other quantized operations. Several users report challenges when working with multi-GPU setups, multiprocessing, and CUDA tensor serialization, often caused by storage mismatches, cache invalidation, or memory overwrites, with proposed fixes involving cache key modifications or programmatic workarounds. There are questions about how to properly build and test AMD ROCm support and concerns over debugging and traceability of internal errors, including how to disable progress bars and capture more detailed trace outputs. Additionally, users seek clarification on internal fixes, best practices for multiprocessing with GPU tensors, and alternative solutions for unsupported hardware configurations. Overall, unresolved technical issues involve stability, debugging, and compatibility across hardware, operators, and PyTorch features."
2019-01-21,pytorch/pytorch,"The issues primarily revolve around enhancing CUDA kernel implementations for specific tensor operations like `matmul(S, S)` and addressing performance or compatibility problems in GPU computation, such as memory leaks in `gloo` backend and GPU usage updates. Several discussions concern resolving compilation and runtime errors in PyTorch's C++ API and Python interfaces, including handling model serialization, setting GPU IDs, and managing model constants. There are concerns about the correctness and stability of multiprocessing with GPU distributions, alongside requests for clearer documentation on ownership models and API usage, especially in the context of scripting modules and storage management. Some threads highlight the need for updating build environments and dependencies, such as installing `psutil` in CI pipelines, and optimizing code constructs for efficiency. Unresolved questions include verifying compatibility of specific PyTorch versions with advanced features like GANs, and clarifying best practices for managing tensor lifetimes and device placement in multi-processing and C++ environment setups."
2019-01-22,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting efforts within PyTorch, including updates to C++ functions, ATen operator modifications, and support for features like namedtuple support in JIT. Key technical concerns involve ensuring backward compatibility of serialized models, resolving errors related to tuple handling in torch.jit, and addressing issues with operator implementation (such as `_unique` and `Gather` for specific tensor types). Several questions remain about progressing with features like gradient support in non-symmetric matrices, efficient error checking, and submodule handling in JIT-compiled models. Additionally, there are unresolved setup and environment issues, including CUDA compatibility, build system adjustments, and multi-version tensor conversion behaviors, all of which are critical for stable, efficient development and deployment."
2019-01-23,pytorch/pytorch,"The discussions highlight several technical concerns, including compatibility issues between PyTorch and CUDA versions (notably CUDA 9.2 and driver mismatches), where users seek guidance on installation mirrors, environment setup, and bug fixes. There are questions regarding the support for features like zero-sized tensors, dynamic shape handling in ONNX export, and improvements for multi-GPU training consistency and memory management. Several issues involve bug fixes and code refactoring, such as moving certain functionalities from THC to ATen, addressing protobuf build problems, and optimizing memory allocations, with some being resolved (e.g., #15092) and others pending further work. Moreover, concerns about build system configurations for Android and OpenCL compatibility for AMD hardware are raised, alongside code usability enhancements like handling tuple outputs in C++, and ensuring that code changes don't regress existing features. Unresolved questions remain about maintaining backward compatibility, testing new features across platforms, and streamlining build processes on different hardware and OS configurations."
2019-01-24,pytorch/pytorch,"The discussions highlight several technical concerns including the lack of native support for 'same' padding in PyTorch, which impacts developer productivity, and the potential performance implications of padding calculations. Memory management issues are repeatedly raised, particularly relating to CUDA IPC synchronization, stream handling, and leak detection, with suggestions to improve synchronization and allocate tracking (e.g., using RDTSC or user-defined counters). Compatibility and robustness questions are also prominent, including the correctness of type annotations, handling of non-contiguous tensors, and environment-specific issues like OpenMP and driver versions. Additionally, enhancements are proposed for the JIT and serialization APIs to better support namedtuples, custom state, and debugging, as well as extending feature support for sparse gradients and ONNX conversions. Several unresolved questions concern the implementation details and tradeoffs of these features, as well as testing approaches for non-contiguous or memory-sensitive operations."
2019-01-25,pytorch/pytorch,"The discussions primarily revolve around runtime errors, segmentation faults, and memory management issues in PyTorch, often related to system library conflicts, CUDA driver compatibility, or incorrect usage patterns (e.g., CUDA IPC memory, tensor cloning, and multithreading). There are concerns about improving error messages, especially for driver-related problems, and handling memory leaks or leaks-like behaviors caused by lists versus tensors. Some issues involve integrating features like namedtuple support in JIT, extending operator functionalities, or optimizing performance for specific workloads (e.g., quantization, convolution). Additionally, several discussions address build failures, dependency mismatches, and compatibility issues with different hardware (like Turing GPUs) or software environments, with proposed solutions including code fixes, environment adjustments, or better documentation."
2019-01-26,pytorch/pytorch,"The discussions highlight several critical issues, including a persistent KeyError when using multiple data loader workers with HDF5 datasets, which might be mitigated by adjusting the start method or exception handling. There are ongoing performance and compatibility concerns with PyTorch's support for sparse tensors, including slow operations with non-standard sparse tensor creation and functions in sparse tensor math. Several threads address optimizing GPU operations, such as convolution algorithms, outlier quantization techniques, and integrating int8 support via MKL-DNN or FBGEMM, with considerations about hardware-specific benefits like VNNI or different workload scenarios. Additionally, there is interest in enhancing JIT support for dynamic data structures like dictionaries, as well as refining the implementation and documentation of features like shape manipulations, custom operators, and type annotations. Unresolved questions include verifying bug fixes on master branches, supporting nested data structures in TorchScript, and managing cross-language module interoperation, all indicating an active effort to improve stability, performance, and usability."
2019-01-27,pytorch/pytorch,"The discussions highlight several technical concerns, including potential issues with the correctness of the current AdamW implementation in PyTorch, which some users find diverging from the original paper's decay behavior. There are questions about proper usage and support for sparse gradients in embedding layers, especially regarding C++ class implementations, and performance discrepancies in sparse tensor operations, notably the impact of coalesce() on CPU vs. GPU. Several reports of runtime errors, memory management challenges, and build compatibility issues—such as CUDA version mismatches and linked library configurations—are also prevalent. Additionally, there's a call to update documentation for clarity on vector-Jacobian products, along with maintenance suggestions like fixing build issues or improving performance for specific operations, while unresolved questions concern how to best integrate or fix these aspects within the PyTorch ecosystem."
2019-01-28,pytorch/pytorch,"The discussions mainly revolve around improving PyTorch's usability and deployment, such as enabling more flexible network cloning and parameter freezing (with issues surfacing in DataParallel with non-leaf variable modifications), enhancing support for shape inference, nested dictionaries, and negative strides, and fixing memory and runtime errors on various platforms. Several threads highlight the need for better error messages, backward compatibility considerations, and ensuring correctness across different hardware and library versions, including CMake build configurations and CUDA/cuDNN compatibility issues. There are ongoing efforts to accept incremental patches, improve code clarity, and fix bugs, such as out-of-memory errors, inconsistent operator registrations, and memory leak detections, while some debates focus on architectural choices like tensor serialization in C++ versus Python. Unresolved questions include how to implement cross-allocator memory tracking, supporting non-leaf variable modifications, and improving documentation or testing to prevent regressions. Overall, many concerns aim to stabilize, optimize, and make PyTorch more robust and user-friendly in varied deployment environments."
2019-01-29,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's capabilities, including implementing GPU-accelerated 1D interpolation, expanding support for structured outputs like namedtuples in TorchScript, and improving interoperability in JIT and C++ frontends. Key technical concerns involve ensuring performance and correctness, such as optimizing sparse tensor operations and addressing memory leak issues, as well as ensuring compatibility with various compiler and environment configurations across different platforms. Some discussions suggest potential API or infrastructure changes, like enabling unstructured 2D interpolation, adding support for serialization of complex IValues, and integrating support for user-defined classes with state. Multiple unresolved questions pertain to debugging runtime errors (e.g., cuDNN BAD_PARAM), managing build and dependency issues (like OpenMP, protobuf), and ensuring backward compatibility while evolving APIs. Overall, these threads reflect active development and troubleshooting efforts aimed at broadening PyTorch’s functionality and robustness."
2019-01-30,pytorch/pytorch,"The discussions highlight several technical concerns including persistent issues with deadlocks and performance regressions related to CUDA, ROCm, and MKL-DNN, often requiring rebuilding from source or upstream fixes. There are also notable questions about API consistency and usability, such as the appropriate handling of `view` versus `reshape`, the design of the `smooth_l1_loss` with an additional `beta` parameter, and the implications of changing function signatures or internal structures like `KernelState`. Several discussions address build failures, compatibility issues on different systems (Windows, CentOS, Ubuntu), and ensuring backward compatibility when modifying existing APIs or behaviors. Furthermore, improvements are proposed for enhancing ONNX export support, debugging workflows, and the integration of custom kernels in multi-GPU environments, alongside concerns about proper testing approaches and CI reliability. Overall, unresolved questions remain around optimal API design, performance tuning, interoperability, and robust testing."
2019-01-31,pytorch/pytorch,"The discussion highlights several core issues: the need for better support of shared memory strategies and multiprocessing configurations across different operating systems, with specific challenges on Windows versus Linux; the handling of parameter sharing and control flow during model tracing and scripting, especially to support complex models with weight sharing; and the importance of enhancing the dispatch and dispatch table mechanisms for thread safety, kernel lookup, and extensibility. Additionally, concerns are raised about hardware and driver support, such as CUDA version compatibility, MKL-DNN bugs, and ROCm stability, which impact performance and stability. There is also emphasis on improving user-facing API clarity, including tracing varied data structures like namedtuples and dictionaries, as well as debugging and testing infrastructure. Unresolved questions include strategies for supporting complex data structures in tracing, fixing platform-specific multiprocessing bugs, and optimizing kernel dispatch and memory management for advanced hardware."
2019-02-01,pytorch/pytorch,"The discussions highlight ongoing challenges with supporting complex tensors in PyTorch, including performance bottlenecks, GPU autograd support, and C++ extension compatibility, with suggestions to optimize custom implementations and improve ecosystem support. Memory management issues are a common concern, particularly fragmentation, CUDA out-of-memory errors, and the need for better profiling tools and allocator unification, along with strategies like `drop_last=False` and setting `cudnn.benchmark=True`. There are also broad architectural questions about the design of dispatch mechanisms, including the use of hash maps versus vectors, kernel registration, and metadata handling for scripted modules, emphasizing the need for clearer APIs and data handling consistency. Multiple contributions discuss improving test robustness, reducing boilerplate, and ensuring platform compatibility, especially with heterogeneous hardware and CUDA versions. Unresolved questions remain regarding the support for nested data structures, the handling of undefined tensors in autodiff, and optimizing multi-GPU and NUMA-aware memory allocation strategies."
2019-02-02,pytorch/pytorch,"The discussions primarily revolve around the challenges of accurately measuring FLOPs for non-traditional operations like top-k, sparse sum, and indexing, as traditional tools are insufficient. Several issues address build failures and compatibility concerns, especially regarding dependencies on dependencies like MKL, OpenMP, and cross-platform issues (e.g., ppc64le, macOS). There are concerns about memory management and potential memory leaks in complex models, with suggestions to monitor OOM behavior and thread settings. Code review and rebasing are ongoing for numerous patches, asserting that some failures are unrelated to current PRs or infrastructure issues. Lastly, there are conceptual considerations around registry registration and dependency management in the framework's extension and build systems."
2019-02-03,pytorch/pytorch,"The discussions highlight ongoing concerns about performance regressions and accuracy issues in PyTorch, particularly related to CUDNN algorithm selection, solver behaviors, and optimizer implementations (e.g., AdamW). Several comments seek clarifications on the root causes and fixes for these regressions, including questions about internal functions like cudnnSetConvolutionMathType and the proper application of weight decay. Users report issues with installation hurdles, such as installing Caffe2 and compatibility with different versions, and request updates on unresolved bugs and upcoming releases. There is also interest in ensuring robustness through testing for regressions and confirming correctness of new features like AdamW. Overall, the community urges transparent communication about fixes, clear explanations of root causes, and improvements in testing to prevent future regressions."
2019-02-04,pytorch/pytorch,"The discussions highlight several technical issues, including the complexity of implementing a Maxout layer in PyTorch and the debate over whether to include a general reference implementation in the core library. There are concerns about the robustness of JIT features, specifically about querying the JIT state and its impact on scripting compatibility, with suggestions for exposing such a flag. Several issues relate to ONNX model export errors, memory leaks in Gloo, and CUDA compatibility, emphasizing the need for better error handling, build environment consistency, and support for newer GPU architectures. Additionally, there is discussion around improving the design of distributed training interfaces, such as DataParallel, by allowing more flexible device and gradient handling, and around limitations or bugs in PyTorch's operator implementation and testing infrastructure. Many unresolved questions focus on fixing existing bugs, handling environment-specific incompatibilities, and enhancing the user experience with better debugging, error messaging, and feature support."
2019-02-05,pytorch/pytorch,"The discussions highlight several core issues: (1) Challenges with memory management and resource sharing in CUDA, especially regarding stream synchronization and allocator behaviors; (2) Limitations and bugs in serialization of non-tensor attributes within `script::Module`, with proposals for enhancing flexibility through handling extra files and supporting various `IValues`; (3) Compatibility and interfacing concerns between `torch.nn.Module`, `torch.jit.ScriptModule`, and C++ modules, emphasizing the need for better interop and user-friendly API adjustments like support for `Device` parameters in Python bindings; (4) Regressions and build issues stemming from recent commits, including potential bugs in CUDA algorithm selection, support for CUDA in the PyTorch builds, and the impact of compiler flags; (5) Ongoing enhancements to testing frameworks incorporating `xdoctest`, and improvements in handling model attributes, metadata, and probabilistic modules, alongside unresolved questions about serialization support for complex data types and stream management semantics."
2019-02-06,pytorch/pytorch,"The discussions highlight ongoing challenges in supporting complex and sparse tensor operations in PyTorch, including performance issues in custom complex number implementations, and the need for optimized support for large sparse matrices and block-sparse methods. Several comments address specific bugs and performance regressions introduced by recent PRs, particularly related to autograd, CUDA kernel configurations, and memory management, especially on different hardware like V100 and ROCm environments. There are concerns about compatibility and API changes, such as refining `torch.split` argument types, handling attribute errors with `__getattr__`, and ensuring threading and data loading efficiency across different platforms. Additionally, multiple users inquire about the development timeline for mathematical functions like `expm` and `logm`, and the integration of probabilistic modules, with some suggestions to align with frameworks like TensorFlow's Bijectors. Overall, unresolved issues involve performance bottlenecks, stability across hardware/OS, and expanding support for advanced math functions and probabilistic models."
2019-02-07,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality and usability, including adding advanced operations like LocalConv2d, support for complex number arithmetic, and padding options such as 'SAME'. Several threads address engineering challenges, such as reducing memory usage for locally connected layers, optimizing custom CUDA kernels, and resolving build issues related to third-party dependencies like MKL-DNN, MAGMA, and HF libraries. There are concerns about maintaining consistency with existing interfaces, especially regarding parameter sharing in autograd and the design of saving/loading models with extra metadata. Additionally, questions about correctness, such as ensuring accurate behavior of batch normalization during inference and proper shape analysis for in-place operations, remain, alongside suggestions for better testing practices and compatibility improvements across PyTorch versions."
2019-02-08,pytorch/pytorch,"The discussions highlight persistent issues with PyTorch's compatibility and build processes, including segmentation faults when importing certain libraries (e.g., nmslib), and the need for improved autocompletion and type support in IDEs like PyCharm, which has been addressed via stub patches. Several threads focus on low-level implementation details, such as adding support for `torch.tensor()` in JIT, handling `beta` parameter in custom C++ extensions, and optimizing MKLDNN support for non-contiguous tensors, often requiring careful build configuration adjustments. Concerns are raised about the proper management of shared pointers in custom Python bindings, especially regarding `shared_from_this()` usage and ensuring correct memory semantics. Additionally, there are ongoing validation and performance considerations around CPU feature dispatching, multi-threading controls, and compatibility across hardware architectures, along with systematic tests and regression fixes needed for robustness across different environments."
2019-02-09,pytorch/pytorch,"The discussions highlight ongoing challenges in managing CUDA and GPU memory, particularly monitoring rapid CUDA memory changes and cleanup after kernel interruptions, with some users seeking solutions for better memory tracking and flushing. Documentation gaps are evident for deprecated tensor creation APIs, C++ API clarity, and build dependencies, especially regarding NCCL and ROCm support, causing build failures and configuration confusion. Several issues address compatibility and integration problems with editors like PyCharm, type stubs, and autocompletion, along with efforts to improve code modularity by refactoring compiler and parser components while avoiding disruptive dependencies. Performance regressions and correctness concerns are also discussed, notably in random number generation quality, shape exporter correctness for ONNX/ONNX-TRT, and attribute lookup speed, with proposals for new algorithms and structural improvements. Finally, unresolved questions remain around build environment setup, testing workflows, and ensuring the stability of complex multi-component compilation processes."
2019-02-10,pytorch/pytorch,"The discussions highlight ongoing efforts to extend PyTorch's capabilities, such as integrating deformable convolutions and improving Android support, with questions about future plans and existing limitations. Several issues pertain to build environment challenges, including compatibility with specific compilers, CMake versions, and hardware toolchains, indicating the need for improved CI and documentation. There are concerns about ensuring correctness and stability in features like unified ONNX export semantics, uniform random number generation, and memory management, often requiring additional tests and code standardization. Additionally, users seek guidance on code customization and handling edge cases, such as supporting kwargs in autograd functions and managing GPU sharing on Windows. Unresolved questions remain around optimizing build processes, fixing test failures, and extending compatibility across platforms."
2019-02-11,pytorch/pytorch,"The discussions primarily revolve around technical challenges in building, configuring, and deploying PyTorch, including resolving compatibility issues with various Visual Studio, CUDA, and CUDA-related libraries (like cuDNN, libcuda.so), as well as system dependencies on older Linux distributions with outdated glibc versions. Several questions address build environment inconsistencies, such as compiler support for instruction sets (AVX, AVX512), threading safety, and linking behaviors, especially in relation to MKL and NCCL. There are ongoing efforts to improve automation, testing robustness, and feature enhancements, such as support for named tuples in JIT, deterministic hashing, and better CUDA/GPU multi-process handling. Unresolved issues include build failures due to compiler warnings treated as errors, flaky CI test failures, and incompatibilities with system configurations like CircleCI or specific hardware architectures. Several suggestions aim to optimize or troubleshoot build processes, runtime behavior, and dependency management for improved stability across diverse environments."
2019-02-12,pytorch/pytorch,"The discussions highlight challenges with environment-specific issues such as DLL loading failures on Windows, DLL load errors in CUDA-related components, and CUDA driver compatibility, notably related to driver mode and TCC configuration. Several comments address build and compilation problems, including CUDA version mismatches, GCC compatibility issues particularly with CUDA 9.2, and errors caused by macro definitions or unsupported compiler versions. There are concerns about maintaining correct and consistent versioning, especially in relation to Git tags and PEP compliance, as well as handling deprecation or feature support across different Python versions (e.g., tuple versus list annotations). Additionally, robustness and correctness issues in code, like in-place tensor operations, asynchronous CUDA error handling, and support for complex data structures in TorchScript, are raised, along with suggestions for improving build scripts, test coverage, and documentation clarity."
2019-02-13,pytorch/pytorch,"The discussions highlight several key concerns: the need for improved error messaging and handling for matrix conditioning issues and CUDA kernel synchronization problems; the challenges of maintaining compatibility and performance across different hardware configurations and software environments, including issues with MKL-DNN and CMake settings; potential API overhauls for modules like `nn.DataParallel`, including attribute forwarding and device management, balanced against breaking changes; efforts to optimize performance, such as fixing integration issues with mxnet, refining operator implementations, and benchmarking backward compatibility; and ongoing environment stability and CI reliability issues, with questions about version compatibility, build configuration, and testing coverage, especially for platform-specific builds. Unresolved questions include how best to support list of dicts in C++ frontend, managing version-dependent behaviors, and standardizing environment setup procedures for reproducible, efficient workflows."
2019-02-14,pytorch/pytorch,"The discussions highlight several technical concerns: unresolved GPU compatibility issues, particularly with AVX instruction sets, and the need for better handling of dynamic shapes during model export and optimization (e.g., static vs. dynamic ONNX models). There are recurrent problems around DLL loading errors on Windows environments, often linked to environment configuration or dependencies like MKL-DNN, and issues with library linking and build configurations, such as variadic macros causing compiler errors on MSVC. Additionally, there are ongoing efforts to improve the robustness and usability of model serialization, memory management, and graph representation, including handling exceptions like deadlocks and NaN propagation, as well as extending support for control flow, alias analysis, and tensor layout optimization. Some questions remain unresolved, such as how to best support dynamic shapes in ONNX, refine error messaging, and improve cross-platform build consistency—particularly for performance-critical components like MKL-DNN."
2019-02-15,pytorch/pytorch,"The discussions highlight various technical issues and enhancements within PyTorch, including memory management bugs, data loader threading vs multiprocessing, and runtime dispatch failures, often requiring detailed debugging, code refactoring, or environment adjustments. Notable concerns include potential shape mismatches during tensor operations, warning mechanisms for broadcasting, and ensuring compatibility across Python versions and CUDA setups. Several issues are related to build and environment configuration problems, such as dependencies, external library integrations, and compiler flags. Contributors frequently suggest creating additional tests, refactoring code, or rebasing branches to mitigate CI failures or improve stability. The overall focus is on debugging, optimizing performance, extending functionality, and improving tooling for the PyTorch ecosystem."
2019-02-16,pytorch/pytorch,"The discussions predominantly revolve around enhancing the flexibility and usability of hooks in PyTorch, specifically the ability to modify or access inputs and outputs during the forward pass, which is currently limited. There's a recurring request to support manipulating forward outputs within hook functions, with community contributions welcomed via pull requests. Several issues concern build and environment compatibility, especially on Windows, as well as the need to support newer Python features such as named tuples and structseq, and fixing multiple installation and import errors. Additionally, ongoing efforts aim to improve documentation clarity, handle deprecation and API design concerns, and optimize performance in specific operations like batch matrix multiplication and resampling methods. Unresolved questions include the prioritization of these feature requests and fixes, and the confirmation of fixes or workarounds through further testing or backtraces."
2019-02-17,pytorch/pytorch,"The discussions highlight several technical concerns including environment-specific issues with DLL loading and GPU memory management, particularly on Windows systems, with some solutions involving Python version downgrades or installing specific PyTorch versions. There are questions about compatibility between CUDA versions and GPU hardware (e.g., RTX 2080Ti requiring CUDA 10), and challenges related to proper installation in conda versus pip environments. Performance and correctness improvements are discussed, such as optimizing batched operations like triangle solves through reshaping and permuting tensor dimensions, and the implementation of batch-capable functions like `trtrs` in ATen. Additionally, some unresolved matters involve ensuring reproducibility through testing, managing external dependencies like ONNX files, and addressing build and integration issues such as linking errors on Windows."
2019-02-18,pytorch/pytorch,"The discussions primarily revolve around optimizing CUDA resource usage and resolving runtime errors on specific hardware, with users modifying kernel configurations and code to reduce resource requests. Several issues highlight memory leaks and discrepancies in memory management between different PyTorch versions and environments, with suggestions to update dependencies like MAGMA or utilize `torch::NoGradGuard` in C++. There are concerns about compatibility and correctness when exporting models to ONNX, particularly related to assertion checks and model structure details, alongside questions on how to effectively serialize model inputs and metadata. Some threads address performance discrepancies due to tensor contiguity and the need to implement efficient tensor swapping functions or better profiling tools. Unresolved questions include troubleshooting runtime errors after code modifications, ensuring stable model serialization, and improving compatibility across hardware and software configurations."
2019-02-19,pytorch/pytorch,"The discussions mainly revolve around enhancing PyTorch's capabilities, such as supporting complex number types, improving dtype promotion consistent with NumPy, and handling tensor subclassing behaviors. Several issues address performance and correctness concerns, including CUDA kernel optimizations, asynchronous error handling, and multithreading interactions with OpenMP and multiprocessing. There are also recurring themes about API design choices, like DataParallel attribute forwarding, device management during serialization, and input validation in the JIT compiler, often balancing backward compatibility and usability. Some threads focus on debugging and environment-specific problems, including driver or DLL mismatches, CUDA error handling, and dependency versions. Overall, the discussions highlight ongoing efforts to improve PyTorch's robustness, efficiency, and user experience, with many unresolved questions about implementation details and best practices."
2019-02-20,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) Efforts to support complex tensor types in PyTorch, including external packages and ongoing integration work, with some progress but delayed due to limited resources; (2) Performance improvements for sequence reversal on GPU using optimized gather operations, with benchmarking indicating GPU reversal is faster; (3) Compatibility and build issues across platforms, notably with MSVC, CUDA versions, and DLL loading conflicts, prompting suggestions such as reverting specific patches or changing load order strategies; (4) API stability and correctness concerns, especially regarding in-place operations on tensors with zero strides, and the importance of proper deserialization, serialization, and operator implementation; (5) Broader discussions on design decisions such as the movement of code from Python to C++, handling of sparse gradients, tensor promotion protocols, and code maintenance practices. Several unresolved questions remain around optimizing GPU operations, improving build stability, and clarifying API behaviors to avoid user confusion."
2019-02-21,pytorch/pytorch,"The discussions primarily revolve around implementing and supporting advanced RNN modules like Convolutional LSTMs, with questions about official integration and support, highlighting existing draft implementations. Several issues concern environment and compatibility problems, including CUDA interfacing on Windows and Linux, numpy and Python version conflicts, and threading/multithreading complications in multiprocessing contexts, especially related to OpenMP and CUDA support during fork operations. There is a recurring focus on improving documentation, test coverage, and handling of tensor subclasses, including custom tensor extensions, type safety, and serialization/deserialization via TorchScript. Multiple discussions address performance regressions, code refactoring, build system inconsistencies, and infrastructure failures affecting CI processes. Unresolved questions include proper handling of sparse tensors in collective operations, enhancing cross-platform support, and establishing best practices for tensor subclassing, all with an emphasis on robustness, correctness, and usability improvements."
2019-02-22,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, including support for batched operations like `logdet` and `slogdet`, and the implementation of constraints for optimization algorithms such as L-BFGS-B. Several concerns revolve around aligning PyTorch APIs with NumPy, including API parity, documentation, and user experience issues, alongside efforts to improve support for sparse tensors, batched matrix operations, and the handling of diverse tensor memory formats like blocked layouts. There are technical challenges related to CUDA and protobuf build issues, performance regressions, and integration of MKL-DNN for optimized CPU operations, with some discussions focusing on improving error messaging, performance benchmarks, and build stability. Additionally, there are unresolved questions about feature adoption (e.g., numpy-like API support, automatically selecting sparse optimizers), and the need for clearer documentation and systematic testing to address these gaps."
2019-02-23,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionality, including implementing advanced optimization algorithms like L-BFGS-B, improving numerical and autograd testing, and expanding API compatibility with NumPy. Significant concerns involve resolving circular dependencies among core modules such as torch/jit, torch/functional, and torch/nn, with proposed solutions like decoupling or refactoring attribute forwarding in DataParallel. Addressing multi-GPU training efficiency, particularly ensuring correct use of DistributedDataParallel and potentially evolving DataParallel as a proxy for future scalability, is scrutinized. There are also questions on handling constant tensors in JIT, managing reference counting with autograd functions, and refining error handling and determinism, especially regarding atomic operations on CUDA. Unresolved issues include ensuring proper test coverage, avoiding regressions, and optimizing performance bottlenecks in parallel execution."
2019-02-24,pytorch/pytorch,"The discussions primarily address challenges with BatchNorm behavior during inference, especially the impact of setting models to `.eval()` mode versus `.train()`, and how batch size (including batch size=1) affects batch statistics and model performance. Several issues relate to CUDA memory management and synchronization, including proper stream handling during copy operations and growing memory leaks linked to data loader implementations. Other concerns involve compatibility and build issues across different environments, such as differences between debug and release builds, and reproducibility and debugging of complex error states in various hardware setups. There are recurring questions about optimizing multi-GPU training, updating dependencies like MAGMA, ensuring proper installation, and adding tools for memory profiling and performance benchmarking to improve development and debugging workflows."
2019-02-25,pytorch/pytorch,"The discussions encompass various technical concerns including the implementation of a new optimizer in PyTorch, with guidance on existing optimizer structures, and issues related to DataLoader worker failures, often due to insufficient memory or thread management across different Python versions. Several issues address model behavior differences between training (`model.train()`) and evaluation (`model.eval()`), especially concerning BatchNorm and inference performance, with clarifications on correct usage and expected behavior. Runtime errors such as CUDA-related problems, including misconfigured dependencies like NCCL or CUDNN, are frequently discussed, often requiring environment-specific fixes like correct paths and version compatibility. Additionally, there are concerns about deprecations, documentation clarity (e.g., for `torch.load()`), and the need for better internal tooling, testing, and build instructions, especially for Windows and distributed environments, as well as suggestions for future enhancements like list operations in TorchScript and support for mixed device types. The conversations also include ongoing improvements, such as refining memory management, threading issues, and introducing new functionalities while balancing backward compatibility and user experience."
2019-02-26,pytorch/pytorch,"The discussions highlight several technical concerns including memory management issues with GPU variables, particularly the use of `torch.cuda.empty_cache()` and garbage collection for referenced variables, and challenges with backward compatibility and consistency when handling dynamic input sizes and tensor operations, such as in packed sequences and sparse lookups. There are questions about optimizing distributed training workflows, specifically regarding the `replicate()` function's invocation in `DataParallel` versus `DistributedDataParallel`, and the impact of certain API modifications on existing modules and user code. Several discussions also mention build and environment setup challenges, especially on Windows and with dependencies like CuDNN, along with the importance of clear documentation and robust error reporting. Unresolved questions include fixing specific test failures, improving failure notifications, and establishing better testing and development practices for new features like LazyTensor support and output shape inference."
2019-02-27,pytorch/pytorch,"The discussions highlight various technical concerns including CUDA and GPU memory management, with questions about proper usage of CUDA streams and stream guards, as well as memory optimization and debugging memory leaks. There are ongoing efforts to improve PyTorch's extensibility via subclassing tensors with methods like `__array_ufunc__` and `__tensor_ufunc__`, aiming for seamless integration with NumPy and better type propagation. Several issues address build, compatibility, or installation problems, such as resolving linker errors, CUDA version incompatibilities, and dependency management for different hardware architectures. Additional discussions focus on improving testing robustness, version tracking with build hashes, and ensuring backend support across platforms. Unresolved questions remain around the best practices for multistream kernel execution, handling sparse tensors, and enhancing the Dispatch and autograd mechanisms for custom subclasses."
2019-02-28,pytorch/pytorch,"The discussions highlight ongoing challenges with ensuring compatibility and stability in PyTorch, including issues with DLL loading errors on Windows, build and version management, and CUDA compatibility, particularly with RTX and CUDA 10.1, where hardware and library support limitations are evident. There is also concern about precise numerical behaviors and rounding modes, especially when dealing with half-precision floating points, and the need for consistent implementation of tensor subclassing, including support for `__array_ufunc__` and related methods to improve type propagation and interoperability with NumPy. Additionally, performance and memory management issues are raised, such as memory leaks during backward passes, out-of-memory errors with large batch sizes, and the need for better debugging tools or API support (e.g., reservoir sampling). Finally, there are suggestions for expanding and improving PyTorch's modularity and functionality, such as adding batch linear algebra operations, support for probabilistic modules, lint checks, and contribution to core APIs like `TransformModule`. Many of these discussions remain unresolved or call for further investigation, especially around handling numerical precision, build process improvements, and enabling more flexible tensor subclassing."
2019-03-01,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's functionality and performance, including enabling output manipulation via forward hooks, addressing dynamic batching issues in DataLoader (especially with variable batch sizes and shuffling effects on loss stability), and supporting batch linear algebra operations on GPUs. Several questions concern proper implementation details, such as the proper use of `check_generator` in code generation, the impact of in-place operations on autograd, and ensuring compatibility with various hardware architectures. There are also concerns about build and deployment issues, such as outdated dependencies (e.g., git versions in Docker), compatibility errors on macOS and Windows, and minimizing binary sizes for specific architectures. Unresolved issues include handling nested ModuleLists, improving the subgraph matching API, fixing flaky tests (notably in DataLoader), and clarifying the support scope for certain types and dispatch macros."
2019-03-02,pytorch/pytorch,"The comments highlight several technical concerns, including platform and Python version compatibility issues with PyTorch packages, and the necessity of extending interpolation capabilities in PyTorch to support unstructured grids beyond current regular sampling. There are ongoing discussions about optimizing batch normalization for large batch sizes, addressing inefficiencies in cuDNN operations when input shapes exceed certain limits, and improving tensor lifecycle management in CUDA IPC to handle shared tensors more effectively. Additionally, several issues involve code evolution, such as refining tensor transposition behaviors, simplifying function serialization, and reducing binary size by targeting specific GPU architectures. Unresolved questions focus on how to automate protobuf configurations, handle potential performance regressions in JIT or code generation, and implement missing features like 0D/1D tensor transposition and convolution support, emphasizing the need for careful stability and compatibility considerations."
2019-03-03,pytorch/pytorch,"The discussions highlight several technical concerns: compatibility issues between CUDA versions and GCC 7, particularly with CUDA 9.x, leading to longstanding unresolved errors; challenges in model conversion workflows, such as converting models to ONNX and deploying in frameworks like TensorRT; difficulties in accurately inferring output shapes for custom `nn.Module` classes and the burden on module developers to implement shape inference; potential memory management problems related to shared memory usage and process crashes, prompting suggestions for improved handling and user notifications; and API design considerations, notably clarifying and distinguishing similar flags like `check_trace` and `check_inputs` to prevent user confusion. Proposed solutions include enhancing compatibility documentation, providing helper functions for shape inference, refactoring or clarifying API flags, and adding robust benchmarking and error handling to improve stability and usability."
2019-03-04,pytorch/pytorch,"The discussions highlight several key issues: the occurrence of bugs related to network architectures (e.g., missing Sigmoid in models), and performance bottlenecks in multi-GPU training often linked to excessive checkpointing and memory management strategies. Compatibility and build issues are prevalent, especially on macOS and Windows, with solutions involving compiling dependencies from source, environment configuration adjustments, and architecture-specific optimizations. There are ongoing concerns about CUDA-related errors, such as unknown runtime errors post driver updates, and discussions about improving tensor subclass behaviors through mechanisms like `__array_ufunc__` to enhance dtype and type propagation. Additionally, some discussions focus on code organization (e.g., placement of CPU-GPU functions), performance regressions, and infrastructural issues like build size optimization, all reflecting efforts to improve robustness, efficiency, and extensibility of PyTorch."
2019-03-05,pytorch/pytorch,"The discussions highlight several technical concerns including the verification of cuDNN's weight handling and indexing behavior in RNN implementations, and the need for a detailed review of weight multiplication matching across different approaches. There is also a recurring demand for implementing inverse hyperbolic functions in PyTorch, emphasizing the necessity for numerically stable versions. Multiple issues address build and compatibility problems, such as platform support restrictions (notably 32-bit Python) and CUDA-related errors after driver updates, with suggestions to clarify requirements in documentation. Additionally, there are ongoing investigations into dataset deadlocks on Windows, shape inference automation for modules, and the organization and placement of CUDA backend code, alongside concerns about linker errors due to MSVC configuration mismatches."
2019-03-06,pytorch/pytorch,"The discussions highlight several technical concerns including the need for support and improvements in the transition from TH to ATen, optimizing distributed training (e.g., addressing deadlocks, performance regressions, and gradient synchronization issues), and enabling better hardware and platform compatibility (such as GPU stream management, handling CUDA versions, and 32-bit Python support). There are questions about accurate error reporting in CUDA, especially regarding asynchronous device errors and their impact on debugging, as well as the integration of sparse tensor support and enhancing ONNX export stability. Some suggestions propose refactoring or augmenting existing APIs—like adding attributes, custom module containers, or error flags—while unresolved issues include handling async CUDA errors reliably and clarifying API behaviors. Overall, the comments reflect ongoing efforts to improve robustness, compatibility, and usability, with several open questions about best practices and future feature integrations."
2019-03-07,pytorch/pytorch,"The discussions highlight a range of technical concerns including compatibility issues with specific system configurations (e.g., MATLAB environments, Windows architectures, CUDA versions), and potential bugs in PyTorch's handling of tensor hashing, dataset shuffling, and linear algebra routines such as Cholesky decompositions. Several questions pertain to the correct and efficient use of CUDA streams and NCCL for multi-GPU training, emphasizing the importance of synchronization and stream management for performance and correctness. There are ongoing efforts to improve documentation clarity and the structure of internals like Variable and at::Tensor, alongside code stability issues such as test failures and merge conflicts. Additionally, the community discusses architectural decisions related to module construction, MKLDNN tensor implementation, and consistency across different hardware and software environments, with some unresolved bugs and performance considerations remaining open for future resolution."
2019-03-08,pytorch/pytorch,"The discussions primarily focus on improving and extending PyTorch functionalities, such as ensuring custom RNNCell compatibility with JIT and packed sequences, and optimizing tensor operations like circular padding and tensor broadcasting. Several issues address performance regressions, including CPU and CUDA speed discrepancies, related to denormal float handling, CuDNN heuristics, and cublas implementation details, with suggestions like using `torch.set_flush_denormal` and adjusting launch configurations. There's interest in refining the API for module construction to support named submodules with minimal copying, and in better handling multi-device tensors with device assertions. Unresolved questions include the integration of MKL-DNN in libtorch versus Python, fixing broken build configurations (e.g., compiler flags), and ensuring correctness and efficiency of low-level BLAS operations amidst hardware and compiler variances."
2019-03-09,pytorch/pytorch,"The discussions highlight multiple technical issues, including challenges with CUDA library detection inside Singularity containers, which may require manual library path specifications or environment variable adjustments; the potential removal of the unimplemented and buggy `pstrf` LAPACK function in favor of the reliable `potrf` due to pivoting differences; and concerns over `DataParallel` attribute forwarding and `state_dict` compatibility, suggesting possible attribute delegation or interface adjustments. Additionally, repeated occurrences of the “too many open files” error in DataLoader with multiprocessing point to the need for sharding strategies or resource limit modifications, and there are ongoing efforts to improve build processes, such as transitioning some optimization passes from Python to C++. Finally, some discussions involve version-specific bugs or incompatibilities, like issues with `torch.cuda.is_available()` or serialization protocols, requiring targeted fixes or environment setup guidance."
2019-03-10,pytorch/pytorch,"The discussions primarily address integrating and optimizing PyTorch functionalities, including extending RNN cells with packed sequences, exposing aten's multinomial alias setup, and implementing effective FLOP counting methods for various operations. Several issues concern dependency management and build configurations, such as handling MAGMA on different CUDA versions, cross-platform and device-specific determinism, and ensuring compatibility on ARM and GPU hardware. There are questions about API design choices, such as device handling in autograd, the behavior of specific operations like `argmax` and `sort` with NaNs, and in-place tensor operations, emphasizing clear semantics and avoiding data races. Additionally, discussions highlight maintaining consistency, portability, and performance, alongside resolving bugs like incorrect `requires_grad` propagation and environment setup errors. Overall, unresolved questions revolve around API enhancements, experimental features, and platform-specific stability improvements."
2019-03-11,pytorch/pytorch,"The discussions encompass various technical issues and feature requests in PyTorch, including troubleshooting build and installation problems related to Python environment management, package version mismatches, and platform-specific compatibility. Several comments address performance optimization, such as reducing CUDA init delays, optimizing MKLDNN usage, and supporting sparse tensors in distributed operations. There are suggestions for API improvements, like implementing __array_ufunc__ for subtype propagation and enhancing module containers with named submodules, alongside debates on whether to extend or limit existing functionalities like ModuleList/ModuleDict. Unresolved questions include how to efficiently serialize non-tensor attributes in TorchScript, integrate complex number support, and handle specific performance regressions or platform limitations, reflecting ongoing efforts to refine PyTorch's internal mechanisms, usability, and cross-platform robustness."
2019-03-12,pytorch/pytorch,"The discussions mainly revolve around memory management and performance issues in PyTorch, particularly regarding shared memory limitations, efficient implementation of functions like bincount and histogram, and correct handling of overlapping memory in tensors. Several threads address CUDA out-of-memory errors and difficulties with multi-worker data loaders, emphasizing the importance of proper resource cleanup and environment configuration. There are concerns about the compatibility and stability of C++ features (like ABI compatibility and overload handling) and build issues on Windows and Linux, including AVX support and specific compiler flags. Additionally, there are questions regarding API design, such as the behavior of tensor expansion, nested function registration, and support for new or missing operations, alongside discussions on documentation, testing, and potential feature enhancements."
2019-03-13,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch’s features and stability, including implementing a `SAME` padding mode and refining pairwise distance functions, with suggestions for a generic map-reduce interface for distance calculations. There are concerns about serialization compatibility, particularly with `torch.jit.ScriptModule` and the naming conventions for model files (`*.pth` vs. `*.pt`), as well as issues with CUDA build errors, memory management, and multi-threaded GPU usage. Several threads address compatibility and build issues, such as NCCL and NCCL-related build errors, and environment setup inconsistencies, alongside troubleshooting model loading, NaN gradients, and out-of-memory errors. Additionally, questions about extending autograd with callbacks, handling tensor subclassing, and enabling more deterministic behavior for operations like `argmax` are raised. These discussions collectively emphasize the importance of improved documentation, stable APIs, and performance-aware feature development."
2019-03-14,pytorch/pytorch,"The discussions highlight several core issues: the challenge of accurately tracing models with dynamically defined layers such as `Flatten` in `forward()` due to trace limitations, and the need for enhancements to support sparse tensor operations, including sparse weights in `linear()` and sparse gradients. There are concerns about the stability and correctness of CUDA/NCCL operations, especially around environment configuration and memory management, with some suggesting workarounds like disabling specific NCCL features or managing allocator behavior. Additionally, debates emerge around the proper design of Python-C++ bindings, such as extending `Tensor` capabilities, handling internal buffer allocations with third-party libraries like `ideep`, and managing autograd's graph reconstruction to prevent memory leaks and NaNs during training. Finally, the maintenance of robust and stable CI systems is a recurring logistical concern, affecting timely detection of issues across platforms, with suggestions to improve reliability and reporting mechanisms."
2019-03-15,pytorch/pytorch,"The discussions highlight the need for improved debugging tools in PyTorch, such as overflow/error raising behaviors similar to NumPy, and controlling build configurations like debug/release modes in MSVS. Several issues concern performance overheads, specifically the costs of cudaGetDevice/cudaSetDevice calls, and potential strategies to optimize or bypass these for single-GPU scenarios. There is interest in extending tensor functionalities, including support for custom attributes, more efficient batched convolution layers, and better handling of in-place operations on expanded tensors, with suggestions on restriding and checking memory contiguity. Additional topics include resolving build and environment issues on Windows, macros and linking behaviors, and API evolutions such as function schema and hook interfaces. Unresolved questions involve the best approach for in-place restriding logic, fixing platform-specific build issues, and ensuring precise profiling benchmarks for PyTorch performance assessments."
2019-03-16,pytorch/pytorch,"The discussions highlight challenges with properly building and installing PyTorch, often resulting in module import errors like ""No module named torch._C,"" which are related to build environment setup, header resolution issues, and plugin version mismatches. There are questions about managing device and data types when tracing or scripting models in TorchScript, emphasizing the need to handle model conversions explicitly before saving or tracing, especially across CPU and GPU. Performance concerns are raised regarding the overhead of CUDA operations such as cudaGetDevice and cudaSetDevice, and suggestions involve removing or caching these calls to improve efficiency. Additionally, users encounter issues with using certain functions (e.g., torch.flip) and parameters (e.g., long_size in torch.load), revealing gaps in documentation and feature support in specific PyTorch versions."
2019-03-17,pytorch/pytorch,"The discussions highlight several technical concerns including the challenge of deep copying models with private attributes in PyTorch, especially when using `copy.deepcopy()` which fails on non-leaf tensors; the difficulty of managing CUDA out-of-memory errors during large model training and inference, with suggestions to break models into smaller parts or implement CPU-GPU memory sharing strategies; and issues related to `DataParallel`—specifically how to properly handle attribute access, `state_dict` compatibility, and model saving/loading without causing inconsistencies or requiring API changes. Additional concerns include ensuring the profiler and benchmarking tools accurately measure overheads without interference, handling custom allocators for opaque types such as `ideep::tensor`, and debugging internal memory errors and test crashes arising from recent code reorganizations. Overall, there's a focus on improving API robustness, memory management, and developer experience through clearer documentation, better abstractions, and careful refactoring."
2019-03-18,pytorch/pytorch,"The discussions highlight several core technical issues, including the need for differentiation support for `torch.gels`, the challenge of maintaining consistent behavior in BatchNorm across different GPU configurations, and efforts to improve the PyTorch operator registration interface, particularly with regard to handling function schemas and naming. There are ongoing discussions about improving model serialization via enhanced pickle protocols and handling model loading errors, especially when dealing with corrupt files or cross-OS compatibility. Performance considerations are raised in contexts such as CUDA kernel determinism, GPU memory management, and the impact of atomic operations in training versus inference, with suggestions to implement deterministic modes and optimize memory handling in opaque tensors. Additionally, numerous concerns focus on code refactoring and API enhancements, such as enabling custom user-defined tensor types (e.g., Voltage, Current) with preserved metadata, improving the autograd graph mutation semantics, and ensuring robust, consistent behavior in JIT compilation, alias analysis, and graph optimizations."
2019-03-19,pytorch/pytorch,"The provided comments span multiple issues related to PyTorch development, highlighting various technical concerns including compatibility and reproducibility (e.g., deterministic CUDA behavior, fixed PR builds), performance optimization challenges (e.g., data loader determinism, threading effects, kernel speedups), and potential API and implementation improvements (e.g., support for complex types, improving error messages, enhancing auto-differentiation behavior). Several discussions suggest reworking internal mechanisms, such as replacing certain function implementations (like `__array_ufunc__`), adding support for nuanced features (e.g., mixture distributions, deterministic modes), or refining error handling and user guidance. Unresolved questions include how to incorporate new features without breaking existing stability (e.g., backward compatibility, BC), how to improve documentation and test coverage, and how to address specific bugs (such as non-deterministic RNNs or CUDA initialization issues). Proposed solutions involve code refactoring, expanding test suites, and improving configuration management, though some issues (like kernel correctness or compatibility with legacy code) remain open for further investigation."
2019-03-20,pytorch/pytorch,"The discussions highlight several technical concerns, including the challenge of proper implementation and support for custom tensor subclasses, especially for `__array_ufunc__`, with debates on whether to implement it in Python or C++. There are issues with CUDA kernel errors, notably NVRTC compilation errors and the need to manage dependencies like MKLDNN and NCCL, sometimes requiring workarounds such as disabling MKLDNN or adjusting environment variables. Compatibility and reproducibility across diverse hardware, compiler, and environment configurations remain problematic, with questions on ABI compatibility, compiler flags, and build procedures. The complexity of ensuring correct behavior in autograd, especially for sparse tensors, in-place operations, and version counters, is underscored, along with the need for better test coverage and debugging tools. Finally, several unresolved questions concern maintenance, code integration, and robustness of new features, including multi-stream autograd, distributed training, and handling of legacy or external dependencies."
2019-03-21,pytorch/pytorch,"The discussions highlight several core issues including build failures and compatibility problems with different compiler versions (notably GCC and Clang) and environments, especially on macOS and Windows. There is concern over current limitations and bugs in the PyTorch/ONNX export process, such as proper shape handling in models, missing documentation, and issues with support for complex or sparse tensors, including their gradients and CUDA interactions. Multiple threads address the need for better testing, benchmarking, and robustness, particularly when dealing with gradient checks, API consistency, and internal tensor representation, as well as documentation for new features. Unresolved questions include improving the model export fidelity, ensuring backward compatibility, and streamlining multi-platform build and runtime environments."
2019-03-22,pytorch/pytorch,"The discussions highlight ongoing performance regressions, such as increased benchmarking times and efficiency bottlenecks in backward passes, prompting considerations for optimization and profiling improvements. Several issues address compatibility and environment-specific problems, notably ABI incompatibilities, outdated NDK versions, and DLL loading behavior, often suggesting building from source or updating dependencies as solutions. There is attention to correctness and robustness concerns, including handling unused model parameters in distributed training, safe tensor resizing with MKL-DNN, and precise error handling in gradient checks and shape validation. Additionally, there are proposals for enhancing debugging, test coverage, and documentation, including better support for model export, tracing, and C++ API clarity, as well as considerations for backward compatibility and CI stability. Unresolved questions revolve around establishing reliable, portable configurations for environments like Android, CUDA, and various backends, along with ensuring the correctness and efficiency of core functionalities amid evolving hardware and software architectures."
2019-03-23,pytorch/pytorch,"The discussions primarily revolve around technical challenges in building, installing, and integrating PyTorch with various environments, including CUDA and GPU driver compatibility, as well as issues with distributed training frameworks like Apex DDP and `torch.nn.parallel.DistributedDataParallel`. Several threads highlight problems related to memory management, deadlocks, and threading behavior, along with efforts to improve profiling, subclass interoperability, and broadcasting handling in loss functions. Additionally, there are questions about build compatibility, compiler errors, and configuration issues on different operating systems and hardware setups. Many suggestions involve upgrading dependencies, adjusting environment variables, or modifying internal code (e.g., macro setups, code generation) to resolve compatibility and stability concerns. Unresolved issues largely concern ensuring robust distributed training support, cross-platform compatibility, and user-friendly error reporting."
2019-03-24,pytorch/pytorch,"The comments highlight significant challenges with hardware compatibility, particularly relating to older processors lacking support for certain instruction sets (SSE4, FMA4), and the difficulties in building or running optimized PyTorch binaries on such systems. There are ongoing concerns about ensuring reproducibility and determinism across different devices and platforms, complicated further by non-deterministic CUDA operations and differences in floating-point calculations. Several issues involve discrepancies between CPU and GPU implementations, notably in operations like `rsqrt`, and the impact of legacy APIs on performance and usability, prompting suggestions to modernize or clarify behaviors. Additionally, users seek guidance on building from source, matching nightly builds, and porting code to newer APIs, indicating a need for clearer documentation and robust build practices to improve compatibility and developer experience. Unresolved questions remain around optimizing builds for legacy hardware, ensuring cross-platform reproducibility, and standardizing APIs for consistency and clarity."
2019-03-25,pytorch/pytorch,"The discussions highlight recurring issues related to environment setup and package compatibility, especially on Windows and with virtual environments, emphasizing the importance of matching package structures and versions. Several threads address specific PyTorch features or bugs, such as the need for deterministic behavior across devices, handling sparse tensors, and improvements in documentation or API stability, with some fixes already merged or under review. Notably, there's concern about non-deterministic results across platforms, the complexity of supporting multi-GPU and distributed training, and challenges with C++ extension building and linking, especially on Windows. A few threads focus on feature requests or experimental functionalities like `repeat_interleave`, `no_train`, and timing tools, along with discussions on code maintenance, testing, and ensuring consistent API behavior. Overall, unresolved questions remain about deterministic operations, cross-platform reproducibility, and infrastructure enhancements to improve build and runtime reliability."
2019-03-26,pytorch/pytorch,"The discussions highlight several technical challenges in extending and maintaining PyTorch's infrastructure: the proper management of virtual environments and package installations to prevent 'ModuleNotFoundError' issues with torch; ensuring compatibility between CUDA driver versions and installed libraries, including driver updates and correct NCCL/Bazel configurations; implementing robust testing and validation for features like `torch.bincount`, `lu` function, and new backend integrations (e.g., TVM), along with the need for clear documentation on build processes; addressing internal bugs such as incomplete autograd analysis, aliasing issues in JIT CSE passes, and the handling of unused or partially contributed code—especially in distributed and multi-device contexts—while ensuring code modularity and ease of extension for third-party backends. Several discussions also revolve around improving the build system, dependency management, and CI setup for better scalability and troubleshooting. Unresolved questions include the best approach to abstract and plug in external backends, handling multi-stream device operations, and creating reliable tests for complex features."
2019-03-27,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's debugging and diagnostics capabilities, such as integrating NaN detection utilities and improving error messages for common issues like batch normalization with small batch sizes. There is an emphasis on providing more informative model summaries, parameter counts, and shape information, with some proposing custom implementations. Concerns regarding device configuration, such as CUDA environment setup, GPU memory management, and driver compatibility, are recurrent, along with challenges in supporting complex features like MKL-DNN layout support, distributed training, and tensor layout interoperability. Several conversations address code stability, correctness, and performance, including refining autograd behavior, optimizing distributed communication, and ensuring compatibility with hardware accelerators. Unresolved questions remain about better tooling, documentation, and handling of backend-specific features, with the community encouraging contributions and discussion across official GitHub issues."
2019-03-28,pytorch/pytorch,"The discussions highlight several technical concerns including the complexity of handling data layout optimizations such as NHWC versus NCHW in PyTorch, with debates on exposing user-facing options versus internal graph rewriting solutions; issues with functional implementation correctness and API consistency, exemplified by problems with AdamW, bincount, and gradient anomaly detection features. There are also ongoing efforts to improve autodiff and autograd robustness, including handling view operations, version counters, and serialization in scripted modules, as well as fixes for CUDA kernel errors and synchronization issues in distributed training. Additionally, questions about performance regressions, memory usage, and the need for better documentation or API design suggestions are raised, with several PRs under review or proposed for infrastructure improvements, bug fixes, and feature additions. Unresolved questions include optimal handling of data layouts, serialization formats, and the integration of type annotations or stubs to improve static analysis and developer experience."
2019-03-29,pytorch/pytorch,"The discussions highlight several key issues: (1) Memory and performance challenges during package installation, especially in limited environments like Docker or low-memory setups, with suggestions to improve installation methods and simulate problematic environments. (2) Enhancements to core PyTorch functionalities, including implementing features like `return_counts` in `unique`, type propagation in autograd via `__tensor_ufunc__`, and handling multi-GPU optimizations, with debates on code design, backward compatibility, and API consistency. (3) CUDA and GPU-specific issues, such as CUDA version incompatibilities with hardware (e.g., RTX cards), installation misconfigurations in CI, and addressing bugs related to device placement and memory allocation. (4) Development and refactoring of PyTorch modules, like `Sequential` with named submodules, and improvements to code maintainability, including reducing unnecessary copying, better documentation, and integrating external backends (e.g., TVM). (5) CI stability, test reliability, and validation concerns, with emphasis on fixing flaky tests, improving benchmarking accuracy with CUDA synchronization, and proper handling of version mismatches or build failures."
2019-03-30,pytorch/pytorch,"The discussions predominantly revolve around extending and integrating custom RNN cells with packed sequences in TorchScript, highlighting limitations in JIT support for operations like in-place assignment (`hidden[0:batch_size] = ...`). Multiple threads inquire about examples and support for custom RNN modules, indicating challenges in compatibility with packed sequences and full RNNBase features. Several issues address performance concerns, such as optimizing batch matrix multiplication kernels, enabling ONNX support for expand, and ensuring correct timing measurements for CUDA code. Additional discussions focus on autograd multi-device execution, specifically scheduling nested backward operations to preserve parallelism across devices, with proposals to modify task queueing to improve reentrancy behavior. Lastly, issues related to build environments, CUDA driver compatibility, and bug fixes in APIs or memory management are recurrent, emphasizing ongoing efforts to stabilize and optimize PyTorch's core functionalities."
2019-03-31,pytorch/pytorch,"The discussions encompass several technical concerns, including the behavior and synchronization of `data_ptr` when using GPU tensors, and issues related to achieving determinism across CPU and GPU computations, often emphasizing the importance of thread management and proper seeding. There are ongoing challenges with compatibility and support for different CUDA compute architectures (e.g., sm_70, sm_75), particularly regarding the binary support and expected release timelines. Some discussions address implementation bugs or incompatibilities, such as errors in serialization on Windows, mismatched expected behaviors in `multinomial_alias_draw`, and issues arising from recent code merges breaking functionality on certain platforms. Additionally, there is concern about package installation compatibility, especially automatic selection of CUDA versions during `pip install`, and suggestions for enhancing testing frameworks and code correctness, such as handling `strict` mode in `state_dict` loading and improving test coverage for multi-dimensional inputs."
2019-04-01,pytorch/pytorch,"The discussions cover a range of PyTorch technical issues, including DLL load failures on Windows and CUDA toolkit compatibility, where solutions involve environment adjustments and CUDA version considerations. Several issues pertain to the internals of the autograd engine, such as deadlock prevention in nested reentrant backward calls, with proposed approaches involving task prioritization and worker thread management; however, complete deadlock avoidance remains complex. There are bugs related to model serialization, loading state dictionaries with missing or unexpected keys, and UDF/incompatible module handling, with suggestions for API improvements and better error reporting. Performance concerns are raised regarding JIT batching and the support for batched tensor operations like `torch.bmm`, with ideas for more optimized batch processing and support for sparse tensors. Lastly, issues involve platform-specific compilation errors, especially on Windows with MSVC, CUDA, and compiler version mismatches, prompting solutions like macro adjustments, environment variable settings, and potential codebase modifications to enable proper compilation and functionality."
2019-04-02,pytorch/pytorch,"The discussions highlight several key technical issues: the need for improved support and optimization of complex tensors and advanced indexing in PyTorch, including ONNX compatibility and efficient CUDA kernels; challenges with CUDA and GPU driver compatibility, which can cause deadlocks, incorrect behaviors, or performance regressions; the importance of robust profiling and memory management tools, especially related to caching and kernel execution; and the ongoing development of features such as reentrant autograd handling, operator namespace management, and static runtime optimizations. Several proposals focus on fixing specific bugs, enhancing internal APIs, and streamlining build and deployment processes across platforms. Unresolved questions remain about managing nested backward calls safely, ensuring backward compatibility and proper API deprecation, and optimizing batching and parallelism without risking deadlocks or performance losses. Overall, the community emphasizes balancing correctness, usability, and performance while maintaining clear documentation and extensibility."
2019-04-03,pytorch/pytorch,"The discussion covers a wide range of technical issues related to PyTorch, including installation challenges (e.g., wheel files and CUDA compatibility), CUDA errors and profiling, autograd deadlocks, and improvements in operator support such as multi-dimensional reductions and optional tensor handling. Several comments highlight the difficulty of debugging complex issues like deadlocks caused by nested autograd or CUDA bugs, suggesting the need for better documentation and user warnings. There are ongoing efforts to enhance profiling tools, operator functionality, and build processes, with some fixes integrated via pull requests and others pending further investigation. Unresolved questions involve ensuring compatibility across CUDA versions and systems, optimizing performance in small-dimension reductions, and making debugging information more accessible. Overall, these discussions reflect continuous development efforts to improve stability, usability, and feature support in PyTorch."
2019-04-04,pytorch/pytorch,"The comments span various topics, including the need for comprehensive documentation and tutorials for custom RNNs, especially jit-compatible ones, and issues related to complex number support, including type promotion and promoting compatibility. Several reports highlight CUDA memory management problems, driver compatibility issues, and hardware-specific bugs, suggesting workarounds or emphasis on driver updates. There are discussions on improving the build process, especially cross-platform scripting (batch vs. bash), and clarifications on the behavior and implementation of functions involving autograd, such as the correct handling of saved tensors in custom autograd.Function subclasses. Additional concerns include improving test coverage, performance regressions after certain PRs, and making internal mechanisms more discoverable. Overall, unresolved questions revolve around correct implementation of advanced features (like complex types, optional tensors, and distributed training), debugging hardware-specific errors, and enhancing developer experience through documentation improvements."
2019-04-05,pytorch/pytorch,"The discussions highlight several key technical issues, including the need for better error messages to detect CUDA breakage during CI runs, and the challenge of ensuring proper memory management and resource cleanup on GPU devices, especially across different backends like ROCm. There are concerns about the stability and correctness of certain operations, such as the derivative of the Cholesky decomposition and the consistency of gradients, with suggestions to improve testing and validation, including shape analysis and handling of optional outputs. Questions remain on how to implement custom backward functions that support double backward or handle complex models with in-place or nested autograd operations, with workarounds like explicit dependencies proposed. Additionally, some concerns relate to build and integration issues, such as versioning, build options, and reproducibility of tests in CI environments, emphasizing the need for clearer instructions, better error diagnostics, and more robust build configurations."
2019-04-06,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA dependency management, particularly on environments lacking GPUs, and the need for robust detection and handling of CUDA failures during CI testing. Several issues concern memory management and stability, such as CUDA out-of-memory errors during large batch processing, potential memory leaks in the autograd engine, and the complexities of weight reordering with MKL-DNN, including serialization implications. There are also suggestions for improving API usability and maintainability, such as automating stub generation for PyTorch modules, clarifying tensor reshaping versus resizing behaviors, and ensuring compatibility with different compiler and ABI configurations. Additionally, the community emphasizes thorough testing, reproducibility, and clear documentation to support these enhancements and troubleshoot bugs effectively."
2019-04-07,pytorch/pytorch,"The discussions highlight the need for an official, stable implementation of synchronized batch normalization within PyTorch, as current unofficial versions are unstable and incompatible across versions, risking PyTorch's competitiveness. Several threads address compatibility issues and bugs related to multi-GPU training, CUDA support, and environment setup, emphasizing the importance of robust, cross-platform solutions. There is also attention to transparency and documentation, particularly regarding correctly annotating functionalities like `ignore_index`, and improving user accessibility through API enhancements (e.g., cuDNN algorithms, flexible `randint` behavior). Some threads focus on code maintenance aspects, such as refining autograd behavior, proper serialization, and efficient code structures, often seeking guidance or proposing patches before formal review. Overall, unresolved questions remain about optimal multi-GPU design paradigms, backward compatibility, and feature support (such as buffer interface integration and device mapping), underscoring ongoing efforts to balance performance, stability, and usability."
2019-04-08,pytorch/pytorch,"The discussions highlight several key technical concerns, including memory management issues during large-scale model installation and training, especially related to CUDA/OOM errors, with suggested mitigations like reducing batch size or adjusting shared memory. There are queries about improving the autograd and backward support, particularly for complex operations like unfold and matrix gradients, emphasizing the need for manual gradient definitions or handling symmetry constraints. Support for extending supported hardware features such as SKIP support, CUDA version detection, and device compatibility is also debated, alongside potential API and internal implementation changes like handling View sharing, argument promotion bugs, and operator schema updates. Unresolved questions include the proper way to support double backward operations, ensuring backends like ROCm handle vectorized code correctly, and how to improve testing and error reporting for these low-level functionalities. Overall, the focus is on robustness, performance, and correctness across hardware and software configurations, with some issues pending further review or design clarification."
2019-04-09,pytorch/pytorch,"The discussions highlight several core issues: the need for clearer, more consistent error handling and messaging in PyTorch, such as refining tensor size errors and graphics backend issues; the importance of extending functionality with features like model parallelism, backend configuration, and advanced normalization layers (e.g., LayerNorm with LSTM); and technical challenges related to API design, including GPU memory management, data sharing in distributed contexts, and ONNX model exporting with broadcasting operations. Several discussions also emphasize code quality improvements, such as better testing, reorganization of build scripts, and handling of multiprocess data loading. Unresolved questions include API improvements for advanced features, backcompat considerations, and ensuring stability across system configurations. Overall, these conversations reflect ongoing efforts to enhance PyTorch’s robustness, usability, and flexibility while addressing technical limitations and usability concerns."
2019-04-10,pytorch/pytorch,"The discussions highlight several critical issues: unresolved bugs related to in-place operation hooks and their documentation, particularly affecting autograd and feature compatibility; compatibility challenges with ONNX export and opset version limitations impacting downstream frameworks like TensorRT; performance discrepancies and nondeterministic behaviors in RNNs when using cuDNN, potentially due to non-determinism or bug reports; underlying architectural concerns such as the inefficient handling of MKL threading, OpenMP issues on CPUs, and the integration of different modules like batchnorm and layernorm. There are also ongoing efforts to improve testing coverage, API consistency, and code refactoring, including handling of default tensor types, on-device module conversions, and constants like pi. Unresolved technical questions include fixes for in-place hook limitations, improving error messaging for out-of-bounds indices, and optimizing GPU kernel implementations for various operators. Overall, the discussions reflect active development, with many issues requiring further investigation and validation before resolution."
2019-04-11,pytorch/pytorch,"The discussions highlight several main concerns: GPU memory management and efficiency, especially in training loops and cuDNN integration; API improvements like overloading `to()` for layout support (e.g., MKLDNN, sparse), and maintaining backward compatibility; usability and stability issues such as CUDA runtime errors, binary build inconsistencies, and threading in JIT; and potential enhancements in PyTorch's core features, including constants like `pi`, tensor operations, and function signatures to improve correctness, performance, and user experience. There are also ongoing debates about deprecating certain data types or functions, testing regressions, and ensuring proper build environments across diverse systems. Many suggestions involve code refactoring, API refinements, and cross-module consistency, with several unresolved questions about implementation details, testing frameworks, and long-term plans for features like variable handling and layout support. Overall, the discussions emphasize balancing backward compatibility, performance optimization, and API usability amid ongoing development and complex internal dependencies."
2019-04-12,pytorch/pytorch,"The comments highlight ongoing challenges with GPU memory management during PyTorch installation, especially in Docker environments, suggesting alternative offline installation methods to mitigate OOM errors. Several discussions focus on extending PyTorch functionality, such as adding 'SAME' padding modes, flexible `to()` layout conversions (like `to(layout='mkldnn')`), and handling batch normalization with small batch sizes, including improving error messages for BN issues. Compatibility and stability concerns are raised regarding distributed training with NCCL (e.g., GPU idle issues, multi-node setups), optimizer state sharing, and CUDA bugs; some suggest hardware-specific support and platform-specific build issues. There are also technical proposals for optimizing CPU kernels (e.g., specialized loops in normalization code) and enabling better control flow in the JIT compiler, alongside questions on testing practices and backward compatibility for new features or modifications. Unresolved questions include the exact source of certain NCCL errors, the impact of hardware and driver versions on failures, and how to best implement new API features such as gradient-based evaluation mode toggles."
2019-04-13,pytorch/pytorch,"The discussions primarily revolve around issues related to reproducibility and determinism in PyTorch, such as variations in results across runs, nondeterministic behaviors in DataLoader with multiple workers, and potential sources of randomness. Several entries address problems with specific functions like `cross_entropy`, `empty_like`, and `gradcheck`, highlighting challenges in ensuring consistent behavior across platforms and configurations. There are also concerns about hardware-related stability issues, particularly CPU power draw affecting multi-GPU training, as well as build and installation hurdles, especially on Windows and with different CUDA versions. Additionally, some threads discuss internal implementation details, like optimizer update semantics, cache management, and the placement of loss functions in the codebase, suggesting ongoing efforts to improve PyTorch's correctness, efficiency, and flexibility."
2019-04-14,pytorch/pytorch,"The discussions mainly focus on improving PyTorch's functionality and usability, such as adding an identity module for flexible network modifications, implementing `index_select` for sparse tensors, and addressing performance bottlenecks in linear algebra routines like `btriunpack`. Several issues highlight high memory usage during GPU initialization and discrepancies in memory layout handling, prompting suggestions for better cache management and layout validation. There are also concerns about the robustness of the API, compatibility across different environments, and the stability of features like CUDA stream parallelism and operator support for compute capabilities. Additionally, multiple threads share challenges around installation, build errors, and environment-specific bugs, with solutions ranging from code optimizations to version downgrades or configuration adjustments. Unresolved questions include the implementation of atomic operations like `min` in advanced indexing, the behavior of caching mechanisms across devices, and ensuring API stability amid future changes."
2019-04-15,pytorch/pytorch,"The discussions highlight several technical concerns including the performance trade-offs of propagating NaNs for error detection versus their potential impact on device-specific algorithms, and the need for consistency across CPU and GPU implementations. There are ongoing efforts to improve API features such as named tensors, conditional batch normalization, and support for native support of gradient calculations in eval mode; some proposals involve significant refactoring and API design debates. Memory management issues like CUDA memory initialization overhead, kernel launch configurations, and cache strategies are also prominent, with suggestions for optimizing kernel launches, per-device caching, and understanding CUDA memory allocation behavior. Additionally, there are unresolved questions about proper handling of tensor formats (e.g., blocked formats, NHWC) and the need for clear migration or deprecation plans for features like byte vs. bool masks, alongside questions about compatibility and testing of new features or changes to the internal build system and distribution packaging."
2019-04-16,pytorch/pytorch,"The discussions highlight several technical issues in PyTorch development, notably challenges with CUDA and NCCL synchronization causing hidden errors and performance bottlenecks, and the complexity of integrating multi-stream execution without unintended side effects. There is interest in expanding and stabilizing features like synchronized batch normalization, dynamic padding calculations for convolution operations, and the implementation of a consistent interface for handling nested modules like nn.Sequential and checkpoint_sequential—particularly in relation to tuple handling and model flattening. Additionally, difficulties in building and deploying PyTorch with proper compiler settings, dependencies (such as protobuf and libtorch binaries), and environment configurations—especially on different system architectures—are recurrent, alongside concerns about API stability, documentation gaps, and maintaining compatibility during refactoring. Unresolved questions include how to best support multi-device models within the existing API, error handling in distributed GPU contexts, and ensuring reproducible, performant builds across platforms."
2019-04-17,pytorch/pytorch,"The discussions primarily focus on troubleshooting runtime and memory errors in PyTorch, such as device-side asserts, CUDA out-of-memory issues, and kernel compatibility problems related to hardware and software versions. Several comments highlight the importance of matching model parameters, dataset labels, and hardware architecture (e.g., CUDA compute capabilities) to prevent errors, along with solutions like disabling cuDNN or setting environment variables for debugging. There are also technical suggestions concerning code optimization, such as specializing loops for performance gains and refactoring legacy tensor handling to improve memory layout management. Additionally, questions about expanding support for quantization, model serialization, and tensor operations indicate ongoing efforts to enhance PyTorch's functionality and robustness. Unresolved issues remain around precise error diagnostics, hardware compatibility, and best practices for large model handling and performance profiling."
2019-04-18,pytorch/pytorch,"The discussions highlight several key technical concerns, including the non-deterministic behavior of PyTorch's random number generators across versions and hardware, which complicates reproducibility; the need to properly leverage peer device access information for multi-GPU setups; and the challenges introduced by hardware-specific kernel execution errors, such as those caused by incompatible CUDA architectures. Additionally, there are proposals to improve API clarity and consistency, such as clarifying gradient enabling/disabling functions, and refactoring tensor copy operations for performance and reliability. Some discussions focus on build system intricacies, dependency management, and issues related to compiling and integrating third-party libraries like protobuf and ONNX, particularly regarding compatibility and environment stability. Unresolved questions include the best way to handle internal API visibility, the impact of memory management strategies like memcpy optimizations, and whether certain code patterns or design changes should be deprecated or reworked for better maintainability and performance."
2019-04-19,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) Device and device-agnostic saving/loading of model states, with solutions involving CPU-based storage and device identification strategies. (2) Reproducibility and determinism in CUDA computations, emphasizing proper seeding, deterministic backend settings, and acknowledging inherent non-deterministic modules. (3) Enhancements to tensor operations, including asymmetric padding in 3D, new memory formats (e.g., NHWC, channels_last), and support for nested or optional data structures in TorchScript, with considerations for API consistency and backward compatibility. (4) Performance optimizations on CPU and GPU, such as efficient copying and thread pooling, and addressing flaky or infrastructure-related CI failures. (5) Maintenance, testing, and API consistency issues, including handling of large models, serialization limits, and proper validation of new features, with ongoing refactoring and documentation improvements to ensure robustness and usability."
2019-04-20,pytorch/pytorch,"The discussions highlight several key issues: ensuring proper dataset label processing and loss function compatibility for segmentation tasks; addressing intermittent and environment-specific DataLoader errors, especially related to multi-processing and debugging; implementing support for `namedtuple` in `torch.jit.trace` and handling complex input structures; fixing flaky test failures and non-deterministic behavior in JIT, particularly with tensor operations and input shapes; and improving multi-stream random number generation on GPUs by transitioning from `curand` to thread-safe `philox` generators, alongside managing environment-specific compatibility, such as `slots` macro conflicts in Qt. Unresolved questions include the precise handling of input structures in JIT, the best approach to streamline multi-stream RNG safety, and methods to reliably reproduce flaky tests for consistent robustness."
2019-04-21,pytorch/pytorch,"The discussions primarily focus on improving code inspection, auto-completion, and type hints for dynamically generated or global members in PyTorch, with suggestions like explicitly importing specific members or configuring mypy and pylint. Several issues address runtime errors and warnings in various contexts, including model tracing failures, CUDA device initialization problems, and NCCL backend errors, often mentioning specific fixes or workarounds such as environment modifications or downgrades. There are concerns about memory overhead caused by small tensor allocations, with suggestions to modify documentation and develop more efficient data structures. Some questions relate to the stability and correctness of model serialization with torch.jit.trace, especially after certain PyTorch versions. Overall, discussions encompass debugging runtime issues, enhancing static code analysis, and optimizing memory and serialization workflows."
2019-04-22,pytorch/pytorch,"The discussions reveal several recurring themes: the importance of data type consistency (e.g., float vs double) in tensor operations and loss functions, and the impact of target tensor types on gradient computation; the need for clearer, more robust error messages and handling in scenarios prone to memory issues, especially in multiprocessing and multi-GPU setups; and the desire for enhanced API design regarding tensor memory format (e.g., NHWC vs NCHW) to optimize performance and compatibility across devices. Several questions are raised about the technical correctness and API choices, such as the handling of nested nn.Sequential modules, the export and tracing behaviors in TorchScript and ONNX, and the internal consistency of model serialization limits. Unresolved issues include testing and fixing specific bugs like crashes during model export, clarifying the relationship between backend and tensor type identifiers, and improving error reporting and documentation for users confronting hardware or version incompatibilities. Overall, these discussions emphasize the need for clearer interfaces, better error diagnostics, and thorough testing to ensure reliability and scalability of the framework."
2019-04-23,pytorch/pytorch,"The discussions highlight ongoing developments and challenges within PyTorch, including enhancing support for custom CUDA kernels, integrating extensions, and improving ONNX export compatibility. Several threads address hardware and performance issues, such as GPU stability under stress, sparse tensor operations, and CUDA/CUDNN configuration problems, emphasizing the need for better tooling, testing, and documentation. Feature requests include support for certain operations like bicubic interpolation, handling complex data structures (e.g., dicts) in TorchScript, and maintaining backward compatibility with existing models and APIs. There are also concerns about build issues, environment configuration, and performance optimizations across different hardware and software setups. Overall, the discussions reflect active efforts to evolve PyTorch’s flexibility, stability, and usability while tackling technical nuances and integration challenges."
2019-04-24,pytorch/pytorch,"The discussions cover several technical areas including adding support for bicubic interpolation in PyTorch, especially with autograd, and ensuring its results are comparable to OpenCV; implementing ""same"" padding for convolution layers with correct shape calculations; improving performance and stability in data loading and memory profiling, including issues with multiple converging threads, hardware differences, and CUDA kernel behaviors; addressing serialization and API consistency in the JIT and autograd functions; and dealing with build complications on different platforms, such as compiler and environment mismatches, deprecated CFFI extensions, and CUDA/Python interoperability issues. Several suggestions involve refactoring existing operator patterns, updating documentation, or introducing new APIs to improve usability and compatibility, with some discussions about the maintenance or deprecation of legacy codebases like THC modules. Notably, unresolved questions remain about hardware-specific behaviors, build failures on certain environments, and the impact of suggested API changes on the broader framework's optimization passes. Overall, the conversations reflect ongoing efforts to enhance PyTorch’s functionality, performance, and robustness across diverse use cases and platforms."
2019-04-25,pytorch/pytorch,"The discussions highlight several technical concerns, including the non-determinism of certain GPU operations like scatter_add_ due to atomic operations, and the potential impact of changing the implementation of core operators such as nn.Linear to direct aten::linear calls, which may affect JIT optimizations and shape analysis. There are questions about ensuring reproducibility across different devices and platforms, specifically involving non-deterministic CUDA kernels and differences observed in floating-point results. Debugging and performance issues are also prevalent, such as exceptions during CUDA kernel compilation, illegal memory accesses, and speed discrepancies between scripted and eager models—often related to device placement or environment inconsistencies. Unresolved issues include handling the synchronization of custom operations across different build environments, fixing known bugs related to CUDA kernel compatibility, and managing complex interactions with external tools like MATLAB and third-party libraries, with some problems attributed to environment configurations or hardware limitations."
2019-04-26,pytorch/pytorch,"The discussions highlight several key areas of concern: issues with reproducibility and non-determinism in GPU operations, particularly with functions like scatter_add_ and consistency across different hardware (e.g., CUDA versions, CPU vs. GPU); challenges in ensuring deterministic behavior in multi-process and multi-threaded environments, especially involving external libraries or subsystems like cuDNN and OpenMP; difficulties in integrating new features or modifications into the core PyTorch codebase, such as optimizing `Linear`, handling ONNX export, and adjusting internal APIs for better compatibility across hardware and software configurations; and maintenance concerns related to build configurations, specific hardware mismatches, and system dependencies. Many of these require careful patching, better documentation, and targeted workarounds, with an emphasis on improving reproducibility, compatibility, and ease of use in diverse environments."
2019-04-27,pytorch/pytorch,"The discussions highlight several core concerns: the difficulty in installing PyTorch on Windows due to package availability issues, which was remedied by downloading wheel files; the challenge of implementing or optimizing functions like SSIM loss and SVD for GPU performance; and the need for accurate modeling of specific functions in JIT or through custom CUDA kernels, such as `kl_div`. There are also questions about the best practices for code compatibility across different PyTorch versions, particularly when modifying C++ extensions, and about maintaining share-memory and multi-process safety in optimizers. Additionally, specific feature proposals like `AverageUnpooling` and handling multiple inputs in `Sequential` are discussed, with suggestions for cleaner implementation and compatibility improvements. Unresolved issues include ensuring CUDA kernels handle NaNs safely and maintaining API consistency across backend changes."
2019-04-28,pytorch/pytorch,"The discussions highlight several technical issues, including the delayed implementation of `median` for CUDA tensors in PyTorch, which has now been addressed in the master branch. There are concerns about adding `weight decay` correctly in optimizers like Adam, emphasizing that it should be scaled by the learning rate, and discussions about potential impacts on training procedures. Multiple threads address troubleshooting and optimizing CPU and CUDA operations, such as LAPACK functions, `trtrs`, and kernel compilation hang-ups, often suggesting trying different tensor types or cleaning build caches. There's also ongoing interest in implementing SSIM as a loss function for GANs, with recommendations to use external repositories or official approaches. Overall, unresolved questions include proper CUDA batching operations, efficient custom kernel development, and comprehensive support for specific models and metrics within PyTorch."
2019-04-29,pytorch/pytorch,"The discussions highlight various technical issues and feature discussions within PyTorch, including the non-differentiability of `l1_loss` at zero and potential options to handle it, and the challenge of supporting multi-dimensional slicing and unsqueeze operations in scripting. Several issues address performance and correctness concerns, such as the behavior of BatchNorm during training versus evaluation, and the implementation of `mean_std` functions with considerations for computational efficiency. There are also discussions on improving user interface and API usability, including better documentation, API flexibility for chunk data handling, and more intuitive device management in distributed training. Addressing this, some unresolved questions involve refining the interface for tensor operations in scripting, debugging specific errors related to ONNX export and TensorRT, and ensuring compatibility and performance across various hardware setups. Overall, the development process continues to balance feature enhancement, bug fixing, backward compatibility, and usability improvements."
2019-04-30,pytorch/pytorch,"The discussions highlight various core issues in PyTorch development, including the need for reproducibility (such as setting seeds and deterministic operations), handling deterministic behavior in non-deterministic modules (e.g., upsampling, interpolation), and the importance of clear API design (e.g., differentiating between `torch.arange` and `Tensor.new_*` methods). There are concerns about performance optimizations and correctness in low-level components, such as the handling of NCCL, CUDA, and the deprecation of certain operators and APIs (e.g., `new_*` constructors, in-place operations affecting tracing). Several discussions also address build and environment setup reliability, including dependency management (CUDA, cuDNN, compilers), and compatibility issues across platforms and libraries. Finally, enhancements in documentation, error messaging, and API consistency, particularly around the new `IterableDataset`, nested sequential modules, and the `add_video` input expectations, are ongoing considerations."
2019-05-01,pytorch/pytorch,"The discussions highlight ongoing development and debugging challenges in PyTorch, including incomplete or pending feature implementations such as median support on CUDA tensors, and regressions in performance due to cublas, cuDNN, or threading behavior changes in recent versions. Several issues address compatibility, primarily involving device-specific bugs, such as errors with torch.load on certain GPUs, and concerns about correct ABI adherence, especially between build configurations and external libraries like Apex. There are also repeated concerns about proper API evolution, deprecated or legacy constructor functions, and documentation clarity for autograd behaviors and device handling, as well as the need for clearer memory and threading control—especially when using multiple GPUs and distributed settings. Some discussions focus on improving build stability, debugging support, and ensuring the correctness of code interoperability with other frameworks or lower-level libraries, alongside handling of specific bugs and undesirable regressions introduced in recent releases."
2019-05-02,pytorch/pytorch,"The discussions highlight several key technical concerns: the development of a NumPy-like API for PyTorch's tensor objects, including API extension strategies, argument translation, and seamless conversion between `torch.Tensor` and `torch.numpy.ndarray`; the handling of higher-order derivatives and gradient semantics, especially how `.backward()` might be wrapped or integrated with `autograd.grad()` for consistent API behavior; and various build, CUDA, CuDNN, and environment-specific issues such as driver compatibility, multi-GPU support on ROCm, and cross-platform reproducibility. Additionally, there are concerns about maintaining stability across PyTorch versions, improving documentation, and dealing with deep integration challenges like memory management, performance regressions, and the correct handling of numpy-like features (e.g., order, dtype promotion). Many of these issues involve balancing backward compatibility, performance overhead, and usability, with proposed solutions including API refactoring, code generation improvements, and careful handling of environment-specific bugs. Unresolved questions remain around the best architecture for API extension, differentiation support for custom objects, and managing complex build and runtime environment discrepancies."
2019-05-03,pytorch/pytorch,"The comments span a wide range of topics, including improving GPU-based interpolation in core PyTorch, handling multiprocessing and memory leaks during distributed training and DDP, and refining APIs such as `torch.jit`, `nn.LSTM`, and `memory_format`. A recurring concern is CUDA/ROCm out-of-memory errors during large batch training, especially when using features like DDP, with suggestions to optimize synchronization and memory management. Several issues address breaking changes or bugs introduced in recent versions, with proposed code fixes and refactoring strategies, such as adjusting `sample` shape handling in distributions and fixing tensor serialization limits. There is discussion about enhancing user experience and API expressiveness for tensor layouts, data structures, and tracing, as well as dealing with build and environment compatibility challenges on various platforms. Unresolved questions include ensuring backward compatibility with shape and API changes, improving test reliability in CI, and clarifying behaviors of new features like `memory_format` transformations and jit support for complex data structures."
2019-05-04,pytorch/pytorch,"The discussions cover a broad range of technical concerns, including the implementation details of PyTorch's random seed management across CPUs and GPUs, especially in multi-GPU environments. They address the handling of gradient computations with small or zero values, suggesting practical solutions like clamping or masking to prevent explosions. Several issues involve the integration and compatibility of features such as `im2col/col2im`, `nn.Identity`, and custom RNN support with JIT, aiming for improved usability and performance. Additionally, there are ongoing considerations about the proper handling of data types and device-specific tensors, as well as debugging and reproducibility challenges in CUDA extensions and distributed training setups. Overall, the conversations reflect efforts to optimize performance, ensure compatibility, and clarify best practices for extending and debugging PyTorch-based pipelines."
2019-05-05,pytorch/pytorch,"The discussions highlight ongoing efforts to optimize core functionalities in PyTorch, including enhancing GPU interpolation, kernel implementations, and memory handling, with a focus on migration from TH to Aten and optimizing kernel dispatch. There are concerns about the impact of recent API changes—such as scheduler adjustments and gradient calculations on integers—on existing workflows and compatibility issues with different PyTorch versions and external libraries like TensorRT and ONNX. Several threads address performance regressions and bugs, notably in convolution routines, inference speed, and API stability, emphasizing the need for careful testing and backward compatibility. Suggestions for improving user APIs, such as adding ND transpose support in RNNs and better tensor utilities, are recurrent themes, alongside requests for code refactoring and feature enhancements. Unresolved questions include the stability of recent patches, optimization strategies for CUDA and mkldnn, and specific fixes for export and performance issues in complex models."
2019-05-06,pytorch/pytorch,"The discussions cover a range of technical concerns, including the implementation of NumPy-like APIs in PyTorch, with ongoing efforts to unify tensor types and support seamless conversions while maintaining performance and compatibility. Several issues highlight challenges with CUDA and GPU memory management, driver compatibility, and stream safety, as well as specific bugs related to autograd, tensor reductions on empty tensors, and multi-GPU memory limitations. There are also discussions about improving documentation organization, code generation, and build configurations to streamline development and debugging. Additionally, some questions focus on enhancing the C++ API, serialization, and pybind bindings, with unresolved issues around core dumps, OpenMP support, and ensuring feature stability across environments. Overall, these discussions indicate persistent efforts to refine PyTorch's core functionalities, performance, and usability."
2019-05-07,pytorch/pytorch,"The discussions cover a broad range of technical issues, including inconsistencies in behavior between CPU and CUDA tensors, particularly with data types returned by functions like `torch.histc`, and the need for more stable and consistent interfaces for pairwise distance calculations, with suggestions for batched implementations and a `squareform` utility. Several comments highlight problems with multi-GPU training efficiency, especially related to checkpointing overhead, and suggest optimizing checkpointing strategies or adopting improved model implementations; there are also discussions about the implications of checkpoint granularity on backward speed. Other concerns involve build and environment issues, such as compatibility and ABI-related problems on Linux and macOS, handling library dependencies like `libomp`, and ensuring reproducible, efficient builds across different toolchains and systems. There are questions about transitioning to `fork` for multiprocessing, extending support for quantized tensors, handling serialization failures, and understanding the behavior of functions like `argmax` and indexing in PyTorch compared to NumPy. Ultimately, the discussions point to ongoing stability, compatibility, and performance improvements, as well as API usability and clarity, with several proposals for refactoring, feature enhancements, and better documentation."
2019-05-08,pytorch/pytorch,"The comments reflect several key technical concerns:
1. Memory management and sharing issues in multiprocessing, especially with CUDA tensors, suggested fixes involve setting start methods or copying data explicitly.
2. Performance optimizations, such as reducing the number of checkpoints in models like DenseNet, improving checkpointing efficiency, and managing CUDA kernel implementations, are discussed to enhance training speed.
3. API stability and backward compatibility, including handling renamings, deprecations, and BC-breaking changes in RNG, tensor subclasses, and API functions like `add_graph`, suggest careful migration strategies.
4. Build and environment configuration challenges, such as library linking errors, dependency management, and build system adjustments (e.g., for CUDA, ROCm, and Windows), require targeted fixes or environment setups.
5. The proposals for code improvements include better error handling (e.g., replacing macros with exceptions), safer source code practices, and infrastructure updates to support new functionalities (like half-precision, sparse tensors, and on-the-fly reordering)."
2019-05-09,pytorch/pytorch,"The discussions highlight challenges in managing multiple versions and dependencies for libraries like NumPy, CUDA, and PyTorch, with specific issues around uninstallation consistency, environment setup, and build configurations. Concerns about optimizing performance are evident, especially regarding sparse matrix support, device memory management, and the efficiency of distance computations like `torch.cdist` compared to SciPy. Several threads address infrastructure, such as proper build processes, testing frameworks, and documentation updates to improve reproducibility and maintainability. Additionally, there are repeated calls for better error handling, clearer API design, and ensuring compatibility across hardware platforms and software versions. Overall, unresolved questions pertain to build robustness, performance improvements, and clear guidelines for development and deployment workflows."
2019-05-10,pytorch/pytorch,"The discussions highlight persistent issues with CUDA memory management, especially Out-Of-Memory errors during large batch training and in distributed setups, with suggested workarounds like adjusting shared memory size and process spawning methods. Several comments point to performance regressions and the need for better profiling tools or more efficient parameter sharing, particularly in context of JIT tracing, parameter sharing practices, and tensor layout handling. There are concerns about complex refactorings such as the TensorIterator optimizations, API inconsistencies, and build failures on Windows, some of which are addressed through fixes, tests, or configuration changes. The need for clear documentation, correct handling of fallback behaviors, and thorough review of reorganizations or new features like `trace_module` are also emphasized. Unresolved questions include memory leak issues during autograd backward passes, better error handling for device-specific problems, and ensuring backward compatibility amid significant code reorganizations."
2019-05-11,pytorch/pytorch,"The discussions highlight several technical concerns including potential CPU instruction set compatibility issues in prebuilt PyTorch binaries due to FMA4 instructions on AMD processors, which may require patching Sleef's dispatcher; GPU memory management and garbage collection, with emphasis on eliminating reference cycles to prevent memory leaks during model reloads; performance disparities in CPU distance computations between torch.cdist and scipy's cdist, possibly due to implementation differences; and the need for better handling of large or long-sequence data that exceeds RAM capacity, suggesting potential development of memory-aware layer operations. Additionally, issues with multi-GPU communication latency and synchronization, as well as build environment stability (e.g., NUMA support and build configuration inconsistencies), are noted. Some discussions also propose improvements in documentation, test procedures, and maintainability for more robust and scalable deep learning workflows."
2019-05-12,pytorch/pytorch,"The discussions highlight ongoing efforts to optimize and extend PyTorch's functionality, including implementing circular padding in convolution via im2col/vol2col, and exposing SSIM as a native loss function with community support. Key concerns involve ensuring correct kernel selection and backward formula adjustments for circular padding, alongside rigorous testing and integration into both CPU and CUDA implementations. Several issues address runtime errors and synchronization challenges, such as model saving errors during CUDA operations, effects of forking and device initialization, and understanding non-blocking CUDA memory copies. There's an emphasis on improving interoperability through protocols like `__cuda_array_interface__` and DLPack, and on refining internal APIs like `__repr__` messages for clarity on model loading status. Overall, unresolved technical questions remain around kernel optimization, synchronization stability, and providing robust, native implementations for advanced GPU operations and metrics."
2019-05-13,pytorch/pytorch,"The comments encompass a wide range of topics, including improvements in tensor masking and nonzero operations, challenges with CUDA and cuDNN compatibility, and optimization requests such as support for LARS and more efficient memory handling. Several issues highlight bugs or performance bottlenecks, with suggestions for fixes, performance benchmarks, and feature enhancements like support for `__cuda_array_interface__` and int-type `AvgPool`. Discussions also address build processes, environment configurations, and system-specific errors, often requesting or providing guidance for troubleshooting and implementation. Overall, the issues reflect ongoing efforts to optimize PyTorch's functionality, fix bugs, enhance compatibility, and expand features based on user needs and hardware considerations."
2019-05-14,pytorch/pytorch,"The discussions encompass a wide range of technical issues related to PyTorch development, including performance optimizations (such as faster `torch.cat` and improved tensor operations), compatibility and porting challenges (notably with ONNX, C++ APIs, and environment configurations), and bug fixes (like double backward support, shared memory errors, and specific operator behaviors). Several threads highlight ongoing efforts for feature enhancements (e.g., support for sparse tensors, layout propagation, and new API functionalities) and request for guidance on testing, code review, or environment setup. There are also issues related to build stability, compiler and library compatibility, and platform-specific problems, especially on Windows and with third-party dependencies. Overall, the threads reflect active maintenance, feature expansion, and troubleshooting to improve reliability, performance, and usability of PyTorch across different environments."
2019-05-15,pytorch/pytorch,"The discussions highlight several technical issues, including the Bit's suggestion to add a `set_sharing_strategy('file_system')` call (Question: where to place it), concerns about memory management and CUDA out-of-memory errors during large-scale training, particularly in multi-GPU and distributed contexts, and the need for improved support and testing for advanced features like ONNX export, sparse tensor support, and module structure preservation. Several bugs related to build failures, compiler warnings, and compatibility (e.g., Eigen, MKL, and CMake configurations) are also prevalent. Additionally, there are questions about implementing meta-optimization, better handling of shared parameter reloading, and ensuring backward-compatible, efficient quantization and pruning workflows, with ongoing discussions about best practices and potential code modifications. Unresolved issues include precise placement of sharing strategy commands, debugging CUDA memory leaks, and extending or improving support for specific tensor and model operations."
2019-05-16,pytorch/pytorch,"The discussions highlight various technical concerns including difficulties with GPU memory management and data loading performance when using PyTorch with GPU tensors, as well as issues with CUDA initialization, power supply constraints, and multi-process data handling. Several threads question the correctness and efficiency of specific functions such as `view`, `zeros`, and `load_state_dict`, with suggestions to refactor code generation and improve consistency across CPU and GPU implementations. There are also open questions about new features like support for sparse tensors in JIT, the behavior of `size`, `.T`, and large tensor indexing beyond 2^31 limits, alongside concerns about build processes, ABI compatibility, and runtime performance for specific kernels and operations. Many comments call for added tests, better documentation, or clearer code structure to ensure correctness and maintainability. Overall, unresolved issues include ensuring compatibility, efficiency, and correctness in diverse scenarios involving large tensors, multi-GPU configurations, and evolving APIs."
2019-05-17,pytorch/pytorch,"The comments reveal several recurring technical issues, including performance discrepancies between `torch.cat` and `clone`, in-place mutation concerns during gradient computations, and challenges with CUDA kernel initialization and thread safety. There are also notable discussions around build and compatibility issues, such as the support for old CUDNN features, ABI inconsistencies in libraries, and errors arising from unsupported operations or unsupported input tensor shapes. Several suggestions and fixes are proposed, including modifying code generation strategies, adding specific feature support like GELU activation, and adjusting environment variables or build options for stability. Unresolved questions mainly concern ensuring thread safety for RNGs, proper handling of in-place operations with autograd, and clarifications needed for build and deployment procedures across different platforms."
2019-05-18,pytorch/pytorch,"The discussions primarily revolve around compatibility and consistency issues in PyTorch. Several comments address API stability and deprecation concerns, such as the support status of `.shape` versus `.size()`, and the existence and placement of functions like `one_hot`. CUDA-related problems are prominent, including CUDA out-of-memory errors, the need for proper multiprocessing start methods (spawn vs. fork), and issues with CUDA support in forked processes, especially when using `DataLoader` with `num_workers > 0`. Some discussions focus on build failures due to missing files, compiler incompatibilities, or environment configuration, and there are suggestions for code fixes, such as using `detach()` for loss functions and fixing specific operator implementations. Overall, unresolved questions remain about best practices to ensure CUDA compatibility in multiprocessing workflows, handling memory errors, and extending functional API support, indicating ongoing stability, performance, and usability challenges."
2019-05-19,pytorch/pytorch,"The discussions primarily revolve around installation and environment management issues, such as importing `torch` in different contexts (e.g., Anaconda Prompt, scripts, Jupyter notebooks) and ensuring proper setup in conda environments. Several issues address module import errors, ABI compatibility, and building problems on various platforms, including Windows, macOS, and Docker, often linked to CUDA, driver, or compiler version mismatches. There are ongoing efforts to fix symbol exposure, build failures, and dependency resolution, with some concerns about ABI inconsistencies in shared libraries and the need for clearer API or naming conventions. Additionally, users seek feature enhancements like Python API fixes, JIT fusion optimization, and GPU support for specific modules, alongside standard troubleshooting guidance."
2019-05-20,pytorch/pytorch,"The comments reflect ongoing efforts and challenges in improving PyTorch, including issues with deadlocks and hangs in data loader operations, performance discrepancies between CPU and GPU implementations, and the complexities of static and dynamic memory management across different hardware and libraries. There are concerns about inconsistent behavior and support for features like `batch_first` in RNNs, non-deterministic loss functions like `CrossEntropyLoss`, and compatibility with CUDA 10 and various compilers, often tied to ABI compatibility and static linking issues. Additionally, questions arise around the correctness and usability of certain API behaviors, such as `torch.Size`, `size` attributes, and parallelism in multithreaded environments, as well as the need for better documentation, testing, and debugging practices. There is also an emphasis on fixing specific bugs in disjoint components like `MultiheadAttention`, `interpolate`, and `distances` functions, and addressing internal assertions and version mismatches to ensure stability across updates. Overall, unresolved questions center on performance optimization, compatibility, correctness, and clearer API semantics."
2019-05-21,pytorch/pytorch,"The discussions reveal ongoing challenges with GPU memory management in data loading, especially when combining GPU preprocessing (like RAPIDS) with PyTorch's multiprocessing DataLoader, leading to CUDA initialization errors and performance bottlenecks. Several issues address the need for ensuring deterministic behavior with `seed` and `shuffle` in multi-worker datasets, emphasizing the importance of consistent data ordering across epochs and processes. There are concerns about maintaining API consistency for tensor operations (e.g., `permute` vs. `transpose`, `reshape`), alongside requests for extending functionality such as multi-dimensional reductions, sparse tensor support in JIT, and optimizing sparse matrix operations. Additional topics include improving build system compatibility (VS versions, CUDA support), fixing serialization, and ensuring correct behavior in mixed environments like XLA or when integrating external libraries (e.g., dart). Overall, unresolved questions center on API clarity, robustness of multi-device and multi-threaded code, and expanding support for advanced tensor types and optimization features."
2019-05-22,pytorch/pytorch,"The discussions reveal several key technical concerns in PyTorch development, including the persistence of CUDA memory leaks and out-of-memory errors, often related to improper memory management or complex interactions with backends like MKL or NCCL. There are ongoing efforts to improve API consistency and usability, such as adding an `align_corners` parameter to `grid_sample` and fine-tuning memory format preservation. Performance regressions with functions like custom convolution backward passes, and the need for stream-safe random number generation, are also highlighted, along with challenges in build system configuration and cross-platform compatibility, especially on Windows. Additionally, improving testing, code movement transparency via `ghstack`, and maintaining backward compatibility (e.g., with `torch.Size`) are important unresolved topics."
2019-05-23,pytorch/pytorch,"The comments highlight several key technical issues: the limited support of `DistributedDataParallel` for non-CUDA networks and backend MPI support, indicating ongoing development and platform-specific challenges; performance and implementation concerns with operations like `topk()`, `grid_sample()`, and `cdist()` on CPU and GPU, including the need for optimization and API enhancements such as `align_corners`; difficulties with memory management and resource cleanup in multi-process/multi-GPU scenarios, including CUDA context handling and MKL linking; and potential design improvements, such as consolidating tensor creation APIs, removing or refactoring legacy classes like `torch.Size`, and ensuring compatibility and correctness of models serialization/deserialization and JIT code, especially regarding attribute handling and in-place operations. Several unresolved questions relate to how best to abstract and implement profiling, pruning, and memory management features, and whether to standardize or remove legacy APIs for simplicity and maintainability."
2019-05-24,pytorch/pytorch,"The discussions primarily revolve around hardware support and performance optimization, such as enabling CPU-based DistributedDataParallel, improving cuDNN convolution backward functions, and fixing GPU kernel regressions. Several issues address reliability and correctness concerns, like preventing out-of-bounds indexing in CUDA kernels, handling large tensors, and ensuring deterministic behavior in sampling functions like `choice` and `exponential`. There are also multiple questions about code organization, deprecating or refactoring legacy functions, and managing build complexities across different platforms, especially with in-source build size issues and legacy support. Lastly, discussions include extending or removing support for specific features (e.g., complex numbers, bfloat16, tracing fixes) and ensuring backward compatibility and test coverage for ongoing development."
2019-05-25,pytorch/pytorch,"The discussions highlight several technical issues in PyTorch, including kernel build errors related to AVX/AVX512 instructions not being properly defined during compilation, which can be resolved by adjusting header includes and build configurations. There are concerns about CUDA and GPU stability, especially with multi-GPU setups and power supply issues, indicating a need for better hardware compatibility guidance. Several performance and usability features are under consideration, such as providing more comprehensive benchmarking tools, supporting dictionary outputs in ONNX models, and handling explicit `.numpy()` conversions to clarify device and gradient-related semantics. Thread safety and process safety in PyTorch's distributed and parallel operations are questioned, especially around reference cycles and GIL management. Lastly, there are ongoing questions about tensor identity comparisons, with suggestions to rely on data pointers and tensor attributes rather than object identity, to better understand data sharing and detachment behaviors."
2019-05-26,pytorch/pytorch,"The discussions highlight ongoing technical issues including a bug with the `pixel_shuffle` operation during ONNX export, which was fixed by updating the symbolic function; build failures on Windows due to dependency cycles and large binary sizes, necessitating updates to build scripts and environments; and a potential default alignment problem in the `grid_sample` function related to coordinate conventions, with suggestions to add an `align_corners` option. Several questions address enhancing API robustness and usability, such as supporting `training_only_param` in modules, managing `track_running_stats` in BatchNorm, and improving `torch.utils.benchmark` for GPU inference. Unresolved concerns include build errors on Windows requiring CMake updates, and whether to remove or support certain arguments in documentation. Overall, the conversations encompass bug fixes, build system stability, API improvements, and compatibility considerations across frameworks and hardware."
2019-05-27,pytorch/pytorch,"The discussions highlight development progress and ongoing issues in PyTorch, including the lack of certain features like ModelParallel and DataParallel, and requests for official modules such as convolutional LSTM layers. Several issues concern compatibility and build system challenges, especially on Windows and Linux, with specific problems like dependency cycles, version mismatches, and environment conflicts related to CUDA and MKLDNN libraries. There are also inquiries about expanding API functionalities, such as enabling per-layer cuDNN algorithm configuration, adding support for model re-serialization, and tools for extracting intermediate activations for model introspection. Lastly, unresolved bugs around multi-GPU memory management, non-deterministic backward, and JIT compilation support remain, with some discussions proposing refactoring and feature enhancements to improve usability and robustness."
2019-05-28,pytorch/pytorch,"The discussions highlight several technical challenges and questions, including issues with installing PyTorch on Windows and macOS due to environment mismatches, and the need for clearer documentation and build instructions, especially for libtorch and CUDA configurations. There are concerns about specific kernel and optimization behaviors, such as handling CPU feature detection (SSE/AVX), and the complexities of supporting various hardware and backend environments like ROCm or different CUDA versions. Some discussions focus on enhancing API flexibility, such as exposing tunable cuDNN algorithms or providing ""unsafe"" variants of index operations, and ensuring backward compatibility with existing models and serialization formats. Additionally, several questions arise around improving testing coverage, diagnostics, and handling of corner cases like empty tensors or APIs involving optional parameters, along with proposals for refactoring or clarifying code practices."
2019-05-29,pytorch/pytorch,"The comments reflect a range of discussions including the implementation of advanced modules like ConvLSTM for video processing, improvements to data handling and device compatibility, and performance optimizations such as better multiGPU data pipelines and lightweight hooks. Several comments highlight issues with build environments, dependencies, and platform-specific complications, especially on macOS, Windows, and with CUDA driver compatibility, alongside suggestions for clarifying installation instructions and refining build procedures. There are ongoing proposals to enhance API modularity and usability, such as support for hookable weights, device-agnostic optimizer state management, and more flexible argument handling in modules, as well as code refactoring to improve clarity and maintainability. Some unresolved questions deal with performance disparities between CPU and GPU, serialization consistency, and ensuring safety and correctness of operations across different hardware and software configurations. Overall, the discussions aim to address usability, compatibility, performance, and robustness enhancements for PyTorch."
2019-05-30,pytorch/pytorch,"The discussions highlight several key technical concerns: support and compatibility issues with TorchScript, including for-loops over tensors; limitations and bugs related to cuDNN with small channel inputs and batch sizes, especially exceeding 65535, with ongoing fixes; challenges in implementing features such as hookable weights, sparse matrix support, and efficient batched operations like top-k and SVD, often due to hardware, library, or framework limitations; compatibility and build infrastructure challenges, including CMake version requirements and Windows size limits; and considerations around backward compatibility, serialization quirks, and ensuring correctness in mathematical operations like sinh/cosh, with proposals for improvements like CMake integration, native function support, and more modular design."
2019-05-31,pytorch/pytorch,"The discussions encompass a variety of technical concerns including installation difficulties across platforms and package managers, with specific emphasis on issues related to dependencies such as MKL, CUDA, NCCL, and LMDB, and how installation procedures impact reproducibility. Several threads highlight bugs and API reactions, such as unexpected behavior in `einsum` broadcasting, deprecated or breaking modules in serialization and backward compatibility, and potential performance regressions or bugs in convolution padding modes, especially with circular padding. There are concerns about build system configuration, notably the use of CMake flags, VS version compatibility, and in-source build restrictions, often coupled with suggestions for better CI coverage and testing. Additional issues include deadlocks with embedded Python interpreters, stability and correctness of mathematical function implementations (like `sinh`, `cosh`, and `dirichlet_grad`), and the need for proper testing of new features or bug fixes, including handling edge cases and ensuring code coverage."
2019-06-01,pytorch/pytorch,"The discussions highlight a variety of technical issues including difficulties with installing PyTorch packages on Windows and through pip, often resolved by alternative methods such as conda or manual wheel downloads. Several reports involve runtime errors like NaN gradients, CUDA out-of-memory errors, and bugs in specific layers or features (e.g., cyclic learning rate, CTC loss, kernel caching), with suggested fixes involving code refactoring, better testing, or parameter adjustments. There are also efforts to improve build configurations, documentation, and bug fixes related to deprecated macros, tensor operations, and build system integration, especially on Windows and for specific hardware. Unresolved questions include how to properly seed data loaders to avoid nondeterministic results, fix specific deprecated macros, and ensure reproducibility and stability across different environments and versions. Overall, the conversations suggest ongoing maintenance, bug fixing, and feature enhancement challenges in both code correctness and usability."
2019-06-02,pytorch/pytorch,"The discussions highlight several key technical concerns: on Windows, issues with DLL loading and CUDA header completeness suggest installation or environment setup problems, with solutions involving proper CUDA installation and environment variable configuration. There are ongoing challenges with performance and functionality, such as enabling multi-threaded execution and parallelism in PyTorch's autograd engine, which may require architectural adjustments like virtual CPU devices. Test reliability on Windows is a concern, with flaky tests identified and mitigation strategies, such as setting random seeds, discussed. Additionally, there are feature requests and compatibility considerations, such as adding GPU support for the GELU function, maintaining parity between CPU and GPU implementations, and addressing build system limitations for 32-bit OSes. Overall, unresolved questions revolve around stability, cross-platform consistency, and enhancing performance through architectural changes."
2019-06-03,pytorch/pytorch,"The discussions reveal recurring challenges with PyTorch's data loading stability, especially involving multi-threaded DataLoaders and the impact of external libraries like OpenCV, which the user mitigated by setting cv2.setNumThreads(0). There are performance and correctness concerns around custom RNN modules such as conv-LSTM implementations and the integration of control flow support within Distributed Data Parallel (DDP), prompting suggestions for API design and API stability improvements. Several issues focus on correct handling of tensor operations in different contexts, including managing tensor device transfers (e.g., CPU to XLA, CUDA) and ensuring consistent behavior when moving models or converting between data types, especially in ONNX exporting and mixed precision scenarios. Additionally, there are discussions on software build, merge conflict resolution, and the robustness of test frameworks, with some patches aimed at improving code correctness, error handling, and compatibility (e.g., boolean tensor support, reworking `_apply`). Unresolved questions include how to best support evolving features in a way that remains backward compatible and predictable across diverse hardware and integration contexts."
2019-06-04,pytorch/pytorch,"The discussions primarily revolve around correcting and clarifying the behavior of tensor operations such as `torch.flip` versus `torch.tensor([1,2,3])` reversal, with emphasis on differentiating dimension order reversal from element order reversal. Several issues concern memory management, particularly CUDA out-of-memory errors, which are addressed through batch size adjustments, device handling, and implementation fixes. There is ongoing work to improve backward compatibility, including in the serialization of modules, the handling of in-place modifications during autograd, and migration guidance for API changes. Updates on the implementation details of parallelism, threading, and hook support in the JIT, alongside fixes for build failures, platform-specific bugs, and bugs in specific functions like `qlinear_unpack` and `log_beta` initialization, feature prominently. Overall, the discussions reflect active maintenance, bug fixing, and feature enhancement efforts—some of which involve refactoring for performance or compatibility, with questions about proper integration and testing still open."
2019-06-05,pytorch/pytorch,"The discussions primarily revolve around addressing bugs and API limitations in PyTorch, such as the need for proper in-place tensor methods (`_to_inplace_`) to maintain version tracking when moving models across devices, and handling module references in scripting and JIT contexts, especially when using `ModuleList` or `ScriptModule`. Several issues highlight challenges in supporting dynamic or complex operations like variadic tensor reductions, tuples as constants, and multi-tensor einsum interfaces, with concerns about IR complexity and memory management. There are also technical questions about compatibility with various CUDA versions, nvcc, and external dependencies, as well as concerns about proper benchmarking and timing measurement of GPU inference. Lastly, some discussions concern the stability and structure of internal graph nodes in JIT, deprecations, and BC-breaking changes in learning rate schedulers and distributed training, along with unresolved questions on clean migration paths and code consistency."
2019-06-06,pytorch/pytorch,"The discussions highlight several key issues: (1) Compatibility and installation challenges across different OSes, CUDA versions, and Python environments, including DLL/load failures on Windows and macOS/Omegad; (2) Bugs and inconsistencies in PyTorch functionalities such as L1Loss subgradients, optimizer behaviors when moving models to CUDA, and handling of in-place operations affecting autograd; (3) Pending or ongoing feature improvements, like enhanced support for script modules, custom LR schedulers, and extended function dispatching (e.g., `grid_sample`, `einsum`), as well as code portability and refactoring efforts (e.g., moving from TH to ATen, removing or revising deprecated API); (4) Correctness and stability of tests, numerical precision in model verification, and reproducibility issues, especially in distributed training and ONNX export scenarios; (5) Maintenance and integration updates, including merging patches, addressing build failures, and planning for future releases, with emphasis on fixing existing bugs before introducing new features."
2019-06-07,pytorch/pytorch,"The discussions highlight several technical concerns, including discrepancies in model training speed and accuracy between Keras/TensorFlow and PyTorch, with particular interest in sparse updating mechanisms such as SparseRMSprop. There is clarification needed on the semantics of batch_first in RNN modules, especially regarding hidden state dimensions, and how to best document or improve handling of variable sequence lengths and parameters in scripted modules. Support and supportability of sparse tensors within JIT, along with the need for exposed parameters for operations like SSIM, also feature prominently. Additional questions focus on debugging build issues related to CMake, proper handling of non-contiguous tensors, and ensuring code compatibility across platforms or hardware configurations, such as Windows or ARM architectures, with some proposals for improved testing and more explicit API behaviors."
2019-06-08,pytorch/pytorch,"The discussions highlight several technical concerns, including the efficiency and memory usage of custom CUDA kernels versus flexible approaches like im2col for locally connected layers, with some suggesting building specialized kernels for performance gains. There are also questions about improving or replacing the current `choice` and `multinomial` sampling methods to enhance speed and correctness, especially on the GPU, with benchmarks showing significant performance differences. Additionally, issues around PyTorch's C++ API, specifically the migration from `torch::Generator` to shared pointers and compatibility with CUDA 10, are raised, alongside difficulties with JIT tracing of functions requiring explicit tensor types and handling shape-dependent parameters. Overall, the discussions focus on optimizing kernel implementations, ensuring compatibility and stability across different versions, and improving user-facing API support for JIT and GPU workflows."
2019-06-09,pytorch/pytorch,"The discussions highlight several technical issues and questions related to PyTorch, including dependencies and build configurations (e.g., hardcoded CUDA paths, OpenCV compatibility with libtorch on Windows, and system environment inconsistencies affecting build processes). There is a need for improved documentation, especially regarding `ignore_index` naming conventions, and the potential renaming to `ignore_target` to reduce confusion. Challenges in optimizing batch operations such as QR, SVD, and convolution (`SpatialDilatedConvolution`) on GPUs are noted, with suggestions to investigate their internal implementations and timing. There are also ongoing efforts to extend PyTorch functionalities—like adding new tests for internal methods, implementing batch SVD, and supporting custom C++ extensions—while addressing build failures and increasing robustness. Overall, many questions remain about implementation details, correct usage, and building workflows, with some issues demanding further testing, code review, or clarification."
2019-06-10,pytorch/pytorch,"The discussions highlight several key technical concerns, including handling ConnectionResetError due to multiprocessing with DataLoader, the performance and implementation of vectorized tensor copy operations, and issues related to tensor dimension conventions in RNNs versus documentation clarity. Other recurrent topics involve CUDA memory management and compatibility, the need for better error messaging and robustness in JIT scripting, and the challenges of integrating CMake with Python build scripts. Several suggestions propose refactoring or reverting code that causes breakages or inconsistencies, such as updates to learning rate schedulers, memory format handling, and default argument behaviors. Unresolved questions center around improving tooling (e.g., for FLOP counting), making APIs more intuitive, and ensuring build system reliability across platforms."
2019-06-11,pytorch/pytorch,"The discussions highlight persistent GPU memory inefficiencies with the unfold-based local linear layers, prompting interest in a more native CUDA implementation to reduce memory footprint, especially when batching. Multiple issues address CUDA compatibility and installation problems, often tied to environment mismatches, CUDA versions, or missing shared libraries, indicating a need for better build and deployment guidance. There are concerns about the correctness and stability of nonlinear functions like `torch.finfo` in JIT contexts, affecting models such as super-resolution, and a call for clearer documentation and testing strategies for such corner cases. Several PRs and features (e.g., throughput benchmarks, modified LR schedulers, and extended ONNX support) are pending review or integration, with attention to maintaining backward compatibility and reducing system complexity. Lastly, errors associated with multi-process memory management, threading, and GPU stream synchronization suggest the need for robust debugging, testing, and possibly structural changes to ensure consistent and efficient multi-GPU behavior."
2019-06-12,pytorch/pytorch,"The discussions highlight several technical issues and proposed improvements: correcting padding and output size calculations for convolutional layers to match other frameworks like TensorFlow, including support for `'same'` padding in Conv2d; addressing inconsistencies and bugs in CUDA tensor management, especially around stream recording and allocator behaviors; enhancing ONNX export robustness and debugging tools; improving model initialization schemes and default behaviors; and refining internal kernel parameters and parallel execution strategies for performance and correctness. Questions arise about handling shared storage views, proper testing for non-default streams, and correct model serialization/loading practices, with some suggestions for API enhancements and safer type inferences. Various insights into environment setup, device compatibility, and build configurations also emerge as ongoing concerns."
2019-06-13,pytorch/pytorch,"The discussions highlight several core issues within the PyTorch codebase: (1) the need for clearer guidance and more robust handling of multiprocessing sharing strategies, especially concerning `set_sharing_strategy`; (2) challenges around improving API consistency, such as making `generator` keyword-only and handling default initializations; (3) debugging and fixing specific bugs related to batch normalization with small batch sizes, memory management, and CUDA kernel performance—particularly for large kernel sizes and FFT-based convolutions; (4) addressing build and linking issues on Windows, Linux, and integrating with external dependencies like MKL, YAML, and ONNX, including static vs shared linking concerns; and (5) refining error messages, type inference, and profiling tools for better developer experience and reliability, with several patches aiming to enhance testing, determinism, and compatibility across platforms and versions."
2019-06-14,pytorch/pytorch,"The discussions highlight several key technical concerns, including the need for more precise error messages when CUDA runtime errors are triggered, particularly for device-side asserts and NaNs, as well as improved debugging tools like `CUDA_LAUNCH_BLOCKING=1`. There's a recurring theme of ensuring compatibility between CUDA, driver versions, and PyTorch/PyCUDA setups, especially in contexts like driver backward compatibility and multi-version installations. Several issues address the robustness and correctness of core components, such as handling `IOError` interruptions in Python 2, guard conditions in tensor operations to avoid invalid memory accesses, and the support for various tensor types and stream synchronization. Additionally, the need for API stability and clear deprecation paths is emphasized, alongside efforts to improve testing, documentation, and internal code consistency (e.g., namespace and macro management). Unresolved questions remain around better error handling for CUDA assertions, stream safety during operations, and the support for exporting models to ONNX and scripted modules."
2019-06-15,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's usability and performance, including integrating `Declarations.yaml` into libtorch packages for better language binding support despite the lack of stability guarantees, and addressing significant GPU and CPU performance discrepancies in functions like `torch.cdist`. Multiple comments emphasize the importance of correct timing methods for CUDA operations, synchronization, and benchmarking, revealing concerns over inconsistent performance measurements. There are also technical debates about API design and support, such as the support for long tensors in dilated convolutions, handling `col2im`/`im2col` operations, and maintaining compatibility with different backends like XLA, CUDA, and ONNX. Unresolved issues include fixing build failures, addressing bugs related to device guards with CUDA 10, and improving error reporting and type inference, all while considering the impact of changes on existing extensions and backward compatibility. Overall, the discussions reflect a focus on optimizing performance, ensuring platform compatibility, and refining API robustness and usability."
2019-06-16,pytorch/pytorch,"The discussions highlight several technical issues: the resolution of a bug in the `pixel_shuffle` layer export process, which was fixed by updating the symbolic.py implementation; questions about the availability and correct referencing of symbolic.py in the ONNX export pipeline, suggesting a need for version-specific updates or PyTorch version upgrades; synchronization problems related to CUDA's `requires_grad` propagation, with considerations about the limitations of optional gradient tracking; performance concerns with THNN and cuDNN kernels, especially for depthwise convolutions on FP32 versus FP16; and build/compatibility challenges, such as linking errors with Caffe2, threading deadlocks during GIL management, and environment inconsistencies affecting reproducibility and correctness."
2019-06-17,pytorch/pytorch,"The discussions highlight numerous technical concerns, including the potential integration of parameter and FLOPs counting tools directly into PyTorch, and the need for a standardized off-the-shelf solution. There are questions about the proper usage and behavior of KL divergence, especially regarding log-probabilities, and the stability and reproducibility issues stemming from GPU operations, such as non-deterministic behaviors in `F.interpolate`, `cross entropy`, and CUDA kernel launches. Several issues address build and compatibility challenges, notably with different CUDA versions, compiler tools like Visual Studio, and cross-version support for features like `libtorch` and ONNX export. Additionally, there are suggestions for improving API consistency, such as deprecating certain threading or context management functions, and for enhancing testing coverage and code organization to handle various corner cases efficiently. Unresolved questions remain around enabling deterministic training, simplifying distributed data loading, and optimizing operations like `multinomial` sampling and tensor conversion paths."
2019-06-18,pytorch/pytorch,"The discussions highlight various technical challenges, including the implementation and documentation of bitwise tensor operations in PyTorch, and compatibility issues with exporting models to ONNX, especially concerning model output formats and data types in tracers. Several issues address startup and build errors related to compiler versions, platform-specific macros, and header include paths, reflecting ongoing platform compatibility and build configuration concerns. Performance bottlenecks, particularly with large kernel sizes in convolutions and the efficiency of CUDA, CPU, and MKL-DNN operations, are also examined, with suggestions for kernel optimizations and better resource management. Additionally, there are discussions on correctness and stability in autograd and training workflows, such as handling repeated backward passes, managing gradients with zero targets in CTC loss, and ensuring proper synchronization during distributed training. Unresolved questions remain about bug fixes' completeness, appropriate naming conventions for kernels, and maintaining consistency across codebase modifications, with an emphasis on thorough testing and documentation."
2019-06-19,pytorch/pytorch,"The discussions highlight several key issues: the need for clearer and more consistent handling of device contexts and tensor memory formats (especially with channels_last and layout propagation), as seen in the tensor expansion, layout, and autograd-related debates; concerns about the stability and correctness of CUDA/cuDNN backends, particularly regarding deterministic algorithms, mixed precision, and backend-specific behavior such as with ROCm or ONNX exports; questions about extending or refactoring existing APIs—like `torch.trace`, `torch.cuda.set_device`, and deprecated functions—to improve usability, BC, and clarity; and the challenge of managing build environments, dependencies, and cross-compatibility, including CMake version issues, third-party library versions, and internal code modifications. Several discussions also touch on API design, performance optimizations, and ensuring backward compatibility while introducing new features or deprecations. Unresolved questions remain about how to best handle layout and type promotion consistency, CUDA resource management, and the proper integration of advanced features like perturbation training or custom operators."
2019-06-20,pytorch/pytorch,"The collected comments encompass a wide range of topics, including the implementation of specialized neural network layers such as LocalLinear and modifications to existing modules, with emphasis on correctness, performance, and BC-breaking risks. Several issues address software bugs, including deterministic behavior, memory management, and compatibility across hardware and software versions, highlighting the importance of testing, reproducibility, and detailed documentation. There is focus on developing utility modules such as nn.Flatten, handling of device and data types, and on the integration of features like quantization, pruning, and extensions, asking for clear interfaces, BC considerations, and practical examples. Discussions on the impact of CMake versions, CUDA support, and XLA compatibility underscore the complexity of build and deployment environments. Additionally, thematic concerns include API design principles, community contributions, and process improvements such as testing strategies, review procedures, and feature acceptance criteria, emphasizing the importance of balancing innovation with stability and usability."
2019-06-21,pytorch/pytorch,"The discussions primarily focus on resolving compatibility and functionality issues within PyTorch, such as the handling of `IntArrayRef` with size discrepancies affecting functions in `ATen/native`, and ensuring consistent API behavior between Python and C++ (particularly for `max_pool2d` and `max_pool2d_with_indices`). Several concerns involve improving reproducibility, randomness, and jit capabilities, including better documentation of `torch.Generator` and expanding support for activation-based pruning and quantization techniques. An underlying theme is optimizing build configurations, especially cross-platform and compiler differences, such as handling symbol export macros on MSVC versus GCC, and resolving binary dependencies like `libomp`. Additionally, there are ongoing debates on code API design choices—such as how to implement model state reloading, the handling of in-place tensor operations, and whether to deprecate certain STL-based containers—in pursuit of performance improvements and clearer interfaces. Unresolved questions include aligning C++ API semantics with Python's behavior, managing binary and symbol visibility issues, and defining the future direction of PyTorch's core abstractions and extension mechanisms."
2019-06-22,pytorch/pytorch,"The discussions highlight concerns about memory management and performance, such as increasing memory usage during batch loading, differences in convolution results between CUDA and ROCm backends, and potential regressions from in-place operations and kernel fusion optimizations. Several questions address how to better handle model loading, especially regarding checkpoints and serialization of complex objects like samplers, suggesting possible API improvements or alternative approaches. Issues related to PyTorch's API design are discussed, including the appropriateness of constructor-based loading, in-place operation behavior with mixed data types, and the consistency of existing scalar and tensor operations. There are also technical challenges in cross-platform compatibility, kernel fusion, and maintaining precise behavior across different hardware and backend configurations. Unresolved questions include how to implement flexible serialization, ensure performance stability, and streamline in-place tensor operations with type safety."
2019-06-23,pytorch/pytorch,"The discussions highlight challenges in exporting models to ONNX, particularly with complex structures like encoder-decoder architectures, where deep copies or module encapsulation improvements are suggested. There is significant attention on the non-deterministic behavior of certain kernels (e.g., MaxUnpool2d) and the need for reproducibility across different platforms and hardware, with proposed solutions including deterministic kernels, setting thread limits (OMP_NUM_THREADS), and barriers during distributed initialization. Concerns about model initialization are raised, advocating for explicit, configurable initialization methods with clear visibility and flexibility, rather than built-in defaults. Additionally, there are ongoing issues with library dependencies and build configurations, such as missing dylibs and compile issues, which impact installation and runtime stability. Overall, the discussions emphasize improving robustness, reproducibility, API clarity, and hardware compatibility across PyTorch's various features."
2019-06-24,pytorch/pytorch,"The discussions highlight ongoing challenges with specific functional implementations like `logsumexp` and `col2im`, particularly concerning compatibility, broadcasting, and numerical stability across different hardware backends and versions. There are concerns about the design and API consistency between C++ and Python interfaces, especially regarding argument handling and in-place operations, with calls for clearer, more maintainable solutions. Several threads address performance and backend support issues, such as CUDA versus ROCm discrepancies, MKL-DNN integration, and hardware-specific bugs, emphasizing the need for robust testing, benchmarking, and conditional support. Routing questions on documentation, package dependencies, and build environments to proper channels reflects a focus on usability and reproducibility. Unresolved questions remain about API deprecations, feature additions like sparse support, tensor options, and operational bugs in core components, indicating areas requiring further review and testing."
2019-06-25,pytorch/pytorch,"The discussions highlight several key technical concerns including the implementation of legacy or missing functions such as `potrs`, the need for a standardized `nn.Flatten` class following `torch.flatten`, and the support for ""same"" padding across convolutional layers and pooling, with debates on whether to depend on input data-dependent calculations. There are significant issues related to exporting models with torch.onnx, especially handling tuples and custom operators, as well as problems integrating libtorch with different environmental setups and dependencies like Eigen, Thrift, and NCCL. Memory management and potential leaks, particularly in CUDA contexts, are also a recurring theme, alongside questions around the correctness of new functions, testing, and ensuring backward compatibility. Lastly, some discussions revolve around build system inconsistencies, CI failures, and ensuring proper support for diff contexts like GPU, ROCm, and nested tensors, as well as the proper design of serialization and load mechanisms."
2019-06-26,pytorch/pytorch,"The discussions cover several technical concerns, including the implementation and support of sparse gradients and optimizers like SparseAdam, and the need for making hyperparameters in SSIM implementations tunable. Issues around ONNX export errors and the static nature of kernel attributes are highlighted, with suggestions to staticize kernel parameters. Multiple questions relate to CUDA and MKL-DNN performance, compatibility, and build issues, including ABI concerns and proper linking on CPU-only environments. There are also concerns about deep integration of features like NestedTensor, autograd support, and the design of tensor/parameter behaviors, as well as discussions on the correctness of shape calculations and the overall API semantics for schedulers, in-place operations, and backend support. Unresolved questions include how to best structure ptype propagation, support dynamic model modifications, and improve build and compatibility workflows."
2019-06-27,pytorch/pytorch,"The discussions highlight several key technical concerns, including: the complexity of correctly implementing and integrating `__array_ufunc__` and `__array_function__` for seamless interoperability between PyTorch and NumPy, with emphasis on the correct delegation order (`Tensor`'s protocols should delegate to underlying implementations rather than the reverse); the challenge of ensuring shape and data layout consistency, especially with convolutions and memory formats (`channels_last` vs. `channels_first`), and the need for proper propagation rules; data transfer and GPU memory management concerns for GPU-based `SSIM` implementation, including minimizing CPU-GPU data movement; compatibility issues and performance regressions related to specific hardware, backend changes, and dependencies; and some design suggestions for enhancing the API, such as dynamic parameter freezing in `DistributedDataParallel`. Many of these questions remain open, requiring careful design, testing, and deeper understanding of both framework internals and user workflows."
2019-06-28,pytorch/pytorch,"The discussions predominantly revolve around implementation details and potential issues in PyTorch’s functionalities. Key concerns include the correct formulation of weight decay in AdamW, memory overheads and memory leak risks in distributed training and CUDA kernel code, and the proper support for deprecated or missing features across various hardware backends. There are questions about improving model portability, especially regarding device handling and compatibility with different libraries and system configurations, such as issues with MKL, protobuf, and torchvision. Several discussions suggest potential fixes, workarounds, or feature enhancements—like overloading operators for new data types, supporting mixture distributions, or refining the internals of PyTorch modules—and some of these remain unresolved or under review. Overall, the main technical themes involve improving robustness, compatibility, and efficiency of PyTorch’s core functionalities and extensions."
2019-06-29,pytorch/pytorch,"The discussions highlight ongoing challenges and enhancements in PyTorch, including issues with package resolution and post-releases in pipenv, discrepancies in model conflict during inference, and the need for better support and testing for new features such as SSIM, MS-SSIM, and layout-aware operators. Several comments address bugs related to C++/CUDA extensions, incorrect handling of tensor layouts, and build system inconsistencies, with solutions like replacing deprecated headers or modifying build configurations. There is also interest in improving usability features, such as supporting different tensor dimensions for loss functions, enabling multi-GPU training and inference workflows, and ensuring backward compatibility and stability across various environments. Certain unresolved questions concern performance pitfalls, like memory consumption in the ideep library, and ensuring the correctness of autograd operations (e.g., `cdist` gradients), indicating areas for future refinement. Overall, the discussion emphasizes iterative improvements, bug fixes, and feature expansions to strengthen PyTorch's robustness and flexibility."
2019-06-30,pytorch/pytorch,"The discussions predominantly focus on enhancing PyTorch features, including adding support for 'same' padding in ConvND layers, which involves dynamic input-dependent padding calculations with potential performance trade-offs. There are ongoing efforts to improve support for sparse gradients, batched determinant operations, and matrix exponential implementations, often encountering compatibility or linkage issues. Several conversations address integration challenges, such as adapting the `~` operator for bitwise operations, refining model conversion from PyTorch to ONNX and OpenVINO, and implementing deterministic scatter operations, which require careful device and concurrency considerations. Additionally, there is discussion around extending tensor functionality, supporting multiple data types, and ensuring compatibility across platforms, with some pending unresolved questions about performance impacts and backward compatibility. Overall, the thread reveals an active roadmap for feature additions, bug fixes, and interoperability improvements in PyTorch and related tools."
2019-07-01,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's features and stability, such as implementing new functionalities like dense-sparse matrix multiplication support, MixtureSameFamily distributions, and robust distributed training methods, alongside refactoring efforts like moving NVRTC support to core ATen. There are concerns about performance issues and bugs, particularly related to CUDA operations (e.g., kernel crashes, non-deterministic behaviors, and build errors due to mismatched dependencies), as well as compatibility challenges with hardware (e.g., GPU architecture incompatibilities). Several threads focus on optimizing existing modules—such as enhancing einsum broadcasting, improving max pooling reliability, and fixing build configurations—and ensuring backward compatibility with deprecated APIs and inference precision. There is also an emphasis on cleaning up build systems, supporting diverse tensor input dimensions, and clarifying testing and documentation practices for new features. Unresolved questions include handling hardware-specific bugs, ensuring deterministic operations, and integrating user-defined CUDA snippets in a safe, portable manner."
2019-07-02,pytorch/pytorch,"The comments reflect concerns around stability, performance, and correctness in PyTorch. Notable issues include difficulties ensuring reproducible randomness (e.g., improved uniform random generators), challenges in handling NaNs and edge cases in gradients, and the need for improved support for sparse tensors and higher-order gradients. There are recurring questions about ensuring deterministic behavior, efficient custom memory handling, and the proper merging or refactoring of internal modules (like MKL-DNN support and CUDA-related code). Several discussions address API design and usability, such as the support for broadcasting in einsum, API consistency for metrics/losses, and the placement of new features or deprecations. Unresolved questions include integrating new features without breaking backward compatibility, optimizing computation across CPU/GPU, and improving testing coverage for API stability."
2019-07-03,pytorch/pytorch,"The discussions highlight persistent issues with dataloader hangs and data loading stability, especially with multiple workers and environment conflicts such as OpenCV's threading affecting training halts. There are ongoing efforts to implement advanced features like 'padding=""same""' in convolutional layers, with considerations about their dynamic behavior, performance implications, and compatibility with export formats, alongside code refactoring and testing challenges. Compatibility and build issues surface related to external libraries like NCCL, MKL-DNN, and CUDA, with specific attention to correct detection, consistent memory reporting, and potential race conditions involving cudaFree and NCCL group launches. Furthermore, questions around extending standard Python built-in functions (like range, max, set) into PyTorch's API, especially with regard to quantization and gradient computation, are being discussed, along with the need for clearer documentation and API consistency. Lastly, issues with environment configuration and build process errors, particularly on different operating systems and Python versions, emphasize the importance of environment setup and dependency management."
2019-07-04,pytorch/pytorch,"The discussions highlight a range of technical concerns including the absence of certain features such as flexible frequency axes in FFT operations, causal padding in convolution layers, and improved handling of batch normalization errors in training with small batch sizes. Several comments propose enhancements like adding support for asymmetric 'same' padding in Conv layers, controlling axes in FFT, and expanding metrics and loss functions for better monitoring and evaluation. There is also focus on optimizing performance through better utilization of MKLDNN, fixing memory overlap detection, and addressing import/export issues with ONNX and TorchScript. Unresolved questions include the implementation details for features like custom metrics, handling of quantized MKLDNN tensors, and resolving build errors related to CUDA, NCCL, or environment mismatches."
2019-07-05,pytorch/pytorch,"The discussions raise several key technical concerns: the need for enhanced control over FFT operations, specifically specifying axes for transforms; the requirement for more efficient circular padding implementations; and progress toward enabling batched determinant and log-determinant operations, which depend on batched SVD support. Additional questions include how to improve or replace slow documentation rendering potentially caused by KaTeX, and how to extend PyTorch's functionalities such as custom C++ operations, normalization in SSIM, and handling tensor serialization/deserialization issues. There are also challenges related to debugging distributed training, performance optimization of data loading, and supporting non-default CUDA streams for parallel kernel execution. Unresolved questions encompass how to support these feature requests effectively and the timeline for significant enhancements like batched SVD and higher-level wrapper integrations."
2019-07-06,pytorch/pytorch,"The discussions primarily revolve around the implementation and correctness of weight decay scheduling in optimizers, with suggestions to decouple weight decay from learning rate scheduling for better adherence to theoretical frameworks. Several questions concern performance optimization of models, particularly scripted models and their inference speed, along with issues related to CUDA compatibility, driver versions, and environment setup, especially on Windows and GPU-specific configurations. There are also multiple inquiries about extending PyTorch functionalities, such as sparse tensor operations, custom C++/CUDA operators, and better support for metrics, metrics libraries, and training utilities like ignite, aiming for more flexible, reliable, and user-friendly tools. Some technical concerns point to stability issues, numerical edge cases, and proper API design, including handling of autograd functions, serialization formats, and proper documentation, which are critical for robust and maintainable deep learning workflows. Overall, the discussions highlight ongoing efforts to improve optimizer behavior, model inference efficiency, environment compatibility, and ecosystem extensibility within PyTorch."
2019-07-07,pytorch/pytorch,"The discussions highlight several technical issues and questions, including potential module import conflicts due to files named like 'torchvision.py,' and challenges with integrating specific CUDA and cuDNN versions, such as replacing PyTorch's bundled cuDNN. There are ongoing developments regarding matrix exponential functions (expm, logm, sqrtm) and their integration into PyTorch. Users also inquire about performance limitations with GPU-based batch QR and eigendecomposition for small matrices, with suggestions to optimize by batching larger matrices or leveraging external libraries like MAGMA. Additional concerns involve build errors related to protobuf, DLL load failures on Windows, and API design questions such as enabling flexible loss functions like MS-SSIM, with some issues requiring updates to dependencies or patches to improve functionality."
2019-07-08,pytorch/pytorch,"The discussions highlight several core concerns: first, the development of out-of-tree CUDA device/layout/dtype extension capabilities and their timelines, with questions about inferred features and implementation plans; second, issues related to CUDA/cuDNN bugs, such as specific errors with large batch sizes, and performance bottlenecks in linear algebra routines, especially on small matrices, suggesting potential optimization strategies like batching with block diagonal matrices; and third, compatibility and build issues, notably with the NCCL detection in CMake and protobuf compilation failures on Raspberry Pi, along with questions about supporting CPU Half precision and handling environment variables like OMP_NUM_THREADS for optimal performance; additionally, there are discussions on integrating metrics and evaluation tools into core PyTorch, and API design considerations such as supporting dynamic padding in convolutional modules, as well as quality assurance actions like adding tests for RNG functions. Overall, unresolved questions concern feature timelines, performance optimization, robustness of build configurations, and API consistency, indicating ongoing efforts to improve PyTorch’s extensibility, reliability, and usability."
2019-07-09,pytorch/pytorch,"The discussions highlight several technical challenges and considerations: issues with CUDA kernel selection, cuDNN bugs, and the need for better hardware compatibility (e.g., C++ ABI issues, GPU driver mismatches). There are ongoing efforts to improve PyTorch's internal components, such as supporting batched linear algebra operations, proper handling of shape analysis, and reworking autograd for views and nested modules. Concerns about performance optimization are raised, especially regarding small matrix operations, memory pinning, and kernel launch overheads. Additionally, broader development questions include build system robustness (e.g., version mismatches, third-party dependencies), feature completeness (e.g., metrics in core, quantization support), and API design considerations for modules like `nn::Sequential`. Unresolved questions pertain mainly to improving hardware support, build stability, and API consistency."
2019-07-10,pytorch/pytorch,"The discussions primarily revolve around the consistency and implementation details of core PyTorch functionalities, such as batch normalization correction practices, model serialization, and distributed training strategies, with concerns about correctness, performance, and usability. Notable points include the need to unify variance correction during training/testing, proper model loading and class definition order, and enhancing distributed data parallelism support across platforms, especially on Mac. Several issues address the impact of implementation choices on CUDA kernel launch overheads, memory management, and reproducibility, emphasizing the importance of accurate profiling and environment configuration. Additionally, there's ongoing interest in developing features like metrics, quantization, and flexible API extensions, alongside maintaining build integrity and compatibility with various toolchains. Overall, unresolved questions include balancing performance with correctness, establishing clear API behaviors, and improving debugging and testing workflows."
2019-07-11,pytorch/pytorch,"The comments predominantly address technical challenges related to PyTorch's stability, performance, and compatibility. Key concerns include GPU memory management issues and fainting errors during model training or inference, particularly under specific hardware conditions and configurations. Several discussions highlight the need for better debugging and reproducibility tools, such as benchmarks, detailed error messages, and methods to handle sparse and quantized tensors efficiently. Additional questions focus on improving the robustness of distributed training across different platforms (Windows, MacOS, Linux), and clarifying how features like JIT, TorchScript, and API serialization will evolve, especially regarding function/module saving and operator support. Overall, proposals suggest incremental fixes, architectural adjustments, and better documentation or tooling to address recurring stability and usability issues."
2019-07-12,pytorch/pytorch,"The discussions highlight several key areas: first, the importance of robust power supply and proper cabling for GPU stability, as well as hardware-related issues like PSU and motherboard failures; second, the challenge of debugging and profiling backward computations in PyTorch, with current hooks and profilers limited in scope; third, compatibility issues stemming from outdated system libraries (e.g., libstdc++) and package conflicts, with suggested solutions involving environment configuration and package management; fourth, API design and maintainability concerns, such as the handling of in-place functions, operator promotion rules, and deprecated features, emphasizing the need for clear deprecation paths and semantic clarity; finally, the complexity of build environment settings, including CUDA, NCCL, and compiler compatibility, as well as ensuring reproducibility and correctness across multi-GPU and multi-node setups; overarching themes involve hardware stability, library compatibility, API correctness, and build robustness."
2019-07-13,pytorch/pytorch,"The discussions highlight ongoing challenges with distributed training in PyTorch, specifically issues with `DistributedDataParallel` causing timeouts and failures, and the need for better control over Gloo IBVerbs usage. There are concerns about the correctness of second-order gradients when using `DataParallel` and the limitations of current implementations with bidirectional RNNs, leading to bugs and potential model training failures. Compatibility issues are frequently raised, such as build errors on macOS with deprecated libraries, and mismatches in ONNX export functions due to tensor scalar conversions. Moreover, there are proposals for API improvements, including making the `upper` argument in certain functions ternary for clearer behavior and shifting to `OpaqueTensorImpl` for efficiency, alongside unresolved questions about performance benchmarks and build processes. Overall, the discussions reflect a focus on improving stability, correctness, and flexibility in distributed and optimized operations within PyTorch."
2019-07-14,pytorch/pytorch,"The discussions raise several technical concerns, including the lack of native support for certain functionalities in PyTorch such as batched determinant and log-determinant operations, which are being addressed through separate branches like `batch-svd`. There is an ongoing debate about integrating features like HParams support into tensorboard within PyTorch, with suggestions to implement them at a higher wrapper level rather than core. Compatibility issues are evident in installation and environment setup, especially related to CUDA versions, driver configurations, and support for architecture-specific builds, notably PPC64le and Windows. Several issues highlight errors associated with GPU memory management, NCCl, cuDNN, and bugs in double backward support for RNN modules, emphasizing the need for more robust and precise error handling and API support. Unresolved questions include timing for feature merges into main branches, plans for static binary builds, and the best approaches for integrating external support or features without complicating the core library."
2019-07-15,pytorch/pytorch,"The discussions highlight several core technical concerns: first, the correct handling of automatic device placement and default device settings in PyTorch, with some debate over providing explicit control versus relying on environment variables or API functions; second, challenges related to efficient multi-GPU parallelism, especially with DistributedDataParallel, NCCL errors, and the importance of synchronization and correct environment setup; third, the complexity of extending PyTorch's support for sparse support, custom kernel optimizations, and tensor layouts, with considerations around implementation difficulty versus performance gains; fourth, issues around backward compatibility and API deprecation, emphasizing care in removing or modifying existing functions, sometimes requiring deprecation cycles; finally, there are ongoing questions about low-level optimization strategies (such as memory access patterns for half precision), and ensuring test coverage for edge cases like zero-element tensors."
2019-07-16,pytorch/pytorch,"The discussions highlight several main concerns: Firstly, issues with hardware configurations and power/PSU connections affecting GPU stability and system reboots, suggesting improved cabling and BIOS updates may be necessary. Secondly, there are persistent bugs related to CUDA, NCCL, and multi-GPU/CPU interactions, often requiring environment variable tweaks, software upgrades, or code refactoring for stability and performance. Thirdly, the complexity of PyTorch's internal implementation presents challenges for memory management, autograd, and tensor operations, with suggestions for clearer API design and enhanced safety (e.g., warning messages for dangerous functions like `as_strided`). Fourthly, many discussions propose or review feature enhancements, such as metrics APIs, tensor pattern rewrites, and expanded autograd/deprecation management, often contingent on broader refactoring or API stability considerations. Lastly, several unresolved questions reflect ongoing development tasks, such as support for nested tensors, improved threading control, and correct handling of tensor shape and type promotion behaviors, emphasizing the need for continued testing, verification, and systematic documentation."
2019-07-17,pytorch/pytorch,"The discussions highlight several technical concerns, including intermittent stability issues on Threadripper systems often linked to BIOS or RAM configurations, and GPU communication lockups primarily with multi-GPU setups lacking proper PCIe peer-to-peer access or due to driver incompatibilities. Additionally, there are challenges with performance regressions in PyTorch documentation loading times due to KaTeX, and potential bugs with dynamic behaviors such as autograd hook support, and specific operator implementations (e.g., `symeig`, `resize`, `flatten`) that deviate from expectations or other frameworks like NumPy. Several questions focus on improving usability and consistency, including clarifying API deprecations, adding helpful attributes (`ndim`, `get_lr`), and refining the handling of onnx export limitations or missing features in serialization. Solutions include code refactoring, better hardware and driver diagnostics, updating dependencies, and API improvements, with some issues awaiting further review or bug fixes."
2019-07-18,pytorch/pytorch,"The discussions cover various technical issues including compatibility and stability challenges with PyTorch's distributed training on multi-GPU systems, with specific attention to NCCL configurations, CUDA driver versions, and system-level logs. There's concern over the integration of third-party libraries like OpenCV and ONNX, highlighting issues with threading, operator compatibility, and export behaviors, especially for models with specific operators such as Upsample or custom functions. Performance optimization stays a recurring theme, especially in relation to memory management (caching allocators, buffer reuse), kernel efficiency (half-precision, atomic operations), and API design (merging or deprecating functions like `logdet` or `flatten`) to improve runtime and user experience. Several points regarding code correctness, such as index handling, ensuring deterministic behavior, and proper error reporting, are also raised without definitive resolutions. Overall, the conversations reflect ongoing efforts towards stabilizing distribution, upgrading interoperability, and enhancing performance while maintaining API clarity and backward compatibility."
2019-07-19,pytorch/pytorch,"The discussions highlight several technical concerns, notably the need for efficient parameter filtering during training, the proper handling of padding (including negative and ""same"" padding) in convolution operations, and the clarification of API behaviors such as label smoothing and HParam support in TensorBoard. There are unresolved questions about compatibility issues with different TorchScript types, device memory management, and backward pass behaviors, especially for user-defined functions and nested tensors. The community suggests refactoring deprecated or problematic functions, improving test coverage (especially for edge cases like non-contiguous tensors and deterministic algorithms), and refining API consistency (e.g., for learning rate schedulers, distributed CUDA, and onnx export). Several discussions also concern performance optimizations in convolution algorithms, NCCL backend settings, and CUDA compatibility, alongside logistical issues related to CI, code review, and repository maintenance."
2019-07-20,pytorch/pytorch,"The discussions primarily revolve around robustness and correctness issues in PyTorch, such as handling of errors on GPU versus CPU (e.g., in `multinomial` sampling), and the need for more meaningful error messages and documentation updates. There are concerns about numerical precision and floating-point rounding errors affecting operations like convolution and matrix computations, especially under different hardware and data types. Several issues highlight the importance of API consistency and proper API design, such as enforcing argument types, clarifying behavior of `weight_norm`, and extending support for shared memory to handle variable-length strings efficiently. There is ongoing work to improve determinism, performance, and usability, with questions about best practices for disabling or revising internal features, and efforts to unify interfaces, such as LRScheduler inheritance and tensor operations, especially across different backends and data types. Unresolved questions include handling of in-place operations, ensuring API clarity (e.g., padding strategies in convolutions), and managing large-scale data and model precision in high-volume applications like medical imaging."
2019-07-21,pytorch/pytorch,"The discussions raise several key concerns: the need for clearer and more informative tensor display representations (particularly including shape, dtype, and device info) to improve debugging efficiency; the challenge of managing multithreaded CPU kernels and controlling thread usage through environment variables vs. API calls; and issues with tensor serialization and model loading, especially regarding zipped vs. unzipped states in C++ deployment. There's also a focus on improving performance and usability, such as refining the behavior of `default_collate` to handle variable-length collections and updating the handling of large tensors beyond certain size limits supported by cuDNN. Additional points include managing dependencies' thread locks, refining the behavior of nested parallel calls, and advancing features like the 1cycle policy for learning rate scheduling, with demonstrations across various applications. Several unresolved questions pertain to tensor display formatting, initialization of CUDA, and handling large tensor operations that exceed hardware limitations."
2019-07-22,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's autograd and data loading systems, including error handling in autograd profiler, and improvements to error messages related to CUDA and data loader worker failures. Several issues address numerical stability and precision, such as eigenvalue perturbations, NaN detection, and floating-point reproducibility, often tied to specific hardware or kernel implementations. There is interest in enhancing API consistency with NumPy, such as adding properties like `element_size()`, and extending support for new tensor types like `bfloat16`. Build environment stability and correctness issues surface, especially related to CUDA version compatibility, build system configurations, and integration with external libraries, warranting careful configuration and re-evaluation. Finally, proposals for API improvements include better support for weight/bias fusion, gradient scaling, and more descriptive error messaging, with some concerns about backward compatibility and the complexity of refactoring core components."
2019-07-23,pytorch/pytorch,"The discussions highlight several technical concerns: inconsistencies and limitations in tensor operations, such as the support for boolean subtraction and the desire to include tensor shape information in debug outputs; issues with CUDA and OpenMP threading behavior affecting performance and error reporting; challenges in integrating and documenting features like chunk-based data loading from databases, and handling complex data sources; dependencies and compatibility concerns related to library versions (e.g., cuDNN, MKL, and libtorch builds), as well as build system intricacies on Windows and ensuring proper linkage; and enhancements to JIT, distribution, and quantization workflows, including ensuring backward compatibility, proper API usage, and the need for clearer documentation and code refactoring to support new functionalities like UTF-8 support and fused operations. Unresolved questions involve improving code robustness, correctness, and usability, especially around error handling, user-facing APIs, and performance optimizations."
2019-07-24,pytorch/pytorch,"The discussions highlight several key technical concerns, including challenges with CUDA resource limitations and the need for adjustable thread configurations, especially on embedded platforms like Jetson. There are ongoing efforts to improve API consistency and usability, such as enabling nested tensor operations, scripting support, and chainable learning rate schedulers while maintaining performance and backward compatibility. Compatibility issues across different OS, compiler, and library versions (e.g., CMake, MKLDNN, protobuf) are prominent, with suggestions for better build configuration practices and environment management. Concerns about model serialization compatibility, especially with evolving model formats and SDKs, are addressed through version checks and build strategies. Lastly, questions remain on how to extend or modify internal APIs, such as incorporating environment-awareness into TorchScript, better handling of sparse or optional data structures, and improving documentation and testing to ensure stability across diverse use-cases."
2019-07-25,pytorch/pytorch,"The discussions highlight persistent issues related to device-specific tensor state management, especially concerning saving/loading optimizer states and ensuring device consistency, with suggestions for using CPU versions during save to streamline device resumption. Circular padding implementations, including circular convolution and several padding utilities, are addressed, with recent resolutions confirmed via PRs, though hardware-specific errors (e.g., on Jetson or specific CUDA kernels) remain problematic and often require manual tuning or patching. There are ongoing challenges with auto-differentiation behaviors, such as handling in-place modifications, gradients through non-standard control flows, and the complexities of deferred or uninitialized parameter initialization, prompting proposals for explicit deferred initialization mechanisms. Additionally, issues around model serialization, scripting, and runtime behavior (e.g., mismatched versions, missing protobuf fields, or serialization of complex internal states) are discussed, with suggestions to improve API clarity and robustness. Lastly, there are concerns about build configurations, environment variable handling, and platform-specific quirks (e.g., Windows build environment, compiler flags, or MKL threading) requiring careful management to ensure portability and correctness."
2019-07-26,pytorch/pytorch,"The discussions raise several technical concerns, primarily around the implementation and correctness of features such as 'timedistributed' tensor operations, parameter sharing in modules, and accurate handling of memory formats during tensor reshaping. Notably, there are questions about reproducibility and robustness of GPU/NN architectures, including NCCL and CUDA version compatibility, as well as environment dependencies affecting build stability. Additionally, there is critique of documentation clarity, testing coverage, and the design philosophy of API consistency—particularly for complex functionalities like parameter sharing, ONNX export, and custom kernel selection. Unresolved questions include how to best automate testing of low-level CUDA code behaviors, improve dependency handling in build systems, and support mixed memory layouts without confusing users. Overall, these discussions highlight ongoing efforts to enhance reliability, usability, and maintainability of the PyTorch codebase amidst evolving hardware and software environments."
2019-07-27,pytorch/pytorch,"The discussions highlight several recurring issues: the challenge of ensuring data loader reliability in multi-GPU and multi-worker settings, with potential conflicts caused by third-party dependencies like OpenCV or memory fragmentation. There are complexities related to CUDA build configurations, specifically handling CUDA library paths, static versus shared linking, and platform-specific library suffixes, which impact reproducibility and ease of setup. Implementation of new features such as support for sparse tensors, edge detection in model training, and handling large tensors exceeding cuDNN limits are discussed, with concerns about compatibility, performance, and API consistency. Additionally, there are ongoing efforts to enhance debugging tools, user documentation, and test coverage to prevent regressions, especially for CUDA-related functionalities and non-standard workflows like ONNX export. Overall, unresolved questions include improving build system robustness, maintaining API clarity amid feature additions, and ensuring platform compatibility."
2019-07-28,pytorch/pytorch,"The discussions highlight several key technical concerns, including the challenge of ensuring deterministic behavior and reproducibility in PyTorch, especially when using multiple DataLoader workers, with suggested solutions like setting seeds and using `worker_init_fn`. There are concerns about non-deterministic modules (e.g., upsampling, interpolation) affecting reproducibility, and whether modifications to tensor operations (like `.reshape()` and `.clone()`) might introduce unexpected behavior related to memory layout and strides. Issues related to multi-GPU training and distributed setups are also discussed, with potential fixes involving environment configurations and code adjustments. Additionally, some CI failures are attributed to infrastructure or package repository issues rather than code changes."
2019-07-29,pytorch/pytorch,"The discussions highlight several key technical concerns: the need for more informative error messages and handling for ill-conditioned matrices and numerical issues; the challenges of supporting batched eigen/decomposition functions on GPU due to hardware limitations; and complexities related to CUDA library path resolution, especially with non-standard or variable CUDA install locations. Additionally, there are ongoing plans for long-term features such as deferred module parameter initialization, enhancements to autograd, and refining build configurations for CUDA dependencies. Several proposals suggest improving user experience through explicit APIs, better error messaging, and more transparent support for custom or edge-case operations. Unresolved questions remain around supporting under-determined systems, efficient batched matrix operations, and improving build system robustness for diverse environments."
2019-07-30,pytorch/pytorch,"The discussions primarily revolve around memory management and performance optimization in PyTorch, including monitoring swap usage and system I/O bottlenecks during large batch processing. Several issues address CUDA and cuDNN compatibility, including build errors due to mismatched library formats, missing dependencies, and compilation failures on specific hardware like Jetson boards. There are concerns about scriptability and code maintainability, particularly in implementing new features such as metrics, quantization, and JIT compilation, with suggestions to add testing, documentation, and better API design. Additionally, discussions include system dependencies, environment setup, multi-GPU training issues, and the need for clearer versioning, build consistency, and external library support. Unresolved questions include the proper handling of mixed precision, support for different device architectures, and ensuring backward compatibility and ease of use."
2019-07-31,pytorch/pytorch,"The discussions highlight several key issues: the need for better documentation and usability regarding features like batch support for linear algebra operations (e.g., det/logdet), and ensuring that performance improvements (e.g., fuse-operations via JIT, efficient handling of nested tensors) are properly benchmarked and validated. There are concerns about the clarity and maintainability of complex code changes, such as the refactoring of RNN modules and the handling of different tensor memory layouts, especially with regard to ONNX compatibility and dynamic shape support. The community emphasizes the importance of transparent contribution practices, such as notifying before large PRs and documenting APIs thoroughly, to foster open source collaboration and avoid alienation. Unresolved questions include the integration of features like named tensor support across different operators and the handling of numerical stability in linear algebra functions (e.g., logdet for semi-definite matrices)."
2019-08-01,pytorch/pytorch,"The discussions highlight concerns about the discontinuation of TUNA's Anaconda mirror and the reliance on pip versus conda for package installation, with recommendations to switch to pip or alternative mirrors. Several issues address CUDA compatibility and driver detection, especially on Windows and with different CUDA versions, noting the importance of proper driver installation and environment configuration. There are ongoing efforts to improve PyTorch features such as implementing nn.Flatten, adding support for additional tensor operations, and refining quantization workflows, emphasizing user-friendly debugging and flexible configurations. Performance optimization topics include enhancing data loader startup time, kernel fusion, and memory management, with suggestions to introduce better reuse and parallelization strategies. Lastly, numerous issues raise questions about build system robustness, versioning, and packaging, particularly regarding wheel files, nightly builds, and ONNX export support, alongside discussions on maintaining code clarity and avoiding regressions through careful testing and review."
2019-08-02,pytorch/pytorch,"The discussions highlight several key technical issues including: the proper handling of environment-specific build configurations, especially with CMake and CUDA versions; ensuring compatibility and reproducibility of data loading processes across different systems; managing differences in numerical results caused by floating-point precision and implementation details (as seen in SSIM and other benchmarks); modifications to the PyTorch autograd system to avoid reference cycles and improve memory management; and improving the robustness and correctness of distributed training, including NCCL and Gloo backends. There are suggestions for refining API consistency, such as adding `**kwargs` for modules, and better test coverage for new features. Some discussions concern the impact of recent patches on stability and performance, including in-place operations, inlining, and function restructuring. Overall, unresolved questions involve validation of build configurations, environment setup, and ensuring backward compatibility while pursuing improvements in functionality and performance."
2019-08-03,pytorch/pytorch,"The discussions highlight ongoing issues with DataLoader stability and performance, especially relating to multi-worker configurations, system resource limitations, and platform-specific behaviors (e.g., Windows vs. Linux). Several comments suggest tuning `num_workers`, pin_memory, and controlling threading via environment variables to mitigate deadlocks, stalls, and crashes. There are also concerns about bug fixes implementing updates from external libraries (like MAGMA) and ensuring compatibility with various PyTorch versions, alongside requests for feature improvements such as support for symmetric tensors and dynamic shapes in ONNX. Additionally, unresolved issues include intermittent download failures, complex error traces in quantization and MKL/DNN operations, and the need for clearer documentation and testing of new features."
2019-08-04,pytorch/pytorch,"The discussions primarily highlight issues related to environment and dependency management for PyTorch, such as offline installation challenges, CMake version incompatibilities, and CUDA/CUDNN configuration problems leading to compilation errors and crashes. Several comments address build system nuances, including CMake version thresholds affecting CUDA code generation and the need for environment variables like TENSORRT_ROOT, as well as specific errors like CUB dependent compile errors and mkldnn-related pointer corruption. There are concerns about ensuring reproducibility and determinism in convolution operations, with suggestions to use `torch.manual_seed()` alongside `torch.backends.cudnn.deterministic` settings. Additionally, issues around proper test coverage, size optimization of debug info, and the timing of bug fixes reaching PyPI are mentioned. Overall, the discussions focus on improving build stability, reproducibility, and deployment efficiency while troubleshooting specific technical errors."
2019-08-05,pytorch/pytorch,"The discussions highlight several key technical concerns in the PyTorch repository, including the need for unstructured 2D/ND interpolation functions similar to scipy.interpolate.griddata, and the challenge of implementing unstructured interpolation on GPU. There are performance and correctness considerations around CUDA/cuDNN deterministic behaviors, especially for convolution and other GPU operations, with some solutions involving seeding and backend configuration. Several issues address build and dependency management, such as handling long file paths on Windows, ensuring proper package versions, and fixing build regressions related to PyTorch's internal structure and external tools. Additionally, maintaining compatibility, enhancing testing strategies, and clarifying API design choices (e.g., overload handling, in-place view management) remain ongoing concerns. Unresolved questions include the integration of new features like override APIs, memory leak detection, and better abstractions for data loading and backend operations across different environments."
2019-08-06,pytorch/pytorch,"The discussions cover several key technical concerns, including environment and dependency management for PyTorch installation, especially related to CUDA and cuDNN versions; compatibility and support issues for features like `torch.linalg.eigh` and eigen decomposition; and challenges in implementing batched Jacobian computations efficiently in PyTorch, with suggestions to leverage or develop parallel and vectorized solutions. There are also recurring questions about serialization efficiency, debugging and testing practices across device configurations, and the design of APIs such as `record_stream()` and the custom dispatch mechanisms. Additionally, some discussions focus on performance optimizations, API enhancements, and proper handling of in-place operations and autograd graph lifetime management. Unresolved questions include how to best support multi-device, multi-threaded workflows, and how to integrate new features while maintaining compatibility and performance."
2019-08-07,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for true batched matrix operations like `triu` in PyTorch, which could improve efficiency for batch processing. There are ongoing developments and considerations around the `autograd` system, especially regarding view operations, in-place modifications, and the management of `grad_fn` lifetimes, with questions about how weak pointers impact backward pass correctness. Several PR reviews point to installation, compatibility, and build issues, such as differences in support for various hardware like ROCm, WSL, and OS X, as well as the importance of thorough testing and proper API design (e.g., `rpc` APIs). Concerns about performance, such as GPU bandwidth and operation latency, are raised alongside proposals for benchmarking and micro-optimizations. Lastly, there are questions about API consistency, proper documentation, and integration of new features like `record_stream()` and `align_corners` in core modules, indicating ongoing efforts to improve scalability, usability, and correctness."
2019-08-08,pytorch/pytorch,"The discussions highlight several technical concerns, including the proper configuration of shared memory in Docker and cloud environments, with specific questions about setting `--shm-size` for Cloud ML jobs. There are recurring issues with CUDA CUBLAS errors, invalid resource handles, and GPU-related crashes, often associated with multi-GPU setups and NCCL/NCCL2/IB networks, emphasizing the need for better support and fixes in parallel/distributed operations. Discussions also address performance regressions in matrix operations like `triangular_solve`, suggesting fixes in implementation to handle broadcasting and shape assumptions efficiently. Additionally, there are concerns about API consistency and usability, such as overloading functions like `view` versus clearer naming schemes (`renamed`) and the necessity for more comprehensive tests, especially for sparse tensors and memory formats. Overall, the conversations reflect ongoing efforts to stabilize distributed training, optimize performance, improve API clarity, and ensure compatibility across hardware and software configurations."
2019-08-09,pytorch/pytorch,"The discussions encompass a range of technical concerns, including compatibility and build issues with different PyTorch versions, CUDA architectures, and libraries such as ONNX, TensorFlow, and OpenCV. There is emphasis on improving PyTorch functionalities, such as adding new interpolation modes, better handling of shape and device mismatches, and optimizing performance, especially in autograd and batched Jacobian computations. Several threads also address stability and correctness, including issues with reproducibility, serialization, memory formats, and backward compatibility, alongside suggestions for better testing, documentation, and API design. Unresolved questions largely focus on ensuring seamless integration across different execution environments (CPU, CUDA, mobile), and streamlining the registration, dispatching, and scripting mechanisms of operators and modules."
2019-08-10,pytorch/pytorch,"The discussions primarily revolve around various technical challenges and feature requests within PyTorch, including issues with DataLoader's CPU utilization when using pin_memory, CUDA errors during tensor operations, and the implementation of backward-compatible features such as default device settings and shape mismatch allowances in state_dict loading. Several conversations address environment setup problems, such as numpy version conflicts and CUDA-related failures, as well as updates to improve performance, especially for large matrix operations and certain GPU architectures like Volta. There are also considerations around API improvements, including adding test coverage, managing backward compatibility, and clarifying behavior under new configurations or in different runtime scenarios. Overall, the issues highlight ongoing maintenance, optimization, and feature enhancement efforts to improve robustness, usability, and performance of PyTorch."
2019-08-11,pytorch/pytorch,"The discussions reflect several core concerns, including the integration of contrastive loss functions and the API design for loss functions like Dropout, with suggestions to unify or deprecate older variants. There are technical questions about low-level tensor operations such as `data_ptr` on GPU, and issues related to environment configuration, Docker memory settings, and support for specific models and operators like `det`, `logdet`, and shape inference problems in ONNX exports. Several issues address version compatibility, with concerns about precise CUDA and PyTorch version support, as well as dependencies like NumPy. Additionally, users raise questions about implementation details, default behaviors affecting Caffe2 or ONNX exports, and the impact of recent PRs on stability or functionality, with some unresolved issues awaiting further review or fixes."
2019-08-12,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch, including integrating features like HierarchicalSoftmax, sparse tensor support, and specialized operators such as tridiagonal solvers and batched pairwise distances, with considerations for performance and proper API design. Several issues address performance optimizations and correctness, such as addressing GPU kernel performance, floating-point accuracy, and consistent behavior across different hardware and software environments, including CUDA, cuDNN, and compiler compatibility. There is concern about documentation, API consistency (e.g., handling `**kwargs`, outputs in JIT tracing), and compatibility issues with serialization and different Python versions, with debates on best practices and backward compatibility. Discussions also include infrastructure improvements like build configurations, environment management, and integration with tools like TensorBoard, alongside bug fixing and maintenance tasks (e.g., rebase, deprecation, error handling). Overall, these threads reflect active development balancing feature addition, performance tuning, stability, and infrastructure robustness."
2019-08-13,pytorch/pytorch,"The discussion covers multiple technical concerns, including the need for enhanced API flexibility (e.g., controlling axes in FFT operations, adding support for empty tensors), improvements in distributed training (e.g., supporting batching, better checkpointing with DDP, delaying gradient reduction), and performance optimizations (e.g., threading, CUDA stream parallelism, kernel compilation). There are also questions about bug fixes and feature implementations such as eigenvalue subset computation, integrating deformable convolutions, and proper handling of tensor data types/models in tensorboard and DLpack conversions. Several proposals aim to improve usability, robustness, and compatibility across platforms and configurations, often seeking better error handling, clearer documentation, or simplified workflows. Unresolved issues include compatibility bugs, performance regressions, and API limitations, with many suggestions for future improvements and considerations for ongoing maintenance."
2019-08-14,pytorch/pytorch,"The discussions cover a wide range of technical issues encountered in the PyTorch project, including implementation details, build and environment problems, performance regressions, and API behavior inconsistencies. Key concerns involve ensuring ABI compatibility between Python and C++ libraries, addressing slow kernel operations such as max reduction kernels on CUDA, improving build configurations for various platforms (especially Windows and macOS), handling dynamic input shapes during ONNX export, and clarifying API semantics (e.g., tensor in containment checks, naming conventions for tensor views). Several discussions propose modifications to internal mechanisms (like caching allocators and scheduler behavior), corrections to documentation, and enhancements for testing and reproducibility. Unresolved questions include the correct approach to quantization parameter representations, best practices for build configurations versus using provided binaries, and ensuring consistent behavior across different hardware and software environments."
2019-08-15,pytorch/pytorch,"The discussions encompass several key technical concerns: the need for better understanding and management of build system intricacies (especially regarding CUDA, MKL, and external dependencies), and the complexities of ensuring reproducibility and performance consistency across environments. There are questions around optimizing kernel compilation times, especially with new instruction sets like AVX512, and handling the gradual deprecation and replacement of components such as `BUILD_ATEN_ONLY` or the proper support for different module serialization and parameter sharing patterns. Additionally, issues regarding API clarity, documentation accuracy, and versioning—particularly for particular PyTorch features, the stability of binary releases, and compatibility with external tools like ONNX and CMake—are highlighted. Several discussions also point to unresolved bugs, performance regressions, or build failures, indicating ongoing debugging efforts with potential fixes in progress."
2019-08-16,pytorch/pytorch,"The discussions highlight ongoing development challenges and feature requests in PyTorch, including the absence of GPU-supported unstructured 2D/ND interpolation functions, with suggestions to leverage SciPy or triangulation techniques, and an interest in implementing total FLOPS and shape output visualization for models. There are various build issues, particularly related to environment configurations, such as compatibility with MKL-DNN versions, CUDA architecture detection, Windows segmentation faults, and cross-platform compilation difficulties, especially with mingw-w64 and MSVC. Several patches and PRs address performance bottlenecks (e.g., autograd cycle fixes, TensorIterator improvements, sampling efficiency) and usability enhancements (e.g., better serialization, clearer error messages, documentation for record_stream). Unresolved questions include the best strategy for shared build scripts via bot re-rendering, fixing build breaking changes in different environments, and ensuring backward compatibility for module type checks and porting lower-level kernels."
2019-08-17,pytorch/pytorch,"The discussions highlight several technical concerns, including compatibility issues with CUDA and GPU architectures, particularly the invalid device function error stemming from build or runtime mismatches, and the need for proper OpenMP compilation flags. There are questions about handling numerical instability in specific loss functions, like `MultiLabelSoftMarginLoss`, and suggestions to move reductions to TensorIterator for performance improvements. Concerns are also raised regarding the build process, such as the implications of build isolation in pip, linking to MKL and OpenMP libraries, and potential fixes like disabling MKL-DNN build if MKL isn't detected. Additionally, questions about error diagnostics, such as improving exception messages and traceback propagation, point to ongoing efforts to enhance debuggability and build robustness."
2019-08-18,pytorch/pytorch,"The discussions highlight a variety of technical issues and enhancements related to PyTorch. Notable concerns include improving the efficiency and clarity of custom RNN modules like LayerNormLSTM, addressing build and compatibility errors such as multiple inclusion of `libiomp5.so` during compilation, and fixing critical bugs like tensor overflow in CUDA kernels and handling of special values like NaN and Inf in grid sampling, which can cause memory corruption or crashes. Other topics involve usage patterns, such as the necessity to call `torch.cuda.current_device()` to avoid CUDA errors, and concerns about code maintainability, including proper rebase workflows and clarifying argument documentation. Several unresolved questions focus on the nuanced behavior of functions with default parameters, handling of specific hardware or library configurations, and improving support for fused operations, reversible in-place ops, or adapting to different device types and data layouts. Overall, the discussions aim to enhance stability, correctness, and performance while clarifying usage and development practices."
2019-08-19,pytorch/pytorch,"The discussions highlight several technical concerns including the need for optimizing DataLoader performance by tuning `num_workers` and thread affinity, especially when using libraries that spawn multiple threads like MKL and OpenMP, and managing GPU memory contexts effectively. Several issues involve build system challenges, such as handling multi-threaded compilation errors, compiler bugs, and dependency conflicts, particularly on Windows and with CUDA versions; solutions include patching kernel parameters, adjusting build flags, and reordering dependencies. There is also a focus on improving support for exporting and scripting models, especially handling dynamic shapes, non-contiguous tensors, and robustness in backward hooks, as well as upgrading support for newer features like Bfloat16 on CPUs and ensuring compatibility with ONNX opsets. Unresolved questions revolve around better build orchestration across repositories, managing multiple versions of dependencies like MKL, and improving error reporting and debugging for complex artifacts such as multi-threaded build failures or GPU context issues. Overall, many suggestions aim at enhancing stability, performance, and developer experience in building, running, and deploying PyTorch models across diverse environments."
2019-08-20,pytorch/pytorch,"The discussions encompass several technical topics: optimizing GPU memory management and data loading performance, with suggestions such as per-worker CUDA contexts and adjusting `num_workers`; addressing build and compatibility issues across different operating systems and compilers, including Windows mingw-w64 and PyTorch's interaction with VS versions; enhancing PyTorch's support for dynamic shapes, custom data structures, and exporting models to ONNX, particularly for operations like `roiAlign`; and performance and correctness concerns related to specific operator implementations (e.g., `cumprod`, `cummax`, logical tensor operations), with emphasis on litmus testing, correctness validation, and code refactoring—sometimes avoiding macros—to improve maintainability. Unresolved questions remain about ensuring backward compatibility, improving build stability (e.g., compilation times for large CUDA kernels), and clarifying design choices for features like layer normalization order in the Transformer models. Overall, the discussions highlight ongoing efforts to enhance robustness, performance, and usability across diverse hardware and software environments, as well as expanding operator and model support."
2019-08-21,pytorch/pytorch,"The discussions center around improving robustness and usability in PyTorch, with key concerns including making optimizers map parameter names rather than relying on parameter order, and introducing a `nn.View` module to simplify model modifications involving reshaping, especially for model export and abstraction. There are questions about supporting static and dynamic tensor operations, such as in ONNX exports and the handling of optional or nested outputs, as well as issues related to multi-threaded data loading performance, CUDA context management in multiprocessing, and compatibility across different hardware and software environments. Several discussions also suggest improvements in documentation consistency, testing coverage, and build configurations to prevent regressions. Some unresolved questions involve the design of backends and device support, handling of in-place operations, and ensuring backward compatibility amid ongoing feature enhancements."
2019-08-22,pytorch/pytorch,"The discussions highlight various technical issues in PyTorch development, such as the lack of in-place hooks and support for in-place modifications of views (#598), and the support for deformable convolutions (#2260) and their integration. There are concerns about certain features not being supported on CUDA (e.g., under-determined least squares #3205), and the need for improved error messages and debugging tools (e.g., NaN detection #8069, shared memory leaks #5040). Several issues relate to compatibility and build system challenges, including problems with CUDA, compiler versions, and environment setups across different platforms (e.g., #9204, #9310, #24937). Additionally, there are ongoing discussions about improving testing infrastructure (#24851, #24853, #24919), handling NaNs and special floating-point values (#25016), and ensuring correctness and stability of extensions like quantization and ONNX export (#23852, #24968). Overall, the issues emphasize both feature enhancements and addressing stability, compatibility, and usability concerns within PyTorch's evolving ecosystem."
2019-08-23,pytorch/pytorch,"The discussions primarily revolve around optimization and correctness issues in PyTorch, such as the behavior of BatchNorm during evaluation and training, with recommendations to set `momentum=0` or `model.eval()`. Several issues address memory management concerns, especially GPU out-of-memory errors, CPU thread utilization, and potential memory leaks in data loading, with suggestions like limiting `set_num_threads` or implementing background data transfer. There are questions about the proper integration of ONNX operations, the design of testing frameworks, and the handling of specific operators (like `flatten` and `avg_pool2d`) for export compatibility and performance. Concerns about correctness and stability in CUDA operations, including reproducibility, deterministic behavior, and device synchronization issues, are noted, alongside discussions on code safety, including clang-tidy checks and robust error handling. Overall, unresolved questions focus on improving efficiency, debugging complex runtime errors, and ensuring API consistency and clarity."
2019-08-24,pytorch/pytorch,"The discussions highlight several technical challenges in PyTorch, including potential CPU-related issues linked to BIOS updates and hardware configurations, and the need for deeper investigation into PyTorch stress behaviors. There is concern about incomplete or missing namespace support, such as for `torch.from_numpy`, and the necessity of fixing build and dependency issues, especially on Windows and ARM platforms. Several issues involve improving the functionality of core components like `grid_sample`, particularly for downsampling, by implementing wider receptive fields and filtering methods to better mimic low-pass filtering and prevent artifacts during resizing. Additionally, there are reproducibility and stability concerns in multiprocessing data loaders, with reports of queue corruption and potential data races, alongside ongoing efforts to improve test coverage and platform support."
2019-08-25,pytorch/pytorch,"The discussions highlight several technical challenges and feature requests in PyTorch, including issues with DataParallel handling tuple outputs, the need for improved saving/loading strategies across devices, and the desire for advanced features like sparse PCA support and 3D PixelShuffle. Queries about model export to ONNX, particularly handling dynamic axes and operator support (e.g., roiAlign, unbind), are prevalent, alongside concerns over ensuring compatibility and correctness across different environments and hardware configurations. There is interest in developing more efficient parallelism techniques (e.g., batch Jacobian computation, tensor network support), though these are recognized as complex, requiring significant architectural work. Additionally, several system-level issues are discussed, such as build configurations, Windows environment compatibility, and multiprocessing/tensor deallocation bugs, often seeking better diagnostics and more robust implementations. Overall, the threads reflect ongoing efforts to enhance PyTorch's flexibility, performance, and deployment capabilities amidst ongoing development challenges."
2019-08-26,pytorch/pytorch,"The discussions highlight a need for better documentation and support for advanced functionalities such as multi-dimensional PixelShuffle, tensor support in torch.jit, and improvements in handling sparse matrices for PCA. Several reports indicate performance regressions in PyTorch versions 1.2.0 and later, especially regarding slower execution times and memory leaks compared to earlier versions, with possible causes linked to inefficient GPU operations and the handling of `detach()` references. Compatibility issues related to the C++ ABI, CUDA versions, and library dependencies (like MKLDNN and ONNX) are also recurring, emphasizing the necessity for clearer guidance and consistent build environments. Additional concerns include ensuring proper implementation of `grid_sample` for downsampling, and enhancing error and edge case handling in modules such as `CrossEntropyLoss` with `ignore_index`. Unresolved questions involve support for specific operators, robustness of model serialization/loading, and performance optimization for specialized use cases."
2019-08-27,pytorch/pytorch,"The discussions highlight several core technical issues: the absence of a PyTorch-provided function to directly compute total parameter count, and the proposal to add control over axes in Fourier transforms (`fft`, `ifft`) for more flexible signal processing; the need for optimized `fftshift`/`ifftshift` implementations to minimize data copying; and challenges with memory leaks and GPU memory management during repeated operations, especially on newer PyTorch versions. Additional concerns include compatibility issues related to ABI and C++ extensions, limited support for 5D tensors in certain functions, and inconsistencies or bugs in functionalities like `torch.choice`, `multinomial` sampling, and exporting operators such as `roi_align`. Suggestions comprise improving test coverage for MKLDNN, clarifying documentation and usage of complex features (e.g., `cdist`), and expanding tutorials or videos to enhance adoption. Unresolved questions focus on integrating user-friendly APIs for sampling, ensuring backward compatibility, and resolving specific implementation bugs or performance regressions in edge cases."
2019-08-28,pytorch/pytorch,"The discussions highlight several technical concerns including the inefficiency of summing model parameters, with suggestions to use `nn.ModuleList()` for parameter handling. There are questions about the implementation and availability of negative sampling in PyTorch, and issues with specific functionalities like `torch.cdist` performance discrepancies, especially on GPU, which are being addressed with optimized implementations. Compatibility and support for features such as deformable convolutions, nested tensors, and complex data types (e.g., `torch.complex64`) are debated, along with challenges in exporting models to ONNX, including unsupported operations like `max_unpool2d`. Concerns about CUDA memory leaks, build failures, and the proper handling of auto-grad hooks and serialization are also prominent, with ongoing efforts to improve robustness, documentation, and auxiliary tools like tensorboard visualization. Unresolved questions revolve around integrating advanced schedulers, supporting higher-dimensional data (e.g., 5D tensors), and ensuring API consistency across platforms and versions."
2019-08-29,pytorch/pytorch,"The comments reveal ongoing efforts and questions surrounding PyTorch's development, including the introduction of out-of-tree device/layout/dtype extensions, support for complex numbers on CPU and CUDA, and the management of operator registration and schema naming for dispatching. Several issues pertain to build compatibility problems, such as CUDA version mismatches, MKL linkage, and build system configurations, especially on Windows and macOS. Debugging and test failures are commonly discussed, often linked to specific PRs or changes like in autograd, ONNX export, and JIT scripting, with suggestions for improvements like better testing, code refactoring, and documentation clarity. Some comments address performance optimizations, API redesigns, and versioning/deprecation strategies, indicating active iteration and feedback cycles within the PyTorch community. Overall, unresolved technical challenges include ensuring compatibility across platforms, maintaining correctness in complex features (e.g., double backward, quantization), and managing build and runtime stability."
2019-08-30,pytorch/pytorch,"The discussions highlight several key issues: numerous bugs and inconsistencies in CUDA/OOM and Tensorboard graph visualization; ongoing challenges with PyTorch's interoperability with ONNX, particularly around control flow (loops) and data types; concerns about version compatibility (e.g., CUDA 9.0 vs 10.x, MKL support in wheels) and build configurations (e.g., compiler flags, linking issues); references to internal refactoring and API stabilization, such as reorganization of distributed and autograd code, and support for sparse/structured tensors; and general development logistics, including CI setup, test robustness, and the need for clearer documentation and error handling to improve debugging and user experience."
2019-08-31,pytorch/pytorch,"The discussions highlight a range of technical concerns including reproducibility and stability issues with PyTorch's autograd and model serialization, notably reference cycles and the need to avoid saving entire tensors in backward passes, especially for double backward. There are questions about the proper and efficient ways to detach tensors for reference cycle prevention, and handling of numerical stability in functions like softmax with masking, specifically with `-inf` values. Several issues pertain to build and environment management, such as linking paths in libtorch, CUDA compatibility, and platform-specific bugs on Windows, along with discussions on improving error handling, warnings, and debugging experience. Additionally, there are ongoing efforts to optimize and clarify the interface and internal mechanisms of components like `PackedSequence`, `spectral_norm`, and ONNX export, with some unresolved questions about error diagnosis, code robustness, and maintaining compatibility across different environments and PyTorch versions."
2019-09-01,pytorch/pytorch,"The discussions raise several technical concerns, including the need for accurate documentation updates, such as aligning `atan2` docstrings and declarations with implementation, and the potential addition of deformable convolution support to PyTorch. There are questions about accessing module information for parameters, error handling and error reporting in NCCL and CUDA operations, and performance issues related to multi-GPU and multi-process utilization, especially on Windows and specific hardware like Jetson TX2. Additionally, there is interest in refining code structure and dispatch macros for kernel implementations, ensuring proper linking with MKL on macOS, and improving debugging and static analysis tools like `clang-tidy`. Unresolved questions include error reporting behavior during NCCL failures, GPU resource management for concurrent kernels, and whether new API features like inverse functions or in-place operations are justified."
2019-09-02,pytorch/pytorch,"The discussions highlight a variety of technical concerns including the absence of deformable convolution in PyTorch's core library, leading to reliance on third-party implementations; compatibility issues and installation bugs across different operating systems, especially macOS and Windows; challenges in exporting models to ONNX, particularly with dynamically sized inputs and list constructs; and intermittent failures or flakiness in GPU-based operations such as embedding and SVD, possibly related to driver, hardware, or memory management issues. There are also discussions on code organization, including proper namespace placements and structure for distributed and autograd components, as well as dependencies management, especially with CUDA and related libraries like nvrtc. Unresolved questions include plans for native support of advanced operators like deformable convolutions, handling of dynamic input sizes in model export, and ensuring stable, reproducible performance across environments. Overall, the issues indicate a need for improved documentation, robust cross-platform support, and clearer guidance on model export, dependency management, and distributed training configurations."
2019-09-03,pytorch/pytorch,"The discussions reveal concerns about environment-specific DLL and DLL load failures on Windows due to mismatched CUDA versions, missing dependencies like `requests`, and build inconsistencies related to protobuf and protobuf-generated files. Several issues highlight performance regressions, such as slower inference in JIT-compiled models and decreased throughput after tracing, prompting suggestions for benchmarking and performance testing. Compatibility and correctness issues are raised with CUDA kernel launches causing device-side asserts, often linked to index out-of-bounds errors, and with operations involving tensor lists and indexing that require careful bounds checking. Additionally, there are questions about proper device management APIs in C++ (e.g., device guards), and general requests for better documentation on features like deformable convolutions or tensor serialization, emphasizing the need for clearer support, reproducibility, and environment robustness."
2019-09-04,pytorch/pytorch,"The comments reflect ongoing development challenges and feature requests in the PyTorch project, including integration of deformable convolutions, support for batched operations like pinverse, and improvements in distributed training (e.g., bucketing for DDP). There are technical issues with CUDA and ROCm compatibility, build system modifications, and environment dependencies that affect reproducibility and performance (e.g., MKL support, CUDA version detection, and kernel compilation). Several discussions also involve API consistency and usability enhancements, such as proper handling of `step()` in LR schedulers, improved error messages for CUDA-related issues, and the introduction of experimental features like NestedTensor and freezing graphs. Unresolved questions include maintaining long-term support for forked libraries like QNNPACK, proper testing for flaky components, and environment configuration for building or executing PyTorch across different platforms. Overall, the comments highlight active maintenance, feature evolution, and troubleshooting, with an emphasis on compatibility, performance, and usability improvements."
2019-09-05,pytorch/pytorch,"The discussions highlight several key technical concerns including memory management issues on CUDA, with users experiencing OOM errors even after reducing batch sizes, indicating potential PTX JIT compilation delays or threading-related GPU memory leaks. There's ongoing work to improve reproducibility and deterministic behavior across different hardware, especially concerning RNG outputs and operator performance consistency. Several patches involve refining distributed training, autograd, and RPC mechanisms, with particular attention to process launching methods and process reuse to optimize runtime and stability. Additionally, there are concerns about build configurations, static linking on Windows, CUDA architecture detection, and binary compatibility, especially regarding linking paths, compiler support, and environment consistency. Many unresolved questions relate to automating environment setup (e.g., clang-format versions), handling complex multi-threaded memory issues, and ensuring cross-platform reproducibility and consistency in distributed and inference workflows."
2019-09-06,pytorch/pytorch,"The discussion highlights several recurring issues and technical concerns within the PyTorch community. Key points include the importance of ensuring model output and label dimensions match (Issue #1204), complexities around CUDA and multi-GPU initialization (Issues #8115, #11348), and discrepancies or bugs in specific functionalities such as `cdist` performance and accuracy (Issue #25662) or tensor serialization and indexing (Issues #25483, #25777). Several comments suggest improving testing infrastructure, build reproducibility (Issues #25656, #25689), and user API usability, particularly for quantization support and custom kernel extensions. There is ongoing debate about proper handling of threading, multi-processing, and GPU caching to improve performance and reliability. Unresolved questions include the integration of batch processing optimizations, CUDA kernel consistency, and better support for mixed device operations, alongside hardware and compiler-related bugs requiring further investigation."
2019-09-07,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch with new features such as pairwise ranking loss, improved distribution management, and CUDA kernel fusion, alongside routine maintenance like rebasing PRs and merging contributions. Several technical challenges are raised, including implementing support for ""in"" operator for all types, resolving build and JIT compilation issues for optimized CUDA/PTX usage, and addressing performance discrepancies in inference speed—particularly first-run versus subsequent runs—possibly due to JIT compilation and data transfer bottlenecks. The development of Windows-specific build improvements and the integration of advanced communication routines like MPI for tensor exchange are also discussed. Unresolved questions remain around optimizing inference initialization, ensuring correct ONNX export with control flow, and streamlining build processes across platforms, indicating ongoing work to improve stability, usability, and performance in PyTorch."
2019-09-08,pytorch/pytorch,"The discussions primarily revolve around implementing and improving loss functions such as Dice loss for multi-class segmentation, highlighting issues with gradient requirements and how to properly combine losses like CrossEntropy and Dice. Several threads address PyTorch's internal behaviors, such as module tracking of running statistics, tensor hooks for capturing intermediate outputs, and user interface enhancements like module scripting and distribution parameter access. Concerns about bug fixes, long-term maintenance of external components like QNNPACK, and data loading optimizations are also raised. Users seek clearer documentation, better handling of edge cases such as tensor dimensionality issues, and performance benchmarks for recent changes. Unresolved questions include managing tensor memory leaks, ensuring compatibility with different tensor data types and device configurations, and clarifying the distinct roles of related libraries or modules like QNNPACK."
2019-09-09,pytorch/pytorch,"The discussions highlight various technical challenges and proposals within PyTorch development, including improving optimizer parameter mapping by using parameter names instead of ordering, and handling dynamic parameter changes. There are notable concerns about system stability and reproducibility, especially related to system reboots during training, CUDA memory management, and NCCL topology issues, with suggestions to analyze hardware configurations and environment variables. Several issues revolve around the PyTorch build process, such as supporting MKL detection, reducing binary size, and aligning API consistency (e.g., introducing `new_zeros` in C++ API). There are also questions about improving onnx export, code hygiene (clang-tidy), and serialization without dependency on storage objects, alongside unresolved build errors and test flakiness that complicate ongoing development. Overall, these discussions focus on robustness, system compatibility, API consistency, and performance optimization within the PyTorch ecosystem."
2019-09-10,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch repository, including issues with optimizer-parameter mapping and parameter state association, particularly the challenges around tying optimizers to parameter names versus order, and the potential impact of model modifications on optimizer states. There are ongoing efforts to improve distributed training stability, such as refining NCCL topology handling, process group setup, and addressing potential timeouts, especially in non-adjacent GPU configurations. Multiple issues address build and environment configuration challenges, notably with MKLDNN, CUDA, and CUDA version compatibility, often complicated by environment setup errors with sudo or stale build caches. The complexity of internal serialization mechanisms for different device types (CPU, CUDA, XLA) and their handling of tensor views, storage, and sharing reflects a need for more robust, device-agnostic serialization strategies. Finally, several discussions emphasize the importance of updating, reviewing, and testing new API features and code modifications, including improvements to autograd hooks, tensor types, and control over JIT compilation and graph visualization, often with unresolved questions about backward compatibility and cross-platform consistency."
2019-09-11,pytorch/pytorch,"The discussions highlight concerns over numerical stability and error handling in tensor operations, particularly division by zero for integer tensors and potential improvements in error messages for ill-conditioned matrices during SVD computations. There are ongoing efforts to enhance testing infrastructure, including device-agnostic and per-backend test frameworks, as well as ensuring compatibility across different hardware architectures and software versions. Multiple issues relate to performance regressions, such as in einsum and NCCL peer-to-peer communication, with benchmarking and profiling used to verify stability and efficiency. Several discussions revolve around API design and usability, including support for user-defined module scripting, support for specific functions like `max()`, `range()`, and integrating custom kernel plugins. Lastly, there are concerns about build processes, environment setup, and compatibility, especially for legacy platforms and external dependencies like ONNX and FBGEMM, with proposals for code refactoring and infrastructure improvements to address these challenges."
2019-09-12,pytorch/pytorch,"The discussions highlight several key technical issues, including the challenges of ensuring thread safety in cuBLAS handle pooling, with an emphasis on testing race conditions and handle management strategies. CUDA kernel and stream-related bugs are another concern, prompting suggestions for improved lint rules to prevent incorrect stream usage and address kernel crashes and illegal memory accesses. Environment compatibility and dependency version mismatches, particularly with torchvision and PyTorch versions on different CUDA setups, are discussed, alongside environment configuration issues like SSL certificate and CUDA/PTX architecture detection problems. Some conversations explore the performance impacts of TorchScript and the potential benefits of operator fusion, as well as the need for careful code review around schema matching and type promotion rules. Unresolved questions remain about optimizing NCCL communication in multi-GPU settings, handling mismatched device configurations, and improving CI processes for reproducibility."
2019-09-13,pytorch/pytorch,"The discussions highlight inconsistencies and API design considerations in PyTorch, such as the disparity between Linear and ConvNd layer input types and potential API changes to unify their interfaces, with caution about backward compatibility. Memory management issues, including shared memory leaks during training and changes in NCCL and FBGEMM behavior, are also prominent, alongside concerns about memory growth over training epochs. Several discussions focus on improving the usability and transparency of learning rate schedulers, details of JIT and TorchScript support in C++, and the support of multiprocess environments, including CUDA device handling and environment variable management. Compatibility, build issues, and tooling (e.g., clang-tidy and Dockerfile arrangements) are addressed, alongside efforts to streamline internal data structures, operator registration, and documentation visibility (robots.txt and search indexing). Unresolved questions include the support for exporting C++ models, the handling of certain APIs (e.g., `pin_memory`, `inplace` operations), and ensuring stability and correctness of backend-dependent functionalities across different hardware and software configurations."
2019-09-14,pytorch/pytorch,"The discussions highlight technical concerns about ensuring correct and efficient linking of MKL libraries on macOS, with suggestions to patch build scripts for static linking; the need to implement certain operations like `cummax` natively in `ATen` versus legacy libraries; and the complexities of memory management, especially related to reference cycles and proper object deletion in Python. There are questions about the behavior of CUDNN algorithms on different hardware, potential memory leakage issues, and the challenges of maintaining consistent test environments and handling flaky tests. Additionally, discussions focus on improving module loading mechanisms and managing dataset transformations correctly to conform with expected input ranges. Many unresolved questions revolve around build system adjustments for platform-specific dependencies, enhancing code portability, and ensuring proper handling of Python reference semantics to avoid memory leaks."
2019-09-15,pytorch/pytorch,"The discussions highlight multiple issues with PyTorch, including compatibility and loading problems on macOS related to `libomp.dylib`, and runtime errors with `BCELoss` when input tensors are outside the [0, 1] range, often resolved by proper input normalization or version downgrades. Compatibility and build concerns are prevalent, especially regarding compiler and environment setups—such as VCS version mismatches, CUDA version dependencies, NCCL configuration for multi-node training, and ensuring reproducibility in distributed training. Several issues focus on internal implementation details, like fixing bugs in functions such as `nuclear_norm`, handling operator schemas with the C10 dispatcher, and sharing models across threads without deadlocks. The community suggests incremental fixes, code refactoring, and potential deeper changes like native implementation of certain functions or schema handling improvements, with unresolved questions about the best approaches for these improvements. Overall, the conversations emphasize stability, compatibility, and correctness in both low-level backend implementations and high-level training workflows."
2019-09-16,pytorch/pytorch,"The comments reflect a wide range of technical discussions, including enhancements and fixes for sparse optimizers (e.g., SparseRMSprop), environment setup issues (e.g., PyTorch installation in conda environments, Jupyter Notebook configuration), and platform-specific build concerns (e.g., MKL vs Accelerate on macOS, ABI compatibility issues, linker errors). Several issues involve bug fixes and feature implementations, such as supporting `lstsq` for rank-deficient problems, improving ONNX export semantics, and handling serialization of tensors with views in XLA. There are also questions about performance optimizations, API consistency, and testing strategies (e.g., clang-tidy integration, adding regressions tests). Unresolved questions include platform-dependent build configurations, the best way to support multi-operator registration, and the appropriate design for module unloading in Python."
2019-09-17,pytorch/pytorch,"The discussions highlight concerns about handling division by zero checks in PyTorch, suggesting conditional runtime checks for integer division and the potential impact on performance and consistency across CPU and GPU, pending review of device-side asserts. Multiple issues pertain to build and compilation processes, emphasizing the need to fix build errors, manage dependencies, and improve tools such as clang-tidy and cmake integration, especially relating to compiler compatibility and internal error reporting. There are recurring questions about serialization, especially handling Unicode in torch.load, module exportability from C++, and the difficulties of expanding support for scripting models directly in C++ due to complexity and resources. The threading and deadlock issues in distributed training and sharing models, as well as the memory leaks and performance regressions in various modules, raise concerns about stability, especially on specialized hardware and with particular optimizations like MKLDNN. Lastly, there are discussions about test infrastructure, device-agnostic testing approaches, and the design of APIs for better usability and consistency across different backends and layering, such as in JIT compilation, ONNX export, and operator support."
2019-09-18,pytorch/pytorch,"The comments reflect ongoing discussions about improving error handling for division by zero across CPU and GPU, with proposals to add runtime checks and assertions. There are conversations about adding support for named tensor dimensions, quantization workflows, and better serialization with `torch::save`/`torch::load`, including fixing pickling behaviors and developing infrastructure for IValue serialization. Multiple issues concern build and compatibility challenges, especially on Windows and macOS, with suggestions to use tools like `delocate` or static linking to manage dependencies like MKL. Review and refinement are needed for several pull requests, especially around ONNX export, tensor metadata handling, and kernel fusion; some PRs are blocked or need rebase due to ongoing conflicts. Also noted are concerns over performance testing methodologies (debug vs release builds) and the integration of new features within the existing PyTorch and JIT frameworks, emphasizing the importance of proper testing, documentation, and compatibility considerations."
2019-09-19,pytorch/pytorch,"The discussions highlight several key technical concerns including the need for batching gradients support in PyTorch functions like `cholesky_inverse`, and the challenges of ensuring proper CUDA and environment configurations across different hardware and software setups. There are recurring issues related to memory management, especially CUDA out-of-memory errors, and build failures stemming from incompatible compiler versions, missing dependencies, or package conflicts—particularly on Windows and macOS platforms. Additionally, questions are raised about the potential introduction of a trainer API within core PyTorch, with considerations about its necessity, implementation complexity, and fragmentation of existing training abstractions. Finally, there are technical debates over performance optimizations, such as the use of Sleef vs. compiler builtins and ensuring correctness in numerical functions, as well as practical concerns like reproducibility and testing in diverse environments."
2019-09-20,pytorch/pytorch,"The discussions highlight several technical concerns including issues with the build environment and dependency management, such as compatibility with older glibc, missing headers (e.g., sleef.h), and the need to rebuild from source or adjust compiler flags. There are recurring questions about the correctness and safety of in-place tensor modifications, specifically the difference between `.data` and `.detach()`, along with how these impact autograd and memory management. Concerns also involve API consistency and usability, particularly around device management (e.g., default device settings, channels last formats, and support for new operations like `grid_sample` enhancements). Additional questions focus on performance implications of recent changes, correctness in compiled kernels, and the importance of benchmarks to validate improvements. Unresolved topics include the impact of environment-specific variations on reproducibility and the best practices for integrating new features without breaking existing workflows."
2019-09-21,pytorch/pytorch,"The discussions mainly revolve around improving production deployment methods for PyTorch models, including converting models to ONNX for inference, and supporting model serving frameworks like Kubeflow and BentoML. Several issues highlight performance optimization and hardware compatibility concerns, such as adapting MKL-DNN and AVX usage, as well as debugging specific bugs across different PyTorch versions and environments. There are technical debates on extending function support with better error handling, signature handling, and in-place operation compatibility in autograd, especially in relation to views and checkpointing. Additionally, some conversations focus on infrastructure issues, such as CI failures, code refactoring, and ensuring test coverage for new features or bug fixes. Unresolved questions include whether to rely on existing features like `torch.jit.unused`, and how to best integrate or replace legacy code and dependencies."
2019-09-22,pytorch/pytorch,"The discussions highlight several key issues: the absence of a batched `pinverse` implementation and the desire for a simple `torch.trace` function for improved readability; challenges with CUDA support, GPU-related errors, and profiling (e.g., missing `libcaffe2_detectron_ops_gpu.so` and `libcaffe2_nvrtc.so`), especially in environments with hybrid graphics or specific container versions; the need for enhanced TensorBoard integration, including better logging of hyperparameters and metrics; ongoing work and merge status on functions like `logcumsumexp`, `cummax`, and support for backward and GPU capabilities in new ops; and compatibility concerns with older PyTorch versions and torchvision's unreleased features, alongside development plans for NHWC layout support and scriptability issues."
2019-09-23,pytorch/pytorch,"The discussions revolve around improving CUDA error handling, particularly replacing device-side asserts with asynchronous error reporting mechanisms to enhance robustness and performance. Several proposals suggest associating error status with streams or tensors (NaT concept), enabling error checks post-kernel execution, and modifying error propagation to handle asynchronous failures more reliably, though concerns exist about complexity and potential for confusing behaviors. There is also ongoing work to refactor and optimize the internal implementation, including handling specific operator behaviors, promoting consistent code and API design, and addressing performance regressions, especially related to reduction operations and type promotion. Other topics include ensuring build environment compatibility, addressing multi-threading and resource leaks, enhancing test coverage, and managing version compatibility for serialization. Unresolved questions focus on the best design trade-offs for error reporting strategies, the impact on existing workflows, and clarifying the scope of modifications needed to support these enhancements effectively."
2019-09-24,pytorch/pytorch,"The discussions highlight challenges in implementing autograd hooks, especially with in-place operations and complex graphs, emphasizing the need for better tracking of intermediate states and in-place modifications. Several issues relate to PyTorch’s serialization, versioning, and compatibility, such as handling different serialization protocols, backward compatibility, and ensuring stability across versions. Performance concerns are raised regarding specific operators on CPU and GPU, notably in relation to inlining, kernel improvements, and backend optimizations, with suggestions for clearer benchmarks and better code structure. Numerous PR reviews focus on correct rebasing, conflict resolution, and clarifying the impact of code changes, alongside discussions on improving API consistency and robustness in distributed training, quantization, and model loading. Unresolved questions include precise error causes in certain distributed setups, the effects of recent code alterations on backward compatibility, and how to effectively extend or optimize existing operator implementations and serialization methods."
2019-09-25,pytorch/pytorch,"The discussions primarily revolve around enhancing error handling and robustness in PyTorch, particularly for CUDA operations—such as introducing asynchronous CUDA error checks, proper handling of internal kernel failures, and porting existing checks to more reliable points in the code. Several issues touch on API improvements, like unifying tensor representations, supporting model serialization/deserialization across devices, and API consistency for functions (e.g., `max_norm` in embeddings). There's also ongoing work on feature additions, such as supporting nested tensors, new operators, and larger models, along with infrastructure updates like build configuration, compatibility with different CUDA versions, and build system improvements. Some concerns include ensuring backward compatibility, fixing build failures, and optimizing performance, especially on diverse hardware configurations. Unresolved questions include how to best implement error recovery for asynchronous CUDA errors, API design choices for C++ frontend consistency, and managing distributed or multi-device synchronization reliably."
2019-09-26,pytorch/pytorch,"The comments highlight ongoing challenges with PyTorch's build, installation, and platform compatibility, especially on Windows and older Python versions, with suggested solutions including using conda, downloading prebuilt wheels, or fixing environment issues. Several discussions focus on the performance and correctness of numerical operations, such as CUDA kernels, quantization, and threading, with proposed fixes involving code refactoring, engine selection, and careful handling of data types and memory formats. There are concerns about CI and test flakiness, seeking more robust testing practices, and questions regarding code structure, such as overload resolution, tensor semantics, and code organization between ATen and c10. Additionally, some issues involve deep internals like distributed training, Autograd, IR stability, and backend registration, often requiring thoughtful API design or architectural adjustments. Overall, unresolved questions include ensuring platform and compiler compatibility, performance optimizations, and maintaining API and codebase stability amid ongoing development."
2019-09-27,pytorch/pytorch,"The discussions highlight several key issues including: the need for clearer behavior and implementation details for features such as mixed-precision quantization, support for `torch.Size` conversions, and the handling of dynamic padding in convolutional layers, with suggestions for more explicit API design and better documentation. There are concerns about compatibility and stability challenges, such as ABI issues on certain systems, problematic interactions with external libraries like MAGMA, and the importance of rigorous testing, especially when introducing new features like `__torch_function__`. Several technical proposals involve restructuring code for better modularity, ensuring thread safety (especially in OpenMP contexts), and refining API ergonomics (e.g., enum handling, API parameterization). Many discussions also emphasize the importance of thorough testing to prevent regressions, particularly across diverse hardware and compiler environments, and strategic planning for feature rollout, including staging, deprecation, and cross-library support considerations."
2019-09-28,pytorch/pytorch,"The comments reflect several key concerns: the challenge of enabling module inspection and auto-completion with dynamic members, particularly in IDEs like VSCode, suggesting possible approaches like explicit imports instead of wildcard imports; the difficulty of optimizing cuDNN support for depthwise convolutions on different hardware and verifying latest support status; the complexity of serializing views and their impact on model saving/loading, questioning whether View serialization can be reliably managed; questions about implementing better kernel assertions and error reporting for CUDA kernels to improve debugging without forcing synchronization; and discussions about API ergonomics for enums, advocating for using strongly-typed enums or variants to improve usability and safety, along with considerations on the stability and maintainability of such approaches. Many unresolved questions remain around performance improvements, serialization robustness, CUDA error handling, and API design choices."
2019-09-29,pytorch/pytorch,"The discussions highlight recurring issues with PyTorch, including environment-specific module import errors, particularly when using Jupyter notebooks or different Python versions, and the importance of correctly configuring environments via Anaconda Navigator. There are ongoing concerns about CUDA and GPU memory management, including out-of-memory errors and the need for better control of batch sizes and prefetching strategies, with some solutions involving code modifications or hardware considerations. Several questions revolve around mathematical function implementations, such as the GELU activation and its default module availability, as well as code optimization for convolution matrices and handling system-specific build limitations, especially on macOS and Windows. Community members seek guidance on troubleshooting and testing methodologies, including cross-platform issues, build fixes, and ensuring code consistency and correctness across different hardware and software configurations. Unresolved questions remain about specific performance bugs, build adjustments, and potential code refactoring to improve robustness and flexibility."
2019-09-30,pytorch/pytorch,"The discussions highlight several technical concerns, including the correct handling of environment variables and build scripts to address compilation errors (Issue #2046), and the challenge of network reliability impacting large downloads (Issue #4207). There are issues with ONNX model conversions, especially related to unsupported operations like Upsample and NonMaxSuppression (Issue #19088) and a specific error during version conversion, suggesting a need for better handling or documentation. Data serialization and serialization offset management present stability problems, as seen in test failures (Issue #27048), and there are ongoing efforts to improve profiling, in-place operations, and operator support with concerns about compatibility, performance, and correctness. Additionally, architectural questions about API extensibility, type annotations, and support for custom tensor types suggest an interest in making PyTorch more approachable for diverse backends and users, alongside maintenance and review of PRs to ensure stability and correctness."
2019-10-01,pytorch/pytorch,"The discussions highlight issues related to dynamic library loading and symbol consistency in PyTorch extensions, especially concerning `RTLD_GLOBAL`, ABI compatibility, and C++ symbol exports, which can cause crashes or undefined behavior when loading extensions built with different `-D_GLIBCXX_USE_CXX11_ABI` settings. There is concern over the lack of support for returning `NotImplemented` for in-place operators in the operator dispatch system, and questions about ensuring completeness of operator coverage, especially for operators defined outside `native_functions.yaml`. Several discussions address compatibility and performance considerations across hardware platforms, such as AMD CPUs and GPU architectures, including kernel size impact and BLIS vs. OpenBLAS performance. There's also ongoing work on enhancing PyTorch's type system, including subclassing `Tensor`, static shape inference via type annotations, and improving JIT parser source location reporting. Finally, some issues involve build configuration conflicts like CUDA/ROCm support, and interoperability challenges with tools like ONNX and TensorBoard."
2019-10-02,pytorch/pytorch,"The discussions highlight ongoing challenges related to compatibility and linking issues, particularly involving C++ ABI standards, shared library loading, and symbol resolution, exemplified by problems with torch._C and libstdc++ symbols when using RTLD_GLOBAL and mismatched compilation settings. Several issues address build system complexities, such as integrating new backends like BLIS for improved CPU performance, and managing large codebases with cross-platform considerations, including Android and CMake updates. There are concerns about ensuring correctness and stability, especially for autograd double backward support, native function registration, and model export/export compatibility across different PyTorch versions and ONNX standards. Some discussions focus on improving API design, including support for named tensors, custom modules, and external extensions, with attention to maintaining backward compatibility and providing comprehensive testing. Overall, unresolved questions remain around ABI compatibility, build reproducibility, performance benchmarking, and API robustness for extending and optimizing PyTorch across diverse systems and use cases."
2019-10-03,pytorch/pytorch,"The discussions encompass several key technical concerns: the need for enhanced loss function APIs to return per-instance losses for techniques like OHEM, and suggestions to improve loss computation flexibility; challenges with library linking and ABI compatibility issues, especially in extension modules involving torch._C and C++ ABI mismatches; difficulties with CUDA out-of-memory errors that persist despite reduced batch sizes, and the potential influence of string constants on GPU kernel performance; limitations in support for dynamic axes and constant handling during ONNX export, leading to runtime errors; and the broader question of extending PyTorch's tensor API (e.g., adding view_, reshape_) for better usability, alongside concerns about build system and testing configurations that can cause failures on master even when PRs pass in isolated environments. Several discussions also propose or defer implementing features like constant support, tensor type extension mechanisms, and improved documentation, highlighting ongoing efforts to balance backward compatibility, performance, and usability enhancements."
2019-10-04,pytorch/pytorch,"The comments highlight a variety of technical concerns primarily centered around correctness, performance, and compatibility. Notable issues include the handling of CPU-only installations without unintended CUDA dependencies, tensor size mismatch errors during optimizer state loading, and operations involving dictionary outputs and their support in serialization and tracing. Several discussions address backend-specific behaviors, such as NCCL version logging, ROCm build stability, and cuDNN version differences affecting tests. There are also concerns about build system limits (e.g., library size constraints), flaky CI runs due to nondeterministic behaviors, and the complexity of error handling and debugging in CUDA kernels and distributed setups. Overall, these discussions point to ongoing efforts to improve robustness, usability, and maintainability in PyTorch’s core functionalities and build infrastructure."
2019-10-05,pytorch/pytorch,"The discussions primarily center around integrating advanced features and addressing technical challenges in PyTorch, such as incorporating deformable convolutions—initially proposed to be added to torchvision and potentially moved to core PyTorch if used beyond vision tasks. There are ongoing concerns regarding CUDA out-of-memory errors, prompting suggestions like batching adjustments and kernel-level error handling improvements. Several issues highlight the need for better testing, error reporting, and API consistency, for instance, standardizing assert mechanisms with error codes and refining the IValue API to suit Java or other language bindings. Additionally, questions are raised about supporting non-scriptable observers, merging API functions, and aligning module namespaces, particularly for quantized and different dtype layers, with some suggestions for simplifying or consolidating interfaces. Unresolved technical questions include managing error propagation efficiently, ensuring compatibility across platforms, and maintaining clear, consistent APIs for deep learning components."
2019-10-06,pytorch/pytorch,"The discussions highlight issues with the implementation of a custom `LocalLinear` module, notably the underutilization of padding in the forward method, which could affect model performance. There are multiple debates on backend and hardware optimizations, such as BLIS versus OpenBLAS or MKL, with suggestions for benchmarking and features like batched GEMM or prepacking matrices to enhance efficiency. Several onnx export problems are discussed, particularly with opset compatibility and operator support (e.g., `upsample_nearest2d`, `aten::upsample_nearest2d`), emphasizing the need for better handling of dynamic shapes and supported operators. Persistent environment-related errors, such as CUDA runtime errors and mismatched package installs, suggest the necessity for clearer guidelines and environment management. Lastly, ongoing development concerns include API consistency, user-facing features like named tensors, and the integration and testing of new modules, with particular focus on proper code generation, testing, and community contributions."
2019-10-07,pytorch/pytorch,"The discussions encompass a range of technical concerns including the development timeline and integration of out-of-tree device/layout/dtype extension features, especially for complex number support on CPU and CUDA, with questions about their existing implementation status and future plans. There are ongoing issues related to CUDA kernel overload resolution and implementation correctness, notably with normalization functions and backward operations, and several discussions involve fixing compile errors, optimizing kernel behavior, or clarifying API semantics (e.g., for `batch_norm_elemt_cuda`). Questions about backward correctness for certain functions like `cummax` highlight the need to accurately compute Jacobian matrices versus component-wise derivatives, indicating unresolved issues in gradient implementation. Some comments raise concerns about package management and inter-environment consistency, especially on Windows, with questions about proper installation procedures and environment setup in IDEs like PyCharm. Additionally, there is recurring interest in features like support for unstructured interpolation, improved data handling, and better documentation, alongside structural discussions about PR management, testing strategies, and the evolution of API features (such as `TensorOptions` and `TensorAxes`) within the project."
2019-10-08,pytorch/pytorch,"The discussions reveal ongoing efforts to extend PyTorch's support for complex numbers without relying on AVX or SSE, with proposals to modify tensor storage and dtype invariants to enable low-level reinterpretations. There are concerns about the implementation and registration of custom kernels, especially for quantized operations and ONNX export, highlighting difficulties with operator signatures, namespace issues, and build compatibility across platforms like Raspberry Pi. Multiple threads address bug fixes, performance optimizations, and compatibility issues, and some suggest restructuring or splitting PRs to improve clarity and reviewability. Additionally, several inquiries focus on test robustness, error handling, and build system challenges, emphasizing the need for better documentation, tests, and reproducibility. Unresolved questions include the best approach for in-place tensor modifications, handling of dynamic shapes in operations, and the impact of recent changes on performance and compatibility."
2019-10-09,pytorch/pytorch,"The discussions reveal ongoing challenges in efficiently handling complex number tensors in PyTorch, including reinterpreting tensor memory layouts for FFT and variance operations without unnecessary data copying, with suggestions to relax storage/dtype invariants for complex types. There are issues related to DataLoader's interaction with CUDA and autograd profiling, particularly when using fork-based multiprocessing, which may cause runtime errors; solutions like switching to 'spawn' methods or proper CUDA initialization timing are considered. Several PRs attempt to improve functionality and documentation, but some, such as those involving quantization, padding schemes, and operator registration, face regression or build failures, especially on Windows or in the presence of APIs like ONNX. Some discussions focus on ensuring backward compatibility (e.g., with `torch.arange` dtype) and testing improvements, while others consider maintenance and build system complexities, such as external dependencies (MKL, miniz) and CI infrastructure. Unresolved issues include safe memory reinterpretations of complex tensors, platform-specific build errors, and verification of new protocol compliance for NumPy/CuPy interop."
2019-10-10,pytorch/pytorch,"The discussions highlight various technical concerns including inconsistent behavior and bugs in PyTorch's out= interface semantics, especially with functions like torch.index_select and torch.scatter, where out tensors are resized unexpectedly. There are questions about exposing certain internal parameters, such as kernel size and stride, from C++ layers for model conversion, and debates on whether to create separate classes or namespaces for quantized modules due to their different numerical behaviors. Several issues address compatibility and build problems across platforms like Windows, Linux, and ARM devices, often involving submodule handling, protobuf, or environment configurations, with some suggesting re-cloning repositories or updating submodules. Additionally, there's discussion on reorganizing the `torch.distributed` API, including renaming and structuring for clarity and future extensibility, alongside concerns on testing coverage, CI reliability, and documentation consistency. Unresolved questions remain regarding the appropriateness of API redesigns, handling of complex features like autograd with RPC, and resolving platform-specific build failures."
2019-10-11,pytorch/pytorch,"The discussions highlight various technical issues including the need for better documentation and testing for features like quantization, collective operations, and distributed training. Concerns about compatibility and stability are raised with kernel implementations, operator registration, and environment setup, particularly on diverse platforms such as Windows, Mac, and Android. Several threads address performance optimizations, such as CPU training acceleration with MKL-DNN, in-place tensor operations, and efficient memory use. There are also questions related to framework features, like handling duplicates in top-k operations, support for complex numbers, and improvements to the C++ API, ONNX export, and model deployment mechanisms. Unresolved issues include ensuring proper operator registration, fixing memory errors, and maintaining backward compatibility while advancing framework capabilities."
2019-10-12,pytorch/pytorch,"The discussions highlight several technical concerns, including the glibc TLS/DTV limitation that causes issues with static TLS libraries like libgomp, which can be mitigated by importing libgomp early or updating glibc, especially on older Linux distributions. There are challenges with multithreaded data loading in PyTorch when opening HDF5 files in a multi-worker setup, leading to errors such as wrong B-tree signatures and pickling issues, with workarounds involving SWMR mode or environmental adjustments. CUDA version upgrades (notably to 10.1) are gradually being adopted, with some reports of significantly increased startup times, and solutions involve reinstalling binaries. Several ongoing development efforts involve refactoring or adding features, such as the CPU training optimizations, support for new APIs like `searchsorted`, and improvements to the C++ API; reviewers suggest code improvements, testing, and documentation clarifications. Finally, there's a concern about the stability and compatibility of model serialization, serving multiple models, and quantization workflows, emphasizing the need for clearer API design, error handling, and comprehensive documentation."
2019-10-13,pytorch/pytorch,"The discussions highlight ongoing challenges in implementing complex autograd features, particularly with in-place operations and graph hook mechanisms, emphasizing their difficulty due to autograd engine limitations. Several issues revolve around compatibility and setup problems, such as CUDA version mismatches, environment configuration, and Windows-specific build errors, often requiring reinstallations or driver updates. There are ongoing efforts to extend and improve API support, including implementing new modules like Flatten and BatchNorm layers in C++, and handling data types correctly, especially with complex numbers and numpy/scalar types. Workarounds and fixes for installation issues, CI failures, and performance optimizations are frequently discussed, with some features being deprecated, merged, or marked as WIP. Overall, unresolved questions include the stability of certain APIs, compatibility of features across platforms, and ensuring robustness in new module development."
2019-10-14,pytorch/pytorch,"The discussions mainly revolve around debugging and improving PyTorch's functionality, including resolving specific errors (e.g., handling loss variable definition, CUDA memory leaks, and runtime exceptions), enhancing API consistency (like `searchsorted`, `split`, and quantization support), and addressing performance challenges (such as batched autograd, fused operations, and slow inference). Several comments suggest refactoring or API modifications to support new features, optimize workflows, or ensure compatibility across platforms and backends. There is concern over fixed issues not being fully resolved, with some requests for better test coverage, reproducibility, and stability (e.g., deterministic behavior, memory management, and benchmarking). Overall, the discussions highlight ongoing efforts to debug, extend, and optimize PyTorch, with specific attention to API clarity, backend support, and robustness of core functionalities."
2019-10-15,pytorch/pytorch,"The discussions highlight several core issues: (1) Ensuring correct model configuration, such as matching network output classes to dataset labels, and managing dataset sizes and indexing errors; (2) Improving documentation clarity, especially for functions like `bernoulli_`, function signatures, and handling in-place or in-place view operations in autograd; (3) Addressing serialization and deserialization limitations, such as loading models from buffers or network transmission, and allowing `torch.load` with `BytesIO`; (4) Compatibility and build issues across environments, including CUDA version mismatches, compiler dependencies, and platform-specific problems like macOS build failures; (5) Feature enhancements and bug fixes in core components like autograd's handling of views, TensorIterator optimizations, and support for model export (ONNX, TorchScript), with ongoing work to fix edge cases, ensure reproducibility, and enhance usability in deployment scenarios."
2019-10-16,pytorch/pytorch,"The comments reveal ongoing challenges with hardware-related stability issues, such as system reboots during training on GPU/CPU, often associated with BIOS settings like Turbo Mode or power supply configurations. Several discussions focus on improving PyTorch's internal functionalities, such as moving operators from TH to ATen for performance gains, refining index select and tensor operation implementations, and ensuring backward compatibility in APIs like `load_state_dict_from_url`. There are also concerns about reproducibility and determinism in tests, the correct handling of dynamic control flows and complex data types in JIT and ONNX exports, and ensuring robust error handling in distributed training and autograd engine shutdown. Additionally, the community is actively working on fixing bugs, optimizing performance, and enhancing documentation, with some unresolved issues related to build environments, framework interoperability, and hardware-specific issues."
2019-10-17,pytorch/pytorch,"The discussion covers a range of issues from code management (rebasing, squash commits, moving code from TH to ATen, API design decisions, and code style), to performance optimizations (like operator fusion, faster index_select, and kernel improvements for deep learning workloads), to correctness and stability (such as handling of in-place tensor operations, proper shutdown procedures in RPC, and floating-point precision concerns in large tensor operations). Several reports highlight ongoing bugs or regressions related to backend compatibility (e.g., CUDA, cuDNN, MKLDNN versions and hardware support), with workarounds involving environment adjustments or hardware configuration. There are questions about design choices for APIs (like quantization behaviors, model versioning, and expandability), and suggestions for features (such as SQL-like data handling, dynamic control flows in JIT, and multi-model serving). The central unresolved concerns revolve around ensuring compatibility and stability across diverse hardware/software stacks, improving performance without sacrificing correctness, and refining APIs for usability and extensibility—many issues awaiting fixes, reviews, or further investigation."
2019-10-18,pytorch/pytorch,"The discussions encompass a range of technical issues in PyTorch, including challenges with multi-process synchronization, such as process hangs during distributed training or RPC, and the need for proper barrier synchronization or error handling for uneven batch processing across processes. There are concerns about in-place operations, especially within BatchNorm and the impact on JIT and autograd, with suggestions to support `inplace=True` arguments and improve operator semantics. Several posts highlight the importance of library integration and build compatibility, notably issues with ideep, MKL, CUDA, and PyTorch's integration with external tools or platforms, often requiring environment-specific fixes or upgrades. Additionally, there are discussions about code refactoring, optimizer behavior, binary serialization, and ensuring performance improvements, sometimes involving complex fixes like bytecode analysis or operator registry improvements. Unresolved questions include how to correctly handle module unloading in Python, improve operator registration, and manage platform-specific build issues, reflecting ongoing development and maintenance challenges."
2019-10-19,pytorch/pytorch,"The discussions reveal several key technical concerns in the PyTorch ecosystem: performance regressions in `einsum` and underlying `bmm` operations, with ongoing efforts to optimize efficiency; questions about memory consumption during initialization, highlighting potential kernel and driver overhead; challenges in supporting features like backward hooks and in-place operations in distributed and autograd contexts; and proposed API improvements such as enabling pixel units for grid sampling, standardizing function naming and conventions, and adding new interpolation modes. Additionally, there's active development around fixing border gradients, supporting residual displacements, managing numerical stability in distributions, and refining testing strategies for reproducibility. Some unresolved questions include the best practices for coordinate normalization in grid sampling, handling resource cleanup in distributed setups, and ensuring backward compatibility amid API refactoring. Overall, the discussions emphasize balancing performance, usability, and correctness through ongoing enhancements and collaboration."
2019-10-20,pytorch/pytorch,"The discussions highlight significant performance concerns with PyTorch, notably the potential fallback to naive BLAS implementations, issues with JIT tracing support for dictionaries with mixed types, and slower `einsum` operations compared to NumPy, especially for large arrays. There are ongoing efforts to improve cuDNN support for depthwise and group convolutions, as well as challenges in implementing batched autograd and efficient solutions for large-scale Jacobian computations. Compatibility and build issues on macOS, including compiler configurations and deployment targets, are also prominent topics. Additionally, various feature requests and bug fixes, such as improvements to `Upsample`, DataLoader stability, and implementation plans for API enhancements, remain active and unresolved."
2019-10-21,pytorch/pytorch,"The discussions highlight several key technical concerns, including limitations in the current PyTorch API such as the lack of support for partial freezing of DistributedDataParallel models, issues with large tensor operations (e.g., SVD memory constraints and einsum performance), and challenges related to tensor subclassing, module dictionary handling, and custom operations in TorchScript. There are ongoing efforts to improve memory management, such as enabling user control over GPU memory allocation, and to address bugs in ONNX export compatibility, particularly for operations like max_unpool2d. Some discussions point to potential architectural constraints, such as the need for automatic detection of unsupported patterns, ensuring backward compatibility, and handling operator dispatching with fallthrough and AMP support. Overall, unresolved questions include enhancing API flexibility for tensor subclasses, extending support for complex operations, and ensuring proper synchronization of CUDA device flags, all of which require careful design and testing."
2019-10-22,pytorch/pytorch,"The discussions highlight challenges with package environment management and installation issues, particularly resolving module import errors like 'torch._C', as well as ensuring proper environment activation. Several threads concern performance and correctness issues, such as inconsistent results in distributed training, GPU memory management, and performance regressions after code modifications (e.g., dynamic type checking overhead, support for various tensor operations). There are ongoing efforts to improve API consistency and usability, including making built-in functions more comprehensive, fixing shape and data type issues in quantization support, and enhancing the integration of custom and scripted modules across platforms, especially Windows. Additionally, flakiness in CI testing, interaction of parallel execution with autograd/tensor states, and proper infrastructure for features like fallthrough dispatch demonstrate the need for more reliable and scalable tooling. The unresolved questions mainly revolve around handling complex dispatch and memory management patterns, robust support for custom extensions, and the systematic reduction of flaky tests to improve development productivity."
2019-10-23,pytorch/pytorch,"The discussions reveal ongoing challenges with PyTorch's API evolution, particularly regarding sparse tensors, API stability, and feature consistency, such as the implementation of center cropping and NHWC support. There is emphasis on improving developer and user experience, notably through better error messages, debugging modes, and supporting more flexible data formats and device configurations, including support for channels-last memory format and mixed data types. Several technical concerns are raised about build system stability, compatibility across compilers and environments (notably MSVC versions), and the proper management of CUDA resources and extensions. Serialization, interoperability between C++ and Python, and the correctness of operator dispatch and backend implementations remain key areas, with proposals for clearer abstractions and safer mechanisms. Lastly, testing robustness and performance regressions are critical, requiring careful validation across hardware and software variations, as well as vigilant code review and testing workflows."
2019-10-24,pytorch/pytorch,"The discussions highlight several key technical issues: inconsistencies and unexpected behaviors in PyTorch's `out=` semantics, particularly regarding in-place operations and tensor resizing; the need for improved type hinting and static analysis to maintain API correctness; challenges with CUDA driver compatibility and debugging, especially for mixed or half-precision tensors on different GPU architectures; complexities in handling scalar-tensor operations and type promotion across different API pathways; and architectural considerations around tensor memory formats, stride handling, and proper support for features like distributed data loading and model copying, all with an emphasis on maintaining backward compatibility and debugging support. Many proposed solutions involve clearer error messaging, API refactoring, and adherence to existing conventions to mitigate current bugs and ambiguities. Unresolved questions remain around the best semantics for certain tensor operations, the impact of driver/software version mismatches, and how to streamline the internal API for better maintainability and user experience."
2019-10-25,pytorch/pytorch,"The discussions highlight several core technical concerns: 
1) Compatibility and build issues, including challenges with specific compiler versions, environment configurations, and third-party dependencies like MKL, OpenMPI, and TensorRT, often requiring manual workarounds or updates to build scripts. 
2) API design questions, such as ways to extend `DistributedSampler` with padding and size parameters, or introducing new kwargs like `raise` or `scatter`, aiming for clearer, more flexible interfaces for distributed and quantized models. 
3) Implementation details around core functionalities, such as support for sparse tensor operations (`ne`/`isnan`), improving type modifications (e.g., `Type::__setstate__`), and handling of strides and layout-preserving views, including whether to add new operators or enhance existing ones. 
4) Concerns over correctness and stability, including potential bugs in gradient computations, backward passes, and the impact of certain optimizations on CPU/GPU performance, with multiple discussions on profiling, reproducibility, and benchmarking. 
5) Maintenance and future planning, on issues like test coverage, versioning, deprecation of certain modules (e.g., Caffe2), and the integration of features like named tensors or support for newer hardware/compilers, often requesting explicit refactoring, code review, and documentation updates."
2019-10-26,pytorch/pytorch,"The discussions highlight concerns about the behavior and implementation details of `track_running_stats` in BatchNorm layers, questioning whether the current logic correctly handles training and inference modes, and suggesting more explicit control over its behavior. There are issues with certain builds failing due to missing headers or macro conflicts, as well as limitations in enabling features like cuDNN for larger kernels (e.g., 5x5). Additionally, code maintenance topics such as the immutability of `track_running_stats`, handling of device-specific optimizations (like Power VSX), and custom C++ autograd functions are raised. The importance of improving build configurations, resolving known bugs (e.g., with half-precision utilities), and enhancing API consistency (e.g., for creating undefined tensors) are also noted as ongoing concerns. Overall, these discussions emphasize the need for clearer semantics, robust build and runtime support, and accurate documentation for feature behavior."
2019-10-27,pytorch/pytorch,"The discussions highlight several key issues: difficulties integrating libtorch within ROS due to conflicting library paths and header parsing, as seen in Issue #14352; ongoing problems with CUDA and CuDNN compatibility, especially on Windows (Issue #19106.0) and with specific GPU hardware (Issue #25857.0); challenges in maintaining and updating Python type hints and stub files to ensure consistency and correctness in PyTorch's external API (Issue #16574.0), including the potential dependency on `typing-extensions`; and various bugs related to specific operator behaviors, such as BatchNorm's `track_running_stats` logic (Issue #16793) and the proper implementation of in-place operators' deprecation. Several discussions suggest solutions like adjusting CMake configurations, adding explicit tests for typing and operator correctness, removing or refactoring outdated implementations, and benchmarking impact of new features or changes. Unresolved questions focus on compatibility fixes, improvements in type annotation testing, and ensuring stability across different hardware and software environments."
2019-10-28,pytorch/pytorch,"The discussions highlight several technical concerns, including the potential decoupling of Tensor dtype from storage dtype to handle complex numbers and interleaved real/imaginary data, as well as the need for better support of complex types on CPU and CUDA. There are questions about implementing forward-mode automatic differentiation and Hessian-vector products, including API proposals and underlying mathematical formulas. Issues related to CUDA kernel compilation, including support for clang and addressing illegal memory access errors, are also prominent. Additionally, concerns about API consistency, such as the behavior of functions like `addcdiv`, and the handling of `out` parameters, are raised. Some discussions remain unresolved, especially around improving documentation, ensuring backward compatibility, and fixing specific bugs in core functionalities."
2019-10-29,pytorch/pytorch,"The discussions primarily revolve around ensuring correctness and consistency in PyTorch's core functionalities, including verifying the proper matching of network output dimensions, tensor indexing, and operator semantics, alongside addressing specific bugs such as memory leaks, static quantization performance, and operator behavior with varying input types. Several questions pertain to improving code API design, such as generalizing layer fusion processes, managing backward compatibility, and enhancing user documentation. Concerns are raised about platform-specific issues (e.g., CUDA, cuDNN, MKL, and building environments), with suggestions to relax API constraints, handle edge cases like small matrices or singular matrices, and enable more flexible, robust multi-threaded, and distributed operations. There are also ongoing efforts to rebase, merge, and validate patches related to new features, performance boosts, and bug fixes, highlighting the need for comprehensive tests and code reviews to avoid regressions. Unresolved questions include how to handle legacy code, optimize third-party library integration, and maintain ABI compatibility across versions."
2019-10-30,pytorch/pytorch,"The discussions reveal concerns about the efficiency and correctness of certain tensor operations, such as torch.flip and torch.norm, with suggestions to move towards view-based implementations or deprecate redundant functions. There are notable issues related to GPU memory management, CUDA kernel launch configurations, and platform-specific bugs, such as problems with libtorch on different hardware or software environments. Multiple technical questions address compatibility (e.g., building with specific compilers, ABI issues, or supporting multiple device types), as well as signaling around the stability and testing of distributed and RPC functionalities, which have shown flakiness and regressions. Several patches involve refactoring or new feature implementation (e.g., custom distributions, operator deprecation, and performance benchmarks), with unresolved questions about correctness guarantees, such as rolling back or handling singular matrices in linear algebra or validating model portability between CPU and GPU. Overall, the discussions revolve around improving performance, ensuring stability, and maintaining compatibility across platforms and use cases, with some pending issues requiring further investigation or testing."
2019-10-31,pytorch/pytorch,"The comments highlight several ongoing issues and proposed improvements across the PyTorch codebase. Notably, there's discussion around correct exception types (e.g., KeyError vs ValueError), potential performance optimizations (e.g., comparison between tuple and loop execution, batched autograd), and ensuring compatibility (e.g., fixing deprecation warnings, rebase requirements, and environment setup). Several logs indicate flaky tests or build failures related to dependencies, environment mismatches, or hardware-specific bugs, with suggestions such as specific environment variables, reworking code logic (e.g., using `flatten_parameters()` in RNNs, adjusting `scatter` logic), and more robust testing strategies. Questions also arise around best practices for API design (e.g., handling activation modules vs functions, deprecated public functions, and RPC mechanisms), and addressing environment-specific or low-level issues (e.g., thread affinity, MKLDNN updates, CUDA runtime errors). Overall, the discussions point toward stabilizing features, cleaning up code, enhancing performance, and improving documentation and usability, with many unresolved questions about fixing flaky tests, compatibility, and environment setup."
2019-11-01,pytorch/pytorch,"The discussions reveal several key points: First, there's ongoing work to improve module and type modification APIs, emphasizing safe attribute addition over removal, and the need for distinct module and type creation functions. Second, multiple issues concern CUDA-related bugs, including illegal memory accesses, reproducibility of specific models, and compatibility with different architectures or driver versions, often requiring debugging or re-implementation. Third, enhancements in documentation, testing, build configurations, and CI setup are frequently discussed to ensure code quality and reproducibility across environments, including handling of warnings, deprecation notices, or platform-specific quirks. Fourth, there's interest in extending PyTorch functionalities such as dataset merging via SQL, supporting custom RNNs, and quantized tensor operations, with considerations of performance trade-offs and API design choices. Lastly, discussions also touch upon infrastructure aspects like supporting clang compiler builds, CUDA build flags, and build size optimizations, as well as whether to disable certain features or dependencies to improve maintainability and efficiency."
2019-11-02,pytorch/pytorch,"The discussions highlight ongoing challenges with OpenCL support and GPU backend compatibility, especially for AMD/ROCm and Nvidia CUDA, emphasizing the need for improved hardware support, multi-GPU build configurations, and better documentation. Several issues concern software functionalities and API design, such as the inconsistent implementation of `median`, the support for quantized tensors, and the potential for more standardized and flexible APIs for operations like batch normalization. There are questions about integration with external libraries like PIL/Pillow, onnx, and dill, along with build-related concerns including dependency management, environment setup, and proper configuration flags for CUDA architecture and quantization. Performance regressions, especially related to sparse and batch operations, alongside CI build failures and documentation generation, are also recurring concerns requiring attention. Unresolved questions involve the development of new features like LKJ distribution support, better database integration for data loading, and future plans for static builds and support for newer PyTorch versions and hardware platforms."
2019-11-03,pytorch/pytorch,"The discussions highlight several technical challenges and adjustments in PyTorch related to environment setup, including dependency conflicts such as PyTorch's compatibility with specific NumPy versions and building issues on Windows with CMake, protobuf, and kernel implementations. Memory management concerns such as GPU out-of-memory errors during training or export suggest reducing batch size or optimizing memory usage. Compatibility issues with different CUDA versions, GPU drivers, and deprecated functions like `pthread_setname_np` on older Linux distributions also feature prominently, with workarounds like custom patches or setting environment variables. Additionally, updates and enhancements for ONNX export support, especially for models like Faster R-CNN, are underway but still require testing and integration, and contributions from first-time contributors are being encouraged with guidance. Overall, ongoing development entails addressing environment compatibility, memory optimization, and model export features, along with process improvements for building and testing."
2019-11-04,pytorch/pytorch,"The comments encompass a wide array of issues related to PyTorch's development, including troubleshooting build problems on Windows and Linux, specifically with CUDA, cuDNN, and compiler compatibility; improvements to distributed training, particularly around device synchronization, atfork handling, and API changes for `DistributedDataParallel`; and enhancements to functionality such as ONNX export behavior, new features like bisecting, and support for new data types and backends. Several discussions focus on fixing bugs (e.g., in `rsqrt()`, `cdist()`, and gradient computations), optimizing performance, and ensuring compatibility across different platforms and environments. There are also proposals to deprecate or modify APIs for backward compatibility and BC-breaking concerns, along with maintenance of build systems and dependencies (protobuf, CMake, CUDA libraries). Notably, the thread includes ongoing efforts to improve user experience through better error messages, documentation, and testing (e.g., for custom extensions, tensor types, and integration with tools like TensorBoard). Unresolved questions often relate to system-specific behaviors (e.g., BIOS settings affecting P2P, compiler flags, or hardware configurations) and future feature development (e.g., support for `IterableDataset` with samplers)."
2019-11-05,pytorch/pytorch,"The comments reflect ongoing development and troubleshooting within the PyTorch repository, including issues with conditional device selection and attribute errors across different versions, indicative of potential API or version compatibility problems. Several discussions focus on enhancing functionalities such as adding trilinear interpolation, deterministic interpolation, and support for higher-dimensional grid sampling, highlighting community interest in these features. There are technical concerns regarding the correct implementation and integration of custom CUDA kernels, protobuf serialization, and ONNX export, with specific attention to ensuring compatibility and fixing bugs in recent PRs. Additionally, some discussions revolve around code maintenance, such as refactoring forward functions, managing test flakiness, and dealing with build system and binary size issues, especially on Windows and with dependencies like protobuf and tensorboard. Unresolved questions include handling compatibility across versions, improving test robustness, and optimizing performance, particularly in distributed and CUDA-specific contexts."
2019-11-06,pytorch/pytorch,"The discussions primarily revolve around issues with PyTorch model serialization/deserialization, especially regarding backwards compatibility in `load_state_dict` and model versioning, with some proposals suggesting avoiding internal `_load_from_state_dict` in favor of native `load_state_dict` handling. Several reports concern reproducibility and correctness of ONNX export, particularly with custom operators, version mismatches, and operator support (e.g., `upsample_nearest2d`, `nms`, `index_put_`). Memory management and performance concerns are highlighted, including GPU memory leaks, high CPU usage with DataLoader, and slowdowns in certain operations across different PyTorch versions. Compatibility issues also arise with platform-specific build dependencies, compiler versions, and environment configurations affecting module loading and runtime errors. Overall, the discussions emphasize maintaining robustness and correctness in serialization, export, and deployment workflows amidst evolving features and platform heterogeneity."
2019-11-07,pytorch/pytorch,"The discussion covers multiple technical areas, including the handling of CPU vs GPU tensors in JIT tracing, with questions about whether `jit.trace` inherits CPU-only behavior and how to ensure type consistency across devices. There is concern over model serialization and version management, especially for incremental updates of code and weights, with suggestions for a more flexible API and SDK to support model management and pre/post-processing. Several issues relate to CRITICAL bugs such as CUDA out-of-memory errors, non-deterministic failures, and CUDA kernel or allocator bugs, highlighting the need for robustness, proper memory handling, and thorough testing of new features like sparse tensor support, quantization, and operator registration. Discussions also involve build system considerations (compiler compatibility, environment detection), stability of testing infrastructure (flakiness, flaky CI failures), and API consistency (tensor operators, export formats, normalization defaults). Overall, the focus remains on stabilizing PyTorch's core functionalities, improving backward compatibility and reproducibility, and clarifying API semantics to reduce user confusion."
2019-11-08,pytorch/pytorch,"The discussions primarily revolve around PyTorch's compatibility issues, including `CUDNN_STATUS_NOT_SUPPORTED` errors, GPU memory management, and environment setup, often linked to driver or library versions and system configurations. Several threads address runtime errors such as `RuntimeError`, `RuntimeError: test_dataloader failed`, or OMP/IPC related crashes, highlighting the need for better debugging, CI stability, and environment validation. Updates and fixes to PyTorch components, like `load_state_dict`, `__torch_function__`, and operator registration, are discussed, alongside concerns about performance regressions, binary size inflation, and API evolution (e.g., out parameters, versioning). There are multiple requests for improved testing, documentation, and handling of certain features (e.g., `ReflectionPad3d`, tensor concatenation, tensor fragmentation), with some ongoing issues unresolved, especially related to system-specific or CI-detected failures. Overall, efforts focus on maintaining compatibility, robustness, and performance while managing system and environment heterogeneity."
2019-11-09,pytorch/pytorch,"The discussions highlight core issues around API design, compatibility, and usability enhancements in PyTorch. Notable concerns include support for the `zip()` function within `ModuleList`, with recent support only for zipping with other `ModuleLists` or tuples; the desire for clearer, grouped documentation for linear algebra functions to improve discoverability; and potential improvements to the `logdet` API through additional arguments like `sym_pos_def`. There's also significant focus on ensuring backward compatibility and stability, especially in areas like quantized tensor operations, and addressing CI failures or build issues across platforms. Overall, many suggestions aim to refine interfaces for better stability, clarity, and user experience, often pending further review or implementation."
2019-11-10,pytorch/pytorch,"The discussions highlight various technical issues in PyTorch, including challenges with multiprocessing start methods and the importance of using context objects like `get_context(""spawn"")` to prevent resource leaks; the support for sparse PCA and algorithms like LOBPCG, with some shared progress and pending implementation details; and environment-specific problems such as module import errors caused by incorrect package installations, particularly when using conda versus pip, and platform limitations like lack of 32-bit binary support on Windows. Additionally, there are concerns about profiling and tracing functionalities, emphasizing the need to port trace processing to C++ and improve usability for complex, multi-iteration or asynchronous systems. Lastly, several build, compatibility, and test failures are discussed, with suggested fixes including code reverts, environment adjustments, and more robust testing strategies."
2019-11-11,pytorch/pytorch,"The comments reveal ongoing discussions about code clarity, backward compatibility, and performance enhancements in PyTorch. Several issues involve handling edge cases such as empty tensors, ensuring numerical stability in functions like `atanh`, and supporting various tensor formats (e.g., 2D, sparse, quantized). There are considerations for API design, especially around model versioning, the use of `__torch_function__`, and operator dispatching, emphasizing the need for clear, maintainable, and non-breaking changes. Performance benchmarking, especially relevant to CUDA operations and function precision, indicates efforts to balance speed with numerical accuracy. Additionally, many comments address build stability, testing coverage, and CI reproducibility concerns, reflecting active maintenance and development challenges."
2019-11-12,pytorch/pytorch,"The discussions primarily revolve around issues with PyTorch's compatibility, performance, and API design. Several technical concerns include the support for static OpenMP linkage, handling of dynamic shapes, and the support for various data types and tensor operations in JIT and ONNX export. There are suggestions for code refactoring for better API consistency, such as the handling of `copy_` and `load_from_state_dict`. Other points involve optimizing performance, dealing with library dependencies (like MKL/DNNL), and ensuring backward compatibility, especially relating to the evolving autograd and tensor APIs. Unresolved questions include how to structure API extensions safely, manage external library interactions, and improve user-facing error messages for better usability."
2019-11-13,pytorch/pytorch,"The discussions encompass several key themes: ensuring compatibility between PyTorch and CUDA versions, especially when matching driver, toolkit, and hardware; improving the robustness and clarity of model versioning and deployment strategies, including support for A/B testing and version selection; addressing performance regressions and bugs introduced by recent code changes, such as optimization intricacies and numerical stability issues; refactoring and extending existing PyTorch APIs and internal mechanisms like `copy_`, `batch_norm`, and extensions loading to support new features and better error handling; and resolving build and testing failures across various environments, with considerations for compiler compatibility, dependency management, and test stability. Several discussions also focus on the importance of accurate test coverage, performance benchmarking, and CI reliability to ensure stable development progress."
2019-11-14,pytorch/pytorch,"The discussions reflect ongoing efforts to improve PyTorch's stability, compatibility, and features, highlighting issues with CUDA and ROCm system incompatibilities, memory management, and kernel dispatching. Key technical concerns include handling large batch sizes with cuDNN, implementing named tensor dimensions, and designing robust fallback mechanisms for operator dispatch that can serve across various operator signatures. There is also focus on system-level improvements, such as support for AVX/VSX with conditional runtime registration, and maintaining backward compatibility for operator APIs. Some unresolved questions involve proper integration of backend fallback kernels with the operator registry, and ensuring that system modifications (like CUDA driver updates or system environment configurations) do not cause regressions. Overall, the discussions emphasize balancing technical correctness, system performance, and ease of maintenance in advancing PyTorch's core infrastructure."
2019-11-15,pytorch/pytorch,"The discussions highlight persistent issues with DataLoader's 'num_workers' setting, GPU and CPU synchronization, and multiprocessing stability, suggesting potential improvements in implementation or configuration practices. Several comments address tensor size mismatches and loss function usage errors, indicating the need for clearer API guidance or validation. There are ongoing efforts to enhance weight initialization, operator fallback mechanisms, and model export capabilities, often involving refactoring or replacing existing code with more flexible solutions, such as dedicated dispatch stubs or operator registry extensions. Compatibility and build issues, especially for specific hardware, operating systems, and deployment scenarios (e.g., Android, mobile, XLA, Windows), are also recurrent, emphasizing the importance of robust cross-platform support. Unresolved questions about improving training efficiency, memory management, and the integration of new features—such as JIT scripting, operator fallback, and default initializations—remain, with several suggestions leaning toward decoupling internal mechanisms and enhancing user-facing APIs."
2019-11-16,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's mobile inference API, with emphasis on simplifying setup and expanding JNI bindings, while seeking feedback on usability. There is a concern about implementing matrix functions like exponential, aiming for naive initial implementations to facilitate progress, alongside discussions on numerically stable algorithms and gradient approximation methods for matrix exponentiation. Several issues address compatibility, performance, and debugging, such as improving runtime trace accuracy, handling different model versions, and managing build failures across CI jobs. Concerning model serving, questions revolve around best practices for versioning, deployment strategies (e.g., eager vs TorchScript), and integrating features like traffic splitting and A/B testing for production robustness. Overall, unresolved questions include balancing usability and stability in custom subclassing, ensuring backward compatibility, and defining comprehensive model versioning and deployment workflows."
2019-11-17,pytorch/pytorch,"The discussions highlight ongoing challenges with implementing ""SAME"" padding in convolutional layers, emphasizing the need for an optional API to support quick prototyping despite performance trade-offs. Several issues address device compatibility and API robustness, such as fixing device handling in `PackedSequence.to`, adding comprehensive tests for new features, and resolving ONNX export errors related to operator input counts and broadcast handling. Other concerns include ensuring proper gradient propagation with `isnan()` for sparse tensors, fixing bugs related to type promotion in `nn.Transformer`, and updating documentation for functions like `torch::allclose` in C++. There is also a focus on enhancing test coverage with common input/output checks and on addressing build failures, ensuring compatibility across different hardware and software versions."
2019-11-18,pytorch/pytorch,"The discussions primarily revolve around enhancing hardware support and performance in PyTorch, including AMD ROCm, OneAPI, and quantization parity, with ongoing debates about the appropriate level of abstraction and implementation details. Several issues concern memory management, such as memory leaks, cache capacity, and fragmentation, as well as ensuring reproducibility and consistency across different backends and modes. There are questions about integrating new features like model versioning, auto-casting, and custom extensions (e.g., thrust complex types) into existing frameworks, highlighting challenges in API design, compatibility, and stability. Debugging complexities due to mixed execution modes, multi-process interactions, and environment-specific behaviors are also emphasized, along with suggestions for code refactoring, better testing, and documentation. Overall, unresolved questions include balancing backward compatibility, correctness, and performance improvements while managing complexity across diverse hardware and software configurations."
2019-11-19,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch environment setup, especially ensuring correct installation of torch and torchvision across different Python versions and platforms, often resolved by adjusting Python versions or using correct package managers. Several issues address performance regressions, CUDA memory limitations, and hardware compatibility (e.g., AVX support), emphasizing the need for thorough benchmarking and compatibility checks, including the potential impact of code changes like named tensors or operator modifications. There are recurring concerns about backward compatibility and serialization format stability, especially when handling model versioning, A/B testing, and loading models in older PyTorch versions. Additionally, multiple questions focus on debugging and testing strategies, including how to handle faulty test cases, mock environments, and ensuring reproducibility in CI pipelines. Overall, the discussions reflect a focus on stabilizing core functionalities, improving environment robustness, and maintaining backward compatibility amid ongoing feature development."
2019-11-20,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's autograd, JIT, and distributed systems, such as inconsistent gradient computations, support for custom operators, and robustness of APIs during model serialization/deserialization. Several issues address performance and compatibility problems, including build failures due to compiler or environment mismatches, and the need for safer, more consistent API designs for features like `requires_grad` locking, autograd handling, and dynamic shape support. There are concerns about backward compatibility, especially around serialization, model loading, and API behavior across versions, with suggestions to improve versioning mechanisms and to clearly document best practices. Additionally, discussions involve optimizing lower-level functionalities like memory layout, operator fusion, and reducing code bloat to enhance performance and maintainability. Unresolved questions mainly pertain to ensuring stable, portable deployments, especially in inference on mobile or heterogeneous environments, and refining user-facing APIs for better thread-safety, clarity, and ease of use."
2019-11-21,pytorch/pytorch,"The discussions highlight ongoing challenges with handling variable-length sequences in RNNs, specifically addressing the limitations of `pad_packed_sequence` leading to hacky padding solutions, and the potential integration of NestedTensors or alternative approaches. There are concerns about the correctness and robustness of distributed training, including RRef memory management, multithreading, and threading overhead, with suggestions to improve thread pool configuration and clarify documentation. Several issues relate to ONNX export, such as operator compatibility, versioning, and model serialization, emphasizing the need for precise shape handling and graph transformations. Additionally, questions are raised regarding API stability, documentation clarity, and compatibility, especially with evolving internal APIs, backend support, and external deployment strategies like TorchServe. Many of these concerns remain open, requiring further refinement, testing, and documentation to align with community needs and future-proofing efforts."
2019-11-22,pytorch/pytorch,"The discussions encompass a range of technical concerns including the implementation and placement of unshuffle operations (e.g., PixelUnShuffle) within PyTorch, addressing build issues especially related to CUDA versions and compiler support (e.g., Xcode with libc++), and handling CUDA memory management and out-of-memory errors. Several issues highlight the need for better debugging tools, such as source range support in graph rewriting and clearer documentation for quantized models, ONNX exports, and API usage. There are questions about the stability and correctness of operations like Backward functions, hyperbolic inverse functions, and support for specific hardware backends (e.g., ROCm, OpenCL). Unresolved questions involve the integration of new features into existing APIs, efficient handling of mixed precision/quantization, and maintaining backward compatibility while evolving the codebase. Overall, the threads reflect ongoing efforts to optimize performance, improve robustness, and clarify documentation amid complex hardware and software compatibility challenges."
2019-11-23,pytorch/pytorch,"The discussions largely revolve around complex technical enhancements and bug fixes in PyTorch, including the development of forward-mode automatic differentiation and derivatives management, with proposed API designs that could involve wrapping functions or associating derivatives directly with tensors. Several issues involve framework internals and require careful handling of autograd behaviors, such as the interpretation of as_strided operations, optimization of autograd graph fusion, and proper accuracy in resize and interpolation operations—particularly to match ONNX and other backend implementations, with debates on whether to prioritize exact scale preservation or approximation. Critical comments highlight the importance of proper environment setup when exporting models to ONNX, ensuring compatibility and correctness, especially on certain architectures or platforms like Windows 7. Additionally, there are challenges related to cross-compiler compatibility (e.g., Intel vs. MSVC), build system configurations, and debugging difficulties, especially when dealing with legacy platforms and specific compiler/linker issues. Unresolved questions include how to efficiently implement precision-preserving forward modes, handling of complex autograd edge cases like as_strided with stride zero, and best practices for environment-specific build and export configurations."
2019-11-24,pytorch/pytorch,"The discussions cover several technical concerns: the importance of warnings and messages during compilation/development, and whether they indicate issues or can be ignored; the lack of a standardized, clean CMake/VS2019 project template for building PyTorch with separate debug and release configurations; and difficulties when using Intel compilers with PyTorch, particularly errors in ArrayRef.h and related attributes, possibly due to compiler compatibility or code annotations. Additionally, issues related to loading models correctly across different CUDA driver and library versions, and handling specific failure cases like division by zero or platform-specific build issues, are raised. Some discussions explore enhancements such as integrating JAX features into PyTorch and exposing certain APIs (e.g., `destroy_process_group`) in documentation. Overall, the main concerns involve improving build processes, compiler support, error resilience, and converging on best practices for deployment and development."
2019-11-25,pytorch/pytorch,"The discussions highlight several key issues: the inconsistent handling and performance of batch normalization modes, especially in evaluation versus training; the support and implementation of sparse tensors and sparse operations with autograd; the management of device and backend configurations in distributed and CUDA contexts; and concerns regarding the correctness and stability of certain operators (e.g., softmax, resize with scale factors) and their compatibility with ONNX and other frameworks. Several technical questions around ensuring deterministic behavior, handling upper-layer API design, and integration of different backends and device types are also raised. Additionally, issues with build environments, compiler compatibility, and proper testing coverage for performance and correctness are frequently discussed and require attention. Overall, the conversations focus on improving robustness, correctness, and usability across diverse hardware, backends, and APIs within PyTorch's evolving ecosystem."
2019-11-26,pytorch/pytorch,"The discussions reveal multiple technical concerns: (1) The lack of forward output manipulation hooks in PyTorch modules, with a potential plan to support this feature via PR #22285; (2) BatchNorm CUDA limit issues and workaround recommendations, such as disabling cuDNN or using custom non-CuDNN implementations; (3) Challenges in exporting models with dynamic control flow, especially with nested modules and RNNs, and strategies like scripting submodules to improve ONNX compatibility; (4) The need for clearer, targeted documentation for internal features like AD and serialization, and fixing build issues related to plugins like NCCL and HIP; (5) Broader infrastructure questions about model deployment strategies, distributed training, and API stability, with suggestions for improving robustness and performance in multi-GPU and distributed contexts."
2019-11-27,pytorch/pytorch,"The comments reveal several recurring themes and technical issues, including requests for enhancements to initialization methods (e.g., LeCun, Kaiming), concerns about system library conflicts affecting CUDA and MKL, and the importance of correctly configuring environment variables like `OMP_NUM_THREADS` to resolve performance and compatibility problems. There are multiple discussions around bugs or regressions introduced in recent code changes, such as unsupported operators, shape mismatches, and multithreading issues, often requiring reverts or patches. The maintenance of backward compatibility and correctness during API updates, especially for operator schemas and serialization, is also emphasized. Unresolved questions include how to better diagnose `cuda.is_available()` failures, handling GPU support for older architectures like K40m, and improving user-facing error messages for hardware or environment incompatibilities."
2019-11-28,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's capabilities, including adding complex tensor support with CUDA, improving memory management, and fixing bugs related to nan gradients in loss functions. Several issues involve environment and system dependencies, such as library path configurations, CUDA version compatibility, and proper device handling in C++, indicating a need for clearer documentation and diagnostic tools. There are concerns about stability and correctness in features like ONNX export, fake quantization, and distributed training, emphasizing the importance of more comprehensive testing, error reporting, and API robustness. Additionally, some discussions address improving usability, such as better function interfaces, verbose error messages, and more flexible API design for common tasks like batch normalization updates. Unresolved questions mainly pertain to system compatibility, bug fixes, and detailed reproducibility of specific errors or behaviors."
2019-11-29,pytorch/pytorch,"The discussions primarily revolve around enhancing complex number support in PyTorch, including CUDA implementations and derivative definitions, with ongoing work and open PRs. Several issues address reproducibility challenges with random seed initialization in multi-worker data loaders, with solutions involving seed resets and worker seed setting, though some approaches may lead to identical sequences across epochs. Multiple bug reports highlight NaN occurrences in training, potential GPU kernel inefficiencies, and inconsistencies in functions like `torch.jit.script` and `interpolate`, often complicated by software version mismatches and environment configurations. There are concerns about low utilization of depthwise convolutions on certain hardware, suggesting the need for optimized kernels. Additionally, build and compatibility issues, such as missing headers (`numa.h`) and environment-specific bugs, are discussed, with suggestions to improve user experience and robustness."
2019-11-30,pytorch/pytorch,"The discussions highlight several core issues: (1) a compatibility bug with `weight_norm` and RNN modules on GPU, due to parameter handling and flattening, suggesting a need for careful tensor type consistency; (2) ongoing challenges in computing higher-order derivatives, especially sustainable gradient and Jacobian calculations, with requests for batched and symbolic autodiff capabilities akin to JAX; (3) platform-specific installation problems, particularly on Mac and Windows, often requiring environment cleanup or external dependencies like `libomp`; (4) the desirability of integrating more explicit and batched derivatives, auto-differentiation enhancements, and better support for higher-order functions; and (5) infrastructure and support concerns, including support for legacy GPU architectures, build environment clarity, and improving error messaging for user clarity."
2019-12-01,pytorch/pytorch,"The discussions highlight challenges in improving Windows build support for ideep, including issues with inline assembly and configuration, and questions about maintenance by Intel. There is a strong suggestion to develop a `torch.metrics` subpackage with a design inspired by TensorFlow 2.x metrics, featuring a class-based interface with methods for resetting, updating, and computing metrics, to enhance usability and consistency. Concerns are raised regarding performance discrepancies between CPU and GPU, especially with FP16 in cuDNN on various architectures, with recent improvements from cuDNN updates noted. Additionally, issues related to environment setup, such as Python version compatibility affecting scripting and tracing in TorchScript, are identified, alongside other build and compatibility problems involving dependencies like OpenSSL and magma. Overall, the discussions emphasize enhancing cross-platform support, user-friendly metric APIs, and addressing specific performance and compatibility bugs."
2019-12-02,pytorch/pytorch,"The comments highlight ongoing work to improve support for complex numbers in PyTorch, including derivative calculations (Wirtinger calculus), complex functions like `exp()`, and limited GPU and CPU kernel coverage via extensions. There are discussions about the best practices and API design for subclass preservation (e.g., `__torch_function__`), including naming and feature API changes, with considerations on backward compatibility and clarity. Several issues concern performance optimizations, especially around gradient computations, memory management, and operator implementations such as `index_select`, `index_put`, and tensor resizing, often requiring careful handling of contiguous vs. non-contiguous tensors and broadcasting semantics (`align_corners` in resizing). Clarity and robustness in distributed training, model deployment, and loading mechanisms, including serialization compatibility and multi-host orchestration, are recurring themes with suggestions for architectural improvements. Unresolved questions pertain to the support for new device types in the backend, handling of dynamic and shared models in production, compatibility of third-party dependencies like `dill`, and API consistency across PyTorch components."
2019-12-03,pytorch/pytorch,"The discussions primarily focus on advancing complex number support in PyTorch, including complex derivatives, support for functions like `exp()`, and the integration of complex kernels into CPU and GPU backends, highlighting the need for comprehensive operator support and performance considerations. Several issues address ONNX export inconsistencies—particularly around `interpolate()` and `upsample`—noting discrepancies in `align_corners` behavior and opset compatibility, with proposals for more explicit operator handling and version-specific support. The repository also contemplates API design for autograd, debating between function-to-function mappings versus in-place or stateful functions, emphasizing the importance of making the autograd system more flexible, predictable, and user-friendly. Additional concerns include improving CUDA device flag handling, memory management, test organization, and build system robustness, alongside infrastructure improvements like better configuration, profiling, and multi-host model serving architectures. Overall, unresolved questions revolve around performance, compatibility, API consistency, and scalability across CPU, GPU, and distributed systems."
2019-12-04,pytorch/pytorch,"The comments highlight various technical discussions including enhancements to PyTorch functionalities such as supporting scale factors less than 1 in pixel shuffle, the development of named tensors, and improvements in distributed training and autograd mechanisms. Several issues relate to correctness, performance, and API design, such as the behavior of `torch.max`, `torch.argmin`, and gradient computations, with ongoing efforts to refine these. Profiling and benchmarking challenges are recurrent, especially concerning CUDA operations, memory usage, and the performance impact of recent changes. Additionally, there are concerns about build system configurations, compatibility issues, and documentation consistency, indicating active development and troubleshooting efforts across multiple components of the PyTorch ecosystem."
2019-12-05,pytorch/pytorch,"The discussions highlight several key issues: the need for improved sparse-dense batch matrix multiplication support in PyTorch, with particular focus on efficient encoding and hardware acceleration; concerns about performance regressions introduced by `__torch_function__` dispatch mechanisms, especially regarding in-place and nested function support; challenges in accurate profiling and measurement of CUDA operations due to asynchronous execution and synchronization overhead; limitations in certain operations like `index_select` with non-contiguous tensors and how to optimize them; and the desire to enhance distributed training features, such as explicit buffer synchronization and refactoring of module attribute management, while ensuring compatibility and minimal performance impact. Unresolved questions remain about balancing Pythonic API design with performance constraints, proper testing to prevent regressions on code refactors, and infrastructure changes needed to support new features reliably across platforms."
2019-12-06,pytorch/pytorch,"The discussions highlight several recurring technical concerns in the PyTorch repository, including the importance of correct API usage (e.g., proper `torch.cuda.is_available()` spelling), handling edge cases such as empty tensors in pooling and normalization layers, and supporting advanced features like sparse tensors, `padding=SAME`, and custom control flow within JIT-compiled models. There are debates around API stability versus backward compatibility—especially when fixing bugs that break existing API behaviors—and suggestions for improving usability through clearer warnings, better testing, or alternative API designs (e.g., context managers for SWA). Implementation challenges also arise regarding low-level optimizations (dispatch mechanisms, compiler constraints), coverage of in-place operations, and ensuring correct tensor data access in cross-platform and multi-GPU contexts. Some unresolved questions involve maintaining API consistency, supporting sparse serialization, and balancing performance trade-offs with code maintainability."
2019-12-07,pytorch/pytorch,"The discussions predominantly address extending PyTorch's support for complex numbers, including implementing functions like `exp()` for complex tensors and ensuring correct gradient derivation, with ongoing efforts to port relevant kernels from TH to ATen and enhance GPU support. There is concern over compatibility and correctness when handling tensor operations such as `expand`, with suggestions to use `repeat` as a workaround and considerations for how to handle breaking changes and backward compatibility, especially regarding `dim=()` behavior. Package and environment management issues are also highlighted, including conflicts with pyarrow, pybind11, and build tool compatibility, advocating for conda-based solutions and proper setup procedures. Additionally, some discussions focus on improving Native distribution implementations, testing infrastructure, and code migration strategies, such as reorganizing test modules and handling version-specific bugs. Unresolved questions include best practices for integrating subclass behaviors with `__torch_function__`, handling deprecation of API behaviors, and ensuring stable builds across diverse hardware and compiler environments."
2019-12-08,pytorch/pytorch,"The comments highlight several technical concerns, including inconsistencies and bugs with BatchNorm behavior when cuDNN is disabled, especially under syncBN, and issues related to tensor contiguity causing runtime errors. There are discussions about the need for sparse tensor support during training, including autograd compatibility, especially for large sparse adjacency matrices used in finite element methods. Several issues address performance and stability problems in numerical algorithms such as LOBPCG and binomial sampling, with suggestions for more stable implementations and native support. Compatibility and reproducibility challenges are also noted, notably with onnxruntime, ONNX versions, and CUPTI/LLVM dependencies in CI, alongside the need for better documentation on PyTorch internals. Overall, unresolved questions include potential BC-breaking changes in APIs, the implementation of efficient sparse operations with autograd, and stability concerns in numerical routines and deep learning workflows."
2019-12-09,pytorch/pytorch,"The discussions highlight several key areas: firstly, uncertainties regarding autograd support and behavior of element-wise max functions, and the need for clearer documentation on max/argmax behaviors across platforms; secondly, challenges in implementing and testing advanced features like multi-recomputation checkpointing, with references to related literature and potential PyTorch internal mechanisms; thirdly, issues with the reliability and reproducibility of GPU memory usage and performance, possibly affected by hardware and environment conditions, leading to concerns over debugging and stability; fourthly, technical considerations around the integration of quantization, especially regarding backend dispatching strategies (e.g., DispatchStub) and support for various data types; and lastly, ongoing development and maintenance concerns such as build system conflicts, deprecation of certain APIs (e.g., `.data`), versioning impacts, and ensuring compatibility with evolving hardware and software environments. Many topics remain unresolved, requiring further clarification, testing, and infrastructure improvements."
2019-12-10,pytorch/pytorch,"The collected comments highlight ongoing efforts and challenges in extending PyTorch's support for complex numbers, including implementing kernels on CPU and limited GPU support, as well as supporting complex derivatives in autograd, with mentions of external projects like ChainRules.jl. Several issues relate to ONNX export compatibility, particularly with dynamic shapes and operator behavior, requiring careful updates to ensure correctness and standard adherence. Problems with CUDA version mismatches, driver support, and hardware compatibility (notably older GPUs) are evident, with suggestions to disable or conditionally handle tests depending on environment specifics. The conversation also touches on internal infrastructure topics, such as the design of the `intrusive_ptr` and tensor subclassing, compatibility of serialization formats across versions, and build system configurations, often accompanied by plans for phased improvements and incremental refactoring. Finally, there are discussions about test management, environment configuration, and maintaining backward and forward compatibility to ensure robustness across diverse deployment scenarios."
2019-12-11,pytorch/pytorch,"The discussions highlight multiple issues related to PyTorch's build, deployment, and training workflows. Key concerns include the proper configuration and cleanup of CMake/VS projects, especially on Windows and with different compilers, as well as ensuring consistent support across different CUDA versions and hardware setups. Several questions focus on improving distributed training and model management systems—e.g., handling multiple models per server, load balancing, and model versioning—aiming for scalable, user-friendly orchestration. There are also recurring technical questions about performance regressions (e.g., in NCCL, batch norm, and tensor serialization), autograd behaviors (e.g., non-leaf grads, backward multiple times), and maintaining backward compatibility in APIs or model states. Finally, improvements to testing, CI infrastructure, and documentation are emphasized to enhance stability and developer experience, alongside unresolved issues about support for certain features like tensor subclassing, named tensors, and compiler toolchains."
2019-12-12,pytorch/pytorch,"The discussions primarily revolve around improving the support for sparse tensors and their operations in PyTorch, including indexing, autograd compatibility, and ONNX export support. There is also focus on refining the internal API, such as unifying certain functions like `to_here()` and `local_value()` for RRef, and handling the specifications for tensor attributes and their metadata. Several issues concern build and compiler compatibility, especially with Windows, ROCm/HIP, and compiler-specific errors, often due to missing headers or version mismatches. Additionally, there are performance considerations related to assertions and internal checks, with suggestions to disable verbose assertions in release builds for efficiency. Finally, there are infrastructure and testing workflow questions, such as managing generated files (like dependency graphs) and ensuring CI tests are stable and reproducible across platforms."
2019-12-13,pytorch/pytorch,"The comments span a wide range of issues related to PyTorch's development, including reproducibility and deterministic behavior, ONNX export bugs, performance optimizations (e.g., gradient computation, depthwise convolution speed), API stability and deprecation plans, multi-GPU and distributed training intricacies, and environment inconsistencies (e.g., CUDA/ROCm configurations, Docker builds). Several discussions involve fixing specific bugs, improving performance benchmarks, refining API design, and ensuring compatibility across hardware and software versions. The overarching themes are ensuring stability, performance, and usability while managing complex dependencies and hardware variations. Some comments also highlight the need for better documentation, testing strategies, and build reproducibility, particularly around external tools (e.g., ONNX, CUDA, ROCm). Unresolved or ongoing issues include non-deterministic results, gradient handling for multiple outputs, and ensuring collaborative code review and testing practices."
2019-12-14,pytorch/pytorch,"The discussions cover a range of issues encountered in the PyTorch codebase, primarily related to CUDA and hardware compatibility, such as differences in CPU architectures affecting numerical precision, and build failures caused by header or third-party library mismatches (like NCCL, CMake, and MKL configurations). There are concerns about non-determinism in floating-point operations and the behavior of functions like `torch.max`, which return indices that may vary across platforms; advocating for better documentation and understanding of such platform-dependent behaviors. Other topics include best practices for implementing SWA (Stochastic Weight Averaging), particularly how to facilitate early stopping and parameter averaging, and issues with multiprocessing support on Windows requiring correct placement of code under `if __name__ == '__main__'`. Additionally, there's discussion about improving performance and eliminating overhead in tensor operations by replacing certain thrust functions with cub implementations, and ensuring compatibility with third-party interfaces like `__cuda_array_interface__`. Overall, the focus is on code robustness, platform compatibility, performance optimization, and usability improvements."
2019-12-15,pytorch/pytorch,"The comments discuss ongoing challenges and potential improvements in PyTorch, including the development of better tensor thresholding functions, sparse tensor indexing support, and dense-sparse operation support. Several issues relate to build failures caused by missing headers (notably 'ATen/native/cpu/Loops.h') and incompatibilities with various compilers and platforms, indicating ongoing integration and compatibility difficulties. There's also an emphasis on the importance of CI stability, build infrastructure, and the handling of generated files for ONNX export, suggesting organizational and tooling concerns. Discussions touch on semantic clarity and consistency of macros like `TORCH_CHECK` and `TORCH_DCHECK`, and on the complexity of memory management issues such as fragmentation and OOM handling, with proposals for more sophisticated memory refresh or compaction techniques. Overall, unresolved technical challenges include build errors due to header issues, platform compatibility, and organizational workflows for generated artifacts."
2019-12-16,pytorch/pytorch,"The comments encompass various issues, primarily technical and development-related, such as performance optimizations, bug fixes, and API clarifications. A recurring theme is the need for better communication, including clearer documentation, precise error messages, and structured API design (e.g., separate classes for in-place versus clone operations). Several discussions address build failures and compatibility issues, emphasizing the importance of dependency management, build configurations, and environment setup. There’s concern over naming conventions (e.g., for macros like TORCH_CHECK vs. TORCH_DCHECK) and API consistency, aiming to improve clarity for developers and users. Additionally, some comments focus on performance enhancements and developer productivity tools, highlighting ongoing work to make PyTorch more robust, maintainable, and user-friendly."
2019-12-17,pytorch/pytorch,"The comments highlight multiple issues and discussions related to PyTorch development. Key points include clarifying that the DataLoader can accept a list of indices directly rather than a separate SubsetSampler, addressing bug reports such as segmentation faults with empty tensors in pooling layers, and proposed enhancements like adding metrics support in PyTorch's core. There's concern about build failures caused by upstream issues or misconfigurations, particularly on Windows and with specific hardware or software setups (e.g., ROCm, CUDA, or compiler errors). Additionally, discussions cover API improvements such as integrating `torch.eval()` with `torch.no_grad()`, handling of ONNX export issues (including operator support), and memory management strategies like arena allocators or fragmentation handling. Overall, the comments reflect ongoing troubleshooting, API refinement, and feature planning efforts within the PyTorch ecosystem."
2019-12-18,pytorch/pytorch,"The comments cover a spectrum of issues in the 'pytorch/pytorch' repository, including potential performance differences between tensor slicing methods on GPU, proposals for supporting overlapping tensor splits with `tensor.unfold` for various dimensions, and the implementation of sparse matrix operations like sparse BMM for GNNs. Several discussions troubleshoot build or export failures related to ONNX, CMake configuration, and environment dependencies, emphasizing the need for better error handling, consistent API design, and environment reproducibility. There are also feature requests for adding support for LARS optimizer, enhanced visualization, and handling multi-backend models, alongside technical considerations for code refactoring, macro definitions, and testing practices. Unresolved questions include how to handle non-standard tensor operations during export, ensuring backward compatibility with different API versions, and managing external dependency configurations across diverse environments."
2019-12-19,pytorch/pytorch,"The comments reflect multiple recurring concerns: the importance of choosing clear, unambiguous macro or function names (particularly distinguishing between debug assertions and user error checks), which impacts code correctness and maintainability; performance measurements indicating that the current implementation may degrade GPU performance compared to previous versions; and the need for better documentation and examples to clarify expected behaviors, especially regarding support for in-place or zero-sized tensors, and nuanced features like `momentum=None` in BatchNorm. Additionally, there are technical challenges related to build system configurations, cross-platform compatibility (including Windows and macOS), and handling environment-specific issues such as memory limits or environment setup. Several discussions highlight the importance of refactoring duplicated code into common utilities and the necessity of precise, minimal reproducible examples to diagnose bugs. Lastly, some comments question the correctness of certain implementation details, especially when the observed behavior (like gradients being zero in saturation) aligns with theoretical expectations, necessitating further clarification or code review."
2019-12-20,pytorch/pytorch,"The discussions primarily focus on technical challenges and feature requests within the PyTorch codebase, including extending tensor unfold capabilities for higher-dimensional overlapping splits and debugging DataLoader worker failures in CUDA mode, which are caused by initialization errors after forking. Several comments highlight the need for better support and documentation around profiling multi-worker DataLoaders, as CUDA's incompatibility with forked processes complicates profiling with autograd or nvprof. Other discussions involve API changes, such as the support for `momentum=None` in BatchNorm for SWA, and addressing backward-incompatibility concerns when updating operator schemas or supporting new dispatch mechanisms. Additionally, there are ongoing discussions about build system issues, especially around correctly integrating third-party libraries like libtorch in different environments, and ensuring backward compatibility with existing models and APIs. Unresolved questions include how to properly handle profiling in multi-process data loading, and how to manage backward compatibility for evolving operator schemas and APIs."
2019-12-21,pytorch/pytorch,"The discussions highlight issues related to CUDA and PyTorch's CUDA compilation, such as the necessity of correctly installing CUDA, setting the appropriate environment variables, and ensuring PyTorch is compiled with CUDA support, especially on Windows and Mac platforms. Several threads address challenges with multiprocessing on Windows, stressing the importance of using the `if __name__ == '__main__':` guard and `freeze_support()` to prevent runtime errors. There are also concerns about ensuring consistent and compatible SVD outputs between NumPy and PyTorch, as well as memory management for large medical volume data in 3D convolutions. Additionally, some discussions suggest improvements in testing practices, API stability, and handling specific behavior in custom operations, with some potential bug reports and feature requests for PyTorch's internal functions."
2019-12-22,pytorch/pytorch,"The discussions primarily revolve around enabling support for zero-dimensional tensors in specific PyTorch operations like `ConvTranspose1d`, addressing input validation and backward compatibility issues. Several issues concern build failures and compatibility across different environments and hardware, including support for various CUDA versions, compiler support (notably MSVC), and integration with libraries like ONNX and TensorRT. There's also mention of platform-specific challenges such as GPU resource allocation errors when using multiple GPUs, and build failures due to outdated dependencies or misconfigurations. Some discussions focus on fixing internal API inconsistencies, ensuring backward compatibility, and improving deployment and build processes on different systems. Several open and pending issues remain unresolved, requiring further investigation or environment-specific adjustments."
2019-12-23,pytorch/pytorch,"The discussions reveal concerns regarding the handling of certain CUDA kernels, synchronization overheads, and compatibility issues, particularly with compiler and library versions (e.g., Visual Studio, MKL, nvcc). There are recurring themes around improving serialization between C++ and Python (including C++-to-Python tensor save/load), and ensuring compatibility with external systems like ONNX and XLA, which sometimes fail due to unsupported tensor types or system-specific limitations. Several issues highlight performance regressions or inaccuracies resulting from specific kernel implementations or input conditions, prompting suggestions for more robust testing, documentation, or workarounds. Additionally, questions around proper management of sparse tensors, multi-device synchronization, and build issues (e.g., missing PDB files, compiler flags) suggest ongoing efforts to improve portability, correctness, and usability across different environments. Overall, unresolved challenges include system compatibility, kernel correctness, serialization consistency, and performance stability, with many discussions focused on incremental fixes, better tooling, and more comprehensive testing."
2019-12-24,pytorch/pytorch,"The discussions highlight recurring issues related to configuring model parameters such as `num_classes` and dataset labels to prevent shape and output mismatches. Several users encounter runtime errors, especially CUDA-related, due to device state assertions or launch failures, often linked to incompatible driver versions, CUDA configurations, or hardware power limitations. There is mention of challenges integrating cross-language serialization between C++ and Python, especially with newer serialization formats and protocols, requiring specific flags or workarounds. Some discussions propose enhancing flexibility in device support (e.g., open registration for TensorTypeId) and improving documentation for advanced features like autograd hooks or custom backends. Unresolved questions include handling edge cases in loss functions (like BCE with small input values), and dealing with build failures caused by environment or upstream changes, with suggestions leaning toward better error handling, default parameter controls, and clearer documentation."
2019-12-25,pytorch/pytorch,"The discussions primarily revolve around enhancing reproducibility and determinism in PyTorch, suggesting the addition of documentation and annotations regarding functions affected by randomness, such as convolutions and loss functions like ctc_loss, with attention to CUDA and CUDNN versions' impact. Another key concern is automating device placement during data loading to prevent common errors, proposing to support a `device` argument in DataLoader. There's also interest in expanding PyTorch's functionality with operators like `meshgrid`, `cartesian_range`, and `combination_range` to facilitate index and product generation for complex applications, with suggestions to document multiple use cases. Issues related to environment management, compilation, and build consistency, especially when dealing with different build types or versions, are addressed, highlighting challenges in reproducing errors across setups. Lastly, some discussions involve optimizing depthwise convolutions and profiling performance improvements, along with addressing CI build failures due to upstream issues and dependency mismatches."
2019-12-26,pytorch/pytorch,"The discussions highlight issues related to the clarity and completeness of documentation, particularly around optimizer derivations and API behaviors, as well as technical challenges such as handling NaNs in loss functions, device and dtype compatibility in tensor operations, and the stability of CUDA operations (notably launch failures and bugs with zero-sized tensors). Several comments suggest potential enhancements, including the implementation of advanced autodiff functionalities (VJP, JVP, Jacobian, Hessian), proper type/shape handling especially for mobile and C++ API, and features like support for multiple models or network workflows, along with improving the reproducibility and robustness of tests amid infrastructure and upstream reliability issues. The discussions also address build system challenges (compiler support, package management, external dependencies), and the need to prioritize or defer certain features or fixes based on their prevalence and impact. Overall, the concerns span both user-facing API/documentation improvements and low-level stability and performance bugs, with a recurring emphasis on thorough testing, clear messaging, and incremental development."
2019-12-27,pytorch/pytorch,"The comments span a range of topics including feature requests for missing functions like `potrs` and axes-controlled FFT, improvements to debugging and profiling tools, and interface adjustments for RRef equality and sibling checks. Several discussions highlight performance regressions on CUDA, caused by dispatch grid/block configuration differences and overhead in CPU dispatch code, emphasizing the importance of optimizing GPU kernels and dispatch strategies. Issues with build failures are frequently tied to environment mismatches, such as incompatible compiler versions or missing dependencies (e.g., MAGMA), or code syntax errors (e.g., unresolved merge conflicts). There's an overarching concern about packaging, code maintainability, and build automation, with suggestions to integrate external frameworks (like Uber's petastorm) to handle data loading at scale. Unresolved questions include the proper way to support multi-threaded database connections, improvements for distributed process management, and ensuring future features and fixes are compatible across various platforms and configurations."
2019-12-28,pytorch/pytorch,"The issues primarily revolve around subtleties in PyTorch's checkpointing mechanism, particularly regarding implicitly captured variables and memory management strategies. Several discussions suggest workarounds such as controlling `requires_grad` or adding dummy functions, indicating ongoing challenges with variable scoping and state capture during checkpointing. There are also multiple reports of build failures, often related to platform-specific compilation problems, serialization errors, and resource constraints, which require careful debugging and environment configuration. Additionally, users are concerned about performance regressions, CUDA instabilities, and compatibility across different hardware and software setups, such as MLC/MLC2 support, GPU memory limitations, and compiler-specific issues. Overall, these discussions highlight the need for clearer interfaces, robust handling of variable capture, and improved CI testing to address platform variability and subtle bugs."
2019-12-29,pytorch/pytorch,"The discussions highlight several technical concerns: increasing consistency with NumPy conventions by adding tensor properties like `itemsize`; evaluating and improving the performance of GPU kernels such as BCELoss by analyzing dispatch configurations and kernel balancing; porting scatter/gather functionality to use public ATen APIs to replace internal TensorIterator usage; addressing build failures related to deprecated or unsupported C++ constructs and internal APIs; and managing data parallelism more efficiently across heterogeneous GPUs, including splitting strategies that optimize for different GPU speeds. Questions also arise around support for abstract types in JIT, and ensuring that functions and types like `as_tensor` maintain precision and correct inference behavior. Many unresolved issues relate to kernel dispatch optimization, compatibility in specific build environments, and ensuring safe exposure of internal APIs."
2019-12-30,pytorch/pytorch,"The discussions highlight several key technical issues, including challenges with parameter sharing in traced modules and the importance of adding debug information for better traceability. There are concerns about supporting 0-dimensional tensors in convolution operations, specifically ConvTranspose1d, to handle edge cases without extensive edge case handling code. Additionally, there are questions about parameter sharing best practices, the impact of different GPU architectures and CUDA versions, and ensuring consistency between Tensor and torch.* functions. Some issues relate to build configurations, dependency management (particularly CUDA library linking), and performance regressions, with suggestions to improve testing, debugging, and platform support (e.g., supporting variable-length inputs in DataParallel). Unresolved questions involve support for certain quantized operations, bug fixes needed for dynamic operators, and ensuring compatibility across hardware, software, and Python versions."
2019-12-31,pytorch/pytorch,"The discussions highlight ongoing challenges with the implementation of custom loss functions like Dice loss, particularly related to gradient requirements, and issues with GPU kernel performance, including memory access errors and kernel optimizations. There's concern about the impact of recent changes on batch normalization behavior with zero batch size inputs and the correctness of running statistics updates. Several issues relate to the usability and API design of JIT scripting, especially around script modules and script export behaviors, where the current API either breaks encapsulation or lacks clarity. Troubles arise from environment-specific failures, build system limitations (e.g., ROCm support, Windows build errors), and GPU-specific bugs (e.g., cublas errors, memory leaks). Unresolved questions involve ensuring backward compatibility, improving kernel performance via better parallelization, and clarifying API semantics for scripting and device management."
2020-01-01,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's features, such as adding sparse batch matrix multiplication for GNNs, and addressing performance and usability challenges with scatter/gather operations, especially regarding non-unique indices and parallelization safety. There is interest in implementing more flexible data parallel strategies, such as manual batch splits to optimize multi-GPU workloads. Several issues pertain to build and runtime errors on various systems, including missing CUDA support, distribution package availability on ARM, and test failures, often related to specific PyTorch versions or configurations. Additionally, there's consideration of API documentation accuracy, particularly for scatter_add, and complexities introduced by Python's GIL affecting distributed operations. Overall, these discussions reflect active development, bug fixing, and feature enhancement efforts within the PyTorch community."
2020-01-02,pytorch/pytorch,"The comments highlight several key issues across the GitHub discussions:

1. **Determinism and Convolution:** Notably, there's a concern about deterministic behavior in convolution and loss functions like CTC loss, affected by CUDA and cuDNN versions, warranting documentation updates for `torch.nn.Conv*` modules.

2. **CUDA Errors and Compatibility:** Multiple reports of device-side asserts and CUDA runtime errors suggest potential mismatches between CUDA versions, drivers, and PyTorch builds, with some solutions involving updating drivers or changing code practices.

3. **JIT and Serialization Limitations:** There's ongoing discussion about limitations of TorchScript, especially regarding class inheritance, scripting modules, and handling of dynamic or list inputs, with proposals to relax some constraints or provide clearer APIs for scripting modules and functions.

4. **DataParallel and Variable Input Lengths:** Users express interest in supporting variable-length inputs, possibly via nested tensors or list-based data, which raises questions about the best API design (e.g., `PackedSequence`, list of tensors, or new data structures).

5. **Testing and CI Failures:** Many failures are related to external dependencies (e.g., missing packages, upstream failures), highlighting the need for better tests, environment management, and possibly more explicit instructions for reproducing or fixing issues.

Overall, these discussions emphasize compatibility, deterministic reproducibility, scripting flexibility, and robustness of testing infrastructure."
2020-01-03,pytorch/pytorch,"The comments highlight multiple ongoing challenges in PyTorch development, including issues with CUDA memory management and kernel loading, leading to high memory usage and startup delays (e.g., memory allocated on heap, kernel image loading). Users express concerns about API design choices, such as whether to support tensors in place of Python scalars across different backends and how to effectively document or standardize such usage. Several reports address reproducibility and performance problems, often tied to specific hardware or software versions (e.g., PyTorch versions, CUDA/cuDNN compatibility, threading models), with workarounds like adjusting batch size or using `torch.no_grad()` suggested. There are also discussions about build failures, dependency/version management (e.g., PyTorch, ONNX, Pillow), and the need for clearer testing or more systematic handling of non-deterministic operations. Overall, unresolved issues revolve around memory efficiency, API consistency, and build robustness across diverse environments."
2020-01-04,pytorch/pytorch,"The comments primarily highlight ongoing development challenges and feature gaps in PyTorch, such as the absence of native 'padding=same' support, issues with CUDA memory management, and compatibility with older CUDA versions. Several discussions address build failures and flaky tests in CI, often related to environment setup, compiler issues, or upstream bugs, emphasizing the need for better build configuration and stability. There are questions about the support of specific features like CUDA 9.0, as well as efforts to migrate code to more efficient or modern implementations, such as porting operators and improving build processes. Additionally, some discussions focus on test coverage, such as ensuring correct behavior of functions like `add_histogram` or handling complex model scenarios. Unresolved issues include build failures due to compiler/linker limits, CUDA version compatibility, and specific operator support, warranting further debugging, environment management, and feature completion."
2020-01-05,pytorch/pytorch,"The discussions primarily revolve around build failures and runtime errors caused by incompatibilities or missing features, such as undefined symbols (_cvtu32_mask16) related to CPU-specific instruction sets (AVX512) in certain precompiled libraries like XNNPACK, especially on older hardware or with specific compiler/toolchain configurations. Several failures are linked to platform-specific issues, including Windows compilation errors due to missing headers (e.g., pthreadpool.h), linkage problems with MKL libraries in static builds, and large memory consumption in tests causing out-of-memory errors. Some errors suggest that downstream tests (e.g., test_dataloader, test_nn) are failing due to upstream breakages or environment setup issues, not necessarily code regressions. There are ongoing discussions about whether to rebase or revert certain PRs, and suggestions to build from source or adjust test parameters to mitigate hardware or configuration constraints."
2020-01-06,pytorch/pytorch,"The comments encompass a wide range of technical issues and suggestions related to PyTorch development, such as limitations in the `scatter_kwargs` function affecting RNN hidden states, build and environment configuration challenges on various platforms, and issues with CUDA, NCCL, and distributed training workflows. Several discussions focus on improving API stability and usability, like refactoring `Tuple` creation in C++, handling long paths on Windows, or enhancing ONNX export support for custom operators. Others address performance concerns, such as optimizing data transfer with multi-threading, managing memory for large tensors, and addressing GPU kernel inefficiencies and correctness in gradient computations for scatter operations. Multiple reports highlight ongoing CI failures, build errors, or compatibility issues across different environments and hardware, indicating active maintenance and the need for careful rebasing, patching, and testing. Overall, these discussions reflect an active development process focused on robustness, API improvements, and platform support."
2020-01-07,pytorch/pytorch,"The comments involve multiple technical discussions surrounding PyTorch development, including the implementation and API design of sparse batch matrix multiplication, support for specific mathematical operations (like `cumsum`, `svd`), and API deprecations. Several issues highlight concerns over backward compatibility, proper API usage, and the behavior of operations like `scatter_add`, especially regarding gradient computation and the use of indices. There are also discussions on the integration of external packages (like metrics), enhancement of debugging tools, and platform-specific build challenges, such as NCCL, NCCL with threading, and environment setup for CUDA and NCCL compatibility. Some comments address CI failures to distinguish upstream issues from local problems, with suggestions on how to rebase or disable unstable tests. Overall, unresolved questions target the correctness of mathematical operations, API stability, backward gradient formulas, and build environment stability across platforms."
2020-01-08,pytorch/pytorch,"The comments reveal ongoing challenges related to CUDA and NCCL compatibility and stability, particularly with multi-GPU setups, likely due to driver or environment issues. There are concerns about RPC and distributed training reliability under various configurations, including IOMMU settings and process spawning. Several issues involve development workflows, such as debugging deep inside C++ extensions, managing build size limitations, and ensuring tests are reliable across different Python versions and CI environments. Discussions also touch on API consistency, like handling scalar vs. tensor arguments, and the need for clearer documentation or refactoring to streamline user experience. Unresolved technical questions include proper ways to support mixed-inheritance with TorchScript, ensuring reproducibility for complex distributed tests, and code contributions' impact on internal stability, e.g., XLA or ONNX support updates."
2020-01-09,pytorch/pytorch,"The comments reflect ongoing discussions about improving consistency and clarity in PyTorch's API and documentation, particularly around naming conventions, loss functions, and batch of sparse matrices. There is a significant emphasis on addressing performance bottlenecks, especially in CPU and CPU-based sparse matrix operations, as well as back-end implementation details like renaming or deprecating certain loss classes and optimizing functions like `bmm`. Several technical concerns include backward compatibility when renaming loss functions, optimizing CPU and CUDA implementations for sparse tensor operations, and handling issues with serialization formats and internal API changes, especially regarding the `__torch_function__` protocol and type inheritance. Some discussion points also involve system-specific build issues, error handling, and ensuring reliable behavior across different hardware and software environments, including CUDA, ROCm, and mobile platforms. Overall, the dialogues highlight efforts to enhance performance, API consistency, and robustness, alongside addressing build, testing, and documentation challenges."
2020-01-10,pytorch/pytorch,"The comments span several areas of development and bug fixes in PyTorch, with notable discussions on enhancing the autograd API (e.g., adding an `inputs` argument to `autograd.backward()`), improving performance (e.g., exporting chrome traces directly in C++, optimizing model benchmarking, and reducing compilation/linking size issues), dealing with build and environment configurations (e.g., Visual Studio and CMake setups on Windows, fixing ONNX export bugs, and resolving CUDA library linking errors), and API stability concerns (e.g., versioning serialization formats, supporting mixed CPU/GPU operations, and ensuring backward compatibility). There are also ongoing maintenance and testing questions, such as handling flaky tests, testing model export correctness, and managing dependencies for documentation and third-party integrations. Many discussions suggest partial fixes, experimental features, or refactoring plans to improve usability, performance, and robustness, but several issues remain unresolved or require further verification (e.g., build failures on specific platforms, correctness for newer versions, and ensuring compatibility with external tools like ONNX). Overall, the main concerns revolve around API improvements, performance optimizations, build stability, serialization/versioning clarity, and backward compatibility."
2020-01-11,pytorch/pytorch,"The discussions highlight various technical concerns, including the desire for more pythonic and transparent APIs for handling Parameters and Modules in PyTorch, with suggestions to treat Parameters more like Modules by making them callable or allowing `nn.Parameter` to accept Modules. Several issues involve robustness and compatibility in model serialization formats across different PyTorch versions, proposing the need for explicit versioning and human-readable format identifiers. Hardware and CUDA-related bugs, such as illegal memory access on GPUs and inference speed disparities, are also discussed, emphasizing hardware diagnostics and potential code optimizations. Additionally, there are ongoing efforts to unify and clarify serialization formats (e.g., eager vs TorchScript), while some failures in CI builds relate to upstream code incompatibilities and deprecated features. Overall, unresolved questions pertain to API improvements, versioning schemes, serialization standards, and hardware stability."
2020-01-12,pytorch/pytorch,"The discussions highlight challenges related to package management and environment reproducibility, particularly with the impending shutdown of TUNA's Anaconda mirror and the need for reliable CUDA and PyTorch versions. Several issues involve version compatibilities, such as CUDA 10.x versus newer versions, and the necessity of precise environment setup (e.g., calling `set_epoch` in distributed data sampling or ensuring single PyTorch installations). There are also technical concerns about model exporting (e.g., ONNX export issues with specific PyTorch versions), internal API behavior (e.g., `torch.backends.cudnn.enabled`), and stability of CI builds amid external failures or flaky tests. Additionally, various code maintenance topics are discussed, such as adding type checks, fixing mypy annotations, and addressing warning behaviors during testing. Overall, these discussions emphasize ensuring environment stability, version compatibility, and robustness of workflows in PyTorch development."
2020-01-13,pytorch/pytorch,"The discussions highlight concerns about the maintainability and performance of features such as tensor memory format propagation, including how to efficiently represent and cache permutations beyond simple NHWC/NCHW tags, with suggestions to store permutations directly rather than as vectors. There are issues with correct handling of stride normalization and canonicalization, pointing to the need for automatic adjustments during tensor creation. Several technical bugs and failures are related to patch application mismatches, CUDA/nbc library bugs, and the importance of robust testing and build environment consistency, especially across different CUDA versions and Python environments. Questions about the correct modeling of displacement fields in grid sampling and the proper handling of model format/version identification for serialization also arise. Lastly, there are concerns over warning semantics and backward compatibility depending on Python version, with suggestions to use warnings like `FutureWarning` over `DeprecationWarning`."
2020-01-14,pytorch/pytorch,"The GitHub comments reflect ongoing concerns around data loading shuffling, dataset and DataLoader behaviors, and potential memory leaks in PyTorch, especially when using multiple workers or on specific hardware configurations. Several issues highlight performance bottlenecks and correctness bugs, such as the inefficient implementation of weight normalization, and the complexity of supporting batch sparse matrices, which impact CPU and CUDA operations. There are discussions about standardizing model save formats and version reporting, and considerations around API design for tensor and distribution handling, including custom dispatching for subclasses and ensuring compatibility with XLA. Some comments note upstream build failures or environment setup challenges that affect testing, while others address internal BC breaks, such as the renaming of internal functions and the impact on decomposed native extensions, especially for XLA. Overall, these comments underscore the need for improved testing strategies, API consistency, performance optimizations, and build reliability."
2020-01-15,pytorch/pytorch,"The comments highlight several technical issues and ongoing discussions in the PyTorch codebase, including the need for clearer documentation of API changes and new features such as support for per-layer cuDNN algorithms, improved sampling strategies, and more flexible data bucketing. There are concerns about flaky or upstream CI failures and build issues related to dependencies, compiler compatibility, and architecture-specific bugs. Discussions also cover the design and implementation of advanced features like in-place gradient zeroing, backward compatibility inspection, and state management during distributed training. Some entries note regressions or bugs that require attention, as well as areas for potential refactoring, testing, and documentation improvements. Overall, unresolved questions focus on stability, performance implications, and the proper integration of recent API enhancements within the larger PyTorch ecosystem."
2020-01-16,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch development, including unmerged or stalled pull requests related to features like truncated normal distribution, symmetric tensors, or distributed training refinements. Several discussions concern build failures, often due to upstream dependency conflicts, deprecated code, or incorrect configurations—particularly in the CUDA/C++ compilation environment. There are repeated mentions of flaky CI failures, often caused by network or environment inconsistencies, complicating reliable testing and release workflows. Some discussions involve API design choices, such as `__torch_function__` dispatch control and support for new tensor types, as well as documentation and build system clarifications. Unresolved questions include how to properly synchronize dependencies (like CMake flags or system libraries), handle build errors across platforms, and ensure feature support (e.g., CUDA, ONNX, JIT, and quantization) in upcoming releases."
2020-01-17,pytorch/pytorch,"The discussions mainly revolve around improvements and concerns in the PyTorch codebase such as enhancing tensor comparison utilities, supporting batch operations like `torch.dot`, and adding features such as `TransformedDistribution`. Several issues discuss the reliability and correctness of backward pass performance, with some pointing out potential memory leaks related to specific modules like MaxPool2d. There are technical questions about build processes, support for system libraries (like NCCL), and compatibility across different environments and hardware, including CUDA and ROCm. Some comments highlight flakiness and intermittent failures in CI/CD pipelines, suggesting the need for more robust testing and clearer documentation, especially for new or less supported features. Overall, ongoing enhancement of functionalities, debugging, and deployment stability are the main themes, with unresolved questions about test coverage, performance implications of certain implementations, and build consistency."
2020-01-18,pytorch/pytorch,"The discussions highlight several key points: concerns regarding backward compatibility introduced by schema changes and operator modifications, especially with very old PRs and their impact on testing (e.g., backward incompatible OperatorLibrary changes and version-specific behaviors). Hardware and environment-specific issues are mentioned, including GPU faults, CUDA driver discrepancies, and system-level bugs like deadlocks caused by threading and environment configurations. There is interest in improving user-facing APIs, such as simplifying distribution wrappers (`NormalizedProduct`) with upstream support for functions like `.to_event()`. Additionally, compatibility issues with external tools and models (e.g., ONNX version constraints and model conversion failures) are also discussed, alongside the need for better test coverage and code robustness across diverse setups."
2020-01-19,pytorch/pytorch,"The discussions highlight various technical challenges including CUDA runtime errors such as device-side asserts, illegal memory access, and out-of-memory issues, often related to mismatched or outdated CUDA and cuDNN versions, or improper tensor consolidations. Several threads point out potential causes like version incompatibilities, insufficient hardware resources, or bugs in specific modules (e.g., BatchNorm backward pass, RNN parameter application). Several unresolved questions concern how to properly handle model serialization, operator optimization, and graph tracing, especially in TorchScript and ONNX exports, with some suggesting workarounds or improvements. Furthermore, multiple build failures are attributed to upstream issues, environment misconfigurations, or flaky test states, indicating ongoing integration challenges. Overall, the core concerns revolve around ensuring CUDA compatibility, memory management, correct model export/serialization, and stable integration testing."
2020-01-20,pytorch/pytorch,"The discussions primarily raise questions about the proposed API change to support calling parameters as functions in modules, emphasizing the need for minimal invasiveness and caching strategies at the parameter level to avoid out-of-sync bugs. Concerns are expressed regarding the compatibility of certain decorators (e.g., @property) with TorchScript and the potential for breaking backward compatibility, especially when the change involves small code modifications like parentheses. There are also technical issues related to device handling and guard APIs in C++, as well as specific bug reports such as memory leaks, shape handling inconsistencies in tracing, and CUDA errors, which may not directly relate to the API proposal. Additionally, questions about the incorporation of new features (e.g., distribution mode, MKLDNN support, and scatter/add operations) and the stability of test infrastructure are discussed, highlighting areas needing further review before adoption."
2020-01-21,pytorch/pytorch,"The comments from these GitHub issues primarily revolve around debugging and improving reliability of CUDA and distributed training in PyTorch. Several discussions highlight efforts to handle asynchronous CUDA errors, manage device contexts, and ensure backward compatibility, often suggesting more robust error checking (e.g., replacing `cudaGetLastError()`) and model consistency checks in distributed setups. Issues related to build errors, library linking, and environment configurations (e.g., multiple CUDA versions, MAGMA dependencies, NCCL) emphasize the complexity of stable builds across diverse environments. There are also ongoing proposals to enhance scripting and tracing support, optimize performance (e.g., memory allocators, instruction-level assembly verification), and extend features such as quantization and tensor manipulation. Unresolved questions include better runtime error propagation for CUDA, model verification in distributed training, and how to reliably test GPU-dependent performance improvements."
2020-01-22,pytorch/pytorch,"The comments reflect ongoing efforts in PyTorch's development, covering issues such as error-checking improvements (e.g., async CUDA error handling and exception translation), operator crashes, build system complexities, and performance considerations. Several discussions involve refactoring operator kernels, handling zero division errors, or how to implement reparametrization in modules, often accompanied by code snippets or proposed API changes. There are also numerous reports of CI failures—many flaky or caused by external factors like environment issues or upstream bugs—and some that involve build or system configuration problems, including CUDA dependencies and protobuf versions. Some comments suggest adding tests, clarifying API behaviors, or reworking code structure; unresolved questions include details about integration with tools like ONNX, handling of mixed CPU/GPU models, and the impacts of fallback dispatch mechanisms. Overall, the discussion highlights a mix of bug fixing, API improvements, testing, and environment management challenges in the PyTorch OSS project."
2020-01-23,pytorch/pytorch,"The discussions highlight several recurring concerns, notably the need for clearer API documentation and consistency (e.g., list of view operations, proper handling of module cloning, operator support across devices). Several technical proposals involve refactoring existing functionalities for better stability and clarity, such as introducing `deepCopy()` for modules, improving zero-checks in operators, and fixing cyclic padding bugs. There are also unresolved questions about implementation strategies for new features, like selective eigenvector computation, and their testing and benchmarking challenges. Many comments address build failures, CI flakes, and environment setup issues, indicating ongoing integration and portability concerns. Lastly, there’s a persistent emphasis on maintaining backward compatibility while progressing with new features and fixes."
2020-01-24,pytorch/pytorch,"The comments reflect ongoing efforts to enhance error handling, performance, and compatibility across different operators and systems within PyTorch. Key discussions revolve around standardizing exception propagation (notably ZeroDivisionError), porting operators from TH to ATen for improved CPU robustness, and refining profiling and GIL management in multi-threaded contexts to avoid deadlocks. Several issues address build stability, particularly around kernel compilation errors, flaky tests, and external dependency fetch failures, with suggestions ranging from reworking internal APIs to separate module updates for easier maintenance. There's also interest in clarifying operator behavior (e.g., views, in-place vs. out-of-place results, size inference) and improving documentation consistency. Overall, the focus is on making the core framework more reliable, performant, and developer-friendly amid evolving hardware and software environments."
2020-01-25,pytorch/pytorch,"The comments highlight several recurring themes: (1) Implementation details and correctness of memory management, such as ensuring proper tensor freeing to prevent leaks, and clarifying the behavior of optimizer parameter groups (e.g., why LARS shouldn't be applied to batch normalization and biases, and the handling of `clone()` as a public API). (2) Performance and feature support, such as adding batched determinant support, batched inverse, and support for mixed/different tensor shapes or datatypes, along with benchmarking to evaluate improvements. (3) Handling asynchronous operations and backward pass safety, especially the potential for race conditions and memory issues when calling `backward()` multiple times with `wait()`. (4) Dealing with build failures, flaky tests, and upstream issues, often caused by CI environment problems or upstream code regressions. (5) Questions about API usage in multi-GPU and embedded/interop contexts, emphasizing proper GIL management, public API practices, and potential API clarifications."
2020-01-26,pytorch/pytorch,"The discussions highlight several technical issues, including the need for better API consistency in optimizer state management, especially regarding moving models between CPU and GPU, and ensuring proper seed setting in distributed samplers. There are concerns about the lack of native support for loading data from cloud object stores like HDFS or S3 in PyTorch, with suggestions to leverage existing Python libraries or third-party tools such as Petastorm. Additionally, some issues relate to performance and correctness, such as the slowdown observed with cuDNN on certain convolutions, the behavior of the `Zoneout` implementation, and the stability of numerical operations like `argmax`. The discussions also involve code maintenance questions, including API visibility, proper testing, and build stability, with some features being dropped or deferred due to maintenance overhead or lack of adoption. Unresolved questions include the best approach for optimizer state resets, handling of data loading from cloud storage, and addressing performance regressions in specific CUDA operations."
2020-01-27,pytorch/pytorch,"The comments reflect ongoing efforts to enhance PyTorch, including support for AVX and complex numbers, and address various issues such as CUDA errors, binary compatibility, and performance regressions. Several discussions focus on porting kernels from TH to ATen, improving autograd and autograd's nested parallel execution, and expanding platform support, particularly for ROCm and big-endian systems. Notable concerns include handling in-place operations on expanded tensors, correctness of gradient computations (especially second-order gradients), and ensuring reproducibility and stability, especially in distributed contexts. There are also questions about the evolution of the API (e.g., `clone`, `reset_state` placement) and maintaining backward compatibility across versions. Unresolved issues include flaky tests, CI failures due to upstream breakages, and ensuring consistent performance, especially for advanced features like sparse tensors and mixed-precision training."
2020-01-28,pytorch/pytorch,"The discussions primarily revolve around performance considerations and design decisions in handling sparse and dense matrix operations, particularly whether to avoid coalescing sparse tensors to improve efficiency, and the implications for batched matrix multiplication. There are suggestions to improve sparse tensor processing using binary searches for index-range determination to enhance performance, as well as debates on error messaging and API consistency, especially regarding optimizers and scheduling functions. Issues also touch on ONNX export compatibility, graph representation for model pruning, and hardware-specific build configurations, such as support for ROCm, HIP, and CPU vectorization. Several technical challenges remain, including improving error clarity, maintaining backward compatibility, and ensuring platform-specific code works efficiently without regressions."
2020-01-29,pytorch/pytorch,"The discussions mainly revolve around performance and implementation details of tensor operations in PyTorch, specifically around `flip()` behavior and negative stride support, with some suggestions for improvements like using views or batching strategies. Several issues pertain to compatibility, such as ensuring backward compatibility in optimizers or supporting diverse input types, and handling quirks in specific functions like `BCELoss`. There are concerns about external dependencies and build configurations, particularly with CUDA, ROCm, and environment setup, including potential conflicts with hardware capabilities and compiler flags. Workarounds like patching, rebase strategies, and code refactoring are frequently discussed to address build failures, test failures, or API inconsistencies. Additionally, some conversations focus on enhancing testing infrastructure, API design, and documentation clarity for better usability and maintainability."
2020-01-30,pytorch/pytorch,"The comments from the GitHub issues indicate a variety of ongoing and resolved concerns, including bug fixes, performance improvements, and design discussions. Several issues relate to regressions in sparse tensor operations, CUDA kernel behaviors, and integration of new features like `pack_sequence`, `torchsearchsorted`, and support for different data layouts such as channels-last. There are frequent mentions of upstream breakages, environment setup challenges, and the need for better testing, especially for GPU and CUDA-related functionality. Discussions also touch on API stability, backward compatibility, and the transition to TORCH_API macros and C++ implementation improvements. Overall, the main themes involve fixing regressions, improving performance and usability, and ensuring compatibility across platforms and versions—many of which are still actively in progress or require further review."
2020-01-31,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch's features and implementations, including difficulties with copying and deepcopy semantics related to gradients and model weights, performance bottlenecks in sparse tensor Batched Matrix Multiplication (BMM), and support for negative strides in tensors—some of which have been addressed or are under active development. Several discussions also concern the stability, correctness, and usability of JIT and export functionalities, especially regarding exporting models with certain operations or features like hooks, nested tensors, and complex indexing. Additionally, there are issues related to build failures, environment setup, and external library compatibility, indicating ongoing CI and deployment challenges. Overall, unresolved questions include ensuring backward and forward compatibility, performance optimization on CPU/GPU architectures, and improving user-facing documentation and API behavior for complex tensor operations."
2020-02-01,pytorch/pytorch,"The discussions primarily revolve around practical challenges with multiprocessing in Jupyter notebooks, notably issues with setting start methods like 'spawn' and 'forkserver' and their impact on CUDA sharing and basic multiprocessing. Several comments address implementation details and API behaviors, such as the need to add 'return x' in model forward functions, sparse tensor operations, and accessing weights in scripted models. There are concerns about CI build failures, flaky tests, and potential compatibility issues with different environments and hardware, including ARM architectures and GPU configurations. Additionally, some comments highlight ongoing feature development, such as sparse support for `nn.Linear()`, DNNL integration, and performance optimizations like parallelism in FBGEMM. Unresolved questions include simplifying warnings, ensuring backward compatibility, and improving debugging tools for complex distributed and hardware-specific scenarios."
2020-02-02,pytorch/pytorch,"The discussions highlight ongoing efforts and challenges in PyTorch development, including the need for deterministic interpolation, efficient memory management, and model export portability on different hardware configurations. Several comments address installation platform-specific issues, such as compatibility of wheel files on Windows and alternative package commands for CPU-only environments. There are concerns about upcoming release schedules and the integration of support for emerging hardware like FPGA and OpenCL, emphasizing modularity and out-of-tree support to maintain core simplicity. Technical questions also focus on debugging failures in CI builds, especially related to CUDA, ROCm, and static analysis, alongside issues with model deployment and environment variable configurations. Overall, the discussions reflect active troubleshooting, feature planning, and compatibility considerations across hardware, software, and deployment contexts."
2020-02-03,pytorch/pytorch,"The discussions predominantly revolve around build and compatibility issues across macOS, Windows, and Linux, especially related to CUDA, ROCm, MKL, and other dependencies. A recurring concern is ensuring support for multiple GPU architectures and architectures like 'compute_70' in CUDA, as well as supporting different hardware setups like NVidia in Apple environments. There's also emphasis on test robustness, error message clarity, and performance optimization for both CPU and GPU kernels, with some mentions of specific implementation strategies such as dispatch mechanisms and tensor layout considerations. Several comments point out the importance of proper error handling, profiling support, and addressing flaky or upstream build failures. Unresolved questions include how to harmonize shared code between CUDA and ROCm, how to improve user experience on DLL loading issues, and whether certain features like onnx model import/export should be integrated into the core PyTorch codebase."
2020-02-04,pytorch/pytorch,"The collected comments address multiple technical topics related to PyTorch, such as the need to verify whether current `nn.LSTM` implementations support totally independent forward and backward layers (Type-1 behavior), and whether new flags or modifications are required. Issues about specific functionality like segment_* ops, circular padding, and tensor size handling indicate ongoing development and bug fixes. Several discussions concern improving build processes, binary size, and platform-specific support (Windows, ROCm, mobile), often highlighting test failures or build errors due to code incompatibilities or environmental configurations. Enhancements in API usability (e.g., custom libtorch building, dynamic learning rate scheduling, profiler thread safety) are suggested, alongside ongoing efforts to improve debugging, documentation, and error messaging. Unresolved questions include the correctness of current implementations, performance implications of proposed changes, and how to ensure reliable cross-platform compatibility and testing."
2020-02-05,pytorch/pytorch,"The discussions reveal ongoing challenges with CUDA kernel implementations, including optimizing parallel binary searches without branch divergence, and debugging issues like consistent zero matrix results in cusparse operations. There's concern about the complexity of reorganizing internal operator registrations to colocate definitions with options, citing limitations in current API constructors and the intractability of reordering registration code. Some issues involve build failures caused by compiler compatibility, mismatched static/dynamic linking settings, and unsupported features like `torch.triu` in ONNX. There's also a recurring theme of handling special tensor types (e.g., bit-packed, sparse, nested) and maintaining backward compatibility, especially around schema and operator registration, while some discussions focus on improving user-facing documentation and testing to ensure robustness. Unresolved questions include how to better support custom build configurations, handle complex operator registration patterns, and address hardware-specific build limitations."
2020-02-06,pytorch/pytorch,"The comments reveal ongoing challenges and discussions regarding data loading concurrency issues in PyTorch, with solutions like reducing `num_workers` or setting shared memory sharing being explored. There are also technical debates about the design and API consistency for functions like `one_hot`, `logdet`, and large model export to ONNX, emphasizing the importance of exposing `dtype`/`device` parameters and handling large models efficiently. Several reports concern build failures, especially relating to compiler configurations, DLL loading problems on Windows, and compatibility issues with CUDA and specific hardware like V100 GPUs, pointing towards needs for environment validation, better error messaging, and targeted fixes (e.g., for in-place ops or flaky tests). Discussions about integration with frameworks like TorchScript, RPC, and profiling tools highlight ongoing efforts to improve usability and extensibility, but often with unresolved questions about backwards compatibility, performance overheads, and support for advanced features like sparse matrices or nested scopes. Overall, the issues reflect a mixture of stability, usability, and performance optimization challenges that require coordination across testing, API design, and build infrastructure."
2020-02-07,pytorch/pytorch,"The comments mainly revolve around troubleshooting and improving PyTorch's functionality and build stability. Several issues discussed include the importance of defining model classes before loading to resolve pickling errors, handling GPU memory management, and refining automatic differentiation behavior (e.g., version sharing, views, in-place operations). There are ongoing efforts to enhance performance (like profiling, operator fixes, and batching support), ensure compatibility (Windows build issues, large model support), and improve API consistency (e.g., autograd, `__torch_function__`, index promotion). Flaky tests caused by network instability or timeouts are also frequently noted. Overall, the conversations focus on bug fixing, performance optimization, and robustness of PyTorch's core features."
2020-02-08,pytorch/pytorch,"The discussions highlight challenges in implementing multi-model deployment and scaling in PyTorch's model serving ecosystem, emphasizing the need for a management architecture that supports efficient loading, versioning, and high availability across clusters. Concerns include the complexity of orchestrating model updates, synchronizing state across inference nodes, managing dependencies, and ensuring load balancer integration without race conditions. Several proposals suggest separating inference and management services, utilizing a central ""model registry"" for lazy-loading models, and leveraging load balancers with consistent hashing for stability. Additionally, questions remain on handling non-JIT models, dependency packaging, serialization, and maintaining compatibility with existing workflows. Overall, the community seeks a scalable, user-friendly solution that simplifies deployment, version control, and resource utilization while maintaining robustness and flexibility."
2020-02-09,pytorch/pytorch,"The collected comments highlight multiple ongoing technical issues and development discussions in PyTorch, including concerns over data types inconsistencies and improvements for gradient handling, specifically the need to update tensor `__deepcopy__` behavior and address gradient sharing issues with `.grad` attributes. Several entries discuss deep autograd and graph management, such as resolving recursive graph errors, in-place operation warnings, and handling views and version counters to prevent memory leaks and incorrect gradient computations. Compatibility and serialization versioning are also emphasized, with proposals for more explicit TorchScript file format versions and backporting support. Additionally, there are operational and build environment challenges, like CUDA version support, driver mismatches, and CI build failures, especially on specific platforms or with certain dependencies. Overall, the discussions seek to enhance robustness, debugging clarity, and compatibility of PyTorch’s core functionalities."
2020-02-10,pytorch/pytorch,"The discussions cover a range of technical issues in PyTorch development, including ongoing efforts to port kernels from TH to ATen, support for complex numbers and out-of-tree extensions, and compatibility and build system challenges, particularly on Windows and with specific CUDA versions. There are concerns about ensuring deterministic behavior and reproducibility in operations like scatter, as well as maintaining BC (backward compatibility) in line with numpy's promotion rules. Some issues involve ensuring proper CUDA integration, handling tensor data types and memory formats (especially with MKLDNN), and improving APIs for autograd, autograd profiler, and serialization. Additionally, multiple discussions highlight build failures, flaky tests, and the need to rebase or fix upstream issues to stabilize the CI pipeline and support advanced features like forward-mode autodiff and distributed training."
2020-02-11,pytorch/pytorch,"The discussions highlight several concerns: Firstly, the potential complexity of reordering or grouping native function registration code, with constraints on constructor availability and the need for possible codegen modifications, complicate API restructuring. Secondly, implementing features like nested tensors or variable-length argument lists in `native_functions.yaml` presents challenges, such as support for such argument types or the impact on user-facing APIs, with some options like TensorList being limited or not supported for empty inputs. Thirdly, there are technical questions regarding error handling and performance measurement, such as how to properly detect CUDA out-of-memory conditions, handle multi-threaded profiling, or accurately benchmark GPU performance considering asynchronous execution. Lastly, some comments point out specific bugs or design trade-offs, including issues with version tracking in view operations, implementation details of custom bindings with torchbind, or the nuances of numerical types like complex tensors, all requiring further investigation or thoughtful API design."
2020-02-12,pytorch/pytorch,"The comments reveal ongoing work and challenges across various areas of PyTorch, notably CUDA implementation bugs, backend and distributed system improvements, and model serialization issues. Several discussions focus on debugging and performance benchmarking, especially concerning sparse matrix operations with cuSPARSE, kernel dispatch logic, and quantization deployment, often emphasizing the need for careful profiling and incremental testing. There are concerns about robustness and compatibility, especially with different CUDA versions, and the importance of leveraging proper build, testing, and CI workflows, including external tools like ghstack and profiling utilities. Some comments highlight specific implementation details such as handling of complex numbers, efficient locking mechanisms in autograd, and serialization of quantized models, along with questions about documentation, code style, and API design consistency. Overall, the discussions underscore the necessity for rigorous testing, thoughtful API adjustments, and strategic reorganization to enhance stability, performance, and usability in complex system components."
2020-02-13,pytorch/pytorch,"The repository discussions cover a wide range of issues, primarily focusing on implementation and compatibility challenges. Notable concerns include ensuring code like `LocalLinear` works with image inputs, resolving CUDA and cuDNN version mismatches, addressing interpretability and automatic differentiation in complex models, and improving the build system across platforms (Windows, Linux, macOS). Several discussions emphasize fixing test failures, build regressions, and addressing conflicts in upstream dependencies such as gtest and CMake configurations. Additionally, questions about serialization of device objects, enabling deterministic algorithms, and improving API consistency (e.g., for `scatter` and `cumprod`) are highlighted. Overall, unresolved issues largely relate to build stability, testing reliability, and precise handling of deep learning primitives in diverse environments."
2020-02-14,pytorch/pytorch,"The discussions cover a range of topics including the support for 64-bit indices in sparse matrix operations, the design and usability of deterministic algorithms and their interaction with RNG seeds, and API improvements for operations like `packbits`/`unpackbits`. There are suggestions to make features more configurable (e.g., passing a `bool deterministic` argument), to better expose internal flags, and to improve granularity and control over algorithm choices for reproducibility and performance. Multiple concerns about build failures and flaky tests are noted, often related to upstream issues or CI configuration, rather than core code. Additionally, discussions about API consistency, testing practices, and the impact of certain modifications—like shape constraints on `out` parameters—highlight ongoing efforts to balance correctness, flexibility, and usability."
2020-02-15,pytorch/pytorch,"The main concerns across these discussions include the persistent issue with deep copy and gradient preservation of PyTorch tensors and models, especially when parameters are manually updated or deep-copied (Issue #3307), indicating that a fix or workaround is still needed. There are questions about CUDA support, compatibility, and performance optimizations, such as supporting 64-bit indexing in reductions (Issues #32866, #33310) and fixing specific CUDA errors and performance regressions. Several discussions highlight build failures and flaky tests on CI infrastructure, often caused by upstream issues, environment mismatches, or platform-specific errors, requiring reruns or upstream fixes. Additionally, there are suggestions about API design and user-facing features, such as adding support for `List[rpc.RRef[torch.Tensor]]` (Issue #32815) and improving pass registration flexibility (Issue #33261). Overall, unresolved issues include maintaining backward compatibility, optimizing specific tensor operations, ensuring robust CI testing, and enhancing user API support."
2020-02-16,pytorch/pytorch,"The discussions highlight recurring issues with PyTorch package compatibility and installation, such as mismatched CUDA and cuDNN versions, and advice to install specific wheel versions to match environment configurations. Several comments suggest that using `-c soumith` or old instructions should be avoided, and that installing torchvision from source or with compatible versions may prevent reinstallation of PyTorch itself. Performance discussions include questions about cuDNN's impact on execution speed, especially when its support and the environment setup are uncertain, like on a Linux server with missing cuDNN installations. Additionally, there are technical concerns about model conversion and graph analysis in JIT, with suggestions on how to access and parse model components and improve deployment tooling without relying heavily on ONNX or outdated graph representations. Overall, unresolved issues involve ensuring environment compatibility, improving user guidance for installation, and enhancing framework transparency and performance diagnostics."
2020-02-17,pytorch/pytorch,"The discussions highlight several technical issues in the PyTorch repository, including the need to correctly implement padding in the `LocalLinear` class, as pointed out by shir994, and the ongoing deep copy limitations for non-leaf tensors, which remain blocked due to `RuntimeError` exceptions. Additionally, users report problems related to thread management and resource control when using `torch.set_num_threads()` and environment variables like `OMP_NUM_THREADS`, especially with subprocesses and DataLoader workers, notably on Windows and ARM platforms. Sparse matrix support issues stem from CUDA's limited 32-bit index support, with suggestions to add a `deterministic` flag for algorithms, and fixes for performance regressions have been addressed. Many failures in CI builds are identified as upstream or environment-specific, with occasional patch application issues (e.g., failed patches in external scripts), and some failures are acknowledged as flaky or related to external dependencies like tensorboard or driver bugs."
2020-02-18,pytorch/pytorch,"The comments highlight ongoing discussions and proposals related to enhancing PyTorch's functionality, such as adding methods like `.extend` to `nn.Sequential`, improving autograd support for in-place operations like `clamp_`, and supporting complex number operations via torchbind. Several issues involve ensuring backward compatibility and correct behavior in edge cases, such as handling variable sequence lengths in RNNs, and managing model serialization/deserialization. There are also multiple mentions of build and environment stability, including fixing broken CI jobs, addressing memory and linking problems with CUDA, and clarifying behavior of functions like `argmax`. Some discussions involve API usability improvements, such as accepting numpy scalars, documenting behavior, and API signatures, with questions about future roadmap directions for features like flow-based models and complex numbers support. Overall, unresolved questions include implementation details for custom types, in-place backward ops, and comprehensive testing strategies."
2020-02-19,pytorch/pytorch,"The discussions highlight several technical issues, including GPU memory leaks potentially linked to cuDNN (fixed in newer versions), improvements in sparse matrix operations, and enabling AVX512 support on Intel Ice Lake CPUs, with considerations for thermal throttling effects. There's ongoing work to enhance the autograd engine for asynchronous operations, requiring careful management of thread-local states and parent-child relationships, possibly via shared_ptr. Challenges with exporting models to ONNX, especially handling variable-length sequences with torchscript and scripting, are noted, with suggested workarounds involving dynamic axes during export. Numerous build failures due to upstream issues, compiler incompatibilities, and environment configuration problems are prevalent, indicating a need for better build and test stability. Overall, the focus remains on improving performance, interoperability, correctness, and CI reliability amidst complex dependency and platform constraints."
2020-02-20,pytorch/pytorch,"The discussions primarily revolve around enhancing the PyTorch codebase, including verifying and improving the implementation of bidirectional RNN models (Type-1 vs. Type-2 behaviors), introducing support for both modes in C++ and CUDA libraries, and ensuring performance and correctness. There are ongoing efforts to optimize tensor indexing, memory pinning, and large tensor handling to improve speed and stability, along with addressing issues related to serialization, backward compatibility, and framework-internal API robustness. Additionally, support for external frameworks like Petastorm and ONNX export, and enhancements to distributed training and GPU fusion, are discussed to improve deployment, scalability, and interoperability. Many issues remain unresolved, mainly due to upstream dependency failures, infrastructure setup, or need for further benchmarking and refactoring."
2020-02-21,pytorch/pytorch,"The comments highlight recurring issues and potential enhancements in the PyTorch codebase. Notable concerns include the need to update the `__deepcopy__` implementation for Tensors to support `.grad` leaf tensors, and the potential for improved support for deep copies of `.grad`. Several discussions revolve around the proper handling of broadcasting, indexing, and memory management to prevent memory leaks and ensure consistent behavior across CPU and GPU. There are also requests for clearer documentation around complex modules like RNNs and the bidirectional structure, and for better support for distributed training, including DDP integration with RPC and autograd. Some comments point to external or upstream issues (like ONNX compatibility, C++17 features, and build failures) that require further investigation or patching."
2020-02-22,pytorch/pytorch,"The discussions highlight a recurring issue with implementing and integrating custom loss functions, such as Dice loss, into PyTorch training pipelines, emphasizing the importance of correct tensor requirements and grad handling. Several comments address operational challenges like memory limitations in Docker, Dataloader crashes, and compatibility issues across different platforms and PyTorch versions, pointing to potential configuration adjustments or code refactoring. There's also considerable concern about the complexity and review process of substantial PRs, with suggestions to simplify registration mechanisms and avoid unnecessary in-place operations that impair performance. Moreover, questions about proper device management, export/import behaviors, and framework limitations (e.g., support for DDP with remote modules) indicate ongoing development uncertainties. Overall, the main concerns revolve around ensuring correct tensor gradient flow, managing system resources efficiently, and simplifying code modifications for better maintainability and upgradeability."
2020-02-23,pytorch/pytorch,"The discussions address several technical issues including the proper management of dependencies and environment configurations (e.g., reinstalling Pillow, setting CUDA device in CPU-only environments), and the challenges of environment-related errors such as illegal CUDA memory access and segmentation faults. Several comments suggest updating code to current versions or modifying configurations (e.g., disabling Ninja, adjusting opset versions for ONNX export, handling the detachment in optimizers). There are concerns about performance regressions and slowdowns after migrating from older PyTorch versions, as well as issues with floating-point and boolean tensor operations not being well-supported or causing errors. Additionally, some discussions highlight the importance of reproducibility, proper test setups, and the need for clearer, more maintainable code, especially around distributed training and graph transformations."
2020-02-24,pytorch/pytorch,"The comments reveal issues related to build system configurations, such as the impact of using Ninja versus Make and thread/job management, which can cause compilation errors or failures. There are ongoing concerns about mismatches or bugs in CUDA/cuDNN/NN libraries, including potential incompatibilities with certain GPU architectures or CUDA versions, and the need for better diagnostics or error handling. Several bug reports involve performance regressions or correctness issues with specific tensor operations (e.g., `conj()`, `masked_select`, index handling), sometimes requiring more detailed documentation or more robust implementation that accounts for hardware differences. Discussions also highlight the importance of documentation improvements, better testing coverage for edge cases (like non-contiguous tensors), and ensuring that the CI/CD pipeline remains reliable amidst frequent upstream or environment-related failures. Overall, questions remain about compatibility, performance optimizations, and robustness of CUDA features and build processes, with some plans for incremental fixes and improvements, while other issues await further upstream or infrastructural support."
2020-02-25,pytorch/pytorch,"The comments cover several technical discussions related to PyTorch's internal API consistency, experimental features, and performance considerations. Notably, there are debates on whether to unify the input shapes for linear and convolutional layers, with concerns about breaking backward compatibility and API clarity across domains like NLP and vision. Issues also revolve around improving the autograd system, especially handling constants and in-place operations, and enhancing MPI-like distributed RPC mechanisms, including tensor transport and slice operations. Additionally, there are technical questions on threading, synchronization, and build failures, as well as discussions on FPGA support, quantization, and pruning, indicating ongoing efforts to expand hardware and algorithm support. Many unresolved questions pertain to API robustness, preserving backward compatibility, and improving performance monitoring and debugging tools."
2020-02-26,pytorch/pytorch,"The comments highlight ongoing challenges with building and integrating PyTorch, particularly issues related to build configuration, ABI compatibility, and environment setup. Several threads discuss specific build errors, such as undefined references, incompatible library versions, and platform-specific problems (e.g., Windows CUDA setup, Linux compilation errors). There are concerns about build system choices (CMake vs. traditional), ABI and library version mismatches, and test flakiness, especially on Windows or when using specific hardware. Some discussions propose solutions like adjusting environment variables, reworking build scripts, and better error handling or warnings for users. Unresolved questions include how to properly support different ABI versions, ensure reproducibility of results, and streamline complex environment configurations across platforms."
2020-02-27,pytorch/pytorch,"The comments primarily revolve around recent build and test failures, many caused by upstream issues or environment misconfigurations, especially related to CUDA, cuDNN, and external dependencies like scipy and scipy/scikit-learn. Several discussions concern the stability and reproducibility of certain features such as CUDA memory management, non-determinism issues in operations like `repeat_interleave`, and serialization bugs in sparse tensors. There are also suggestions for improving testing practices, such as stricter pinning of dependencies in CI and better handling of device affinity in distributed RPC, alongside concerns about build system complexities on various platforms. Overall, the key unresolved issues include fixing upstream failures, improving the stability and determinism of GPU operations, and refining CI environment management."
2020-02-28,pytorch/pytorch,"The discussions highlight several critical points: Implementation of `torch.multiprocessing.set_sharing_strategy('file_system')` should be properly documented for correct placement within user code; modifications to internal or deprecated autograd functions, such as for SoftPlus or SmoothL1Criterion, require detailed code updates and verification to avoid runtime errors; support for mixed device tensors in distributed RPC and autograd systems entails careful handling of device affinities and error reporting, possibly through stream synchronization or explicit device contracts; testing and benchmarking are necessary when introducing new code paths, especially for performance-sensitive operations like `all_gather`; and ongoing CI test failures, often due to upstream or environment-specific issues, suggest a need for robust failure classification and potential infrastructure improvements."
2020-02-29,pytorch/pytorch,"The discussions focus on memory management and efficiency optimizations in PyTorch, including reducing CUDA out-of-memory errors by adjusting batch sizes, optimizer choices, and cache clearing. There is interest in developing flexible, DDP-compatible metric APIs modeled after Ignite, emphasizing making metrics as modular as `nn.Module` with proper distributed handling. Several issues relate to build failures, mostly stemming from upstream problems, build configuration, or compatibility issues, especially on Windows and macOS, with some discussions on fixing by code changes or environment setup. Concerns exist about maintaining deterministic behavior in scatter operations, especially porting scatter reduction to ATen, with solutions involving sorting indices for deterministic CUDA operations. Some core problems involve build system consistency, operator registration, and ensuring proper runtime behavior for scripted modules and custom operators."
2020-03-01,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch repository, including intermittent and rare bugs such as illegal instruction signals and segmentation faults potentially caused by CPU instruction set mismatches (e.g., AVX2/AVX). There is also concern over build failures related to environment-specific issues like missing dependencies, merge conflicts, and upstream upstream breakages, often tied to complex CI testing environments. A recurring suggestion involves providing manual control over CPU feature detection, such as setting `ATEN_CPU_CAPABILITY`, to bypass problematic instruction-specific code paths. Additionally, some compile-time errors relate to deprecated or incompatible code patterns, such as the use of lambdas with captures for operator registration, and specific bugs in libraries like SciPy's eigenvalue solvers and XNNPACK's C++ class registration. Unresolved questions include how to improve robustness across diverse hardware and environments and how to refine or disable problematic code paths without sacrificing performance."
2020-03-02,pytorch/pytorch,"The comments encompass a wide range of issues, including proper placement of multiprocessing sharing strategy commands, handling of deprecated or missing functionalities (such as complex number operations and non-contiguous tensors), performance regressions (notably in batch operations and integer computations), and build configuration challenges (especially with dependencies like MKL, cuDNN, and compiler environments). Some discussions involve fixing or improving the reproducibility, determinism, and correctness of specific operations (like scatter, reduction, and hash functions), as well as addressing compatibility issues across different hardware (CPU, GPU, ROCm, XLA) and software setups (PyTorch versions, build systems). There are also ongoing concerns about CI failures, dependency pinning, test coverage, and code maintenance (e.g., proper documentation, rebase conflicts, and API stability). Overall, the main themes revolve around stability, performance optimization, environment consistency, and ensuring correctness across diverse use cases."
2020-03-03,pytorch/pytorch,"The comments highlight several key issues within the PyTorch codebase, including the need for safer and more deterministic CUDA kernel implementations (e.g., scatter operations using sorting to ensure determinism), performance regressions and their potential causes (such as the impact of `cpu_kernel()` dispatcher), and the importance of aligning operator registration and type handling with modern best practices (such as using the new op registration macros and ensuring backwards compatibility). Unresolved questions include the proper handling of mixed data types for tensor operations like `remainder`, the correct API design for metrics and their integration with distributed training, and clarification of certain internal behaviors such as the semantics of `Operator` constructors or hooks regarding tensor lifetimes. Additionally, there's a recurring theme of CI build instability due to upstream or environment issues, which complicates merging and testing new features. Overall, the discussions emphasize improving code safety, performance consistency, backward compatibility, and robust testing for new features and internal refactoring."
2020-03-04,pytorch/pytorch,"The discussions predominantly revolve around internal implementation and API clarifications within PyTorch, including the semantics of default data types and how promotion rules apply in various contexts (e.g., combining float16 with float32 or integer tensors). There are concerns about backward compatibility, particularly regarding default behaviors in tensor operations, and how these should be communicated or managed through API updates. Several threads address technical issues such as kernel build errors, linker errors, and test failures caused by missing or incompatible headers, environment setup, or upstream breakages, often requiring rebases, environment adjustments, or code reverts. A notable theme is the emphasis on improving robustness and clarity for both users and developers, including handling missing files, improving test reliability, and clarifying the behavior of specific functions (e.g., `torch.unique`). Unresolved questions include the precise promotion rules when combining complex types or mixed precisions, and how to consistently handle deprecated or incompatible build configurations across different platforms and dependencies."
2020-03-05,pytorch/pytorch,"The comments suggest ongoing discussions around expanding PyTorch's support for sparse matrices, including performance optimizations, new API features such as mixed sparse-dense support, and adding operations like block-sparse modules, with considerations on compatibility and BC-breaking impacts. Several issues also touch on build failures, build environment problems, and CI test flakiness caused by upstream failures or environment misconfigurations, indicating the need for more robust testing and environment setup. Some comments highlight known bugs or limitations in toolchains (e.g., CUDA, compiler errors in MSVC), with suggestions for improvements or workarounds, including code refactoring and API stability considerations. Additional discussions involve model exporting, ONNX compatibility, and dynamic tensor shape handling, with a focus on debugging, testing, and maintaining backward compatibility while advancing features. Overall, many comments reflect active development challenges, performance optimizations, platform support issues, and the effort to improve stability and usability in the PyTorch ecosystem."
2020-03-06,pytorch/pytorch,"The collected comments reveal multiple partial or full code refactoring efforts, including unification of tensor operations, improvements to autograd, and interfacing with different backends (CPU, CUDA, XLA, etc.). Several issues highlight ongoing compatibility and correctness challenges, especially around tensor storage handling, broadcasting, intra-operator assumptions, and device synchronization issues. Discussions also touch on the need for better error reporting, more robust testing, and avoiding BC breaks, particularly in relation to tensor sharing, kernel fusion, autograd backward functions, and operator schema evolution. Many failures appear related to external dependencies, environment configuration, or incomplete build images, rather than fundamental algorithmic bugs. Overall, unresolved questions include how to handle device-specific operations seamlessly, improve diagnostics, and maintain long-term compatibility while evolving the internal APIs."
2020-03-07,pytorch/pytorch,"The discussions reveal multiple ongoing technical concerns, including debugging intermittent and random GPU or kernel issues related to specific CUDA or cuDNN versions, and the need for thorough performance profiling and kernel optimization, especially for depthwise convolution workloads. Several issues address backward compatibility and precise gradient handling, such as the appropriate mode for backward operations and the potential restructuring of distributed autograd context management to handle multi-threading more safely. There are also recurring questions about build failures and integration with external components like torchvision, protobuf, and external Docker images, often due to dependency or environment mismatches. Lastly, some discussions focus on enhancing the internal automation and testing infrastructure, including better failure detection, code cleanup, and precise test re-execution, as well as ensuring a smooth transition for deprecated or overly specialized list operations in the JIT system."
2020-03-08,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch development, including difficulties in contributing new activation functions and handling specific model support like ONNX export issues, especially with operations such as Resize and NonMaxSuppression. There are technical concerns around build failures and compatibility, notably with CUDA versions, CUDA-related runtime errors (e.g., ""unspecified launch failure""), and problems with linker errors that may involve preprocessor or compiler support (e.g., clang on Windows). Several issues pertain to build environment stability, manifesting as failed Docker image pulls and segmentation faults during tests, which may impact future usability and stability. Additionally, there is interest in improving runtime error detection (e.g., autograd anomaly detection) and expanding platform support (e.g., MIPS architecture), along with interest in better documentation and contribution workflows."
2020-03-09,pytorch/pytorch,"The discussions highlight several key issues in PyTorch development: the approach to exposing internal modules for auto-completion and type hints, with debates about whether to switch from `import *` to explicit imports or type hinting; handling of shared memory tensors, especially regarding potential memory leaks in DataLoader and improving error reporting for worker crashes; ongoing work to integrate CUDA/cuDNN functionality more efficiently, including implementation details of multi-head attention (MHA) and performance benchmarks; the need for better broadcast, distributed communication, and testing infrastructure; and addressing build failures due to missing modules (like setuptools) or internal code errors caused by code updates or merge conflicts, particularly in the context of ONNX export, JIT scripting, and operator registration. Many unresolved questions concern error diagnostics, performance impacts of code changes, build stability, and compatibility with different hardware and software environments."
2020-03-10,pytorch/pytorch,"The comments reflect a variety of issues and discussions within the PyTorch development community, including improvements to the `__torch_function__` protocol, migration and compatibility concerns across versions, and performance optimizations for tensor operations and distributed training. Several threads involve fixing bugs related to CUDA kernels, memory handling, and build failures, often suggesting use of contexts like `no_grad()` or refactoring internal mechanisms. Others discuss feature enhancements such as adding API flags, extending support for ONNX, and refining testing infrastructure. Unresolved questions mainly pertain to performance impacts of new code paths, backward compatibility, and the appropriate placement of internal APIs, with community input guiding the proposed solutions."
2020-03-11,pytorch/pytorch,"The comments reflect a broad set of issues encountered in the PyTorch codebase and build environment. Key concerns include compatibility and correct handling of threading, especially related to distributed autograd and the lifecycle of engines; build failures due to missing dependencies or conflicts, notably on Windows, macOS, and CI images; and a need for clearer API and documentation improvements, such as for `torch.norm`, `__torch_function__` behaviors, and the handling of in-place operations. Some discussions focus on the implementation details of linear algebra routines on different hardware backends (MAGMA vs cuSolver), and optimizations for small matrix operations. Several failures are identified as upstream or flaky, often due to environment issues like Docker images, network problems, or external library bugs, which require further investigation or environment reconfiguration. Unresolved questions include the best practices for extending APIs like `__torch_function__`, supporting new backends, and handling the tensor lifecycle in advanced use cases."
2020-03-12,pytorch/pytorch,"The comments reflect ongoing issues and feature discussions within the 'pytorch/pytorch' repository, including troubleshooting paths for specific build/hardware issues, implementation strategies for backend-specific versus generic operator registrations, and concerns about code maintainability and proper documentation for new features like quantization and custom extensions. Several threads highlight bugs or potential regressions in the build system, CUDA interactions, and API evolution, emphasizing the need for clearer documentation and more robust testing (e.g., with gdb or internal tests). There are discussions about API design choices—whether to expose more explicit control over memory formats or to simplify the API with default behaviors and optional overrides, especially for complex modules like RNNs or custom operators. Additionally, the reviews indicate the importance of maintaining consistency and clarity across build configurations and ensuring compatibility with external tools like ONNX, while also managing ongoing internal and external CI failures."
2020-03-13,pytorch/pytorch,"The discussions predominantly center around technical challenges in PyTorch development, such as managing large GPU/CPU memory usage (Issue #7318), ensuring correctness and performance of distributed autograd (Issue #27096), and handling skippable tests and build failures due to upstream or environment issues. Specific concerns include the intricacies of CUDA kernel memory allocation (Issue #12873), the validity of extending community methods with low citation, and the stability of features like GLOO support on macOS and multi-GPU setups on Windows. There are also ongoing efforts to optimize model export, particularly for onnx and JIT, with debates on whether to inline functions or move toward native operator support (Issue #34718). Unresolved questions include how to effectively reduce IR complexity after operator decomposition, improve performance inconsistencies in certain kernels, and handle build failures caused by external dependencies or environment mismatches."
2020-03-14,pytorch/pytorch,"The discussions highlight several areas for potential improvement in PyTorch's internals and user experience. A key concern involves error reporting in multi-process DataLoader usage, where the socket FD behavior can mask underlying bugs or interfere with the expected number of open files, suggesting a need for better signaling or resource management. Another issue pertains to the use of backends like XNNPACK and whether operator dispatch should be backend-specific or value-agnostic, with considerations on serialization and fusion capabilities. Additionally, the complexity of `__torch_function__` handling, especially for tensor lists, raises questions about the impact on runtime overhead and error propagation, with some proposing refactoring to delay `has_torch_function` checks until argument parsing. Lastly, a recurring theme is handling build and test failures caused by upstream changes or environment limits, which may involve rebasings, environment adjustments, or adding specific tests to catch regressions early."
2020-03-15,pytorch/pytorch,"The discussions highlight several key technical areas: (1) Addressing and troubleshooting build and compatibility issues, such as dependencies on specific system libraries like glibc, build failures due to upstream repository or environment misconfigurations, and environment variable effects on performance. (2) Handling complex tensor operations and modeling, such as correct usage of `grid_sample` with normalized coordinates, implementing efficient block-sparse support, and ensuring proper handling of complex number operations for hardware acceleration. (3) Improving API design and usability, including development of a caching system for parametrizations, implementing a checkpointing decorator for memory-efficient training, and precise control over random number generation with `torch.randperm`. (4) Ensuring backward compatibility and addressing backward incompatibility warnings, particularly in operator schemas. (5) Overall, these discussions involve resolving build failures, refining APIs for better usability, and extending hardware and algorithm support, with some unresolved questions about implementation details and system limitations."
2020-03-16,pytorch/pytorch,"The discussions highlight concerns about handling worker process crashes in PyTorch's data/parsing infrastructure, with suggestions to improve crash reporting (Suggestion 1) and warnings about worker tensor lifetime (Suggestion 2). There are debates about the design of a with-statement IR representation, with a preference for a node-based approach (Enter/Exit blocks) for better handling of early returns, break, continue, and exceptions. Additional issues involve compatibility and correctness of the `torch.nn` module's codebase, with certain errors stemming from macro redefinitions and incomplete includes, and some build failures linked to environment constraints or missing files. Overall, unresolved questions include how to effectively track and manage errors in multi-threaded worker scenarios, how to safely extend IR and interpreter representations for context managers, and how to balance backward compatibility with new design choices."
2020-03-17,pytorch/pytorch,"The comments reveal multiple ongoing issues and discussions: several PR merges are faced with build failures on specific platforms, often due to missing files, upstream breakages, or environment misconfigurations; there are multiple bugs in the C++ core, especially related to incomplete type declarations like 'c10::Type' which cause compilation errors, likely due to missing includes or version incompatibilities; the handling of thread-local error states in asynchronous contexts (such as in a new autograd engine) requires careful error persistence and management of GIL scope, with concerns about the safety of failures in worker threads; there's also architectural discussion about how to represent Python's 'with' statements in TorchScript IR—considering either explicit 'Enter/Exit' nodes with the scope preserved or embedding try/finally semantics, each with their own tradeoffs; finally, there are various feature requests and questions about future support for complex types, API design, and potential regressions in performance or correctness that need further investigation."
2020-03-18,pytorch/pytorch,"The comments reflect ongoing discussions on multiple GitHub issues surrounding PyTorch development, including model loading practices, support for sparse operations, and backward compatibility concerns. Several issues involve technical implementation challenges, such as correctly handling model state_dict serialization, integrating sparse tensor support without complex API changes, fixing build errors on various platforms due to missing dependencies like cuda_runtime_api.h, and mitigating compiler errors related to incomplete types or macros. There are also debates about API design choices, such as adding equality comparisons for distributions, managing autograd thread-local state in distributed training, and best practices for module scripting and quantization. Many of these issues remain unresolved or are undergoing re-evaluation, with some requiring patches, split PRs, or specific workarounds to address build failures or design limitations."
2020-03-19,pytorch/pytorch,"The discussions reveal several recurring technical concerns:

1. Operators such as `aten::inverse` and `aten::gesv` are missing in JIT/Tracing, causing crashes when using scripted models; resolving this requires adding operator registration or custom symbolic functions for ONNX export.  
2. Memory leaks and efficient data loading in DataLoader sometimes depend on using numpy arrays versus Python lists, and altering `num_workers` settings; monitoring shared memory (`/dev/shm`) is recommended for troubleshooting leaks.  
3. Cross-device tensor operations like `torch.cat([cpu_tensor, cuda_tensor])` are being addressed with incremental patching, but ensuring correct types and promotion rules remains complicated.  
4. Build failures on various platforms often originate from environment incompatibilities, missing dependencies (e.g., `past` module), and GCC compatibility issues, especially with CUDA and compiler version mismatches.  
5. Enhancements such as custom averaging in SWA, inlining in JIT, and handling dynamic shape inputs in ONNX are proposed, with ongoing work needed to fully support these features, especially ensuring cross-platform and release-compatible code."
2020-03-20,pytorch/pytorch,"The discussions primarily revolve around troubleshooting and improving PyTorch's internal development and build processes, with particular concerns about build failures, test flakiness, and compatibility issues on various platforms (Windows, macOS, Linux, CUDA/ROCm, etc.). Several comments highlight specific errors, such as compilation errors due to ambiguous or missing definitions, deprecated functions, or features not supported on certain hardware or configurations. There are also strategic considerations about infrastructure, including dependency management, build system changes, and feature implementation details like SWA schedulers or operator registration. Unresolved questions include improving error diagnostics, managing cross-platform compatibility, and ensuring the stability of experimental features before merging large refactors."
2020-03-21,pytorch/pytorch,"The discussions mainly revolve around debugging and optimizing PyTorch's internal memory management, operator fusion, and graph representation issues, often focusing on specific error patterns like 'C2672' and 'invalid 'asm''. Concerns include ensuring support for custom or derived modules in features like batch normalization folding and in-place views, with suggestions to potentially restrict in-place operations when in compatibility with autograd. There are questions regarding PyTorch's compatibility with CUDA, cuDNN, and system-installed drivers, as well as how to properly install or build specific versions, sometimes with workarounds or environment modifications. The logs also highlight prevalent build failures, often related to compiler differences (MSVC, Clang) and code generation issues, such as macro expansions and assembly string errors, with some discussions about module support in Python/C++ and build system updates. Residual unresolved issues include handling specific compiler errors, ensuring compatibility between forward/backward graphs, and managing platform-specific build regressions."
2020-03-22,pytorch/pytorch,"The discussions mainly revolve around extending certain PyTorch functionalities, such as support for batched `torch.bincount` and quantized model export to ONNX, with considerations for handling variable output shapes and operator support limitations. There are significant concerns regarding the proper memory management and synchronization mechanisms for CUDA resources, particularly ensuring the lifecycle of tensors aligns with GPU computations to prevent errors or undefined behavior. Several questions address the challenges in exporting quantized models to ONNX, with errors indicating missing operator support or handling of quantized weights. Additionally, there are technical inquiries into the internal implementation details of memory allocators, stream synchronization, and the accuracy of complex tensor operations, along with standard issues like code formatting and build CI failures. Overall, key unresolved issues include achieving reliable quantized operator support in ONNX export, ensuring proper CUDA memory lifecycle management, and enhancing operator compatibility and support for batched and complex tensor operations."
2020-03-23,pytorch/pytorch,"The comments predominantly highlight ongoing development issues and feature requests within the PyTorch repository. Key concerns include the lack of 'padding=same' in Conv2d, with proposed custom implementations for various padding and causal padding scenarios. Several discussions focus on improving the autograd graph and JIT compilation processes, including handling complex operators like `istft`, and optimizing tensor reductions and indexing operations. There are also recurring issues with CI build failures, flaky tests, and dependency management, often related to upstream breakages or environment inconsistencies. Notably, some discussions propose refactoring core functionalities (e.g., parametrization, memory alignment, custom ONNX operators) for better maintainability and performance, while unresolved questions revolve around compatibility, testing, and integration of new features."
2020-03-24,pytorch/pytorch,"The comments cover various technical topics related to PyTorch, such as implementing Maxout layers compatible with different input dimensions, addressing DataLoader and multiprocessing issues (notably on Windows and Linux platforms), handling CUDA and driver compatibility problems, and performance tuning (e.g., for convolution and reduction operations). Several discussions involve debugging build failures, CI/CD pipeline errors, and environment setup concerns, especially around CUDA, cuDNN, and Docker images. Other key points include enhancing ONNX export support, managing backward compatibility, and refining API design (like `SetLR`). Overall, unresolved questions include ensuring tests are robust and reproducible, optimizing performance (e.g., for reductions), and maintaining smooth CI/CD operations amidst evolving system dependencies and platform-specific bugs."
2020-03-25,pytorch/pytorch,"The discussions mainly revolve around implementation details and performance considerations of various PyTorch features, such as loss functions (Dice loss), linear algebra operations (svd, lstsq), ONNX export issues, and distributed training utilities like launch.py. Several issues pertain to bug fixes, compatibility (especially with CUDA, ROCm, and compiler issues), and ensuring correctness and stability of new features across different environments. There are ongoing concerns about test coverage, correctness of numerical results, and documentation accuracy, especially for complex functionalities like quantization, tensor shape semantics, and complex number support. Moreover, some discussions address build environment discrepancies, dependency management, and CI robustness to prevent regressions. Unresolved questions include how to properly support advanced features like custom complex types, handle operator semantics during export, and improve the automation and reliability of tests and debugging infrastructure."
2020-03-26,pytorch/pytorch,"The discussions primarily revolve around three main issues: (1) handling large-scale dataset loading and multiprocessing errors in PyTorch, including specific runtime errors like `ancdata` and shared library loading issues; (2) enhancement of PyTorch functionalities, such as adding `padding='same'` in convolution layers, supporting complex numbers, and improving performance benchmarks, with various suggestions for implementation strategies; and (3) debugging and improving system compatibility and build issues, especially related to Windows DLL errors, GPU support, and CI failures, often involving rebase or environment setup modifications. Additional concerns include ensuring correctness and usability of new features (like `torch.istft` in JIT), maintaining API compatibility with NumPy, addressing upgrade and porting challenges, and managing repo-specific workarounds for internal use. Many unresolved questions pertain to implementation decisions: whether to reimplement features in Python or C++, how to handle specific data layouts, and approaches to fixing build failures or compatibility issues without risking BC-breaking changes."
2020-03-27,pytorch/pytorch,"The discussions mainly revolve around optimizing gradient communication in PyTorch, particularly exploring gradient encoding/decoding for bandwidth preservation, and the implementation complexities involved. Several issues concern the proper integration and support of features such as custom SVD operations, complex number types, and tensor serialization, highlighting the need for robust APIs and compatibility with various hardware backends like ROCm and XLA. Performance considerations are also a recurring theme, with benchmarking and kernel optimizations being discussed to improve efficiency for specific operations such as convolution, matrix multiplication, and tensor transformations across different device types. Cross-platform compiler and build system challenges are evident, notably with CMake configurations, compiler bugs, and integration with external dependencies like MKLDNN, which impact stability and performance. Unresolved questions include best practices for maintaining API compatibility, handling device-specific kernel failures, and ensuring correct functioning for complex types and tensor conversions across different hardware and software setups."
2020-03-28,pytorch/pytorch,"The discussions predominantly revolve around runtime CUDA errors such as illegal memory access, device assertions, and device-side asserts, often triggered during model validation or in parallel/distributed settings. Several issues relate to environment configuration and compatibility, including CUDA and driver version mismatches, GPU support limitations, and the effects of switching devices or simulating failures in code. There are also technical concerns about computational accuracy with complex and floating-point operations, as well as performance trade-offs when modifying kernel implementations (e.g., PReLU backward kernels). Additionally, some discussions focus on build system nuances, such as handling CMake configurations on different architectures (x86 vs PPC), and ensuring proper imports and definitions for successful compilation. Unresolved questions include how to reliably prevent device assertion errors, how to handle device and data type consistency, and how environment or build setup influences runtime stability."
2020-03-29,pytorch/pytorch,"The discussions highlight several key issues: the interaction between OpenCV and PyTorch, where importing cv2 before torch can prevent segmentation faults; challenges with CUDA support on macOS and installation discrepancies; inconsistencies and ambiguities in PyTorch's handling of 'out' parameters (especially for functions like masked_select), suggesting a need for clearer documentation or redesigned behavior; and floating-point precision differences between CPU and GPU computations, which may not be bugs but artifacts of parallel summation algorithms. Additionally, there are ongoing efforts to extend PyTorch's support for quantized models, improve kernel implementations (e.g., clamp functions), and manage build system and dependency issues on different platforms. Many discussions also focus on debugging, supporting new features, and enhancing compatibility, with some unresolved questions around proper handling of tensor properties and integration of new data types."
2020-03-30,pytorch/pytorch,"The discussions in these GitHub comments revolve around several key issues: the proper handling of device and optimizer state when transferring models between CPU and CUDA, especially with regard to optimizer state dicts and device consistency; the implementation and performance optimization of custom CUDA operations like depthwise convolutions and complex number support, including the challenges of achieving hardware acceleration and ensuring correctness across different architectures; the compatibility and correctness of PyTorch's serialization, operator schema, and complex data type support, along with issues related to installation, package management, and build configurations across different platforms; the design and API of functions involving out parameters, in-place operations, and complex tensor conversions, as well as the integration of new features like autocasting and support for new data types; finally, the ongoing efforts to improve test coverage, CI reliability, and maintain backward compatibility while introducing new features. Many discussions involve code refactoring, patch submission, and review processes to address these challenges."
2020-03-31,pytorch/pytorch,"The comments touch on a variety of technical issues in PyTorch development, including the need for higher level abstractions in distributed computing, support for compressed neural network transfer, improvements in CUDA memory allocation, and enhancements in sparse tensor operations. Several discussions consider the implementation and testing of new features like support for complex tensors, index operations, and custom autograd functions, highlighting the challenges of ensuring backward compatibility, consistency between eager and traced models, and cross-platform support. There is also concern over build failures due to environment mismatches, deprecated APIs, or upstream bugs, emphasizing the need for better testing, stability, and documentation. Unresolved questions include how to maintain custom tensor types without forking core code, the best way to handle dynamic shape representations in ONNX export, and how to standardize error and NaN handling across different hardware and precision modes."
2020-04-01,pytorch/pytorch,"The discussions highlight several key concerns: support for `torch.int32` is lacking, especially affecting ONNX export and TensorRT compatibility, as casting from int64 to int32 degrades model accuracy; many test failures, particularly on Windows and involving CUDA, are linked to driver or library incompatibilities, undefined behaviors in integer conversions, or outdated codebases, necessitating rebases or fixes; specific feature requests include implementing `matrix_exp` naively as a temporary solution and extending support for complex number operations, with some feedback on handling dynamic vs static shapes in ONNX exports; there's also discussion about improving in-place operation safety, ensuring correct device handling with `to()`, and performance trade-offs in tensor communication for distributed execution. Several failures stem from upstream issues or environment misconfigurations, and there are ongoing efforts to refine API behavior, improve testing infrastructure, and address low-level implementation bugs."
2020-04-02,pytorch/pytorch,"The discussions cover a range of technical concerns including optimizing DataLoader worker processes (e.g., adjusting `num_workers`) to reduce training time, challenges with CUDA device handling especially when parameters are on multiple devices, and issues with reproducibility and determinism in operations like scatter, especially in the context of integration with TorchScript and ONNX export. There are also concerns about runtime errors related to linking and environment setup on various operating systems and hardware configurations, as well as performance regressions observed in custom CUDA kernels (e.g., depthwise convolution and grid sampling). Several API design questions are raised, such as the behavior of `module.device`, handling of dictionary outputs in TorchScript, and default behaviors for worker resource management. Overall, the discussions highlight ongoing efforts to improve PyTorch’s CUDA support, API robustness, and testing infrastructure amidst platform and hardware variability."
2020-04-03,pytorch/pytorch,"The comments reflect discussions on optimizing the usage of batch_first in RNN modules, suggesting setting batch_first=True or transposing inputs for consistency. There are ongoing efforts to improve the implementation of custom CUDA kernels for depthwise convolutions, exploring performance gains and the impact of different backends. Network-related issues are also discussed, including slow download speeds for binaries related to CDN configurations, and concerns about Windows and macOS compatibility, especially with specific compiler versions and kernel compilation errors. Some discussions address the deprecation and replacement of legacy Python bindings and serialization format changes, emphasizing backward compatibility. Lastly, there's ongoing review of CI failures, test coverage, and infrastructure updates, underscoring some unresolved bugs and the need for more robust testing strategies."
2020-04-04,pytorch/pytorch,"The discussions primarily revolve around implementation details and compatibility issues in PyTorch, including the proper use of `os.add_dll_directory` on Windows for loading dependencies like MKL and CUDA, with concerns about loading order and platform-specific DLL search paths. Several issues highlight CUDA-related errors, such as device asserts and mismatched behaviors across GPU types, which are sometimes mitigated by disabling `cudnn` or adjusting environment variables. There are also ongoing challenges with PyTorch's ONNX export support, especially related to unsupporteded or missing operators like `_thnn_fused_lstm_cell`, and the need for better support for complex data types in functions like `abs` and `isclose`. The community shows concern for test reliability, reproducibility, and ensuring consistent behavior across platforms, especially with updates and changes to underlying libraries like NCCL, CUDA, and compiler versions. Overall, the discussions suggest active development and troubleshooting efforts aimed at improving PyTorch's compatibility, performance, and feature support."
2020-04-05,pytorch/pytorch,"The discussions primarily revolve around memory management and allocation issues, particularly with GPU memory and PyTorch's auto-casting behaviors, which are being phased out due to potential inconsistencies and lack of warning mechanisms. There are questions regarding the compatibility and support of MKLDNN on Windows, with some users experiencing performance discrepancies and runtime errors, especially related to dilation support and specific operators like maxpool2d. Several issues highlight build and installation challenges, especially with conda environments, package versions, compiler compatibility, and dynamic shape handling in ONNX export. Additionally, concerns are raised about correct device handling in tensor creation, the stability of softmax/log_softmax computations, and the need for comprehensive testing, including complex number support, to avoid test regressions. Overall, these discussions point towards ongoing efforts to improve robustness, compatibility, and performance diagnostics in the PyTorch framework."
2020-04-06,pytorch/pytorch,"The discussions primarily revolve around technical issues such as handling string types in Python 2.7, memory allocation and fragmentation in PyTorch, and performance discrepancies between platforms like Windows and Linux. Several threads address the compatibility and correctness of PyTorch's serialization, model loading, and support for various hardware architectures (CUDA compute capabilities, CPU cache behavior). There's also focus on improving error messaging, optimizing specific tensor operations, and extending functionalities such as quantization, `grid_sample`, and custom autograd behaviors. Unresolved questions include whether certain features (like support for quantized models via ONNX) will be supported, how to handle backward pass errors due to in-place operations, and platform-specific build issues. Overall, the conversations involve bug fixes, performance tuning, and feature enhancements, with some ongoing investigations into stability and compatibility across different systems and configurations."
2020-04-07,pytorch/pytorch,"The discussions primarily revolve around improving documentation consistency and standards, particularly for functions shared between `nn.functional` and `nn`. Several issues concern model loading, especially related to model file compatibility and environment setups, highlighting version and configuration sensitivities. There are multiple synchronization and build failure reports across various systems, pointing to upstream breakages, environment mismatches, or build configuration problems, including the handling of dependencies like libtorch, FBGEMM, and CUDA libraries. A prominent concern is the proper management of tensor operations, especially with `masked_select` and `out` parameters, emphasizing correct expectations and behavior for user-supplied tensors versus internal results. Lastly, some discussions focus on extending functionalities (e.g., support for complex numbers, new operations) and code maintenance tasks such as formatting, porting TH functions to ATen, and fixing internal regressions."
2020-04-08,pytorch/pytorch,"The discussions highlight issues related to CUDA driver and driver instabilities, causing CUDA availability problems on some systems. Several comments concern the behavior and implementation of functions like `torch.argmax`, where differences between CPU, GPU, and numpy behaviors, as well as implementation workarounds, are debated. There are questions about a specific patch related to future and callback handling in the C++ API, emphasizing the need for clearer API design for error propagation and callback management. Additionally, multiple build failures (e.g., missing headers, compiler errors, submodule checkout issues) suggest challenges in maintaining cross-platform and multi-version compatibility, especially around the build environment, dependencies, and internal infrastructure. Overall, unresolved issues involve build environment consistency, function behavior semantics, and API usability improvements."
2020-04-09,pytorch/pytorch,"The comments indicate ongoing efforts to improve PyTorch's support for sparse autograd, dense-sparse operations, and complex number handling, highlighting current limitations such as lack of autograd support for sparse tensors, issues with element-wise multiplication between sparse and dense matrices, and incomplete autograd testing for complex tensors. Several discussions focus on performance optimizations, including kernel changes, compiler compatibility (notably with different GCC versions and CUDA architectures), and how to efficiently implement certain functions like `logcumsumexp` or `isclose`. The community emphasizes the importance of thorough testing, debugging, and documenting behaviors, especially regarding internal API changes, backward compatibility, and integration with ONNX. Additionally, there are infrastructure-related concerns, such as build failures due to compiler issues, CI environment constraints, and the need for better build and test automation, particularly on Windows and in distributed or mobile contexts. Unresolved questions remain about supporting new data types (e.g., bfloat16), proper handling of edge cases like infinities or equal elements in autograd tests, and ensuring performance gains without introducing divergence or maintainability burdens."
2020-04-10,pytorch/pytorch,"The GitHub comments cover various technical concerns including memory management issues on GPUs, such as out-of-memory errors despite sufficient reported free memory, solved by techniques like reducing the number of DataLoader workers or calling `torch.cuda.empty_cache()`. There are discussions about the implementation and behavior of PyTorch operations, especially regarding shape inference, broadcasting rules for `binary_cross_entropy`, and complex number support, including how to properly clamp real and imaginary parts. Several build and testing failures predominantly relate to incompatible C++ headers, upstream build system issues, or environment configuration problems, especially when compiling CUDA code or integrating with external dependencies like NCCL. Additionally, maintenance questions involve code generation versus manual registration for custom operators, inference on multi-GPU performance bottlenecks, and how to handle long-standing bugs or build regressions across different platforms and CI environments. Unresolved questions mostly concern optimization strategies, compatibility fixes, and ensuring consistent behavior across diverse hardware and software configurations."
2020-04-11,pytorch/pytorch,"The discussions primarily focus on optimizing and debugging depthwise convolution operations in PyTorch, including potential future support and performance improvements across different hardware and lib implementations. Several issues involve memory management, build failures, and compatibility with CUDA versions and compiler toolchains, especially on internal infrastructure or specific platforms like iOS and ARM. There are also questions about extending support for complex number operations, such as clamp functions for complex tensors, with suggestions to modify existing implementations or add new kernels, ensuring numpy compatibility. Additionally, troubleshooting build errors, runtime hangs, and ensuring correctness of floating-point operations with appropriate tolerances are key concerns raised across the threads."
2020-04-12,pytorch/pytorch,"The discussions primarily focus on implementation challenges and correctness of various PyTorch features, including numerical stability in functions like `logaddexp`, support for complex number operations (e.g., sign, clamp, sorting), and performance concerns for large tensor operations (e.g., `pdist`) on different hardware architectures. Several comments highlight issues with GPU memory management, such as out-of-memory errors and the impact of thread-local versus global object destruction order, suggesting potential fixes like wrapping objects in shared pointers or adjusting destruction sequences. There are questions around the compatibility of C++ frontend optimizers with script modules, as well as discussions on extending functionality, like adding new complex math kernels or improving support for half-precision tensors. Additionally, some conversations involve build system configurations, CI failures, and compiler bugs, indicating ongoing efforts to improve code stability, performance, and feature completeness."
2020-04-13,pytorch/pytorch,"The comments reflect ongoing discussions on various technical issues within the PyTorch repository, including support for multi-node distributed training with MPI backend, documentation consistency for `nn.functional` functions, and handling of CUDA out-of-memory errors, often involving cache clearing methods like `torch.cuda.empty_cache()`. Several issues relate to correctness and compatibility, such as differences in behavior for complex number operations, ONNX model export inconsistencies, and support for specific hardware accelerators (e.g., bfloat16, HIP). Concerns about build failures and merge conflicts highlight the complexity of the codebase and deployment environment, with some patches requiring careful benchmarking and testing before merge. Lastly, there's discussion around API design choices, backwards compatibility, and testing protocols for new features or changes, emphasizing the balance between progress and stability."
2020-04-14,pytorch/pytorch,"The comments highlight a range of issues across the PyTorch codebase and build infrastructure, including handling of Python version compatibility, CUDA memory management, and profiling limitations with dataloaders. Several discussions focus on the design and implementation of new features such as distributed metrics, support for quantized models, and the representation of with statements in TorchScript IR, emphasizing the need for clear API semantics and backward compatibility. There are also operational challenges related to build failures, dependency management, and CI pipeline flaky tests, some of which involve external repositories or tools like onnx, NumPy, and CMake, as well as platform-specific issues on Windows, Linux, and macOS. Unresolved questions include whether to support certain data types like qint8 in quantization, how to handle complex tensors in operations like `sign` and `clamp`, and the best approach for testing, serialization, and architectural changes before release. The discussions reflect ongoing efforts to improve PyTorch's robustness, flexibility, and usability across various hardware and software configurations."
2020-04-15,pytorch/pytorch,"The discussions primarily revolve around technical challenges in PyTorch's internals and ecosystem, such as issues with cuDNN algorithm selection, memory management (fragmentation, allocations, cache behavior), and the complexity of propagating tensor metadata and device/context information through operations. Several comments concern build failures, environment setup inconsistencies, and CI configuration issues, including integration with external tools like ONNX, TensorBoard, and code formatting tools. There are also questions about design decisions, such as how to structure metrics within the API, the behavior of quantized operators, and the transition to supporting new types and features, sometimes conflicting with existing conventions or user expectations. Unresolved issues include reproducibility of memory bugs, the support for particular operators like max_unpool2d, and ensuring consistency across language bindings, backends, and compiler variants."
2020-04-16,pytorch/pytorch,"The discussions primarily revolve around support for edge cases like empty batches in various functions (e.g., Linear, pooling) and the need for comprehensive testing across all nn.functional modules, especially for zero-sized inputs. Several issues relate to performance regressions and bugs introduced by recent PRs, including shape propagation failures, incorrect memory handling, and build failures due to dependencies or code changes, such as in MKL-DNN/DNNL versions and operator registrations. There are concerns about the stability and correctness of the tracing mechanism when handling complex data structures like dictionaries and control flow, along with suggestions for improving testing practices to catch such issues earlier. Additionally, multiple remarks address build environment inconsistencies, network issues affecting CI tests, and the transition to new registration and backend APIs, highlighting ongoing refactoring and maintenance challenges."
2020-04-17,pytorch/pytorch,"The discussions mainly revolve around optimizing PyTorch's internal kernel parallelization, especially for backward kernels and overlap patterns; addressing build and failure issues across different environments and dependencies, such as CUDA, OpenCV, and numerical libraries; and improving the integration and API consistency between Python and C++ (including torchbind and VariableType handling). There are concerns about long compilation times, legacy code, and ensuring correctness in codegen, particularly with autograd, quantization, and runtime dispatch. Several issues also touch on build system configuration, dependency management, and compatibility (e.g., Python 2.7 vs 3.x, handling of `requires_grad`). Unresolved questions remain about how best to implement, test, and document certain features, especially related to device conversions, custom operator registration, and handling of complex types in the JIT."
2020-04-18,pytorch/pytorch,"The comments highlight several concerns: potential misinformation or fake images related to historical figures (e.g., David Shanno), configuration and error messaging improvements in PyTorch, and issues with multithreading, CUDA initialization, and device management that cause runtime errors and flaky behavior. There is also recurring discussion about build test failures, dependency management, and compatibility across different environments, notably with ONNX, CUDA, and ROCm. Developers propose specific code fixes, request improved test coverage, and suggest architectural changes to ensure correctness and stability, especially around asynchronous task handling, future synchronization, and internal assertions. Some unresolved questions relate to build reproducibility, GPU support for certain features (like quantization), and standardizing array conversion interfaces within the community."
2020-04-19,pytorch/pytorch,"The discussions highlight persistent issues with gradient checkpointing in models involving modules like InstanceNorm2d with ReLU(inplace=True), which lead to gradients being lost or undefined during backward passes, possibly due to implementation details of checkpointing and plugin interactions. Several responses suggest that certain configurations—such as removing in-place operations or skipping specific normalization layers—can restore proper gradient computation, indicating potential bugs or limitations in how checkpointing interacts with these layers. Additionally, users inquire about possible workarounds, compatibility with graph mode or JIT, and more robust API or implementation changes to address tensor retention and memory leaks, especially on CPU and GPU with large memory demands. Some questions also relate to low-level issues like instruction selection, compiler dependencies, and build systems, implying ongoing concerns about stability and performance across different architectures and configurations. Unresolved questions include whether recent changes or future updates can fully resolve these fallback and correctness issues, and how to improve diagnostics or control over low-level execution details."
2020-04-20,pytorch/pytorch,"The comments highlight several key technical issues in the PyTorch repository, including discrepancies in different implementations of Dice loss for binary segmentation tasks, with discussions on the mathematical differences and their reasons. There are ongoing efforts to improve tensor conversion methods (e.g., one-hot encoding, numpy compatibility), with considerations on the design implications of a `force=True` parameter for numpy arrays. Several build and CI test failures are attributed to environment-specific issues such as submodule checkout failures, compiler warnings, and platform-specific bugs, with some regressions and bugs identified in CUDA, XLA, and Windows builds. Notably, there are concerns about maintaining backward compatibility, especially around C++ extensions and API changes, and discussions on improving testing and build procedures. Unresolved questions include how to best handle deprecated or legacy features, environment-specific failures, and ensuring correctness versus performance trade-offs in ongoing development work."
2020-04-21,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch codebase, including the need to handle large memory category limits in `torch.multinomial` (limited to 2^24 categories), and the support for multi-process profiling especially with DataLoader and autograd profiler limitations. There are suggestions to improve the profiling experience, such as defaulting to CPU-only profiling, better error handling, and clarifying profiling limitations in documentation. Additionally, issues around efficiency and performance in tensor operations, such as the impact of `mask` vs. `mask_fill_` and the need for benchmarking, are discussed. Some discussions focus on complex support, type hints improvements, and build/continuous integration failures due to platform or compiler specifics. Unresolved questions include how to properly support large categories, prioritize profiling enhancements, and manage build system differences across platforms like PPC and macOS."
2020-04-22,pytorch/pytorch,"The comments highlight several key points: implementation and placement of the 'PixelUnShuffle' layer, with suggestions to add it to `torchvision.ops` or core PyTorch for better accessibility; consistency in function parameter naming and API behavior, especially regarding `Variable` inputs and `argmax` behavior, with proposed workarounds to match NumPy/Numpy-like first occurrence behavior; build and compatibility issues across different environments and architectures, including Linux, Windows, and various CUDA/cuDNN versions, often related to build system updates, dependency linking, or compiler differences; the need for proper testing, benchmarking, and performance evaluation for new features or optimizations, particularly for complex number operations, reduction methods, and pooling implementations; and concerns about upstream regressions, CI failures, and integration challenges with Ninja, ONNX, and internal build tools, along with ongoing efforts to stabilize and improve the build system, testing infrastructure, and cross-platform support."
2020-04-23,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch's build and testing infrastructure, especially on Windows, CUDA compatibility, and internal dependencies such as MAGMA, NCCL, and QNNPACK. Many discussions focus on the challenges of supporting Windows and multi-GPU setups, with concerns about how environment configurations (e.g., environment variables, compiler options, and dependencies) affect build stability and correctness. There are recurring questions about testing coverage for recent changes, ensuring performance regressions are detected, and how to best approach unit testing for features like quantization, asan errors, and complex shape inference. Additionally, several comments suggest improvements to build system practices, including better handling of deprecations, explicit dependency configuration, and improving CI coverage for hardware-specific features. Unresolved questions mainly revolve around specifics of build failures (e.g., compiler flags, dependency linking, platform limitations) and how to enhance testing strategies across various hardware and software configurations."
2020-04-24,pytorch/pytorch,"The comments primarily address a few technical points: first, the potential performance improvement of depthwise and grouped convolutions with the new cuDNN support (notably in 'channels last' format), although some testing shows mixed results; second, challenges with tensor batch size 0 support across various layers, highlighting incomplete existing support and potential impacts on autograd; third, issues with model export and feature support in ONNX, including the handling of operators like `upsample` and `istft`, with some recent fixes and ongoing work; and finally, build and environment configuration problems across different platforms (Windows, macOS, Linux), often related to compiler or dependency issues. There are also discussions on code maintenance, testing strategies, and ensuring backward compatibility. Many issues are unresolved or pending further review, especially related to performance, correctness, and build environment stability."
2020-04-25,pytorch/pytorch,"The discussions primarily revolve around handling numerical instability and NaN/Inf issues in optimization algorithms like LBFGS, with suggestions to improve robustness in the code. There are questions about the proper handling of conversions between CUDA tensors and NumPy arrays, specifically regarding memory sharing and the potential for introducing explicit force flags or modifications to the protocol. Several reports indicate build and compilation failures due to compiler internal errors, undefined references, and issues with version compatibility, especially regarding Python, CUDA, and PyTorch integrations across different environments. Some concerns also involve test reliability, such as order sensitivity in profiling outputs, and the need for better control over test tolerances and precision overrides. Overall, unresolved questions include fixing build errors on various platforms, managing numerical stability in algorithms, and clarifying code changes' intent and impact."
2020-04-26,pytorch/pytorch,"The comments primarily revolve around implementation details and bug fixes related to PyTorch's features. Key concerns include verifying and merging PRs (e.g., minimal padding changes, weight normalization initialization, batch cosine similarity calculations), addressing build failures caused by environment misconfigurations or code issues, and extending support for features like Windows compatibility and sparse tensor operations. Several discussions focus on resolving errors encountered during compilation, testing, or runtime, such as internal assertion failures, operator support in graph mode, and integration complexities with DataParallel and TorchScript. Others involve proposed improvements, like refining the implementation of `isclose` for complex numbers, managing warnings, and supporting kwargs in hooks. Overall, unresolved questions remain about specific bug fixes—especially environment-specific build errors—and the scope of feature enhancements for broad platform and use-case support."
2020-04-27,pytorch/pytorch,"The discussions highlight various technical issues related to PyTorch development, including build failures due to compiler inlining and target-specific flags (e.g., FMA support), dependency management (such as CUDA driver compatibility and broken packages), and errors caused by incompatible or outdated toolchains (e.g., GCC versions, nvcc, CMake versions). Several issues also concern performance regressions with specific operations (e.g., depthwise convolutions, average pooling with channels-last memory format), as well as threading and device memory management problems. Questions are raised about how to safely add features like hooks with support for keyword arguments, the proper way to extend and document onnx support, and best practices for testing and overriding tolerances in numerical comparisons. Overall, unresolved questions involve ensuring compatibility across environments, reducing build flakiness, and maintaining support for mixed data types and hardware (e.g., Windows, macOS, ROCm)."
2020-04-28,pytorch/pytorch,"The comments highlight ongoing concerns about the performance and stability of specific convolution operations with 'channels last' memory format, especially regarding group and depthwise convolutions, and potential speedups from memory format optimizations. There are also significant issues with PyTorch's DataLoader and distributed training on Windows, including file loading, multiprocessing, and NCCL/NCCL-related errors, which require platform-specific handling and better tooling. Additionally, several bug reports and regressions are discussed, such as illegal memory access crashes, shape handling in models, and the need for improved testing and documentation for new features like hardware accelerators, complex number support, and custom backends. Several pull requests address build system issues, documentation updates, API consistency, and performance improvements, with ongoing questions about the best practices for extending hardware support and improving user workflows. Overall, unresolved issues emphasize platform compatibility, performance tuning, and API robustness, requiring targeted fixes and more comprehensive testing."
2020-04-29,pytorch/pytorch,"The discussion covers a range of technical concerns about PyTorch's features, performance, and internal implementations, such as the appropriateness of applying weight decay to biases, handling of NANs in tensor functions, and the design of metrics and evaluation APIs within `torch.nn`. There are questions about potential API changes, especially regarding the introduction of metrics as part of `torch.nn`, and the implications for backward compatibility and module design. Several issues highlight build failures and environment-specific problems, including CUDA kernel errors, memory leaks, and CI failures, often with suggestions for debugging or fixes. The conversation also touches on the complexity of compiler optimizations, runtime performance tuning, and API usage, with ongoing efforts to improve stability, diagnostics, and documentation to support users and developers. Unresolved questions remain about the best practices for certain API designs, CUDA kernel compilation issues, and how to best introduce metrics or evaluation modules in a clean, maintainable way."
2020-04-30,pytorch/pytorch,"The extracted comments from the GitHub threads mostly revolve around issues related to the PyTorch codebase, including the need for better organization of type annotations, improvements in distributed training correctness, and handling of backend-specific operator overloads. Several technical questions are raised about proper integration of custom tensor types, the porting of legacy operators, or the handling of errors and warnings, such as in quantization or backend code support. A recurring theme is the necessity for better error messaging, testing, and documentation, especially concerning new features or platform-specific build failures. Some discussions also address build system challenges and CI failures, advocating for rebasing and more robust testing workflows. Unresolved questions include how to properly leverage autodiff and custom dispatch, how to fix build issues on specific platforms, and whether to disable or improve failing CI tests for smoother development cycles."
2020-05-01,pytorch/pytorch,"The comments touch on a variety of technical topics in PyTorch, including extending support for certain functions in core files like `UnaryOps.cpp`, and the challenges of implementing numerically stable versions. Several discussions involve CUDA and GPU memory management issues, with solutions involving cache clearing, model restructuring, or lower-level API controls, yet some responses suggest these are not definitive fixes. There is ongoing debate about type annotation practices, especially the design of `forward()` methods for type checking and type safety, and whether to define a `torch.types` module for better type hinting. Issues with build systems, dependencies, and environment compatibility are frequent, including problems with linking, compiler bugs, and platform-specific behaviors, often requiring rebase or environment tweaks to resolve. Lastly, some concerns relate to maintaining backward compatibility and handling of non-supported hardware or software configurations, with discussions about long-term solutions and potential framework support extensions."
2020-05-02,pytorch/pytorch,"The comments highlight primary issues related to environment management and compatibility, emphasizing the importance of correctly configuring conda, pip, and CUDA/cuDNN versions to avoid runtime errors and illegal memory access (e.g., #21819), as well as challenges with reproducibility and build failures across diverse platforms. Several discussions address the need for better documentation, especially regarding the support of sparse tensors and the behavior of specific functions like `.is_cuda()` or `.is_sparse()`. There is concern over maintaining backward compatibility, especially when modifying core components like optimizers or architecture-specific files, which can introduce subtle bugs or performance regressions. Some issues also revolve around the difficulty of debugging and reproducing errors, complicated by upstream build failures and environment inconsistencies, signaling the importance of comprehensive diagnostics, environment info, and standardized testing. Overall, these discussions underscore the ongoing need for robust environment management, detailed documentation, and careful handling of API and implementation changes to ensure stability and performance."
2020-05-03,pytorch/pytorch,"The discussions mainly revolve around implementation clarifications and software support issues in PyTorch, such as differences between KL divergence and CrossEntropy Loss, the design of a `device` property for models, and environment setup for compiling libtorch with CMake. Several reports highlight build and compilation errors, particularly related to target-specific CPU instruction sets (e.g., FMA, AVX) failing due to compiler flags or hardware limitations, suggesting potential platform-specific build configuration problems. There are concerns regarding compatibility and support for non-NVIDIA hardware or platforms, especially in Windows and embedded systems, along with some issues stemming from missing or outdated dependencies like cuDNN, CUDA, or system DLLs. Additionally, some discussions address correctness and behavior of functions like `torch.clamp`, along with error handling for edge cases in tensor operations, with unresolved questions on supporting other device backends beyond Nvidia."
2020-05-04,pytorch/pytorch,"The comments reflect ongoing discussions about expanding PyTorch features, such as adding `logm` and batch matrix functions, while balancing API stability, usability, and performance. Several issues involve deep debugging of build failures on different platforms, especially related to compiler errors, unsupported code, and environment setup, highlighting the complexities of multi-platform support. There are concerns regarding backward compatibility and the need for better testing, especially around custom operators, quantization, and complex data types, indicating areas for potential improvements in testing frameworks and documentation. Discussions also emphasize the importance of robust error handling in asynchronous operations, particularly futures and RPC, suggesting possible API enhancements for reliability and correctness. Lastly, the community debates the design and implementation choices for features like model freezing, data loaders, and tensor memory formats, aiming to enhance usability in production while maintaining flexibility and performance."
2020-05-05,pytorch/pytorch,"The comments encompass a range of issues within the PyTorch codebase and build system, including the handling of CUDA and ROCm dependencies on different OS and hardware configurations, and the behavior of reduction operations like argmax/argmin which depend on comparison semantics that could be affected by floating-point tie cases. There are concerns about the reliability and accuracy of numerical operations, especially regarding floating-point equality, and the need for comprehensive testing and validation, including NumPy consistency and CUDA semantics. Some discussions focus on build system improvements, dependency management, and testing infrastructure, such as proper handling of symbolic links, MLIR compatibility, and including more detailed environment validation. Several failures are due to upstream breakages, build environment misconfigurations, or platform-specific issues like missing headers or compiler bugs, all of which remain unresolved or are under active investigation."
2020-05-06,pytorch/pytorch,"The discussions predominantly revolve around improving PyTorch's API and internal consistency, such as exposing layout options before graph rewriting, managing tensor seeds in DataLoader workers, and handling scripted modules inheriting rules. There is concern about API stability, especially regarding scripted modules and optional API parameters, as well as performance implications of new features like the Future API. Several issues highlight build and compilation failures across different platforms, often related to compiler-specific errors, macro definitions, or outdated build environments—these require addressing either by code modifications or environment updates. Additionally, some discussions focus on test reproducibility, random seed control, and ensuring that new code or API changes do not introduce regressions or break existing functionality. Finally, there are ongoing tasks and proposals about better API design for asynchronous callbacks, comprehensive documentation, and build infrastructure improvements."
2020-05-07,pytorch/pytorch,"The discussion highlights issues related to the robustness and correctness of PyTorch's testing and build infrastructure, including intermittent test failures, CI configuration inconsistencies, and build errors due to environment or code issues. Notable concerns include fixing flaky tests and error-prone compile configurations, handling of memory format ops, and the need for more comprehensive testing especially for edge cases (e.g., scripted models with complex data types, custom operators). There is also mention of potential improvements such as exposing metrics via `debug_info`, assessing whether certain deprecated APIs should be removed, and streamlining the build process for different platforms. Some discussions remain unresolved, notably how to best test non-trivial code paths (like custom kernels or model serialization), and ensuring backward compatibility and correct automatic differentiation in complex situations."
2020-05-08,pytorch/pytorch,"The comments reflect a variety of issues encountered during development and testing of PyTorch, including runtime errors, memory leaks, and build failures across different systems and configurations. Several reports concern runtime errors such as CUDA out-of-memory, illegal memory access, and shape mismatches, often tied to specific hardware, driver, or environment issues. Discussions also highlight the importance of proper testing coverage, especially for complex features like custom backends and distributed communication, and the need for clearer documentation and warnings for common pitfalls. Many build failures stem from version incompatibilities, unsupported features, or outdated system dependencies, suggesting a need for better environment management and more robust CI pipelines. Unresolved questions include how to improve test coverage for new features, handle environment-specific bugs, and ensure consistent behavior across hardware and software versions."
2020-05-09,pytorch/pytorch,"The discussions primarily revolve around technical issues with the PyTorch library, including bug fixes, feature implementations, and compatibility concerns. Notable topics include addressing numerical stability in loss functions, concerns about memory usage and data loading strategies, and performance issues related to multi-GPU training and data parallelism. Several existing bugs or regressions (e.g., specific operator support, ONNX export problems, and build errors on different platforms) are highlighted with suggested fixes or workarounds. There are also discussions on code quality improvements, such as type annotation handling, test robustness, and internal API refactoring. Some unresolved questions involve debugging complex build or runtime errors, especially on Windows, and ensuring feature alignment across versions and platforms."
2020-05-10,pytorch/pytorch,"The discussions address several technical issues in PyTorch, including difficulties with model summaries and visualization, notably in handling models with sequential() and parameter counting, and whether to incorporate advanced visualization features like forward pass graphs. There are concerns about handling edge cases in tensor operations such as `max()` on tensors with zero or mismatched dimensions, with proposed improvements to support such cases in a manner consistent with NumPy. Thread safety and device management issues are highlighted in the context of multi-GPU communication, specifically around tensor gather operations when peer-to-peer (p2p) access is disabled. Build failures due to backend incompatibilities, especially with CUDA versions and specific compiler errors, are also frequent, alongside questions about fixing compiler issues or supporting newer hardware through backward-compatible compilation settings. Overall, the issues encompass model introspection, tensor operation robustness, multi-GPU reliability, and build compatibility."
2020-05-11,pytorch/pytorch,"The comments cover a wide array of topics, including feature requests (such as adding PixelUnShuffle, matrix exponential, and named tensor support), bug reports and regressions (e.g., errors in CUDA, Pytorch `median` handling NaNs, and device management issues), and questions about PyTorch's implementation details (like deterministic behavior, hashing, and compiler/platform differences). Many discussions involve proposing code changes, test improvements, or API modifications, often with an emphasis on correctness, performance, and clarity. Several comments also highlight CI failures, build issues, and environment configurations needing attention or fixes, sometimes with requests for contributions or reviews. Unresolved questions include reproducibility of failures, compatibility of certain features across platforms, and whether certain behaviors (like deterministic computations or device sharing) will be future-proofed."
2020-05-12,pytorch/pytorch,"The discussions mainly revolve around the extension and generalization of PyTorch features such as implementing a TimeDistributed-like wrapper, support for in-place operations, handling PackedSequence inputs, and enhancing inference modules like freeze_module. Specific concerns include semantic clarifications and API design choices—such as whether to control behavior with flags or context managers—and compatibility issues, particularly regarding CUDA driver and compiler mismatches, as well as library linking for the C++ frontend. Several discussions highlight the need for better testing, benchmarking, and documentation updates to reflect new features and expected behaviors, especially regarding quantization, deterministic operations, and tensor device management. Unresolved questions involve how to make API usage clearer for end-users, handling legacy code, and fixing build failures due to upstream incompatibilities or environment misconfigurations. Overall, many points suggest cautious API evolution, improved tooling, and detailed documentation to mitigate implementation complexities and user confusion."
2020-05-13,pytorch/pytorch,"The discussions highlight challenges in PyTorch's testing and documentation systems, such as the need to support legacy test runners alongside pytest, with proposals for decorators to generate device- and dtype-variant tests. Concerns are raised about ensuring backward compatibility with `load_state_dict` parameter order and error handling, as well as about improving documentation load times by restructuring or removing tools like KaTeX. There are various technical issues related to CUDA, ROCm, and compiler compatibility, including build failures due to missing dependencies, code specialization issues, and platform-specific bugs. Some discussions focus on extending or modifying core modules, such as `BufferDict` and `ThreadLocalState`, to support new features while maintaining internal consistency and runtime safety, especially around multithreading and autograd contexts. Unresolved questions remain about error detection, handling of complex dtypes, and system integration for new functionalities like sparse tensor reductions or custom operations."
2020-05-14,pytorch/pytorch,"The discussions encompass a range of technical concerns related to performance optimizations, compatibility, and API design within PyTorch. Several issues address performance bottlenecks on specific hardware (e.g., slow `sum()` with AMD CPUs, slow tensor conversions) and potential solutions such as avoiding unnecessary copies or checking for broken instructions. Compatibility and correctness issues are raised about backward compatibility (e.g., renaming internal functions, differences in `view_as_real`/`view_as_complex`) and improvements in operator behavior (e.g., memory format preservation, input shape handling). Additional points include expanding testing coverage for new functionalities (complex tensor support, quantization, distributed RPC), refining profiling and debugging tools, and ensuring CI robustness amid flaky tests and environment inconsistencies. Unresolved questions involve proper handling of control flow for JIT, ensuring enforcement of format consistency in formats, and managing external dependencies and build environment issues on specialized hardware or platforms (Windows, ROCm)."
2020-05-15,pytorch/pytorch,"The discussions highlight multiple technical concerns, including shared memory leaks during multiprocessing DataLoader execution, especially when using more than a few workers, which can cause bus errors and shared memory exhaustion. There are questions around the support and implementation details of new deterministic functions like `F.interpolate`, with some guidance on version availability in PyTorch. Concerns are also raised about maintaining backward compatibility, such as correctly handling dtype conversions in distribution code (`Normal`, `Bernoulli`, etc.), and ensuring consistent operator schema updates without breaking existing code or tests. Flaky tests, especially those related to distributed RPC and internal CI failures, are noted, with suggestions to temporarily disable or rerun flaky tests for stability. Finally, there's discussion around the ongoing ROCm support, build failures due to missing headers (like `ATen/VulkanType.h`), and the general challenge of integrating new features (e.g., TorchScript changes, higher-level APIs) while preserving backward compatibility and stability."
2020-05-16,pytorch/pytorch,"The discussions cover various issues encountered in the PyTorch repository, such as hardware instruction support inconsistencies, installation difficulties, and crashes caused by hardware or driver incompatibilities. Several bugs are related to version upgrades, build failures, or platform-specific problems (e.g., macOS, Windows, Linux, or ROCm). Common themes include efforts to fix or work around memory leaks, bugs in distributed or RPC code, and serialization compatibility issues, with proposed solutions ranging from code patches, configuration adjustments (like environment variables), to API modifications. There are also concerns about the stability and correctness of certain tensor operations, as well as ensuring backward compatibility after updates. Unresolved questions often pertain to the best approach for critical changes, proper testing, and the handling of flaky or platform-specific failures, especially in CI environments."
2020-05-17,pytorch/pytorch,"The discussions primarily focus on testing and CI infrastructure concerns, including how to verify test coverage beyond `run_tests.TEST`, and the potential for extending existing CI report analysis tools to better track test execution completeness. There is a debate about the organization of metrics within PyTorch, with suggestions to keep them within `torch.nn` to maintain consistency with other components like losses and layers. Several issues relate to environment setup and compatibility, such as CUDA version support, driver compatibility, and environment variable configurations like `MKL_THREADING_LAYER`, highlighting complexities in ensuring reproducibility across diverse hardware and software environments. Other concerns involve handling of specific bug scenarios, such as improper behavior in gradient computations (e.g., `tensor.mean(dims=())`) and patching strategies for XLA and autograd, and the importance of reliable, non-flaky CI processes. Overall, the comments reflect ongoing efforts to improve testing, organization, environment management, and robustness in PyTorch development."
2020-05-18,pytorch/pytorch,"The discussions cover several technical topics, including the need for a forward-mode autograd API similar to `torch.autograd.grad`, with suggestions on implementation details and YAML configuration extensions. There are concerns and questions about the consistency and support of certain functionalities, such as `torch.round` behavior and handling of complex tensor types across CPU and CUDA, with some pending fixes and test updates needed. Several issues relate to build and environment setup challenges, including crashes during CMake builds, dependency management, and CI failures, often tied to external repositories, build configurations, or flaky tests. Additionally, proposals for new features or API reorganizations, such as adding metrics to `nn`, supporting specific tensor operations, and improving dataset loading strategies, are discussed with some questions on future directions and compatibility considerations. Unresolved questions include ensuring backward compatibility with models saved with new attributes, handling layout and kernel speed optimizations, and managing flaky CI tests."
2020-05-19,pytorch/pytorch,"The discussions mainly focus on improving testing and contribution workflows, including integrating pytest decorators into PyTorch's generic test framework for better parameterization, and handling test variance (e.g., floating point tolerance changes). There is concern about consistency and backward compatibility when adding features like parameter tags to modules, with suggestions to avoid intrusive core modifications and instead use external or attribute-based solutions. Several issues relate to environment setup, especially CUDA and NCCL behavior on multi-node systems, highlighting the complexity of distributed training, especially on ROCm and different hardware setups. Additional topics include ensuring proper synchronization in distributed contexts, updating deprecated ONNX ops, and fixing environment build failures possibly caused by compiler or dependency issues. Overall, the conversations aim at enhancing test infrastructure, runtime robustness, and reproducibility of complex distributed and GPU-accelerated workflows."
2020-05-20,pytorch/pytorch,"The comments cover a range of issues in the 'pytorch/pytorch' repository, including technical and testing concerns. Several discussions highlight the importance of thorough testing, especially around new features like custom operators, nested tensors, or additional support for specific data types (e.g., complex, bfloat16). There are questions about compatibility, such as handling changes introduced by new ONNX opset versions or differences in operator registration (like for Upsample). Many CI failures are due to external dependencies, build configuration issues, or upstream breakages, often requiring environment or configuration adjustments. Additionally, some discussions focus on API design choices, backward compatibility, and the timing of merging significant refactorings or features into the main branch."
2020-05-21,pytorch/pytorch,"The discussions cover a diverse set of technical topics including improvements and modifications to Dice loss implementations, tensor conversion techniques, issues with PyTorch installation on Windows and Linux, and advanced features like sparse tensors, distributed training, and new operators. Several comments suggest or request the addition of new functionalities, bug fixes, or tests, and there are ongoing concerns about build failures, environment setup, and compatibility issues across different hardware (CUDA, ROCm, Windows, Linux). Contributors discuss best practices for CMake configuration, handling of warnings, and strategies for robust CI testing, especially related to upstream failures, flaky tests, and environment-specific bugs. Some unresolved questions involve the correct way to implement gradient-related nodes in JIT, support for complex number operations, and ensuring reproducibility and performance during multi-GPU distributed training."
2020-05-22,pytorch/pytorch,"The comments cover a range of technical issues, primarily focusing on error handling in data loading (returning None on exceptions and filtering in collate_fn), support for empty batches in various neural network layers, and CUDA-related runtime errors including possible driver incompatibilities and performance regressions. There are discussions about modifying or extending APIs, such as adding bias correction flags in optimizers, introducing new loss functions (e.g. Siamese loss), and adjusting tensor views for complex numbers, with considerations for backward compatibility. Several build failures are linked to outdated CMake versions, compiler errors, and environment inconsistencies, especially in CI configurations and external dependencies. Key unresolved questions involve maintaining thorough support for edge cases like zero-sized tensors, improving crash diagnosability, and managing upstream version dependencies for stable builds and features."
2020-05-23,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch documentation structure, emphasizing a move towards splitting the API pages into individual functions or classes for better searchability and maintainability. There are concerns about CUDA-related performance challenges, especially with tensor core support and benchmarking practices that should account for asynchronous GPU execution timing. Several issues address specific bugs, such as incorrect backward implementations for sparse gradients, SSL certificate errors affecting model downloads, and the proper handling of `track_running_stats` in batch normalization. Additionally, there are queries regarding code compatibility, such as handling standards like `_BitScanForward64` on MSVC and ensuring proper compilation flags. Overall, unresolved questions include how to best balance documentation usability, optimize GPU performance benchmarking, and address specific coding or build system challenges."
2020-05-24,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's CUDA and build configurations, particularly regarding the support for different architectures and environments. Several issues concern errors in compilation due to missing or undeclared identifiers like 'strtod_l' and errors related to the 'tensorpipe' namespace, often tied to mismatched or unsupported dependencies and compiler versions. Users frequently encounter build failures on Windows, macOS, and Linux, often resolved by specific environment variable settings, reinstallation procedures with ON options, or selecting compatible package versions. There are concerns about documentation link consistency and the need for uniform dispatch mechanisms within PyTorch's native functions. Overall, unresolved questions focus on improving build robustness across diverse hardware, clarifying supported configurations, and streamlining the development and compilation process."
2020-05-25,pytorch/pytorch,"The comments predominantly address technical challenges related to PyTorch, such as the implications of importing internal modules like `torch._C`, handling data loading robustness (filtering corrupted samples), and compatibility issues with ONNX export, especially concerning resize operations and opset versions. Concerns are expressed about build and runtime failures across different environments (e.g., Windows, Linux, CUDA versions), GPU hardware limitations (e.g., NVLink with RTX 8000), and integration with system tools like CMake, NCCL, and IOMMU security settings. Several discussions focus on test failures, CI build stability, and performance benchmarking, raising questions on best practices for synchronization, benchmarking methodology, and maintaining backward compatibility. Some comments also cover organizing documentation, deprecating old APIs, and ensuring proper review and merging workflows for submitted code changes."
2020-05-26,pytorch/pytorch,"The discussions highlight several technical concerns, including the handling of distributed training errors, the importance of proper error messages and documentation for DistributedDataParallel, and issues related to compatibility and build failures across different systems and platforms. There is a focus on improving the robustness of operator APIs, especially concerning backward compatibility, deprecation, and serialization, with discussions about versioning, BC-breaking changes, and support for new data types like complex tensors. Additional topics include optimizing performance—particularly in scatter operations and benchmarking—addressing memory leaks, and ensuring consistent API behavior across different hardware backends (CUDA, ROCm, OpenCL, FPGA). Several unresolved questions involve better error reporting, the integration of new features into existing tooling (like the JIT, ONNX, and extensions), and platform-specific build issues, emphasizing the need for comprehensive testing, documentation, and multi-platform support."
2020-05-27,pytorch/pytorch,"The discussions primarily revolve around improving and stabilizing PyTorch's build and extension APIs, especially in contexts involving multiple hardware backends or system configurations. Concerns include ensuring backward compatibility and error handling for extension modules when core APIs change, as well as managing the complexity introduced by features such as custom dispatch, hooks, and the new vmap system that interact with various execution contexts. Several technical issues involve handling compatibility across different PyTorch versions, CPU architectures, and hardware-specific optimizations, often complicated by macro usage and build system intricacies. There are also recurring questions about the best strategies for tracing, codegen, and inlining of tensor operations, particularly for complex and large tensors, with suggestions for API changes to improve usability and stability. Unresolved questions include how to facilitate extension compatibility, error reporting, and consistent behavior across CPU, CUDA, and other accelerators, which need careful API design and build-time strategies."
2020-05-28,pytorch/pytorch,"The discussions primarily address challenges related to enhancing the PyTorch build and runtime environment. Concerns include ensuring backwards compatibility and handling API/schema changes across different PyTorch versions, especially with TorchScript and operator registration. Several comments highlight the difficulty of managing complex backend support, such as the interaction between device dispatch keys, fallback mechanisms, and support for sparse/dense tensors. There are ongoing issues with build failures, compile errors, and runtime errors due to platform differences (e.g., Windows, ROCm) and version mismatches. Additionally, suggestions involve improving testing coverage, error messaging clarity, and the architecture of global hooks, inlinings, and serialization mechanisms for better stability and usability."
2020-05-29,pytorch/pytorch,"The discussions cover a variety of issues related to PyTorch's features, bug fixes, and API designs. Key concerns include: how to implement a unified and efficient API for distributed collective operations, especially coalescing operations, and handling sparse/dense support; the need for clearer and more user-friendly error messages related to operator schema mismatches, especially across versions; the complexity of implementing and testing autograd features, including support for undefined gradients, complex types, and quantization; and the challenge of maintaining backward compatibility while improving APIs, such as in the case of `nn.Module`, `reset_parameter`, and string splitting functions in JIT. Some discussions involve workarounds or improvements for CI failures, build environment issues, and integration with third-party tools like MKL and CMake. Unresolved questions include: how to formalize the API for nested model cloning with differentiable optimizers and how to best support complex types and custom kernel registration without breaking existing functionality."
2020-05-30,pytorch/pytorch,"The comments primarily address several technical issues in the PyTorch codebase, including concurrency and multiprocessing problems linked to OpenMP runtime initialization and fork safety, which can be mitigated by using thread-safe BLAS libraries like MKL or OpenBLAS with pthreads. There are discussions about automating gradient checks in autograd functions, with suggestions to modify code generation scripts for better automation across functions, and considerations about handling tuple vs. single tensor outputs. Other concerns involve compatibility and export issues, such as ONNX operator support (especially the Pad operation's evolution), and ensuring correctness in serialization, default tensor types, and data loader behaviors when default types are set to CUDA tensors. Finally, some comments highlight ongoing CI failures and build environment issues that need resolution before merging certain PRs."
2020-05-31,pytorch/pytorch,"The discussions primarily focus on enhancing the design and usability of a metrics package in PyTorch, with suggestions for clarifying input formats (labels, probabilities, logits) via a 'mode' parameter for metrics like Accuracy. There is emphasis on maintaining control over metric interactions, especially in distributed settings, and ensuring compatibility with current tools like ignite.metrics. Several technical issues are raised related to build failures, CUDA kernel compilation errors, and specific bugs in functions such as division by zero and complex number support, often connected to upstream breakages or platform-specific constraints. Some discussions also involve code refactoring choices, such as replacing custom kernels with existing operations like `mul`, and operational considerations like determinism on GPU. Unresolved questions include fixing build errors related to LAPACK symbols and ensuring compatibility across different Python and CUDA versions, highlighting ongoing development challenges."
2020-06-01,pytorch/pytorch,"The comments highlight several recurring technical concerns, including support and compatibility issues with C11 atomics in CMake and build systems, especially on older Linux distributions and environments lacking proper atomic support. There are discussions about proper error messaging, especially for runtime errors, deprecation warnings, and API mismatches, such as operator schema changes causing import errors or user confusion. Additional issues involve debugging and testing challenges, such as unreliable CI build failures, flaky tests, and the need for better logging, test coverage, or guidance for complex features like batching rules, distributed training, or CUDA kernel behaviors. Some comments also suggest improvements in build configuration, documentation, and user-facing error clarity, while unresolved questions include how to handle compatibility with different hardware, environments, or library versions, and whether certain features or code paths should be removed or refactored."
2020-06-02,pytorch/pytorch,"The discussions predominantly revolve around debugging and improving PyTorch's build, import, and runtime behaviors across various platforms and environments. Key concerns include resolving import errors caused by incorrect module paths, handling of negative padding values in different modes, and robustness of the autograd mechanism with multiple outputs and undefined gradients. Several issues address performance bottlenecks and inaccuracies in reduction operations on large tensors, discrepancies and precision issues with half-precision float formats (float16, bfloat16), and the impact of specific compatibility modes such as TF32 on CUDA operations. There are unresolved questions about static build options, extension package management across different PyTorch versions, and ensuring compatibility with various hardware and library configurations. Overall, the discussions highlight ongoing efforts to stabilize, optimize, and extend PyTorch's functionality and interoperability."
2020-06-03,pytorch/pytorch,"The comments reveal several recurring themes: issues with NaN stability in gradient computations, particularly around division by zero and square root operations; challenges with CUDA and environment setup across different systems; potential inconsistencies with the behavior of pooling functions like AvgPool2d regarding ceil_mode and padding; and the need for clearer documentation, especially for new or non-public APIs like the deterministic flag, optimizer modifications, and custom kernel support. Many discussions involve fixing existing bugs, improving backwards compatibility, or optimizations that avoid large intermediate tensor allocations. There are also multiple troubleshooting tips for environment-specific problems on Windows, Linux, and macOS, as well as handling framework-specific quirks like torchvision, ONNX export issues, and operator registration. Lastly, several comments indicate ongoing efforts to enhance testing, codegen, and maintain compatibility with external tools and standards, with some questions about future roadmap directions and best practices for extending or modifying core components."
2020-06-04,pytorch/pytorch,"The comments reflect a broad set of technical concerns from various issues: the difficulty of reproducing ""illegal memory access"" errors possibly related to GPU inconsistencies or driver updates, and challenges with CUDA kernel behavior, especially regarding reduction operations and computeInline interactions. Several discussions highlight the complexity of maintaining backward compatibility when default behaviors (like epsilon in optimizers, or running stats in BatchNorm) are changed, as well as the difficulty of supporting types such as classes and complex numbers in TorchScript. There are recurring issues with specific CI build failures often due to environment inconsistencies, compilation errors, or upstream breakages, indicating the need for improved testing strategies and environment management. Some discussions involve ensuring proper serialization/deserialization (especially with ONNX and custom modules), minimizing the footprint of API changes, and cleaning up deprecated or unsupported code components, such as submodules or internal flags, with an eye toward long-term maintenance and extensibility. Overall, key unresolved questions include how to improve reproducibility and error diagnostics for GPU errors, how to efficiently support new features in the JIT and serialization formats, and how to handle complex type and memory behavior changes without breaking existing workflows."
2020-06-05,pytorch/pytorch,"The discussions highlight several technical concerns, notably the handling of mutation and aliasing in TorchScript IR, especially regarding in-place operations like `aten::append`, which are not currently well-represented, potentially impacting interoperability with ONNX. There are ongoing efforts to improve batching rules for `vmap` in the JIT, including moving from unpacked to BatchedTensor-based rules, to better manage logical and physical tensor views. Issues related to platform-specific build failures, such as CUDA, ROCm, and iOS support, indicate the need for better build configurations and dependency management. Discussions also address internal API refinement, such as the use of `__annotations__`, and the handling of special operators or compilation features like weak symbols and inlining for performance. Unresolved questions remain about accurately capturing mutation semantics, enhancing compile-time checks, and ensuring cross-platform stability while evolving the internal APIs and build processes."
2020-06-06,pytorch/pytorch,"The discussions highlight several key technical concerns, including the request for a minimal reproducible example of complex code, issues with training performance and memory management, and questions about whether certain operators (e.g., depthwise convolution on ARM) are optimized and how that impacts results. There are also debates about PyTorch internals such as handling in-place mutations and mutation tracking in TorchScript IR, as well as the stability and correctness of fused or optimized operators like `nn.BCEWithLogitsLoss`. Some discussions address CI failures, build issues, and how to improve documentation and debugging guidance, especially around device memory fragmentation, CUDA errors, and library compatibility. Additionally, unresolved questions pertain to the integration of developer features like profiling, the behavior of JIT/FX transformations, and how to handle operator-specific optimizations across platforms."
2020-06-07,pytorch/pytorch,"The discussions highlight several technical concerns: a memory leak issue when using `autograd.grad` with hooks for embedding gradients, which may require clearer documentation or fixes; discrepancies in ONNX model conversion, particularly with different opset versions and operation support like Pad and MHA; handling of `torch.from_numpy` and read-only numpy arrays; improvements to distributed sampling and data loader behavior for reproducibility; and bugs or performance regressions introduced by recent PRs, including failures in CI testing, inconsistent results in quantization, and model conversion errors. Proposed solutions include clarifying documentation on `create_graph=True`, updating ONNX export strategies, adding support for slicing `EmbeddingBag`, and refining distributed sampler implementations. Several unresolved questions remain about the timing and support of certain features (e.g., quantized modules, GPU threading improvements), as well as compatibility issues in specific model or operation conversions."
2020-06-08,pytorch/pytorch,"The discussions highlight concerns about the correctness and behavior of PyTorch's `backward(create_graph=True)` and higher-order differentiation, with suggestions to replace or supplement the current warning system with more explicit notices or refactoring to `autograd.grad` for clarity and memory management. Sparse tensor operations, particularly in relation to deep learning workflows involving sparse matrices, graphs, or linear systems, are a recurring topic, emphasizing the need for more PyTorch functions supporting sparse operations, as well as addressing issues with sparse gradients and in-place operations on overlapping tensors to prevent silent bugs. Several discussions focus on extending or fixing functionalities such as `GridSample` export to ONNX, support for quantization, and improving the compatibility of various operators across different hardware platforms and configurations, often involving PR reviews and bug fixes. A number of critical CI failures, environment issues, and build errors signal ongoing integration challenges, with suggestions to improve test coverage, runtime checks, and documentation. Unresolved questions include better handling of user-allocated streams in CUDA, the implementation of in-place operations on overlapping or broadcasted tensors, and clarifying or enhancing APIs for complex autograd cases and model export workflows."
2020-06-09,pytorch/pytorch,"The discussions reveal persistent issues with multithreading and multiprocessing behaviors affecting DataLoader stability, especially in Windows and CUDA environments, often mitigated by setting `num_workers=0` or disabling specific libraries like OpenCV. Several user-reported bugs involve process hangs, segmentation faults, or shared memory errors, sometimes linked to deep internal changes or the need for environment-specific configurations. There are concerns about compatibility and correctness, particularly around the handling of operator schemas, serialization, and backend implementations (e.g., MKLDNN, cuDNN). Suggestions include enhancing documentation, adding custom collate functions, and refining build processes, with ongoing questions about handling side effects, effect annotations, and improving debugging tools. Unresolved issues include verifying fixes across different hardware, managing build failures, and ensuring deterministic behavior in distributed setups."
2020-06-10,pytorch/pytorch,"The discussions highlight several technical concerns including the complexity of code requiring minimal reproducible examples, issues with model export (particularly with DataParallel and JIT tracing), and problems related to ONNX compatibility such as unsupported operators (`unsafe_chunk`) and model conversion errors. There are questions about the proper way to handle multi-GPU communication, especially regarding the API design of ProcessGroup, and suggestions for improving user experience with better error messaging and support tools like `torch.hub.set_headers()`. Some comments focus on the stability and correctness of distributed training, such as handling collective operation hangs, data type promotions, and runtime device guards, along with ongoing build failures across various CI environments. Unresolved questions include how to properly extend ONNX exports for newer operators, manage broad compatibility issues, and refine APIs for deterministic operations or distributed computing."
2020-06-11,pytorch/pytorch,"The comments highlight several recurring themes across the GitHub issues:

- Compatibility and build issues are prevalent, especially regarding CUDA, cuDNN, and compiler configurations, often addressed by changing build settings or skipping certain tests.
- Certain features such as dataset caching, device property access in models, or support for specific operators (e.g., ONNX symbolic functions) are discussed for potential improvements or are causing implementation challenges.
- There are ongoing efforts to enhance PyTorch's internal APIs, including better support for multi-device models, more informative profiling, and safer operator registration, although some changes require careful consideration to avoid breaking existing workflows.
- Numerous bug reports involve intermittent failures, flaky tests, or environment-specific issues, emphasizing the need for robust testing and diagnostic tools.
- Overall, unresolved questions include how to better support multi-device modules' device property, optimize operator registration, and improve test stability and debugging capabilities."
2020-06-12,pytorch/pytorch,"The discussions mainly revolve around issues related to GPU memory management, such as memory leaks on the main thread, proper GPU reference handling, and GPU context initialization, often suggesting explicit deletion of intermediate tensors or checking process correctness. Several threads address build failures and environment setup problems, often caused by incompatible software versions, missing dependencies, or platform-specific quirks (e.g., Windows, macOS, Android). There are concerns about ensuring reproducibility and correctness of operations, especially when introducing new features like complex tensor support or vectorized kernels, which require extensive testing and API safety mechanisms. Several issues highlight upstream build, configuration, or dependency problems, such as inconsistent CMake or compiler settings, ""could not find remote ref"" errors in Git submodules, and version incompatibilities, often suggesting rebase, environment adjustments, or waiting for upstream fixes. Lastly, questions about long-term API design, backward compatibility, and future platform support (like AMD or DPC++) are also prevalent, indicating ongoing architectural and ecosystem considerations."
2020-06-13,pytorch/pytorch,"The discussions primarily revolve around resolving persistent issues and inconsistencies in PyTorch's functionalities, including the static linking errors on macOS with CUDA, the performance impact of integrating TensorIterator kernels, and handling of collective communication in distributed training. Several questions address build configuration challenges, such as specifying architecture in CMake, ensuring proper dynamic library linkage on Windows, and diagnosing runtime errors related to CUDA and cuDNN. There are concerns about API design choices, such as replacing blocklist/allowlist with deny_list/allow_list for inclusivity and better terminology, and how to handle the introduction of features like `TORCH_FN` in operator registration. Unresolved questions include identifying fixes for static linking issues in older CUDA versions, improving the robustness of auto-differentiation and distributed autograd, and ensuring consistency and completeness in platform-specific build environments. The discussions highlight ongoing efforts to improve build stability, runtime performance, and API clarity across diverse systems and use-cases."
2020-06-14,pytorch/pytorch,"The comments reflect ongoing challenges with loading optimizer checkpoints and ensuring compatibility with different optimizer states, especially with Adam, and the need for better implementation of deep copying and saving/loading state dictionaries. Several discussions highlight the impact of tensor contiguity and permutation operations on model tracing and compatibility, with suggested workarounds such as adding `.contiguous()`. There are issues reported with specific compilation environments and hardware backends (e.g., ROCm, HIP), often related to missing functions or undeclared identifiers. Multiple CI failures and build errors are frequently mentioned, indicating instability across different platforms and configurations. Overall, key concerns include improving model serialization/deserialization, ensuring trace compatibility, fixing environment-specific build errors, and enhancing test coverage for common operations like reductions."
2020-06-15,pytorch/pytorch,"The comments reveal ongoing discussions and issues related to PyTorch's hardware support (including AMD ROCm, OpenCL, and Intel DPCPP), as well as specific implementation challenges such as the need for supporting various tensor formats, norms, and memory overlaps across different backends. There are also questions about the stability and reproducibility of features like RNG and autograd with different PyTorch versions and configurations. Several discussions focus on debugging, CI/build failures, and the extension infrastructure, highlighting areas needing further testing, internal API clarification, or code restructuring (e.g., ensuring proper cleanup, support for complex numbers, and enhancing test coverage). Additionally, there are mentions of potential performance improvements and the need for better documentation or refactoring to improve robustness and maintainability in the codebase. Many unresolved questions remain about integrating new hardware backends, fixing build issues, and ensuring consistent behavior across platforms."
2020-06-16,pytorch/pytorch,"The discussions mostly revolve around support for diverse hardware and frameworks, such as OneAPI, MLIR, and MLIR-based frameworks, with encouragement for porting efforts. A significant concern is handling Python multiprocessing issues, particularly related to DataLoader worker crashes, shared memory limitations, and process management, where solutions such as setting `num_workers=0`, environment tweaks, and process restart strategies are suggested. There are ongoing discussions about expanding support and testing for quantized models, including backward compatibility and API changes, with attention to CUDA, cuDNN, and MKL-DNN-related errors and their solutions. Certain system-specific and build-related issues, such as static linking, TBB inclusion, and compiler flags, are noted, often requiring environment-specific adjustments or build system modifications. Unresolved questions include how to improve error messaging, integration of external tools like TBB, and refining support for complex data structures in TorchScript, with a recurring theme of balancing backward compatibility, performance, and usability improvements."
2020-06-17,pytorch/pytorch,"The comments highlight ongoing challenges with ensuring deterministic behavior, especially in relation to cuDNN versions and CUDA stream management, notably the non-determinism introduced by cuDNN 7.6.5 and newer GPU runtime behaviors. Several discussions focus on improving library compatibility, error handling, and testing frameworks, such as how to effectively identify and diagnose crashes, timeouts, and hardware-specific issues across different platforms. There are concerns about the correctness and stability of features like mixed-precision training, distributed training with RPC, and extensions like `ParameterDict`, requiring careful design and testing to avoid regressions. Some comments suggest enhancements in CI and testing strategies to better catch and reproduce failures, including better stack trace capture and submodule management. Overall, unresolved issues include ensuring reproducibility, proper diagnostics for crashes and performance regressions, and refining APIs for extensibility and robustness in diverse hardware environments."
2020-06-18,pytorch/pytorch,"The captured comments reveal ongoing discussions and issues related to PyTorch's internal development and usage. Core concerns include the stability and robustness of distributed training, particularly process handling and shared memory management, as evidenced by segmentation faults and worker process crashes. There are also API and feature requests, such as exposing tensor operations (like `complex` support), making certain functions more consistent (e.g., `torch.cat`), and improving documentation and test coverage for extended functionalities, including CUDA and ROCm backends. Additionally, some discussions highlight build system intricacies, such as static linking, compiler definitions, and compatibility across various platforms and environments. The unresolved questions often relate to better error handling, proper documentation updates, and ensuring compatibility, especially with newer hardware and software versions, indicating an active but complex development process."
2020-06-19,pytorch/pytorch,"The comments primarily revolve around addressing persistent segmentation fault issues encountered during DataLoader operations, which seem related to shared memory and multi-process handling in PyTorch, especially when reading large JSON files with small dataset sizes and few workers. Some suggest that switching the multiprocessing start method to 'spawn' may help, though this has not fully resolved the problems. Others suspect potential problems with GPU memory management, especially regarding how tensors are retained in Python variables or when using DataParallel, causing out-of-memory errors. Several discussions also touch upon issues with CUDA support for RPC frameworks, NVIDIA driver compatibility, and the configuration of build environments across different platforms, indicating that many failures may lie outside of core PyTorch code but impact stability. Overall, unresolved questions include confirming if the shared memory setup is adequate, whether explicitly managing process spawning or memory releases could mitigate faults, and how to support GPU tensors in distributed RPC and autograd workflows reliably."
2020-06-20,pytorch/pytorch,"The discussions highlight multiple issues related to PyTorch's DataLoader and multiprocessing stability, particularly under different environmental constraints such as shared memory, multi-process configurations, and platform differences (Linux, macOS, Windows). Several reports concern CUDA and cuDNN errors, including illegal memory access and internal errors, often resolved by environment adjustments like CUDA toolkit reinstallation, driver updates, or hardware configuration tweaks (e.g., PCIe slots). There are technical questions about error messaging, data handling (e.g., JSON parsing, json string memory footprint), and API behavior (e.g., `topk`). Some issues stem from mismatched binary builds or unsupported hardware, with suggestions for detailed debugging tools (cuda-memcheck) and system configuration fixes. Unresolved questions focus on improving error reporting, supporting mixed scalar/tensor operations, and extending promotion and promotion-aware operations."
2020-06-21,pytorch/pytorch,"The discussions primarily revolve around addressing runtime errors, such as CUDA and cuDNN internal errors, and issues related to GPU configurations, especially with NVLink and hardware connectivity. Several comments highlight environment setup problems, build system failures, and compatibility concerns on different OS versions, notably Windows 7 and macOS 10.13, often due to missing or misconfigured dependencies and submodules. There are also concerns about CI stability, intermittent test failures, and the complexity of versioning and format compatibility, particularly for TorchScript and PyTorch models. Some suggestions involve reinstalling environments, updating submodules, and verifying hardware connections, while unresolved questions pertain to environment reproducibility and cross-platform build issues."
2020-06-22,pytorch/pytorch,"The discussions primarily focus on performance optimizations and correctness in deep learning operations, such as improving CUDA kernel efficiency, implementing 'same' padding in Conv2d, and enhancing TopK performance heuristics. Several issues address software bugs or stability concerns, including multiprocessing CUDA reinitialization errors, serialization compatibility across PyTorch versions, and CUDA driver or build incompatibilities. There is also conversation about API enhancements like adding forward hooks in C++, better device handling for heterogeneous hardware, and type safety in tensor operations. Additionally, some discussions highlight CI failures and build environment issues, often related to dependencies, compiler flags, or platform-specific limitations. Unresolved questions include whether certain patches should be included before branch cuts, how to detect CPU vs GPU issues for serialization, and long-term plans for multi-device support."
2020-06-23,pytorch/pytorch,"The comments reveal several ongoing technical issues and questions in the PyTorch repository. Notable concerns include challenges with multiprocessing and data loader synchronization, such as preventing duplicate data processing across processes and handling exceptions robustly. There are also discussions around compatibility and build regressions, especially related to nightlies, compiler issues, and dependencies (e.g., CMake, CUDA). Specific feature requests are noted, like adding `torch.quantile`, native support for batched matrix exponentials, and shape inference utilities. Additionally, several build failures and upstream breakages (e.g., compiler errors, missing symbols, docker image issues) are discussed, with some suggestions to revert, rebase, or fix those regressions and improve testing coverage."
2020-06-24,pytorch/pytorch,"The comments mainly revolve around managing GPU memory errors (CUDA Out of Memory), techniques for effective model copying and deep copying, and handling serialization compatibility. Several users highlight the importance of proper tensor lifecycle management to avoid memory leaks, especially when referencing GPU tensors or holding references in variables. Discussions also include improving the robustness of save/load formats across PyTorch versions, the design of lazy or uninitialized parameters, and enhancing user control over environment setups for reproducibility, performance, and debugging. Additionally, there's a focus on fixing build and test failures caused by upstream issues, compiler compatibility, and platform-specific bugs, as well as managing deprecation and feature support issues related to quantization, ONNX, and C++ standards. Unresolved questions include the best approach for cross-version serialization, the implementation of support for certain APIs like `__getstate__`, and how to build more explicit, user-friendly APIs for dynamic shape inference and model analysis."
2020-06-25,pytorch/pytorch,"The discussions encompass multiple topics, including a bug report with segmentation faults likely related to optimized BLAS libraries or memory issues; considerations for implementing new activation functions such as Swish potentially via C++ for performance; and handling of zero-filled tensors, kernel dispatch, or dataset caching strategies. Several issues mention CI failures, flaky tests, build configuration complexities, and the need for better documentation or static analysis tooling. There is a recurring concern about compatibility, particularly with CUDA, Python versions, and CMake, as well as questions on integrating assertions or shape checks into static type systems or runtime checks. Some discussions suggest workarounds, like patching or inheritance, while others prioritize improving core APIs, type safety, and user experience with better error messages and tooling support."
2020-06-26,pytorch/pytorch,"The discussions highlight several technical concerns including potential bugs and regressions in PyTorch, such as inconsistencies in CUDA behavior and issues with lazy parameter initialization, as well as ongoing work to support AMD GPUs, ONNX exporter fixes, and backward compatibility. Several questions focus on build environment issues, like dependencies (e.g., rocrand) and proper configuration for testing, as well as the need for explicit and clear testing strategies, especially regarding lazy modules and distributed training. There are also concerns about the correctness of recent features, like the learning rate scheduler application, and the stability of CI tests across various platforms. Some discussions propose solutions such as rebase and updates to CI configurations, or improvements to API design for explicitness and robustness, though some issues remain unresolved or require further investigation before merging."
2020-06-27,pytorch/pytorch,"The discussions highlight several main issues: system crashes during intensive GPU training suggest power or thermal overload, mitigated by BIOS updates, fan adjustments, or power supply upgrades; compatibility and environment mismatches, such as CUDA, cuDNN, and PyTorch versions, can cause runtime errors like internal errors or launch failures, which may be resolved by aligning software versions or updating CUDA drivers; build failures often relate to missing dependencies or compilation errors, sometimes due to submodule issues or compiler mismatches; certain runtime errors, such as CUDA internal errors, are linked to driver or hardware limitations, sometimes addressed by code modifications like input clamping; finally, CI failures and build issues are frequently caused by outdated or conflicting dependencies, requiring rebasing, environment adjustments, or code fixes."
2020-06-28,pytorch/pytorch,"The discussions encompass a range of technical concerns, primarily focused on debugging and improving PyTorch's features and compatibility, such as reproducing CUDNN errors, supporting complex number operations, and ensuring proper threading and device support across various hardware and software environments. Several users inquire about missing documentation, support for ONNX export, and compatibility issues like mismatched CUDA versions or missing GPU drivers. There are also questions about repository maintenance, including code refactoring, test execution, and merging conflicts, as well as issues related to CUDA library linkage and behavior across different system configurations. Overall, the community highlights ongoing challenges in ensuring stability, performance, and usability of PyTorch across diverse platforms."
2020-06-29,pytorch/pytorch,"The discussions encompass several technical areas including the implementation of matrix functions (e.g., matrix square root, fractional powers, logarithm) potentially via sign functions and Schur decompositions, with considerations about GPU performance and support for complex numbers. There are concerns about the parallelization and efficiency of batch processing, especially on CUDA, along with commentaries on precise timing measurements and synchronization issues in profiling. Other topics involve the manipulation and lazy initialization of neural network modules, especially ParameterLists, and their impact on backward compatibility and serialization, as well as modifying APIs like `torch.norm` to behave more like NumPy. Questions also arise about the support and behavior of distributed training with `torch.distributed`, and handling various failures seen in CI builds for upstream or environmental reasons. Overall, unresolved issues include implementation details for advanced matrix functions, performance optimization, API design choices, and ensuring stability across build environments."
2020-06-30,pytorch/pytorch,"The discussions highlight several technical concerns including the implementation of ""SAME"" padding modes in PyTorch, with proposed solutions involving size-dependent padding implementations that are compatible with dilation and groups. There are questions about maintaining backward compatibility in APIs like BatchNorm, where changes in defaults or parameters could affect models and serialization, as well as discussions about expanding support for complex numbers and their behavior in functions like `finfo` and `finfo`. Some issues relate to build configurations and environment compatibility, such as supporting different ABI versions, CUDA driver interactions, and compiler standards. Additional concerns involve the correctness and performance optimization of functions like spectral normalization, parallel reduction, and convolution routines, as well as handling breakages in CI/test environments due to version mismatches, external dependencies, or platform-specific bugs. Unresolved questions include whether modifications, such as overriding methods or changing API signatures, will break backward compatibility, and how to better support debugging and testing for these extensive changes."
2020-07-01,pytorch/pytorch,"The comments across multiple GitHub issues reflect concerns about high memory usage in PyTorch, often exceeding expected requirements, with some cases where memory allocation doesn't align logically with data/model sizes. Several discussions explore the underlying causes, including CUDA runtime behaviors, kernel loading strategies, and memory mapping areas, particularly on different architectures and environments (e.g., ARM, x86). Certain issues are related to build configurations, compiler versions, and compatibility, with troubleshooting steps involving environment upgrades, build reconfigurations, and detailed inspection of process memory mappings. There are also questions about the internal architecture of CUDA driver memory management and the implications for sharing memory across processes. Additionally, some comments address development workflows, such as adding support for lazy modules, fixing type annotations, and ensuring compatibility with various build systems and platform-specific limitations."
2020-07-02,pytorch/pytorch,"The comments span a wide range of topics including implementation details, support requests, error reproductions, CI failures, and discussions on API design. Several issues relate to the development and support of custom or deprecated features (e.g., Swish activation, Zoneout, ModuleIndexing), with some tasks assigned for onboarding or future enhancement. Notably, there are numerous build failures and environment-specific errors, especially related to compiler incompatibilities, missing dependencies (like MAGMA or cblas), and platform-specific issues, indicating ongoing tooling and environment maintenance challenges. Discussions on API improvements (like broadcasting and barrier mechanisms) highlight a desire for more flexible, robust distributed and autograd functionalities. Many failures are identified as upstream or flaky, requiring either rework or waiting on upstream fixes, with some issues linked to CI infrastructure glitches or environment mismatches."
2020-07-03,pytorch/pytorch,"The discussions highlight the complexity of implementing a binary segmentation loss, with suggestions to improve tensor-to-one-hot conversions using `F.one_hot()` and concerns over code accuracy and efficiency in loss calculations, especially when computing Dice loss per channel. Several issues relate to compatibility and error handling, such as the need to verify matrix singularity without explicit solving and the support for higher-dimensional `addmm`. Questions about the design and API of distributed communication features—like barriers, RPC, and collective operations—emerge, emphasizing the desire for flexible, scalable mechanisms that accommodate dynamic node participation and clean shutdowns. Furthermore, multiple CI failures and environment-specific problems underscore challenges in cross-platform testing, build configuration, and dependency management, which need addressing to ensure stable development workflows."
2020-07-04,pytorch/pytorch,"The discussions cover various technical topics, including the need for public API support for weight normalization in PyTorch, with a focus on incorporating epsilon directly into the compute_weight function to avoid NaNs caused by very small weights. Several issues address the behavior and implementation of distributed data samplers, highlighting concerns about uneven data allocation across GPUs and potential data mixing across epochs, with suggestions for ensuring equal data distribution. There are multiple CI/CD failures and environment setup issues, such as conflicting PyTorch versions, missing dependencies like cblas.h, and compatibility between CUDA, compiler, and PyTorch versions, requiring environment cleanup and configuration adjustments. Other topics include the behavior of addmm and baddbmm functions with tensor dimensions, a proposal to add a __len__ method to the Sampler class for better type hints, and questions about support for batched matrix multiplication for tensors with more than 2D. Overall, the key concerns involve API usability, distributed training consistency, environment stability, and precise tensor operation support."
2020-07-05,pytorch/pytorch,"The discussions primarily highlight a variety of technical challenges and questions related to PyTorch, including module import errors (such as `torch._C`), environment-specific issues (notably on Raspberry Pi and Windows), and CUDA-related runtime errors (like ""unspecified launch failure"" and backward incompatibility warnings). Several comments address compatibility and configuration problems, such as setting correct paths for NCCL, managing GPU memory, and ensuring proper environment setup for efficient training and deployment. There is ongoing interest in improving aspects like model pruning techniques (structured and unstructured), documentation clarity (e.g., loss functions and weighted averaging), and build stability across CI pipelines, with some discussions about existing workarounds and requested feature enhancements. Overall, unresolved questions revolve around fixing specific bugs, improving usability in diverse environments, and clarifying documentation to assist users in troubleshooting advanced use-cases."
2020-07-06,pytorch/pytorch,"The discussions mainly revolve around issues with DataLoader's `num_workers`, file handling in datasets (especially HDF5 files and multiprocessing), and the need for better control of uninitialized parameters in models, potentially via `Infer` sizes or lazy modules. There are concerns about the stability and predictability of boolean flags in functions like `norm`, the API design for shape inference, and the handling of device mismatches and fragmentation issues in allocator management. Additionally, some proposed changes involve debugging profiling overhead, improving documentation for new features like SiLU activation, and tackling performance bottlenecks in autograd and backend operations, alongside addressing CI failures related to dependencies and build configurations. Unresolved questions include the correct way to handle model parameter shape inference, BC-breaking impacts of API modifications, and the integration of better profiling and debug tools."
2020-07-07,pytorch/pytorch,"The discussions primarily revolve around address several technical issues and feature requests in PyTorch, including fixing specific runtime errors (such as tensor expansion issues post version updates, sparse tensor support in various operations, and determinism concerns with CUDA streams), improving API consistency and usability (like unifying BatchNorm modules, handling uninitialized parameters, and supporting dynamic input sizes in TorchScript), and ensuring build and CI stability (resolving Docker image fetch failures, build regressions, and maintenance of code quality checks). Many concerns indicate the need for better error handling, more transparent and flexible API design (especially for model inference and custom weight loading), and enhanced testing strategies, particularly end-to-end end validation and flaky test management. Several discussions propose architectural enhancements, such as consolidating related modules, improving documentation, and adding new features (like `torch.logsumexp` support for multiple inputs and support for complex matrix norms). Overall, the key unresolved questions include how to handle backward incompatible changes, improve reproducibility, and stabilize CI pipelines amid external dependency failures."
2020-07-08,pytorch/pytorch,"The discussions encompass several technical topics: enhancing PyTorch’s memory diagnostics and fragmentation tracking, especially when OOM errors occur; extending support for complex tensor operations like `torch.solve` for complex types; improving quantization support and stability, including handling sparse tensors and revising the `norm` function for complex numbers; integrating distributed training and model synchronization across different tasks and ranks; and addressing build issues, compiler compatibility, and CI/test failures across platforms, including Windows, Linux, and macOS, often related to library linking, CUDA, NCCL, or submodule management. There are also proposals to unify modules like BatchNorm and support additional features such as properties in TorchScript. Several discussions involve improving API consistency, adding testing for edge cases, and planning deprecation or backward compatibility strategies. Unresolved questions include the implementation of memory diagnostics, support for complex operations, robust quantization of sparse tensors, and handling platform-specific build challenges."
2020-07-09,pytorch/pytorch,"The collected GitHub comments primarily discuss various technical challenges and proposed solutions related to PyTorch's development. Key concerns include proper management of resources like file handles in data loaders, maintaining backward compatibility especially with deprecated functions, and performance issues such as slow matrix multiplications or slow autograd backward passes. Several discussions also focus on build failures caused by environment mismatches, compiler issues, or missing dependencies, with some commentaries about CI stability and flaky tests. Additionally, there are questions about new features like support for specific tensor operations, API design choices such as input validation and API consistency, and the impact of changes on existing workflows and performance. Many unresolved issues involve reproducing failures, environment-specific bugs, and ensuring new features do not break existing functionality or compatibility."
2020-07-10,pytorch/pytorch,"The collected comments reveal several recurring themes: issues with ensuring deterministic behavior in CUDA/cuBLAS operations, especially with environment variables like `CUBLAS_WORKSPACE_CONFIG`; challenges in serializing and exporting models with CUDA and TorchScript, such as handling attribute changes like `num_batches_tracked`; performance regressions and overhead concerns related to profiling, fusion passes, and operator implementations; and build failures or environment setup problems on different platforms including ROCm, Windows, and various CI environments. Some discussions focus on fixing bugs like non-contiguous tensor behaviors and supporting new functionalities such as sparse matrix multiplication kernels. Multiple unresolved questions involve compatibility with older ROCm versions, handling unused parameters efficiently in distributed training, and implementation details for introducing new operators or API refinements. Proposed solutions include environment-based settings for deterministic CUDA, code refactoring for more reliable serialization, and improved CI cleanup or build logic."
2020-07-11,pytorch/pytorch,"The comments cover diverse technical topics, including the desire to implement properties for tensor attributes like shape, the intention to add new features such as handling dummy tokens in attention mechanisms, and the importance of consistent dtype handling across sum/reduction operations for different hardware and input types. Discussions also delve into threading and multiprocessing concerns with DataLoader pin_memory, CUDA usage in subprocesses, and the correct way to manage shared memory. Several comments request review, suggest API improvements (e.g., making arguments keyword-only), or indicate ongoing CI issues possibly related to upstream changes or environment specifics, especially on Windows or with specific hardware configurations. There are unresolved questions about error handling, compatibility flags, and benchmarks comparing new implementations against existing ones, with some proposals for workarounds or better diagnostics. Overall, the key concerns focus on correctness of tensor properties, stability and reproducibility of results across platforms, and improving user feedback and usability in complex distributed or hardware-accelerated scenarios."
2020-07-12,pytorch/pytorch,"The discussions primarily revolve around addressing specific technical issues and feature questions in PyTorch, such as resolving errors related to eigenvalue differentiation, CUDA availability with different hardware, and memory leaks during evaluation, with some suggestions for solutions and workarounds. Several questions concern API design choices, like the necessity of the `add_zero_attn` parameter and potential changes to `BatchNorm`'s `affine` options, emphasizing the importance of backward compatibility and consistent documentation. There are also discussions about compatibility and setup problems, including installing specific PyTorch versions and ensuring correct environment configurations, with some troubleshooting steps provided. Some issues involve build failures and CI errors, often due to environmental inconsistencies or compilation errors. Overall, the conversations reflect ongoing efforts to improve robustness, usability, and clarity in PyTorch’s features and documentation while addressing specific bugs and environment-specific challenges."
2020-07-13,pytorch/pytorch,"The discussions center around device management during model saving/loading, with suggestions for always saving CPU-based models and strategies for reloading models onto different devices. The deep copy of tensors raised concerns due to current limitations supporting only leaf tensors, indicating a potential need to support `grad` attribute copying. Another key topic is the behavior of `torch.no_grad()` in JIT-compiled models, suggesting a potential need for a JIT-compatible `NoGrad` implementation. Several threads address performance bottlenecks and correctness issues in sparse matrix multiplications, batch normalization, and convolution optimizations, with proposals for benchmarks, default behaviors, and implementing more flexible initialization methods. Unresolved questions include compatibility of environment variables post-initialization, shape inference consistency, and how to properly handle various edge cases in model serialization, device management, and mixed precision training."
2020-07-14,pytorch/pytorch,"The discussions encompass a variety of technical topics related to PyTorch development, such as performance improvements (e.g., UNIX and CUDA kernel fusion, initializers, sparse matrix multiplication), compatibility and correctness issues (e.g., perturbations in autograd, shape mismatch handling, float16 operations), and infrastructure or CI stability (e.g., build failures, environment configuration, testing strategies). Several proposals suggest API enhancements (like configurable initializers, a `torch.linalg` namespace, and device placement policies for RPC) and code improvements (scriptability of context managers, compile-time optimizations, devirtualization). Unresolved questions include how to correctly handle non-invertible device mappings in RPC, how to fix specific CUDA kernel selection issues, and how to ensure backward compatibility while evolving APIs. Overall, the discussions reflect ongoing efforts to improve performance, usability, and robustness of PyTorch, with many topics requiring further investigation and testing."
2020-07-15,pytorch/pytorch,"The discussions highlight several themes: issues with dataloader deadlocks and memory leaks linked to explicit iterator management; challenges in ensuring unique data processing across multiple processes with multiple workers, addressed by explicit iterator handling; the need for better memory diagnostics, fragmentation tracking, and allocator observability; ongoing efforts for scriptable and inheritance-compatible autograd context managers like `no_grad`; improvements in distributed RPC device mapping and process group control; and maintenance of backward compatibility considerations, especially regarding in-place operations, custom `autograd.Function` behaviors, and schema evolution. These concerns center on robustness, transparency, and extensibility of PyTorch's data handling, autograd, distributed, and API systems."
2020-07-16,pytorch/pytorch,"The discussions encompass various topics including the integration of advanced interpolation methods directly into PyTorch, addressing kernel runtime bugs and memory leaks, and improving the robustness and functionality of distributed training and RPC systems. Notably, there are concerns about handling NaNs in gradient computations, CUDA synchronization issues, and potential crash bugs due to threading and destructor race conditions. Several conversations focus on improving API clarity, such as the support for different input types, shapes, or default behaviors (e.g., `strict` in model loading, default tensor types). Overall, unresolved questions include handling NaN gradients in masked attention, the proper way to manage global state workflows like `rpc.shutdown()`, and resolving various build, test, and environment-specific failures across multiple platforms."
2020-07-17,pytorch/pytorch,"The discussions highlight several technical issues, such as the transition from TH to ATen kernels, particularly complex number support, and the registration of quantization kernels. Questions are raised about the support and compatibility for specific hardware (e.g., Jetson Nano, ARM), as well as the correctness and autograd support for custom operations like `_min_max`. There are proposals to improve API consistency and usability, for example, adding named parameters to `unflatten`, or designing a more flexible `lr_scheduler` system with composition capabilities. Residual concerns involve build failures, code merging conflicts, and ensuring backward compatibility, especially with process initialization, distributed training, and dependencies like glog and NCCL. Unresolved questions center on how to properly debug leaks, errors on various platforms, and how to manage dependencies and process lifecycle in complex distributed or hardware-specific environments."
2020-07-18,pytorch/pytorch,"The discussions primarily revolve around extending PyTorch's support for complex numbers, including autograd functionalities, hardware kernels, and documentation clarity, with several out-of-tree extension efforts ongoing. There are questions about integrating distributed key-value pairing/registering mechanisms using C++ store interfaces versus Python-level solutions, with considerations for performance and ease of use. Concerns are raised about ensuring consistency in API behavior, especially regarding deterministic algorithms, group management, and masking in multi-head attention, as well as maintaining compatibility and performance across different hardware architectures and environments. Additionally, the community seeks clearer documentation on current complex number capabilities, and there are ongoing efforts to improve testing, CI stability, and build configurations. Lastly, several issues relate to build errors, environment setup, and CI failures, often due to missing modules or platform-specific inconsistencies."
2020-07-19,pytorch/pytorch,"The discussions primarily concern difficulties in implementing advanced autograd features, such as module backward hooks and in-place operation hooks, due to engine restrictions and graph handling limitations, including multi-output scenarios and in-place modifications. There are questions about compatibility and API support, especially when inspecting or modifying intermediate tensors in JIT-compiled or quantized models, with a desire to access tensor quantization parameters and insert hooks into JIT graphs. Memory management and debugging tools are also a focus, such as dumping tensor allocations and tracking memory usage, with some interest in external allocator extensions. Several issues highlight ongoing CI failures, often due to external dependencies or environment-specific problems, alongside questions about supporting hooks in JIT, C API completeness, and user-facing APIs for model introspection and debugging. Overall, they reflect a need for more flexible, transparent autograd and JIT tooling, better memory introspection, and stable, comprehensive environment support."
2020-07-20,pytorch/pytorch,"The discussions highlight ongoing challenges in PyTorch related to complex number support, including documentation clarity and evolving implementation strategies across in-tree and out-of-tree extensions. Several issues pertain to experimental features such as complex dtype support, their hardware-specific kernels, and autograd functionality, with calls for clearer high-level documentation on current capabilities. Other technical concerns include ensuring proper device and dtype handling during model parameter loading, as well as correctness in specific function implementations like gather and masked_select, especially across different versions like 1.5.1 and master. Some questions involve the integration of auto-differentiation, support for specialized hardware like XLA and ARM, and managing CI failures due to environment or build issues. Unresolved questions include version support, handling of NaNs in attention mechanisms, and the proper migration path for experimental or deprecated features."
2020-07-21,pytorch/pytorch,"The discussions reveal several technical concerns related to PyTorch, including issues with DataLoader multiprocessing on Windows and Linux, handling of sparse tensor operations, and ONNX conversion errors, often tied to unsupported or not yet implemented features. There are questions about the proper handling of determinism and reproducibility, especially regarding CUDA and cuDNN behavior, and workarounds for specific bugs like the `triu` operator during ONNX export. Discussions also touch on improvements to testing strategies, such as making tests more comprehensive and reliable and fixing various build errors across platforms (Windows, Linux), sometimes due to compiler or environment misconfigurations. Several issues involve versioning, API stability, and BC concerns, alongside suggestions to modify internal functions and APIs or add new supported features for better compatibility and functionality. Unresolved questions include the ongoing efforts to fix multiprocessing support on Windows, support for sparse tensor operations (e.g., sparse-sparse matrix multiplication), and enabling certain operator exports (like `triu`) in ONNX."
2020-07-22,pytorch/pytorch,"The GitHub comments reveal ongoing discussions around enhancing documentation support and support for complex numbers in PyTorch, including clarifying current capabilities and future plans for autograd and related features. Several issues concern performance optimization of specific operations like top-k, sparse matrix multiplication, and convolution padding, alongside implementation details for features such as autobatching, deterministic operations, and device mapping in distributed RPC. There are also recurring troubles with build processes, especially related to environment configuration, compiler issues, and upstream compatibility, often involving CI failures on various platforms. Some comments point to the need for better testing, especially for complex tensor printing, and clarifications about native support for data types like bfloat16 and BF16. Unresolved questions emphasize ensuring correct behavior in edge cases, improving user-facing documentation, and addressing build stability across different hardware and software environments."
2020-07-23,pytorch/pytorch,"The discussions primarily revolve around performance optimization, compatibility issues, and correctness in the PyTorch framework. Several threads address the need for faster `topk` operations in non-innermost dimensions, potential kernel or kernel launcher improvements, and better handling of sparse-sparse matrix multiplications. Additionally, there are concerns about determinism settings, model conversion to ONNX, proper support for complex datatypes, and managing build failures across different environments and architectures, especially related to CUDA, ROCm, and platform-specific build configurations. Unresolved questions include how to extend support for certain tensor operations (like strides handling, complex number derivatives), whether new features (like ""conjugate"" tensor flags) are needed, and how to improve testing, CI robustness, and documentation clarity. Several issues remain open or require careful revisiting, especially around legacy and compatibility challenges, fast kernel implementations, and the correct propagation of errors and configurations across different system setups."
2020-07-24,pytorch/pytorch,"The discussions primarily revolve around PyTorch's deterministic behaviour flags and the accuracy of their documentation, with considerations on how to exhaustively list affected functions and manage API support. Several threads address CUDA and cuDNN memory management issues, including overutilization, overlap detection in in-place operations, and platform-specific bugs, often suggesting possible fixes like environment variable adjustments or build system modifications. There are multiple reports of persistent CI failures due to build environment misconfigurations, platform incompatibilities, or upstream bugs, with some attributed to specific hardware (e.g., Turing GPUs, ROCm setups). Additionally, questions about the proper handling of in-place tensor operations, model serialization, and memory overlap checks indicate safety and correctness concerns. Overall, unresolved questions include ensuring reproducibility and stability across diverse hardware and software environments, accurate documentation, and more robust testing for specific features like deterministic operations and memory management."
2020-07-25,pytorch/pytorch,"The discussions highlight issues regarding the implementation of deterministic operations, including the identification and categorization of nondeterministic functions across platforms, and how to effectively inform users via documentation or test automation. There are technical challenges related to model quantization, especially when overlapping tensors or specific modules (like batch normalization) are involved, resulting in runtime errors or incompatibilities. Concerns are raised about build failures related to compiler flags and environment settings, particularly for compiler-specific issues and sanitizer configurations. Several discussions focus on improving PyTorch's distributed and RPC APIs, including resource sharing, role-based initialization, and the registration backend, aiming for more flexible and scalable distributed communication handling. Lastly, there are recurring build and compilation errors in third-party integrations such as ONNX export and Caffe2, often due to missing declarations or incompatible code, which require meticulous fixes and environment adjustments."
2020-07-26,pytorch/pytorch,"The discussions highlight various technical issues in PyTorch development, including code style improvements (e.g., changing default parameter initialization syntax), platform-specific bugs (notably on Windows and CUDA versions), and memory management concerns (such as proper handling of gradient accumulation and memory sharing in multi-GPU setups). There are persistent challenges with environment and build configurations, particularly with CUDA/cuDNN compatibility, resulting in performance regressions and kernel selection differences across versions. Several conversations also address the need for clearer documentation, better error messages, and handling of unused parameters in distributed training. Some issues remain unresolved, notably reproducibility of certain behaviors on different hardware and software setups, and bugs related to specific compiler or build environment configurations."
2020-07-27,pytorch/pytorch,"The comments indicate several recurring themes: First, there are questions about the reproducibility and determinism of PyTorch operations, especially regarding overlapped tensors, native nonlinear functions, and how to handle nondeterministic library calls. Second, performance concerns are raised about recent changes, such as the memory caching allocator and over-allocation warnings, highlighting trade-offs between efficiency and predictable memory use. Third, several build-related issues are discussed: compatibility with different CUDA and cuDNN versions, linking problems, and the impact of environment setup (e.g., MPS, NCCL, and compiler flags). Additionally, numerous CI failures due to configuration, code incompatibilities, and internal build bugs suggest ongoing struggles with cross-platform, multi-version support, especially for Windows and ROCm. Lastly, some feature requests and maintainability considerations emerge, including the need for aliases, support for specialized memory formats, and handling complex number derivatives, reflecting areas for future development and documentation improvements."
2020-07-28,pytorch/pytorch,"The discussions cover a variety of technical concerns, primarily centered around extending autograd to support new features like forward-mode differentiation, and the API design choices involved. There is debate over whether to build a new API for forward-mode (fwddiff) or to use existing mechanisms like JVP, with considerations about graph building, efficiency, and API usability. Several comments address performance issues, such as the speed of operations involving int64 and float64 types on CUDA, where the underlying kernel implementations and device-specific optimizations are questioned. Integration and support challenges related to multi-GPU communication, CUDA version compatibility, and kernel implementations (e.g., for quantized ops, grid_sample, etc.) are also discussed, along with build system and CI issues. Unresolved questions remain about the best way to represent and support complex numbers in derivatives, the API for optional tensors, and the handling of deterministic behavior across different hardware and software versions."
2020-07-29,pytorch/pytorch,"The discussions encompass various aspects of PyTorch development, including the integration of forward mode automatic differentiation, API design choices such as API stability, named tensor support, and internal API refactoring strategies. Concerns also relate to CUDA kernel performance, memory management, and compatibility issues with complex types and quantization workflows. A recurring theme is the importance of clear documentation, stable API semantics, and handling of edge cases like empty tensors or large array indexing. Several questions remain about the impact of certain design decisions on user experience, performance, and code maintainability, as well as how to best facilitate testing and CI robustness across diverse environments."
2020-07-30,pytorch/pytorch,"The discussions cover various aspects of the PyTorch codebase and infrastructure, including issues with backward compatibility and type unification in TorchScript, and the need for improvements in quantization support on GPU and mobile. Several questions are raised about the implementation details and best practices, such as support for in-place functions, vectorized CPU support, and the use of specific SIMD intrinsics, indicating a desire for consistency and clarity in the code standards. Other concerns involve performance implications of new features, such as partial bucketing in distributed data parallel and device placement validation in elastic multiprocessing, as well as build failures caused by compiler incompatibilities or missing dependencies, highlighting ongoing infrastructure and compatibility challenges. There are also questions about the best way to extend or improve utilities like packbits, the handling of legacy and in-place functions, and the proper way to support future features like torchscript support for custom classes, with a focus on ensuring correctness, efficiency, and maintainability. Unresolved issues include the need for clearer documentation, stable support for new hardware and software environments, and the integration of new features into existing pipelines."
2020-07-31,pytorch/pytorch,"The discussions mainly revolve around the implementation and support of advanced features within PyTorch, such as refining the `pixel_shuffle` to support scale factors < 1 and its property, handling quantization, especially for models like `nn.Embedding` and dynamic quantization, and improving multi-GPU training with variable-sized datasets. There is concern over backward compatibility, especially regarding operator overloads and module behavior in different configurations (e.g., CPU vs GPU, FP16 vs FP32), and the need to ensure proper error messaging and API support for complex tensor operations, including conjugation and unsafe memory practices. The support and performance implications of newer backend features like Tensor Cores and cuDNN settings are also discussed, alongside challenges introduced by ongoing CI failures and upstream bugs. Unresolved questions include how to best integrate custom operators into the existing framework, how to accurately detect hardware-accelerated features (like Tensor Cores), and how to improve the robustness and user guidance around complex configurations, particularly for quantization and distributed training."
2020-08-01,pytorch/pytorch,"The comments reveal ongoing challenges and discussions related to efficient performance testing, especially for CPU performance, and the complexity of quantization support within PyTorch, including issues with custom modules, layers like LayerNorm, and support for quantized operations on different backends. Several entries concern build failures due to environment mismatches, compiler issues, and deprecated or unsupported functions (notably `cusparseSpMM` with Windows and CUDA 11). There are questions about maintaining backward compatibility for operators with schema changes, handling global state references within torchscript, and support for specific features like p2p communication in NCCL. Notably, there are reports of reproducibility issues with specific models and operations across different PyTorch versions and environments, with some failures linked to upstream bugs or flaky CI infrastructure. Overall, the discussions highlight active efforts in bug fixing, feature enhancement, and environment configuration, alongside the need for better tooling, documentation, and support for complex use cases such as quantization and distributed training."
2020-08-02,pytorch/pytorch,"The core technical concerns across these GitHub discussions include: (1) inconsistencies in tensor splitting behavior between `torch.chunk` and `np.array_split`, with a desire to align `torch.chunk`'s behavior for greater numpy compatibility; (2) issues with `ParameterList` and `ParameterDict` compatibility in DDP and model serialization, which complicate model upgrades and checkpoint loading; (3) ongoing compiler and CUDA API compatibility issues, especially with Windows support for CUDA 11+ features such as cusparse, leading to build failures; (4) bugs related to tensor operations, such as incorrect tensor stacking in custom modules and potential index manipulation errors; and (5) general CI failures and environment-specific bugs that require investigation and potentially updated testing strategies. Overall, these discussions highlight a focus on improving API consistency, cross-platform compatibility, and reliable checkpointing in PyTorch."
2020-08-03,pytorch/pytorch,"The discussions span various topics including the inconsistency between `torch.chunk` and `np.array_split`, the handling of `__torch_function__` registration, and the support and behavior of certain PyTorch features such as in-place operations, index manipulations, and quantization, especially on different hardware like CUDA, ROCm, and mobile systems. Several issues concern CI failures, build reproducibility, and platform-specific bugs, often linked to upstream breakages or configuration problems, with suggestions for better testing, documentation, and code structure. There are questions about specific implementation details, such as when to support in-place functions, how to handle aliasing, and compatibility concerns across different PyTorch versions and external libraries like NumPy. Discussions also highlight the importance of improving user experience in distributed training, model export, and cross-platform support, alongside ongoing efforts for bug fixing, feature additions, and build system adjustments. Many unresolved questions focus on ensuring backward compatibility, fixing build errors caused by submodule mismatches, and platform-specific bugs in CUDA, ROCm, and mobile environments."
2020-08-04,pytorch/pytorch,"The discussions primarily revolve around technical challenges and implementation considerations in the PyTorch repository. Key concerns include methods for synchronizing batch normalization buffers across GPUs, ensuring thread safety with NCCL, and handling device-specific issues such as CUDA support, environment setup, and memory management. Several issues address bugs related to operator support, type unification, and compatibility (e.g., handling `Any` types, supporting complex inputs, and consistency with NumPy functions). There are also discussions on infrastructure and build system improvements, such as Docker image support, CI failures, and ensuring reproducible tests. Overall, unresolved questions include proper handling of device states across threads, integration of new features with backward compatibility, and fixing build failures related to external dependencies and environment configurations."
2020-08-05,pytorch/pytorch,"The discussions highlight concerns about the correctness and implementation details of PyTorch features, such as padding mechanisms in LocalLinear, the support for transposed locally connected layers, and the behavior of ordinal functions like `sigmoid` versus `logsigmoid`. Several issues refer to compatibility and build issues on various platforms, including Linux, Windows, macOS, and specific hardware like Raspberry Pi, NVIDIA GPUs, and Google Colabs, often tied to environment misconfigurations, dependency mismatches, or hardware support. There are also recurring themes around CI failures possibly due to upstream bugs, platform-specific bugs, or resource limitations, with some failures identified as flaky or related to known bugs (e.g., CUDA/cuSparse incompatibilities, linker errors). The community suggests improvements like better documentation, environment checks, refactoring for clearer API semantics, and enhancements to tensor dispatching and memory management. Many unresolved questions involve ensuring compatibility across devices, platforms, and PyTorch versions, including build reproducibility, operator support in TorchScript, and support for complex or floating point precisions."
2020-08-06,pytorch/pytorch,"The discussions highlight several concerns including the need for clearer documentation on broadcasting semantics, especially differentiating in-place and out-of-place function behaviors. Multiple issues relate to build system complexities, particularly regarding dependency cycles in CMake, submodule handling, and environment configuration on different OSs, implying a need for better build automation and consistency. There are recurring questions about the expected behavior of specific functions like `torch.bernoulli()`, how to handle data serialization/deserialization with TorchScript, and managing type annotations like `Any` versus `TypeVar`. Several threads discuss test reliability and flakiness, often tied to infrastructure or configuration issues, emphasizing a need for more robust testing practices and clearer environment setup instructions. Unresolved questions also include the future support for features like eigenvalue computation in `torch.linalg`, and plugin management for third-party integrations like ONNX, especially regarding source code availability and reproducibility across versions."
2020-08-07,pytorch/pytorch,"The discussions primarily revolve around various technical issues encountered in the PyTorch codebase, including bugs, performance optimizations, and API design considerations. Several issues concern correctness and stability, such as NaN propagation in functions like grid_sample, and handling of edge cases in quantization and tensor operations, often with specific attention to CUDA and multi-threaded environments. There are also recurring questions about compatibility, including version mismatches (e.g., CUDA, cuDNN, compiler), backward compatibility, and C++ API design, especially regarding Tensor and Storage abstractions and submodule organization. Some discussions suggest improvements in error messaging, test coverage, and build configurations, often with the goal of making the framework more robust and easier to maintain. Unresolved questions include how to handle specific corner cases (e.g., NaN behavior, tensor propagation), API evolution (e.g., support for overwriting activations), and dealing with environmental variability on different platforms and setups."
2020-08-08,pytorch/pytorch,"The discussions encompass several technical concerns such as the implementation and impact of certain fixes or PRs (e.g., PR #16636, support for lstsq, and tensor aliasing), with questions about backward compatibility, performance benchmarking, and proper integration of features like vectorization and distributed support. There are debates on the starting point for low-discrepancy sequences like Sobol’ and Halton, referencing literature and implementation practices. Some issues relate to build failures or environment-specific errors (e.g., CUDA, MAGMA, oneDNN, PyInstaller, and conda/installer crashes), highlighting dependency and compatibility challenges. Various suggestions include adding options, improving build scripts, and clarifying documentation, with unresolved questions about the best approaches to maintain backward compatibility, performance, and user expectations."
2020-08-09,pytorch/pytorch,"The discussions highlight ongoing challenges with cross-platform compatibility, particularly regarding Windows, macOS, and GPU support, including issues with multiprocessing, support for GLOO, and limited functionality for 3D convolutions in ONNX Runtime. Several contributors emphasize the importance of proper build and runtime environments, suggesting the use of 'if __name__ == ""__main__"":' and conda-managed compilers for Windows, and note that some features like 3D ConvTranspose are not yet supported in ONNX. There are concerns about CI failures, build memory limitations, and the complexity of maintaining large libraries, including strategies for slimming down build configurations. Additionally, discussions touch on the need for benchmarking performance impacts of new features, ensuring correctness in operations like min/max, and proper test coverage for new functionalities, especially for complex or non-standard cases like complex number operations and vectorized intrinsics."
2020-08-10,pytorch/pytorch,"The comments reveal several key discussions and unresolved issues within the PyTorch repository. Notably, there's ongoing concern about handling undefined tensors, memory corruption debugging, and the proper implementation of undefined tensor detection with mmap or nullptr usage. Several questions focus on improving multi-GPU training workflows, specifically wrapping models with DataParallel and checkpointing to utilize multiple GPUs effectively. There are also substantial discussions about ensuring backward compatibility and type safety, including moves toward a `torch.types` module and fixing typing-related bugs, especially around complex tensors and operator overloads. Additionally, numerous build and environment issues—particularly related to CI failures, CUDA integration, and submodule updates—persist, highlighting the need for more robust build procedures, environment management, and reproducibility across platforms."
2020-08-11,pytorch/pytorch,"The comments discuss various technical issues, including potential bugs in PyTorch (e.g., handling `ignore_index`, in-place operations, and backward support for certain functions), compatibility concerns (e.g., with ONNX, numpy, and different Python or CUDA versions), and recent failures observed in CI pipelines possibly due to upstream problems or build configuration mismatches. Several threads also address implementation details such as extending operator support (e.g., `torch.maximum`, `torch.minimum`, `torch.vdot`), managing memory overlaps, and integrating features like sharded optimizers with distributed training. Additionally, there is interest in improvements for debugging and testing (e.g., refactoring large test files, adding support for TorchScript IR scope, and enhancing CUDA synchronization), as well as questions about best practices for version compatibility and API design (e.g., deprecating `Tensor.__init__`, how to handle complex numbers, and the proper placement of GPU-related APIs). Unresolved questions include how to handle backward compatibility with `None` gradients, the impact of different opset versions in ONNX export, and strategies for improving CI stability across various environments."
2020-08-12,pytorch/pytorch,"The comments reflect ongoing challenges with PyTorch's internal tensor and graph representations, especially regarding undefined or undefined-like tensors, memory corruption debugging, and the need for better debugging/tracing tools. Several discussions highlight issues with compatibility, particularly around ONNX export, handling of custom enums, and differences between Linux and Windows GPU behavior, including driver and memory limitations on older GPUs. There are concerns about automatic differentiation and autograd behavior, especially with complex eigenvalue functions and tensor options, as well as the need for more robust test coverage and regression checks for API changes or internal refactoring. Multiple comments also suggest improvements in build configurations, CI reliability, handling of legacy options like `align_corners`, and better internal documentation or code structure to avoid breakages and support future features. Unresolved questions include maintaining cross-platform behavior, ensuring legacy support does not break current workflows, and how to better formalize or enforce safety and correctness in tensor options and graph transformations."
2020-08-13,pytorch/pytorch,"The discussions include technical concerns about the stability, compatibility, and functionality of PyTorch features across different versions, hardware platforms, and environments. Several issues relate to build failures, missing dependencies, and differences in behavior between training and deployment (e.g., quantization, mixed precision, ONNX export). There are questions about the correctness of gradient computations, memory management, and platform-specific support, along with suggestions for additional tests, documentation updates, and API clarifications. Some discussions involve suggestions for improving the deployment process, support for multi-GPU or distributed setups, and enhancing user diagnostics. Unresolved questions include specific implementation details, compatibility fixes, and clarifications on certain features' support status or intended usage."
2020-08-14,pytorch/pytorch,"The discussions reveal ongoing challenges with cross-platform compatibility, specifically with CUDA and hardware configurations on Windows and Linux, as well as issues related to CUDA version discrepancies and driver compatibility. Several questions concern the correctness and handling of gradient calculations for operations involving nan, as well as the implementation details of autograd functions, especially with rank-deficient matrices and mixed data types like complex tensors. There's also interest in improving build reproducibility and debugging support, such as better error reporting and debugging tools, along with suggestions for code refactoring and separation of concerns, especially around optimizer sharding and distributed training mechanisms. Many of the questions remain unresolved, often requiring rebase, patching, or further investigation into underlying systems like memory management, build dependencies, and the internal design of CUDA-related components."
2020-08-15,pytorch/pytorch,"The discussions highlight issues with sampling category limits in `torch.multinomial` when dealing with large tensors (>2^24 categories), leading to runtime errors, with some workarounds suggested such as tensor repetition. Several questions are raised about the implementation of second-order forward-mode autodifferentiation, especially regarding handling views, inplace operations, and the design of API for nested Jacobian-vector products, with suggestions to enhance user API and internal mechanisms. There are ongoing CI failures due to environment, compiler, or build system inconsistencies, notably with package dependencies, and questions about supporting different bytecode versions in model serialization and compatibility tests. Multiple issues involve performance regressions or errors in CUDA, in mixed CPU/GPU environments, or in specific models like LSTM, with suggestions to disable certain features or verify tensor device consistency. Finally, some discussions concern package management, index configurations, and the compatibility of new features such as `stft`, `istft`, and polynomial functions, with considerations about API consistency and maintainability."
2020-08-16,pytorch/pytorch,"The discussions highlight concerns about outdated source code and documentation in the PyTorch JIT and related references, with suggestions to prevent Google from indexing older pages. There are debates on implementing efficient CUDA kernels for specific tensor operations, such as correlation and cross-correlation, emphasizing memory layout and kernel looping strategies. Several technical issues involve autograd behaviors with functions like `max_norm`, embedding normalization, and changes in division operations affecting `torch.div` and `torch.true_divide`. Additionally, there are proposals for handling specialized functions such as `fftshift`, `ifftshift`, and support for multidimensional transforms like `stft`, with discussions on compatibility and namespace organization, especially between NumPy, SciPy, and PyTorch. The overall concerns revolve around improving code performance, consistency, documentation clarity, and expanding functionality while ensuring compatibility and correctness."
2020-08-17,pytorch/pytorch,"The discussions largely revolve around optimizing data loading and multiprocessing in PyTorch, especially related to DataLoader worker management, shared memory, and handling dataset indices without duplicates. Techniques include explicit iterator management and using `cv2.setNumThreads(0)` to prevent OpenCV from blocking I/O operations. Several comments address build issues, compiler errors, and integration challenges with external libraries or environments, highlighting the need for specific compiler flags and environment configurations. Upstream failures and CI test flakes are noted, with suggestions to improve testing robustness, particularly regarding model serialization, bytecode versions, and support for complex or new data types like complex32. Unresolved questions include API design choices for forward mode autodiff, the support of double forward derivatives, performance implications of new features, and the reproducibility and testing of complex or hardware-specific code modifications."
2020-08-18,pytorch/pytorch,"The discussions highlight several technical concerns related to PyTorch, including memory management issues with DataLoader on Windows, variability and reproducibility in training results dependent on seed setting and number of workers, and the handling of deterministic algorithms across different hardware and software configurations. There are questions about the correctness and consistency of serialization formats and model loading between different versions and platforms, as well as performance discrepancies observed across different CPU architectures and libraries like MKL and OpenBLAS. Additional concerns involve the compatibility of PyTorch's ONNX export, the support for complex number operations, and the implementation of mathematical functions across different hardware platforms, seeking improvements in code factoring, serialization, and runtime efficiency. Unresolved issues include errors during compiler optimization on Windows, serialization mismatches with model checkpoints, and the need for clearer guidelines on deterministic behavior and external memory management."
2020-08-19,pytorch/pytorch,"The comments encompass a range of issues faced by users, including error message improvements in autograd related to returning lists vs tuples, and the need for clearer error messages when returning scalars on DataParallel. Several discussions involve compatibility and stability issues when exporting models to ONNX, especially around shape and device handling, with specific attention to complex models like MaskRCNN and the intricacies of tensor serialization between different frameworks and versions. Others focus on performance optimization topics such as depthwise convolution speed, the impact of memory format on cuda operations, and the potential of external allocators for better memory management. Upstream breakages and CI failures due to build environments, compiler errors, and library dependencies are also documented, emphasizing the ongoing maintenance challenges. Unresolved questions include whether certain API changes (like in `torch.nn.parallel.DistributedDataParallel`) are acceptable for backward compatibility, and how to better expose or manage external allocators within the PyTorch ecosystem."
2020-08-20,pytorch/pytorch,"The collected comments reveal various technical concerns including how to handle NaN and gradient propagation at zeros, improvements in ONNX export support especially for upsample and max_unpool operators, and the potential for supporting sparse gradients and external memory allocators. There are also discussions about backward compatibility, the implementation of features like adaptive softmax, and the behavior of certain PyTorch functions such as `zero_grad()` and `to()`. Several suggestions aim to improve code maintainability, such as relocating implementation details and enhancing documentation for clarity. Unresolved questions include the support for certain operators (notably max_unpool2d in ONNX), the behavior and implementation of complex number support especially for half precision, and how to better handle external allocators and distributed training issues."
2020-08-21,pytorch/pytorch,"The discussions cover various topics including the appropriate placement of hierarchical softmax-like layers (such as AdaptiveInput) within the PyTorch ecosystem, with suggestions to include it in `adaptive.py` for efficiency and memory benefits; support for supporting both ROCm and CUDA simultaneously; and improvements to tensor indexing and performance in specific workloads like pruning. Several issues involve build errors, compatibility, and testing failures across different platforms, with specific concerns about large class hierarchies and implementation details for functions like `qr_backward`, `triangular_solve`, and tensor type unification. There are questions about adapting or extending existing PyTorch functionalities (e.g., `pack_padded_sequence`, `add_graph`, `torch.div`) and about addressing CI failures and build inconsistencies, especially related to external dependencies and compiler errors. The discussions also include suggestions for better organization of code, documentation clarifications, and more detailed testing to prevent silent regressions."
2020-08-23,pytorch/pytorch,"The discussions primarily address technical challenges related to GPU support and hardware compatibility, including AMD ROCm support for PyTorch, with users expressing interest in AMD GPU and iGPU support pending XNACK availability. There are ongoing efforts to improve build and test stability, such as fixing CI failures, handling upstream breakages, and addressing specific issues like spectral normalization causing export errors and NEON intrinsics support on older gcc versions. Suggestions include simplifying spectral norm removal code and adding fallback mechanisms for NEON intrinsics for broader compiler compatibility. Some concerns also involve build process delays, as seen with lengthy conda installations and CI failures, alongside requests for clearer testing of NEON support in CMake to facilitate easier backporting and patching. Overall, the discussions focus on enhancing hardware support, build reliability, and compatibility across diverse environments."
2020-08-24,pytorch/pytorch,"The comments reflect a variety of technical concerns related to memory management, computational efficiency, and compatibility. Several discussions focus on optimizing GPU memory usage in convolution operations, evaluating the impact of slicing versus padding, and proposing code modifications to improve performance without increasing computation time. Others raise questions about the correct usage of PyTorch's API, such as the proper implementation of `torch.split`, handling hash functions, and exporting models to ONNX, including compatibility issues with symbolic operations and operators. Additional concerns include ensuring correctness of backward computations, especially for functions like QR decomposition, and addressing build failures or CI errors caused by system or environment misconfigurations. Overall, the discussions highlight ongoing efforts to enhance robustness, usability, and performance, alongside debugging and platform compatibility challenges."
2020-08-25,pytorch/pytorch,"The comments reveal ongoing efforts and discussions around improving reproducibility, API consistency, and performance in PyTorch. Several issues relate to seed management and deterministic behavior, where proper seed setting, seed independence across CPU and GPU, and deterministic algorithms are emphasized. There's interest in expanding and documenting features like sparse batched multiplication, histogram-like operations, and flexible `load_state_dict` behaviors, with proposed APIs and implementation strategies discussed. Additionally, challenges around build reproducibility on older hardware, handling non-standard configurations, and managing CI failures due to environment or upstream issues are evident. Overall, the discussions highlight active maintenance, enhancement, and troubleshooting to make PyTorch more robust, performant, and user-friendly."
2020-08-26,pytorch/pytorch,"The discussions encompass various issues in the PyTorch codebase, including handling of `ignore_index` support across different versions, ensuring reproducibility with multi-worker DataLoader, support for sparse-batched matrix multiplication, and ONNX export compatibility for operations like upsampling, cumsum, and others. Several PR reviews highlight concerns about backward compatibility, proper exception propagation, and the need for additional testing, especially around complex operations, quantization, and CUDA-specific behaviors. There are ongoing efforts to improve precision and robustness of numerical routines like `i0`, mitigate non-deterministic behaviors in certain CUDA kernels, and fix build-time errors related to compilers and external libraries. Unresolved questions include the proper implementation of conjugate views with a negative flag, handling of in-place operations affecting auto-differentiation, and integrating new features without disrupting existing CI workflows. Overall, the core concerns focus on maintaining backward compatibility, correctness, and performance while progressively enhancing support for new hardware and operator features."
2020-08-27,pytorch/pytorch,"The comments reflect ongoing development and troubleshooting across various PyTorch features, including issues with CUDA driver errors, sparse tensor support, ONNX export, and integration with external libraries (e.g., RMM, MAGMA). There are concerns about stability and correctness of new features such as sparse autograd, custom operators, and kernel implementations, with specific questions about device management, API changes, and performance implications. Several bugs relate to build processes, conflicting code during automatic merges, or unsupported configurations, raising questions about best practices for version compatibility, testing, and infrastructure setup. There are also discussions about improving developer experience through documentation, testing, and code refactoring, with a focus on robustness, maintainability, and clarity. Overall, unresolved issues include build failures, regression risks, and the need for more comprehensive testing and documentation to ensure reliability of new and existing functionality."
2020-08-28,pytorch/pytorch,"The comments highlight several core issues: difficulties in interfacing ctypes with pybind11 bindings for NVTX functions; design concerns about the API consistency and usability of learning rate fetch functions and the ""get_lr"" renaming; performance regressions in multi-threaded CPU support, especially with OpenMP and multithreading support in compiled libraries; and build system challenges related to missing source files, CMake and Bazel build failures, and inconsistent submodule or environment configurations. Several discussions involve potential fixes, such as refactoring API design for clarity, improving build scripts and dependency management, and fixing code generation or linking issues. Unresolved questions include how to handle complex dtype in tensor functions, better build error handling, and sophisticated support for external or legacy APIs like FFT. Overall, the issues suggest ongoing concerns with build reliability, API consistency, multi-platform support, and performance optimization."
2020-08-29,pytorch/pytorch,"The comments reflect various ongoing concerns and questions about the PyTorch project, including requests for more C++ API support and improvements, especially for users integrating in C++. There are discussions about performance optimization, such as handling unpadded tensors, and the need for better testing and coverage of complex number support, as well as handling of tensor memory formats. Several issues relate to CI/build failures, often caused by upstream breakages, merge conflicts, or environment problems, with suggestions on rebasing or fixing root causes. Additionally, there are questions about exception handling best practices in Python, including when to use `raise ... from ...` versus suppressing exception context, and some requests for documentation or test case enhancements."
2020-08-30,pytorch/pytorch,"The discussions mainly revolve around enhancing autograd's forward-mode differentiation, including the implementation of a new API that enables multiple forward derivatives without recomputation, and the separation of forward and backward AD code for modularity and optimization. There are questions about integrating dual numbers as a core API concept versus an internal implementation detail, especially concerning the interaction with higher-level APIs and perturbation management. Some concerns include handling graph creation with forward AD, the mostly eager evaluation approach versus graph-based execution, and proper exception chaining for clearer error reporting. Additionally, issues such as build failures, dependency management, and compatibility with tools like TorchScript, as well as the handling of deprecated tensor constructors, are discussed. Unresolved questions include the best API design for user-facing features, managing unused parameters in traced models, and fixing build or platform-specific errors during development."
2020-08-31,pytorch/pytorch,"The comments reveal issues related to multiprocessing start methods in PyTorch, particularly the ""spawn"" method causing errors like 'context has already been set.' Solutions involve restructuring code to set the start method before multiprocessing is initialized and avoiding redefinition. There are also discussions about the impact of `mp.set_start_method` on data loading, the importance of avoiding pickling local objects, and adjustments needed when working with shared resources like HDF5 or custom datasets. Some suggest that avoiding changing the start method after it’s been set, and carefully managing dataset creation within processes, can prevent such errors. Unresolved questions include how to handle already-set contexts and ensuring compatibility across different environments and data sources."
2020-09-01,pytorch/pytorch,"The comments primarily revolve around improving documentation and search engine indexing for old PyTorch docs, handling build configuration issues especially regarding unsupported or deprecated compilers and environment setups, and addressing failures in continuous integration (CI) pipelines caused by platform-specific issues or missing dependencies such as 'dataclasses'. Several discussions suggest code and API design improvements, including better way to handle broadcasting and tensor layout conversions, as well as ensuring that custom operations and models are compatible with tracing, serialization, and backend fallback mechanisms. There are concerns about maintaining backward compatibility, especially with autograd and JIT functionalities, and about ensuring tests and coverage are sufficient to catch regressions or incompatibilities introduced by recent changes. Unresolved questions include how to manage platform-specific bugs (Windows, macOS, Linux, and Android), how to better handle CI failures caused by network issues or misconfiguration, and how to improve the stability of build scripts and dependency management in various environments."
2020-09-02,pytorch/pytorch,"The discussion highlights several key issues: the most prominent being the proper use and registration of CUDAHooks, especially in the context of static initializers and dynamic linking, which affects GPU device enumeration and CUDA-related functionalities; the need for better testing, including manual tests and handling of flaky or environment-dependent failures. There's concern about static initializers' reliability, especially with issues like static registration, linker behavior, and static initializers' timing, suggesting future overhaul possibilities. Another core concern involves integrating new API features, like the proposed `load_state_dict`, and managing PyTorch’s behavior around tensor device placement, ownership, and serialization, particularly when involving distributions, the backwards compatibility of schema changes, and model loading issues. Additional discussions address ensuring stability and correctness of backend-specific code, especially on diverse OSes and hardware platforms, handling known build issues (like missing modules and platform-specific errors), and dealing with build failures caused by environment misconfigurations or missing modules."
2020-09-03,pytorch/pytorch,"The discussions reveal several recurring themes: the best practices for deindexing web pages (e.g., using meta tags vs. robots.txt) and automating such updates; performance considerations and profiling techniques for GPU operations, especially concerning synchronization and kernel selection; API design for PyTorch's forward and backward functions, including the impact of code structure on autograd and JIT compilation; challenges related to enabling forward-mode automatic differentiation without breaking existing APIs, and the management of dual numbers and perturbation tags; and the maintenance and robustness of the build, test, and serialization infrastructure, particularly regarding conflicts, environment differences, and upgrade risks. Questions often focus on how to balance API usability, internal consistency, compatibility, and performance, with unresolved issues about input shape handling, codebase stability, and detailed implementation steps for complex features like sparse tensor support, custom C++ operators, or distributed training."
2020-09-04,pytorch/pytorch,"The comments reflect a range of technical concerns primarily related to issues with specific PyTorch functionalities, performance regressions, and build or compatibility problems, often on specific hardware or configurations. Notable points include difficulties with the JIT implementation of custom RNN cells (like the GRU), such as errors during gradient computations and tracing, as well as regression issues linked to recent updates in functions like `multinomial` and `empty`. Several discussions also highlight build failures due to system-level discrepancies (e.g., missing `strtod_l`, module compilation errors) and integration issues with external tools or libraries (e.g., on Windows, with CMake, or on ARM). Solutions proposed involve code patches, rebase strategies, and test updates, while unresolved questions concern the behavior of specific operators, compatibility of extensions, and CI failure diagnostics."
2020-09-05,pytorch/pytorch,"The comments highlight ongoing challenges with tensor shape handling during ONNX export, specifically related to changes in tensor size representation (from list of ints to list of tensors), which impacts model traceability and correctness. Several discussions focus on build failures and compatibility issues across different systems, compilers, and library versions, especially around CUDA, ROCm, and platform-specific dependencies. There are concerns with the limitations of certain control flow constructs (like while loops) in exporting models to ONNX, requiring code modifications for compatibility. Additionally, some comments address CI pipeline failures and flaky tests, emphasizing the need for better testing strategies and environment configurations. The unresolved questions primarily involve resolving build errors, ensuring proper shape handling, and maintaining test stability across diverse platforms."
2020-09-06,pytorch/pytorch,"The discussions highlight several technical issues and suggestions, including the inconsistency in error handling and the potential to improve error messages by using specific macros. There are multiple reports of build or runtime errors, such as missing DLLs on Windows, library load issues on macOS related to rpath configurations, and CUDA compilation errors due to host-device function calls, suggesting ongoing platform-specific compatibility challenges. Concerns also relate to implementation details, like the appropriate way to implement padding modes (Python vs. C++), and whether certain functions (e.g., `tobytes`, `byteswap`) should be supported for CUDA tensors. Additionally, there is active discussion on performance considerations and correctness for floating-point precision (float16, bfloat16), emphasizing safer approaches like casting to float32 during computations, and debates on approach refinements, testing, and proper documentation to prevent over-subscribing hardware threads. Unresolved questions include platform-specific build issues and the best way to handle dtype precision, as well as ensuring consistency across Python and C++ implementations."
2020-09-07,pytorch/pytorch,"The discussions reveal several key concerns: (1) Clarification on the order and retrieval of hidden (`h_n`) and cell (`c_n`) states in bidirectional RNNs, with recent fixes provided; (2) Proper placement and scope of hierarchical embedding layers such as `AdaptiveInput` within the PyTorch modules, alongside questions about memory savings and implementation details; (3) Handling dynamic inputs in ONNX export, especially related to `avg_pool` and operator support across different opset versions; (4) Compatibility issues in protobuf versions and build failures on various systems, including Windows and ROCm, due to missing or incompatible dependencies; (5) Concerns about the design of CUDA stream pooling for RPC, including synchronization strategies, stream reuse, and potential performance trade-offs, along with API consistency for in-place and out-of-place tensor operations. Unresolved questions include the best way to support dynamic input sizes in ONNX, ensuring cross-platform build stability, and integrating new features like global variable management and binary operation variants into the API."
2020-09-08,pytorch/pytorch,"The discussions cover a range of technical concerns including the appropriateness of integrating specific features such as `PixelUnShuffle` into core PyTorch or torchvision, and whether certain functionalities like dataset caching should be embedded within PyTorch core versus handled externally by users. There are ongoing discussions about implementing and optimizing CUDA and ROCm support, particularly in relation to third-party tools like cub, and questions about kernel fusion, performance timing, and compatibility across hardware. Issues related to the behavior of `BatchNorm` in evaluation mode, the correctness of model conversion, and the importance of clear documentation are also prominent. Additionally, there are concerns about stability, test coverage, and build failures on different platforms, with suggestions to improve error handling and test robustness."
2020-09-09,pytorch/pytorch,"The discussions predominantly revolve around framework build issues, especially related to compiler support, CUDA/ROCm compatibility, and environment setup. Many comments address problems caused by mismatched compiler versions, faulty build configurations, or missing dependencies like CUDA headers or HIPify scripts, often resolved by rebuilding, environment adjustments, or explicit flag settings. Several issues concern the representation and handling of dimensions, autograd fallback kernels, and legacy support—highlighting the need for clearer API design and more robust error reporting. There are also ongoing discussions about improving testing coverage, especially for distributed and quantization functionalities, and about frameworks for consistent benchmarking. Unresolved questions include how to handle compile-time errors on various platforms, how to improve user API clarity for dimension semantics, and how to streamline cross-platform build support, especially in Windows and ROCm environments."
2020-09-10,pytorch/pytorch,"The discussions reveal several key concerns: regarding the behavior of `torch.nn.functional.max_pool` padding — PyTorch's implementation currently throws an error when padding is too large, with suggestions to shift to warnings or different approaches; support for sparse tensors in multiprocessing, which is largely unsupported currently and complicated by return types; and the support for reliable `addr` and `ger` operations, where refactoring is proposed to unify their implementation in ATen to improve maintainability and performance. Additional topics include handling of `out=` parameter testing through OpInfo infrastructure, issues with Python 2 style type comments for `TorchScript`, and the need to prevent user errors in C++ API use that could cause device or execution failures. Some discussions also mention CI/test failures, build issues due to missing modules like `dataclasses`, and platform-specific issues like HIP support or CUDA version compatibility. Many unresolved questions relate to improving API robustness, testing frameworks, and cross-platform support for new features."
2020-09-11,pytorch/pytorch,"The discussions mainly revolve around ongoing development and debugging of division operators supporting integer tensors and error handling for zero denominators in PyTorch, with considerations on whether to raise exceptions within kernel loops or outside for thread safety. Several discussions also address handling the device and memory behavior during tensor resizing, especially for sparse tensors, questioning whether resizing should respect tensor strides and device placements, and how high-level APIs should manage such operations. Additionally, there are concerns about compiler and environment issues, such as build failures due to missing modules (`dataclasses`), CUDA-related errors when compiling complex kernels, and the need for better testing, coverage, and build infrastructure to prevent regressions. Some threads propose more systematic and reusable testing strategies for sparse operators, tensor typing, and performance benchmarks, aiming to improve robustness and maintainability. Lastly, there are ongoing issues with CI failures and merge conflicts, often caused by environment or configuration inconsistencies, that need resolution to ensure stable development progress."
2020-09-12,pytorch/pytorch,"The comments largely revolve around environment management in Python, especially ensuring that Jupyter and PyTorch are installed in the active conda environment to avoid module import errors. Several issues highlight compatibility and correctness in PyTorch functions, including JIT compilation errors, optimizer support inconsistencies, and warnings about backward incompatibility introduced by certain PRs. Additionally, there's ongoing discussion on testing coverage, benchmarking performance regressions (particularly on broadcasted ops), and ensuring that new functionalities (like window functions or bitpacking) are well-tested and documented. Some build failures and CI issues are noted, often related to external dependencies or merge conflicts, with suggestions to improve cache handling and more systematic testing. Unresolved questions include the proper handling of device-specific code, the impact of code refactors on performance, and clarifications on PyTorch internals and API expectations."
2020-09-13,pytorch/pytorch,"The discussions highlight recurring issues related to memory allocation errors during model evaluation, suggesting possible misunderstandings of memory usage and allocation sizes. There are proposals to improve usability, such as combining `model.eval()` and `torch.no_grad()` into a context manager to reduce verbosity, or making `model.eval()` itself a context manager, to streamline evaluation mode switching. Several technical concerns involve build and environment setup, including handling of CUDA architecture lists, proper configuration for older GPU hardware, and build system inconsistencies, especially on Windows. Automating cache clearing for performance benchmarking, improving test coverage for sparse autograd operations, and addressing CI failures due to upstream or environment issues are also discussed. Some unresolved questions involve how to better document behavior for multi-dimensional norms and how to handle deprecated or incompatible build configurations without regressions."
2020-09-14,pytorch/pytorch,"The discussions reveal concerns about non-deterministic results in training due to RNG state serialization (Issue #4333), and about the correctness and implications of handling dual numbers and higher-order derivatives (Issue #10223). Several issues relate to inconsistent behavior of functions like `tuple(tensor)` when dealing with scalar tensors, which violates Python's hash contract (Issue #7733). Multiple issues highlight build and compatibility problems, especially around CUDA versions, environment setup, and the use of `cmake`, with some problems tied to build configuration, external dependencies, or environment mismatches (Issues #44616, #44628, #44629). The auto-generated CI failure summaries point to build conflicts, unmerged branches, or unrelated test failures, underscoring the complexity of the test infrastructure and the need for careful triage. Overall, these discussions focus on ensuring reproducibility, API correctness, stable build processes, and maintenance of code consistency amidst evolving environments."
2020-09-15,pytorch/pytorch,"The comments indicate ongoing discussions around PyTorch features and implementation details, including concerns about supporting scalar and broadcasted sources for scatter operations, and clarifications on autograd use of `.data` vs. `detach()`. Several issues related to build failures, compiler errors, and CI test flakiness are also prominent, often involving internal internal compiler errors, conflicts from upstream merge issues, or specific platform constraints (Windows, Linux, macOS). There are questions about the stability and correctness of certain functions (like `torch.outer` and `torch.addr`), potential performance improvements via composite operations, and the proper handling and testing of new features (autograd, quantized tensors, custom classes). Some discussions highlight the importance of adding more thorough tests, fixing existing bugs, and understanding environment-specific failures with detailed profiling. Overall, unresolved issues include ensuring build stability across platforms, enforcing correctness and consistency in function behaviors (especially for custom and quantized types), and validating performance gains."
2020-09-16,pytorch/pytorch,"The collected GitHub comments reveal several recurring technical concerns: the importance of clear documentation for functions like `torch.load()` and the use of `map_location='cpu'` for GPU memory constraints; issues with tensor hashing behavior and its impact on dictionary keys; memory consumption and performance profiling on CUDA-enabled devices, especially relating to kernel loading and memory management; and challenges with the implementation and testing of features such as `torch.unique()` with `dim` and `return_counts`, as well as the handling of complex numbers in operations like `cholesky_cuda`. There are also discussions around the integration of new operators, support for different data types (like int8, uint8, and bfloat16), and ensuring backward compatibility while addressing upstream limitations or bugs, often with suggested fixes or workarounds. Several comments question the appropriateness of current test strategies and the stability of CI jobs, highlighting a need for more robust testing and error handling, especially in distributed or GPU contexts. Overall, unresolved issues include ensuring consistent behavior across different hardware backends, optimizing kernel loading/memory usage, and improving documentation and test coverage for complex features."
2020-09-17,pytorch/pytorch,"The comments reveal ongoing efforts to address various issues in PyTorch, such as improving optimizer support for sparse updates, ensuring consistency with numpy's argument behavior, and supporting complex number operations. Several discussions focus on enhancing APIs and internal mechanisms, like reparametrization strategies, better handling of nested structures, and ensuring deterministic behavior in different environments. There are also concerns about specific bugs, such as incorrect shape sharing upon serialization, and performance considerations for profiling and internal kernel optimizations. Many unresolved questions pertain to compatibility across versions, efficiency impacts, and proper testing strategies, especially for new features and non-standard use cases."
2020-09-18,pytorch/pytorch,"The comments primarily revolve around troubleshooting issues such as runtime errors, especially assertions related to label ranges in loss functions, and the importance of ensuring proper data types when using operations like `autograd`. Several discussions suggest solutions including adjusting label values, checking and correcting input types, or modifying model code (e.g., replacing class labels, ensuring tensor types are compatible). There are also repeated mentions of auxiliary concerns like build environment problems, dependency versions, and CI failures, indicating broader infrastructure challenges. Overall, the main concerns involve correcting data inputs and type support for operations, as well as troubleshooting and stabilizing build/test environments."
2020-09-19,pytorch/pytorch,"The comments indicate ongoing issues with Windows and Linux performance, particularly with DataLoader's slow multi-process data loading and deterministic scatter reductions. Several users report compatibility problems with different Python versions, CUDA architectures (notably sm_86), and difficulties with model tracing and ONNX export involving tensor sizes. Multiple bug reports highlight errors related to DLL loading, DLL conflicts, or unsupported features in specific PyTorch or CUDA versions, often resolved by updates, workarounds, or environment adjustments. There are concerns about upstream build failures, certain compilation errors, and the need for better reproducibility, debugging, and support for features like non-persistent buffers, scatter operators, and custom operators, especially on non-Linux platforms."
2020-09-20,pytorch/pytorch,"The discussions highlight several technical concerns, including potential workarounds for variable batch sizes by tensor repetition or BatchNorm adjustments, and questions about the support and correctness of `torch.where` with scalars. There are comments about installing and supporting specific hardware, such as `libnccl` versions incompatible with architecture, and issues with static linking dependencies like `libcaffe2_nvrtc.so` in shared libraries. Several bug reports and feature requests are present, such as implementing a stable sort/argsort, improving quantization, and adding a histogram function, with performance and correctness considerations. CI/CD failures across multiple systems, potential deadlocks, and the importance of careful rebasing and merging of stacked PRs are also discussed, indicating ongoing integration and testing challenges."
2020-09-21,pytorch/pytorch,"The comments reflect ongoing discussions about various technical issues and feature enhancements in the PyTorch repository. Key concerns include ensuring parity between `torch.chunk` and `np.array_split`, improving performance of certain tensor operations like `torch.addr` and `torch.outer`, and resolving build or compatibility issues on different hardware (e.g., CUDA 11+ on RTX 3080). There are also suggestions for API consistency, such as implementing `hermitian` operators or supporting conjugate views, with considerations around correctness, memory management, and multithreaded safety. Some discussions address test robustness, stability of CI builds, and handling deprecation or backward compatibility. Unresolved questions involve how to best integrate new features like sparse support, customized tensor behaviors, or cross-platform build configurations, as well as balancing performance with correctness in complex scenarios like lazy tensors or conjugate operations."
2020-09-22,pytorch/pytorch,"The discussions predominantly revolve around issues linked to PyTorch's autograd and JIT internals, especially involving how functions like `autograd.grad` and operators are dispatched and registered, with some concerns about backward compatibility, operator collection, and API consistency. Several threads also touch on debugging, profiling, and performance profiling, highlighting the need for better error messages, reproducibility, and profiling tools (like `benchmarks`). Multiple comments mention the importance of internal build fixes, plugin API stability, and the complexities of supporting dynamic or custom operators in TorchScript, including custom class registration via TorchBind. Questions about CUDA, XLA, and compatibility across different hardware and driver versions are also common, along with requests for better API ergonomics and clarification about support for certain features (e.g., `tensor_split`, `torch.tensor_split`). Unresolved issues include compatibility concerns, the need for better documentation, and ongoing build or CI failures that are sometimes related to upstream regressions or internal build environment changes."
2020-09-23,pytorch/pytorch,"The comments reveal several challenges related to PyTorch's internal development and external compatibility. Key issues include the maintenance of compiler and library dependencies (notably, the deprecated Valgrind version and outdated GCC), and the need for better testing and documentation, especially for new features like the dispatch macro system and the `tensor_split` API. Several failures on CI platforms are attributed to upstream breakages, unresolved symbol errors, or build environment mismatches, which require updates to build scripts or external library versions. There’s also discussion on how to improve code organization, such as consolidating macro definitions for enum consistency and handling deprecation warnings. Unresolved questions involve the best approach to documenting new APIs, the safe handling of different compiler and runtime configurations, and how to streamline testing for new features."
2020-09-24,pytorch/pytorch,"The discussions highlight issues related to memory management and DataLoader deadlocks, which can be mitigated by explicit iterator handling and system limits. Several feature requests and bugs remain unresolved, including support for max_unpool2d in ONNX, documentation improvements for new ops, and support for 3D reflection padding. Compatibility concerns are evident with CUDA architectures (sm_86), and some failures are attributed to upstream or environment issues such as network errors, build system mismatches, and missing dependencies. There are ongoing efforts to improve test coverage, stability, and documentation, with some tasks deferred or requiring further investigation, especially in cross-platform and deep learning-specific functionalities like quantization, distributed training, and custom operators. Unresolved questions include performance impacts of new features, better support for hardware variations, and ensuring correctness and consistency across different setups."
2020-09-25,pytorch/pytorch,"The comments reflect a range of technical concerns across multiple areas in the PyTorch repository. Key issues include the need for better support and documentation of sparse data structures, especially graphs, and performance optimizations for sparse kernels post-pruning. Several discussions focus on enhancing distributed training APIs, including batching Jacobian computations and improving process group initialization. There are also concerns about code consistency and maintainability, such as organizing macro definitions for backend dispatch keys, managing backward compatibility, and handling model serialization/deserialization intricacies. Unresolved questions involve the stability and support for new hardware architectures (e.g., Ampere, ROCm), testing strategies for large-scale datasets, and ensuring correctness and performance of newly introduced features and refactorings."
2020-09-26,pytorch/pytorch,"The discussions primarily revolve around the implementation and evaluation details of various PyTorch features, including the effectiveness of NCE loss with different sampling methods, and the behavior of KL divergence on diagonal distributions, with suggestions for upstream contributions. Several issues concern CUDA memory management, multi-GPU and distributed training, and environment setup troubleshooting, with practical solutions like setting `num_workers=0` in DataLoader. Questions are raised about the correctness and consistency of specific functions, like `log_prob` implementations and the behavior of `ceil_mode` in pooling, as well as the development of new features such as OpInfos, functional APIs for distributions, and the integration of backend extension code in documentation. There are also discussions about CI failures, build environment configuration, and the necessity of thorough testing and validation, especially in the context of large-scale datasets and multi-backend compatibility. Unresolved questions include strategies for handling backward-incompatible schema changes, ensuring robust and representative dataset splits, and managing upstream dependency issues."
2020-09-27,pytorch/pytorch,"The discussions highlight issues related to DataLoader's deadlock caused by OpenCV thread management and system resource limits, with suggested solutions like setting `cv2.setNumThreads(0)` or adjusting `ulimit -n`. Several users report problems related to memory allocation errors when loading large models, which can sometimes be mitigated by garbage collection or reinstallation of dependencies like numpy. There are multiple build failures due to missing header files (`torch/csrc/jit/tensorexpr/function.h`), often because required submodules or dependencies are not correctly included or configured in the build environment. Additionally, some concerns include handling multi-GPU setups involving different architectures (NVIDIA and AMD), and the need for better error messaging around model incompatibilities with distributed training. Overall, unresolved issues involve build configuration errors, memory management, and proper dependency inclusion for various hardware and software environments."
2020-09-28,pytorch/pytorch,"The discussions reveal several key concerns: maintenance of NaN handling in gradients and entropy computations, the integration of complex number support, and issues related to build reproducibility and source code accessibility, especially regarding the consistency of git objects across different environments. Several conversations focus on ensuring correct and stable behavior of PyTorch's autograd, CUDA, and ONNX export functionalities, including the accuracy impact of quantization and correctness of shape inference. There are also recurring questions about build system robustness, especially regarding versioning and source code fetch failures, indicating a need for clearer documentation, better precondition checks, and possibly refactoring to improve reproducibility and debugging processes. Additionally, some discussions suggest API clarity improvements, such as the support of `out` variants or the handling of ignored attributes, along with the importance of comprehensive testing and verification across hardware and software configurations."
2020-09-29,pytorch/pytorch,"The collected comments highlight efforts and considerations around improving PyTorch's model parameter handling, such as leveraging `__getattr__` for computed parameters, and discussions on in-place parameter updates for gradient descent. There's a focus on clarifying API behaviors, especially with `device` and `dtype` compatibility, and supporting `out` parameters in functions like `sign`. Several concurrency-related issues arise with the `char_bit` macro in C++, indicating potential portability or compiler compatibility concerns, especially on Windows. Many failures in CI pipelines are due to external repository access problems and possibly outdated or inconsistent code states, which are often addressed by rebasing or updates. Overall, unresolved issues include ensuring backward compatibility, improving API clarity, handling edge cases with mixed precisions and data types, and fixing platform-specific compiler issues."
2020-09-30,pytorch/pytorch,"The shared comments mainly revolve around issues with device compatibility and optimizer state management in PyTorch, emphasizing the importance of correct device placement when loading models and optimizers, especially in contexts involving DataParallel or moving models across devices. Several comments highlight the need for better handling and documentation when switching models and optimizer states between CPU and GPU/other devices, suggesting that the order of loading state dictionaries matters and that reinitializing optimizers might be necessary after device changes. Additionally, there are discussions about improving the PyTorch testing framework—such as introducing better parameterization decorators similar to pytest—to facilitate device and dtype variant testing, along with considerations about compatibility and future-proofing for features like `__torch_function__`. Unresolved questions include whether certain proposed API enhancements (like device-aware loading or argument grouping in codegen) are feasible or are already planned. Overall, discussions focus on ensuring correct device alignment during model loading, improving test infrastructure, and aligning code generation and API design for better maintainability and usability."
2020-10-01,pytorch/pytorch,"The comments cover various topics, including the support for new hardware (e.g., NVIDIA RTX 3080) and the need to update build configurations (e.g., support for sm_86 in CUDA), with some discussions on version compatibility and static linking challenges. There are multiple discussions about API design and implementation choices for features like `torch.linalg.svd`, including providing backcompat and C++ signatures, as well as handling functions that have output types depending on runtime values, suggesting the use of `Literal` annotations or deprecating such functions. Concerns also address testing improvements, such as better test coverage, handling flaky CI failures, and the impacts of certain changes on performance and correctness, including issues with autograd, complex number support, and data-dependent control flow. Additionally, some discussions involve the organization and structure of codegen, improving clarity around tensor arguments, and ensuring proper version bumps and build processes for releases. Overall, key unresolved questions include balancing breaking changes with backward compatibility, performance impacts of new features, and infrastructure bugs affecting CI."
2020-10-02,pytorch/pytorch,"The comments highlight ongoing efforts to improve PyTorch's features, compatibility, and performance, with particular attention to tensor support (especially complex types), API extensibility, and robustness (notably in distributed, quantization, and convolution contexts). Several discussions focus on correcting bugs or inconsistencies, such as handling of `conj()` behavior, `__torch_function__` support, and shape/dimension support, often suggesting the need for more comprehensive tests or API refinements to prevent regressions. Further, there are technical considerations around integrating third-party tools, updating documentation, and addressing platform-specific issues like Windows JNI support or CUDA/cuBLAS bugs. The reviewers emphasize incremental improvements, maintaining API clarity, and avoiding backward-incompatible changes, while also proposing enhancements to profiling, benchmarking, and user-facing features like stream usage or operator attributes. Unresolved questions remain about stabilizing new features, ensuring build reliability across environments, and aligning implementation with external libraries' behaviors (e.g., cuBLAS, NumPy)."
2020-10-03,pytorch/pytorch,"The discussions largely focus on enhancing model analysis, such as adding parameter counts and visualization capabilities, with Python code snippets illustrating summaries and visualization tools akin to torchviz. Several issues concern runtime behavior anomalies, like CUDA illegal memory access errors, which are attributed to driver incompatibilities or environment discrepancies. There are ongoing efforts to improve dynamic shape inference, type checking across TorchScript boundaries, and compatibility in scripting with type-specific `isinstance` checks, highlighting unresolved questions about static vs. dynamic shape handling. Contributors express interest in refactoring or extending existing modules, e.g., integrating `attn_mask` and `key_padding_mask`, with some work already merged or pending review. Debugging discrepancies, environment setup challenges, and ensuring consistent testing across platforms remain prominent, alongside questions about best practices for code maintenance and contribution workflows."
2020-10-04,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch's data loader multiprocessing on Windows, with workarounds such as using `if __name__ == '__main__'` or reorganizing code modules. Several threads discuss limitations with TorchScript support for custom classes, especially regarding `isinstance` checks and class design, indicating active development to improve scripting compatibility. Other concerns involve compatibility and build errors, such as missing static libraries (`libtorch.a`), unsupported GPU architectures in CUDA, and issues with specific hardware like RTX 8000 cards and cudnn configurations. Additionally, there are discussions on feature requests and API improvements—such as the conjugate transpose in SVD, DDP communication hooks, and exporting behaviors—along with observations about CI failures and test coverage impacts. Overall, multiple unresolved technical challenges revolve around system compatibility, scripting support, and robustness of distributed and hardware-dependent features."
2020-10-05,pytorch/pytorch,"The comments reflect ongoing discussions on several core areas: (1) efficient sparse tensor operations in PyTorch, including broadcasting, memory management, and support for formats like CSR/CSC, with consensus on the complexity and need for specialized kernels; (2) API design considerations such as consistent function interfaces, handling of default arguments in overloads, and supporting tensor-like objects in scripting and autograd, with suggestions for improving robustness and reducing code duplication; (3) improvements to the TorchScript compiler, including handling control flow like `assert`, `if`, and `value_or`, along with efforts to maintain backward compatibility and reduce API churn; (4) performance benchmarking, especially on new hardware like the RTX 3090, CUDA version effects, and handling build failures across different environments; and (5) API consistency, correctness, and documentation updates across linear algebra, quantization, and distributed features, emphasizing careful handling of complex types, property attributes, and runtime behaviors such as stream management and versioning. Unresolved questions include the implementation of features like `torch.linalg.norm` and `svd`, support for user-defined types, and safe handling of control flow in FX."
2020-10-06,pytorch/pytorch,"The discussion highlights several technical concerns, notably around the implementation and usability of PyTorch's features. Key issues include handling out-of-memory errors during package installation, the need for more efficient tensor operations like fftshift, and clarifying API behaviors, especially around functions like `svd`, `reciprocal`, and sparse/dense tensor handling—aiming for consistent return types and improved user experience. There are questions about optimizing compilation processes for different hardware architectures (e.g., CUDA compute capabilities) and ensuring compatibility and correctness in complex control flow scenarios within FX. Additionally, suggestions emphasize making the codebase more maintainable through better API design, including the potential adoption of dataclasses, clearer documentation, and more robust testing. Unresolved questions involve the best strategies for supporting backward compatibility, input/output behaviors, and integration workflows for diverse systems and hardware."
2020-10-07,pytorch/pytorch,"The discussed comments reveal several key technical concerns and questions:  
1. There's uncertainty about optimal data types in model training, particularly regarding the slower performance of doubles (`float64`) versus floats (`float32`), and issues with tensor type consistency during loss computations.  
2. Multiple threads discuss memory management and performance optimization in distributed and GPU contexts, including handling gradient buffers efficiently in DDP hooks and reducing peak memory usage during backward passes.  
3. Several questions address API design and backward compatibility, especially related to function overload disambiguation, `__torch_function__` handling, and maintaining consistent behavior for sparse/tensor operations, including fill value implications in sparse vs strided tensors.  
4. There are recurring concerns about build failures, compiler issues, and system dependencies (e.g., MKL, ROCm), with suggestions to improve tests, error messaging, and support for new hardware/feature sets (like CUDA 8.6, newer ROCm, Windows distributed training).  
5. The discussions include proposals for API improvements, such as more precise error handling, documentation clarifications, and better tooling for codegen, with some unresolved questions about implementation complexity, ONNX export issues, and versioning or handling of complex number support."
2020-10-08,pytorch/pytorch,"The comments highlight ongoing challenges with maintaining and automating documentation updates, especially regarding version switching, search indexing, and the handling of new pages or features like master or main branch references. There are multiple issues related to runtime errors, notably with multiprocessing and distributed training (e.g., ""Broken Pipe"", ""RuntimeError: can not pickle torch.futures.Future"") and version-specific bugs in CUDA, cuBLAS, and MKL libraries, some of which appear hardware or driver-specific. Several discussions focus on improving PyTorch's robustness, performance benchmarks, and adding support for new features such as scatter modes, nested tensors, complex number operations, and enhanced onnx export or serialization mechanisms. Some issues also involve refining testing practices, including coverage, flaky test handling, and specific bugs in JIT, autograd, or various AO-specific operators/functionalities. Overall, unresolved questions include automation of tests and documentation, compatibility with newer hardware/software, and handling specific deep learning use cases and API support for complex or advanced tensor operations."
2020-10-09,pytorch/pytorch,"The main technical concerns across these GitHub comments involve issues with building, packaging, and interoperability of PyTorch, especially related to CUDA and ROCm support (e.g., diverging implementations for different backends, performance regressions on newer hardware such as RTX 3090, and build failures on systems with newer Linux kernels). Questions about model conversion to ONNX and maintaining compatibility between TorchScript and ONNX formats are also prominent. Additionally, there are discussions about testing, serialization, type safety, and CI failures due to upstream or infrastructure issues. Unresolved questions include ensuring backward compatibility, correct handling of device-specific nuances, and addressing intermittent build/test failures possibly caused by environment or network issues."
2020-10-10,pytorch/pytorch,"The discussions highlight several core issues with PyTorch, including the difficulty of deep copying leaf tensors (especially parameters), the importance of coalescing sparse tensors for performance, and ensuring consistent support for tensor operations across CPU and GPU, such as `torch.norm` and `torch.rfft`. Several comments address build and compatibility challenges, especially with new hardware like CUDA 11.1, 3080 GPUs, and Windows environments, with suggestions for fixes and the importance of proper error messaging and portable code. There are also recurring concerns about test failures—often caused by environment setup, version mismatches, or internal bugs like missing or misconfigured files—and the need for clearer documentation and more robust inner implementations. Lastly, issues related to internal API stability, fair coverage measurement, and code maintenance are raised, indicating ongoing effort to improve PyTorch's robustness, usability, and compatibility."
2020-10-11,pytorch/pytorch,"The comments highlight ongoing discussions about efficiently computing batched pairwise distances, with recent updates noting the introduction of `torch.cdist` in PyTorch 1.1.0 as the recommended solution, which offers significant performance benefits over previous manual tensor manipulations. Several users inquire about batch processing support, with solutions involving tensor normalization and pre/post-processing to compute cosine similarities efficiently, both on CPU and GPU, emphasizing the importance of handling batch dimensions correctly. There's also a mention of performance testing on new hardware like the RTX 3090, revealing that kernel optimization and synchronization are critical for accurate benchmarking, and that current implementations may not fully leverage the hardware's capabilities, especially regarding CUDA synchronization. Additionally, issues related to building and integrating libtorch with newer CUDA/cuDNN versions and ensuring proper device support for newer GPU architectures like sm_86 are discussed, with suggestions to update documentation and improve build processes. Overall, the main concerns focus on optimizing batch distance computations, hardware support, and build environment management, while recent developments suggest that `torch.cdist` has become the primary method for batched pairwise distances in PyTorch."
2020-10-12,pytorch/pytorch,"The discussions highlight ongoing challenges with NVIDIA's NVTX bindings for PyTorch, as well as issues related to specific performance regressions seen with newer CUDA/cuDNN versions, especially on RTX 30 series hardware. Several technical concerns revolve around ensuring backward compatibility, proper handling of complex inputs in LAPACK functions like svd, and the proper implementation of autograd support for new or custom kernels. There are also questions about the buildup of global state in contexts and device management across different backends, as well as the complexity of maintaining multiple API variants (e.g., svd with or without U,V outputs). Unresolved questions include how to improve tracing/support for dynamic shapes and how to adapt existing functions and APIs for better deprecation/extension strategies to ease future maintenance."
2020-10-13,pytorch/pytorch,"The GitHub comments span a wide range of technical discussions and updates related to PyTorch development. Notable concerns include the need for better handling of deterministic behavior in CUDA operations (e.g., thrust::unique), improved default type promotions for tensors and NumPy analogs, and more flexible API behaviors such as returning tuples or specific functions for certain operations (e.g., `svd`). There are ongoing efforts to improve internal infrastructure like automatic shape inference, better disambiguation of sparse tensor layouts, and more robust test and CI management, including handling flaky tests and environment-specific issues such as CUDA version discrepancies and build failures. Some discussions suggest API deprecation strategies (e.g., `is_sparse`) and interface improvements (like subclassing `torch.Tensor`) to enhance usability and backwards compatibility. Overall, many issues highlight the need for clearer, more consistent APIs, improved runtime diagnostics, and stable build environments to ensure reliability across diverse hardware and software configurations."
2020-10-14,pytorch/pytorch,"The extracted comments across various GitHub issues predominantly raise concerns related to system memory management (shared memory for DataLoader workers, CUDA/GPU memory, build size, and environment setup), API behavior and design choices (e.g., return types of functions like `svd`, handling `is_sparse` in `to()`), and compatibility or debugging challenges (errors in TorchScript, autograd, custom classes, and build failures on Windows or with specific CUDA versions). Several suggestions involve improving documentation for backward compatibility, development workflows (e.g., explicit dry-runs, dataset consistency), and error messaging clarity. There is ongoing discussion about API design paradigms, especially around functions returning tuples versus single tensors and how to handle optional outputs. Debugging efforts also emphasize reproducing issues, understanding underlying causes (e.g., NCCL communicator inconsistencies, compiler incompatibilities), and ensuring build configurations are correct. Unresolved questions include the support for CUDA 11+ in older PyTorch versions, proper handling of mixed device/gpu environments, and how to adapt the codebase for upcoming API or backend changes."
2020-10-15,pytorch/pytorch,"The discussions mainly revolve around certain shortcomings and expectations in PyTorch's autograd and API behaviors. Notably, there's a desire to support explicit deep copying of `.grad` attributes, better handling of higher-order derivatives warnings, and clearer documentation for complex number support and sparse tensor layout semantics. Several issues also address build complexities, such as managing CUDA versions with new hardware, and optimizing library size on mobile/embedded devices. There are concerns about ensuring correctness and stability across platforms, including XLA and Windows, especially with new hardware like RTX 3090. Additionally, improvements to development workflows like profiling, testing, and automatic code generation are also suggested."
2020-10-16,pytorch/pytorch,"The discussions highlight several technical issues in the PyTorch repository, notably around ensuring reproducibility of certain operations like SVD with PyTorch versus numpy, and the appearance of inconsistent behavior in SVD pivot outputs. There are ongoing CI failures, often related to upstream breakages or environment-specific issues, such as build configuration errors with dependencies like ONNX or Vulkan, and compiler or linker errors due to missing or incompatible libraries, especially on Windows and Mac platforms. Some discussions concern the correctness and stability of certain features, such as the backward compatibility of operator schemas and the user experience of new APIs (e.g., handling negative dims in tensor operations). Additionally, there are infrastructure concerns such as integrating new compiler flags or environment checks (e.g., gcc10 testing, boost support) and fixing build system issues for external dependencies. Overall, unresolved questions include how to standardize and document behaviors like pivot representations in linear algebra, managing environment-specific build failures, and ensuring that new API designs are both correct and user-friendly."
2020-10-17,pytorch/pytorch,"The comments highlight performance issues with the backward pass of 3D grouped/depthwise convolutions in PyTorch, which is significantly slower compared to non-grouped convolutions, and note that this slowdown appears specific to 3D convolutions, not 2D. There is discussion about incorporating depthwise convolutions via the `groups` parameter, but current implementations are not optimized, leading to slowdowns. Several issues involve the use of `pybind11` and compiler defines to address build or compatibility problems, with suggestions to improve our handling of compiler flags such as `PYBIND11_COMPILER_TYPE`. Additionally, there are reports of various failures in CI builds, many related to upstream issues or environment misconfigurations, and some discussions about adding API aliases like `swapaxes` and `swapdims`. Overall, unresolved questions include how to optimize the backward pass for 3D depthwise convolutions and how to better manage build and compatibility configurations for extension modules."
2020-10-18,pytorch/pytorch,"The discussions highlight issues related to memory management in CUDA, suggesting that tensor device placement and synchronization are critical to avoid illegal memory access errors and achieve accurate benchmarking results. Some users express concern about the consistency of device-specific operations and the necessity of explicitly moving tensors between CPU and GPU, as well as proper CUDA stream synchronization. Other comments address compatibility challenges, such as building PyTorch on older systems or with outdated compilers, and the importance of correct device configuration, especially with new hardware like the RTX 3080/3090. Several discussions focus on improving code internals, such as creating better support for autograd functions, standardizing output formats, or updating documentation for clarity and standardization. Overall, unresolved questions remain about device compatibility, proper benchmarking practices, and code support for newer hardware or software environments."
2020-10-19,pytorch/pytorch,"The comments highlight issues related to GPU memory management and kernel loading, with specific attention to PyTorch's handling of workspace size and kernel registration, suggesting potential improvements to memory checks and kernel sharing. There are discussions on optimizing kernel loading to reduce unnecessary GPU and CPU memory usage, including kernel removal strategies, and concerns about global state pollution in context management. Some comments address the intricacies of automatic differentiation, especially support for batched jagobiann support and handling multiple inputs/outputs, as well as compatibility considerations for different hardware architectures and system configurations. Several comments mention ongoing or unresolved CI failures, build errors, and platform-specific issues, emphasizing the need for better testing, bug fixes, and feature understanding, such as the support for `grid_sample` modes, autograd support in custom Python functions, and the behavior of the `lu` and `lu_unpack` functions. Overall, the discussions suggest areas for performance enhancement, API improvements, bug fixes, and broader hardware and system support within the PyTorch ecosystem."
2020-10-20,pytorch/pytorch,"The collected comments indicate several ongoing challenges and proposed enhancements in the PyTorch codebase. Key issues include ensuring memory management in CUDA (e.g., size checks for GPU memory), improving the interface and usability of the BatchNorm layer (clarity on flags and behavior), addressing kernel-level support for complex tensors and empty inputs, and fixing build/compatibility problems related to compiler/linker errors and environment setup. There's also interest in optimizing the code generation process, such as integrating lightweight code syntax for operator signatures, and expanding support for features like batched Jacobian computations, model serialization without class definitions, and support for complex data types on CUDA. Many discussions suggest pending work to address bugs, improve performance, and support new features, often involving refactors, test improvements, or external dependencies, with some questions remaining about the best approaches or future directions."
2020-10-21,pytorch/pytorch,"The discussions primarily revolve around optimizing PyTorch's compilation and build processes, addressing specific CUDA and C++ configuration issues, and improving developer/user experience. Concerns include handling unsupported or problematic hardware capabilities (e.g., compute capability 3.5), build failures caused by missing dependencies or environment mismatches, and ensuring proper support for features like complex numbers, in-place operations, and device-agnostic scripting. Several suggestions call for better error messaging, conditional build logic, and explicit support for certain operators or functionalities. Unresolved questions involve compatibility details with external tools/libraries, the best approaches for conditional compilation, and how to support certain features across different hardware and software configurations."
2020-10-22,pytorch/pytorch,"The comments reveal several recurring technical concerns across the GitHub issues. Key among these are challenges related to ensuring compatibility and stability of the PyTorch environment, such as resolving issues caused by conflicting numpy versions or shared memory segment limits. There are also ongoing efforts to improve the framework's internal stability, including handling of indirect or ambiguous type promotion, and ensuring deterministic behavior for operations like `index_copy`. Additionally, multiple discussions address the implementation and testing of new features and optimizations, like the support for CUDA complex tensors, the correctness of gradient computations, and performance benchmarking on various hardware architectures. Lastly, a number of comments mention build system failures and upstream dependency issues, indicating unresolved infrastructure and environment setup problems."
2020-10-23,pytorch/pytorch,"The discussions highlight several technical issues, primarily revolving around implementation details, testing, and compatibility. Notably, there are concerns about handling the start method for multiprocessing, the correctness of backward implementations, and shape inference for complex models. Multiple discussions focus on improving build stability, including addressing upstream build failures with dependencies like LAPACK, fixing vectorized kernel bugs related to NaN comparisons, and ensuring proper inclusion of external libraries such as Boost and MKL. There are also questions about consistency in API behavior across different backends, including support for complex tensors, as well as maintaining clear documentation, especially for new or non-standard features like quantization and variable buffer handling. Unresolved issues include ensuring deterministic behavior, handling backward compatibility for newer operators, and managing multi-backend support for quantized models."
2020-10-24,pytorch/pytorch,"The discussions highlight challenges in implementing functions for positive semi-definite matrices, especially for singular ones, with suggestions involving matrix square root and chain rule derivatives. Multiple issues relate to switching linear algebra operations like SVD, Cholesky, and QR from MAGMA to cuSolver for performance improvements, emphasizing thorough benchmarking for accuracy and speed. The importance of ensuring deterministic results in sum and reduction operations, potentially via parallel prefix sum algorithms, is also stressed. Several bugs are identified, notably compilation errors due to missing or misconfigured components (e.g., 'nullopt' in std namespace, 'getSocketAddr' in 'tcputil'), often linked to environment or build setup issues. Overall, unresolved questions include the best approach for extending functionality with message types versus helper functions, and resolving build failures related to threading, compatibility, and upstream breakages."
2020-10-25,pytorch/pytorch,"The discussions highlight various technical issues in the PyTorch ecosystem, including the lack of official support for ppc64le architecture and the need to build from source, along with user challenges in installing specific versions of PyTorch on Windows and Linux platforms. There are questions about integrating custom sampler state management for data loading, handling DNNL support and its compatibility, and concerns about the behavior of `TensorImpl` size changes influencing bugs and memory optimization. Users also inquire about the handling of manifold-aware tensors, especially regarding subclassing versus attribute-based approaches, and addressing regression bugs in operators like `ne` with NaN handling and vectorized kernels. Additionally, ongoing CI failures and build issues reflect broader infrastructure and compatibility challenges, with suggestions to improve documentation and error messaging related to system requirements like Python bit-architecture."
2020-10-26,pytorch/pytorch,"The discussions cover various topics including configuring shared memory (`--shm-size`) on Google Cloud ML, ensuring reproducibility of results with different seed settings and non-deterministic modules, and making PyTorch functionality more aligned with NumPy (like adding `numpy.diff`). There are ongoing efforts to improve API clarity and correctness, such as adding support for complex types in NCCL and handling non-contiguous tensors, as well as addressing performance considerations in operations like `torch.linalg.svd` by switching to cuSolver implementations. Concerns are also raised about build and CI failures due to environmental issues or outdated dependencies, and suggestions for clearer error messages and documented best practices are proposed. Unresolved questions include API enhancements like `clip_grad_norm_` safety, deterministic behavior for operations, and how to best configure or choose between different backends or device types for operations like quantization or collective communications."
2020-10-27,pytorch/pytorch,"The comments highlight several key issues in the PyTorch codebase. Notably, there are discussions on making operations more deterministic, particularly regarding upsampling and interpolation, to ensure reproducibility across runs, with suggestions to handle non-deterministic modules and operations such as CuDNN. There are concerns about the support for sparse-sparse matrix multiplication in PyTorch, which is currently WIP, and workarounds using sparse-dense multiplication. Additionally, there are issues related to device management in distributed and RPC contexts, especially with device IDs possibly changing after deserialization, and the need for better error messaging or API design for quantized operators to support different backends like QNNPACK and FBGEMM. Finally, some comments address build errors and integration challenges, such as breaking compatibility for subclasses, handling of deprecated or internal-only APIs, and build environment configurations."
2020-10-28,pytorch/pytorch,"The discussions encompass diverse topics including bugs and feature requests in PyTorch, such as issues with tensor operations, CUDA/ROCm support, serialization bugs, and the need for better documentation, tests, and handling of deprecated features. Many comments focus on debugging build failures, platform-specific issues (Windows, MacOS, Linux), compatibility, and performance regressions, highlighting ongoing efforts to improve stability and efficiency. Some threads discuss API enhancements, backward compatibility, and interface consistency, especially regarding quantization, ONNX export, and tensor subclassing. The concerns also include CI/CD pipeline robustness, test flakiness, and internal codebase refactoring, with suggestions for regression testing, improved error reporting, and better user guidance. Overall, community and developer feedback aim to stabilize, optimize, and expand PyTorch’s functionality while managing technical debt and cross-platform support challenges."
2020-10-29,pytorch/pytorch,"The comments highlight several recurring themes:
1. Clarifications and updates to documentation, especially around `batch_first` in RNNs, behavior of `torch.unique`, and the `smoothing` parameter in label smoothing, emphasizing clearer explanations and handling edge cases.
2. Compatibility and implementation details for features like auto-grad support for sequence reversal, the support of complex numbers, and device compatibility issues, with suggestions for test coverage and ensuring proper behavior.
3. Build and environment issues, such as failures on specific Linux and Windows setups, suggestions for robust error handling like `CUDA_LAUNCH_BLOCKING`, and questions about cargo support and binary distribution (e.g., ARM binaries, TorchScript mobile support).
4. Internal API design considerations, such as the separation of `Dataset` and `IterableDataset`, and the need for more robust, scalable data loaders for production.
5. Feature requests and improvements, including adding `scatter_min/max/mean`, better support for fusion/optimizations, support for new backends like NNAPI, and support for features like `untied bias` in convolutions—often with specific suggestions for test coverage, performance, and API semantics."
2020-10-30,pytorch/pytorch,"The discussions cover a wide range of topics in the PyTorch repository, including potential BC-breaking changes in functions like `torch.svd` for complex tensors and the design considerations for `torch.fft` operators, where new dedicated operators are proposed to improve interface clarity and performance. There are concerns about ensuring backward compatibility, such as preserving tensor subclass behaviors during serialization and deserialization, and fixing bugs like non-determinism in `torch.multinomial` and `torch.random`. The importance of improving testing infrastructure is highlighted, including adding coverage for newly supported features and making test discovery and understanding easier, combined with a focus on the internal developer workflows. Additionally, some discussions involve improving model serialization, API consistency, and addressing platform-specific build issues, including CUDA, ROCm, and Windows, as well as cross-platform API uniformity and configuration handling."
2020-10-31,pytorch/pytorch,"The discussions highlight challenges in exporting TorchScript models to ONNX format, questioning whether it is possible with source modifications, and noting the current lack of support for converting TorchScript models to ONNX, particularly in C++. There's a concern about implementation details for making `torch.jit.isinstance` work correctly in both eager and scripted modes, including type covariance issues and registration in C++/TorchScript. Several issues address performance bottlenecks, such as tensor indexing overhead, and potential improvements in type checking and shape inference. Multiple CI failures and system-specific errors are prevalent, including build problems, runtime errors on CUDA (e.g., cuDNN errors), and environment mismatches, reflecting ongoing integration and compatibility challenges."
2020-11-01,pytorch/pytorch,"The discussions primarily focus on the implementation and correctness of label smoothing in cross-entropy loss, with concerns about the sensitivity of the smoothing parameter to the number of classes and the proper validation of the implementation against theoretical formulas. There are questions regarding the equivalence and impact of the KL divergence and label smoothing, specifically whether constant terms can be ignored under certain assumptions. Additionally, several issues relate to CUDA and environment configuration problems, such as mismatched NVCC and CUDAToolkit versions, runtime errors like `cuDNN error: CUDNN_STATUS_EXECUTION_FAILED`, and the importance of explicitly setting GPU devices to resolve driver or hardware-related bugs. Further, there are discussions about build failures caused by missing or incompatible libraries and the importance of properly testing code, especially when making modifications related to onnx exports or model freezing. Unresolved questions include the compatibility of different CUDA versions, handling of floating-point summation non-associativity, and detailed investigation of environment-specific errors."
2020-11-02,pytorch/pytorch,"The comments cover a range of issues in the PyTorch repository, including concerns about non-determinism in certain operations like interpolation backward kernels, reproducibility of results, and whether specific kernels have been updated in new versions. Questions are raised about support for complex/half types, efficient handling of tensor copying, and support for features such as `out=` parameters and handling `isinstance` checks with container types like `Dict`. There are also multiple reports of build failures related to external dependencies, such as missing symbols in libtorch binaries, and issues with CUDA compile and runtime support on older architectures, kernel compatibility, and driver problems, especially for Linux kernels and macOS/Windows. Several discussions emphasize the importance of testing, bug tracking, and documentation updates, with suggestions for better error messaging, handling edge cases like unused parameters and subclass behaviors, and ensuring stable, supported configurations across different platforms and hardware. Unresolved questions focus on support for newer CUDA architectures, complex data types, and the impact of proposed code changes on performance and backward compatibility."
2020-11-03,pytorch/pytorch,"The comments highlight a variety of unresolved or ongoing issues in the PyTorch repository, such as experimental support for complex autograd, GPU-specific performance concerns, and build/test failures across multiple platforms (Windows, macOS, Linux). Discussions include potential API changes (e.g., metrics, hooks, and serialization), the need for development of new features (e.g., `torch.igammac`, advanced GPU support, and extending the metrics toolbox), and bug fixes (e.g., failures in vectorized implementations, unresolved external symbols, and mismatched tensor shape assertions). Several comments indicate that some errors are due to environment misconfigurations, compiler issues, or incomplete upstream fixes, with explicit mentions of CI failures, build conflicts, and necessary patches. The overall focus is on stabilizing builds, improving support for new hardware (such as CUDA 11+ and AMD GPUs), and enhancing developer workflows through API refinement, better testing strategies, and infrastructure improvements."
2020-11-04,pytorch/pytorch,"The discussions highlight several ongoing issues and feature requests in the PyTorch repository. Notably, the searchability and documentation of `torch.is_grad_enabled` remain problematic, requiring potential API improvements or better documentation. Compatibility and build issues on different hardware (e.g., CUDA 10.x, 11.x, AMD ROCm support, and specific GPU architectures) are recurring concerns, emphasizing the need for more flexible build configurations and better environment management. There are also discussions about extending support for specific model types (e.g., ParameterDict, ParameterList), and ensuring API consistency and backward compatibility, especially for newer or experimental features like `torch.jit.isinstance` and the `LSTM` with projections. Finally, numerous CI failures and upstream incompatibilities suggest ongoing integration and testing challenges that require careful rebase, environment validation, or framework updates to resolve."
2020-11-05,pytorch/pytorch,"The comments reveal multiple ongoing issues across the repository, notably with installation and environment compatibility—such as problems fixed by different Python versions, environment variables, or wheels. Several discussions concern the correctness and performance of math operations (e.g., `torch.svd`, `abs` on complex tensors) which have caused test failures and potential bugs, especially related to hardware-specific features like GPU architectures (e.g., missing support or miscompilations for CUDA compute capabilities). There are also blockchain-style merge conflicts and upstream breakages that need resolving, as well as test failures possibly caused by code changes (e.g., in `vec256` implementations, custom modules, or in handling mixed data types). Additionally, some issues concern the proper implementation of API features like `torch.addr` and the support for complex-valued neural network components; questions about their internal workings and performance are raised, with suggestions to leverage existing utilities like `TensorIterator`. Finally, questions about infrastructure, such as CI failure management, test stability, and environment setup, recur, emphasizing the need for careful environment configuration and debugging."
2020-11-06,pytorch/pytorch,"The discussions reveal several recurring technical concerns, including the handling of complex tensors such as the support for complex activation functions, the integration of complex modules, and the support of complex operations in autograd, with ongoing efforts to extend PyTorch's capabilities in this area. There are also concerns about memory management issues like CUDA out-of-memory errors, silent changes in memory format when working with quantized models (notably using QNNPACK on mobile), and performance regressions caused by recent PRs, alongside the need for better documentation and default behaviors for certain features like batch normalization flags. Moreover, questions about the stability, longevity, and design of mechanisms like `__torch_function__`, tensor splitting, and virtual methods in generator classes suggest a focus on both API robustness and long-term maintainability. Unresolved questions include how to efficiently implement support for new features while ensuring backward compatibility, correctness across various hardware and software environments, and how to best structure support for newer data types or device-specific behaviors."
2020-11-07,pytorch/pytorch,"The discussions primarily revolve around the development and maintenance challenges of PyTorch, including issues related to distributed sampling, especially for IterableDatasets, and the need for proper device management in observers. Several questions concern how to implement effectively distributed samplers with large-scale datasets on TPUs, and how to handle device transfers efficiently when copying tensors, especially across CUDA and CPU. There are reports of performance regressions after feature additions, potential bugs with gradient computations (e.g., handling of `kthvalue` gradients), and compatibility concerns with new hardware like the RTX 3080 and 3090. Lastly, ongoing CI failures, environment setup issues, and CLA signing requirements are also mentioned as procedural hurdles affecting development."
2020-11-08,pytorch/pytorch,"The discussions raise several technical concerns including the proper setup of environment variables for package installations with Homebrew and Conda, especially regarding non-standard paths like /usr/local; the need to expand scatter and reduction support in PyTorch, with questions about API design consistency between element-wise and reduction operations and how they interact with initial input tensors; and various CI and build failures related to NCCL submodule checkout issues, mismatched CUDA library loading, and system errors, often linked to specific commit references and environment configurations. Additional questions include the API design choice of adding new message types for RPC versus helper functions, and the handling of quantized/hardswish layers in models, particularly regarding in-place attributes and quantization effects on memory formats. Some issues have been resolved or are specific to certain environments, but unresolved questions remain about fix implementations, environment setup, and best practices for distributed datasets and backend configurations."
2020-11-09,pytorch/pytorch,"The comments reflect ongoing concerns about the stability, correctness, and platform-specific issues of the PyTorch DataLoader, especially regarding multi-processing on Windows and GPU support. Several discussions address build failures related to environment setup, compiler compatibility, and compiler flags, with suggestions including fixing DLL loading issues, incorporating environment detection, and building with the latest compilers. There are also mentions of performance regressions and memory management bugs, with suggestions for profiling, diagnostics, and potential API changes; some issues involve more extensive changes like extending autograd or modifying the core C++/Python interface. Additionally, many comments point to intermittent or upstream CI failures, often due to external dependencies or environment inconsistencies, requiring careful debugging or environment adjustments. Overall, unresolved questions include platform compatibility fixes, environment setup procedures, and functional improvements to profiling, autograd, and memory management for robustness and performance."
2020-11-10,pytorch/pytorch,"The GitHub comments reflect discussions about the development and integration of new features, performance improvements, and bug fixes in PyTorch. Several issues relate to proper handling of tensor memory formats in special contexts such as vmap, with proposals for semantics and implementation changes. There are concerns about backward compatibility, especially regarding functions like `svd`, `tensor_split`, and changes in operator schemas that might break existing code; suggestions include adding explicit `info` tensors or optional outputs to maintain compatibility. CI failures often stem from upstream issues, merge conflicts, or environment-specific problems, requiring rebase or configuration adjustments. Overall, the discussions highlight ongoing efforts to enhance functionality, correctness, and performance, while balancing backward compatibility and CI stability."
2020-11-11,pytorch/pytorch,"The comments from the GitHub issues primarily revolve around advanced technical discussions and troubleshooting related to PyTorch's internal implementation and performance optimization. Common themes include requests for feature enhancements (e.g., unified memory support, complex number support, better memory management), bug fixes, and performance benchmarking. Several discussions highlight the need for clearer documentation, better error handling, and more robust testing strategies. Specific technical concerns include device compatibility issues (GPU architectures, CUDA versions), compiler (GCC/Clang) compatibility, and kernel-level optimizations, often with detailed suggestions for fixes or improvements. Overall, unresolved questions include how to optimize operator implementations without introducing numerical issues, how to better integrate and test new features in CI workflows, and how to handle compatibility across different hardware and software configurations."
2020-11-12,pytorch/pytorch,"The discussions highlight concerns about the safety and correctness of multi-process gradient updates in PyTorch, particularly with the Hogwild approach, raising questions about read-write conflicts and whether gradient tearing affects training. Several issues also address support and compatibility, such as enabling ROCm and CUDA support on various platforms, managing incomplete or conflicting build dependencies like cuDNN versions, and the complexity of ensuring correct behavior with variadic or deprecated functions. There's ongoing debate about the support for complex tensor operations, including their memory and performance implications on CPU and GPU, and about the API design for functions like `torch.linalg.eigh` and `torch.linalg.svd`, especially regarding their return types and in-place options. Additional concerns focus on build system challenges, build cache failures, and ensuring and improving CI robustness and test coverage across multiple hardware and software configurations. Unresolved questions include the feasibility of supporting `create_graph=False` modes for gradient computations, how to support more flexible or incomplete feature sets like `torch.linalg.norm` and slicing syntax, and whether incremental plugin support or API extensions are appropriate for complex features such as multi-device inference or backward-incompatible changes."
2020-11-13,pytorch/pytorch,"The discussions primarily revolve around addressing runtime errors related to label value ranges in classification loss functions, potential fixes involve adjusting label data or modifying loss implementation; version compatibility issues are also prominent, especially with CUDA, ROCm, and Python versions, suggesting the need for better environment management and documentation. There are recurring build and environment setup problems on various platforms, often due to git checkout failures, outdated compiler or dependency versions, and incorrect subprocess handling during tests, which indicate a need for improved build hygiene and scripts. Certain features like deprecated or unsupported operations (e.g., `torch.linalg.norm` with `ord=3`, ONNX export issues, eigenvector degeneracy handling) are under discussion for future support or fixes, but often require additional implementation work or clearer API design. Additionally, there are concerns about the stability of CI testing, error logging, and cross-platform support, particularly for Windows and macOS, which could benefit from more robust error handling, logging, and platform-specific adjustments."
2020-11-14,pytorch/pytorch,"The comments reflect ongoing development and troubleshooting within the PyTorch repository, including proposed feature implementations such as support for spatial transformer networks (Issue #15378) and the addition of new device and scalar type support in the JIT (Issue #31285). Several discussions concern compatibility issues, especially around CUDA compute capability support (Issue #34266), driver and build configuration challenges, and conditional compilation concerns. There are also concerns around testing, logging, and error reporting, particularly in relation to Windows DLL interface issues, error message clarity, and handling of specific backend errors like NCCL timeouts. Additionally, discussions highlight architectural considerations, such as tensor layout support (NCHW vs NHWC), backward compatibility, and more precise shape inference in ONNX exports (Issue #46648). Unresolved questions include how to improve the robustness of error reporting, extend certain features to older versions or architectures, and how to maintain backward compatibility amid ongoing API and internal interface changes."
2020-11-15,pytorch/pytorch,"The discussions raise concerns about recent regressions in training speed associated with versions of `torch.utils.checkpoint`, particularly in multi-GPU setups, and potential root causes linked to specific functions like `torch.random.py`. There are issues related to ABI compatibility when building PyTorch from source, with emphasis on the correct use of `_GLIBCXX_USE_CXX11_ABI` flags and linker errors indicating missing or incompatible symbols. Several users report problems with data loading performance and download times, possibly due to network or build configuration issues. Additionally, some discussions mention API support, such as the support for complex number operations or `view` functions, and whether simplified implementations suffice for backward compatibility. Unresolved questions include detailed root cause analysis of the slowdown and the effort required to fully implement certain C++ APIs like `lu_unpack`."
2020-11-16,pytorch/pytorch,"The comments reflect several ongoing technical topics and challenges in the PyTorch repository. Notably, there is an emphasis on improving sparse autograd support, given current limitations with autograd through sparse matrix operations, which hampers some applications like GNNs. There are also discussions around enhancing the `torch.unique` functionality to support gradients, particularly in multi-dimensional tensors, which currently lacks autograd support due to implementation issues. Benchmarking efforts on new operators like `torch.addr` show performance inconsistencies depending on tensor strides and sizes, with suggestions to optimize vectorization further. Additionally, many CI failures are related to build environment issues such as merge conflicts, outdated toolchains, or environment misconfigurations, which complicate merge and testing processes. The discussions often point to potential API improvements, bug fixes, and the need for more robust testing to prevent regressions and support new features effectively."
2020-11-17,pytorch/pytorch,"The discussions mainly revolve around internal code architecture and API design choices in PyTorch's development, such as how to improve type handling in linear algebra operations, especially in the context of degeneracies and gradient correctness (`symeig`, `eigh`). There are concerns about ensuring backward compatibility, error messaging, and testing coverage, especially regarding complex tensor operations, distributed settings, and scripting of models with non-standard features like `**kwargs`. Issues are also raised about build system robustness, including dependency versions, environment configurations, and handling of third-party libraries like NCCL, MPI, and CUDA, with some failures attributed to upstream updates or environmental mismatches. Additionally, there are discussions on improving developer usability, diagnostics, and documentation clarity, particularly related to the use of scripting, autograd, and system-specific considerations for hardware acceleration (e.g., ROCm, Apple ARM, macOS). Overall, unresolved questions include how to reliably support legacy features, error handling, and cross-version, cross-platform compatibility while maintaining performance and developer productivity."
2020-11-18,pytorch/pytorch,"The discussions primarily revolve around issues related to PyTorch's internal and external build processes, including problematic build failures on some platforms and environments, such as MacOS M1, Windows with Visual Studio, and Linux with diverse CUDA versions. There are concerns about the robustness of code automation tools (e.g., CMake, pybind11, build scripts), environment setup (e.g., compiler and library versions, locale settings), and test stability across platforms, often complicated by external dependencies like NVIDIA's CUDA or third-party libraries. Several questions pertain to improving usability and developer experience, such as better error messaging, handling of complex tensor operations, and more intuitive API design especially for tensor arguments and module scripting. A recurring theme is the need for clearer documentation and better CI testing infrastructure to catch platform or environment-specific issues early. Some unresolved issues involve build configuration errors, external dependency management, and ensuring cross-platform compatibility for advanced features like mixed precision, distributed computing, and JIT compilation."
2020-11-19,pytorch/pytorch,"The discussions mainly revolve around the following points:

1. Compatibility issues with different hardware and software environments, such as CUDA versions, driver supports, and compiler issues, especially with older or unsupported systems like VS2015 or ARM architectures.
2. The management of CUDA/NCCL dependencies, linking errors during build configuration, and the need to adapt the build process for different hardware.
3. The correct handling of sparse tensor operations, including broadcasting support, shape restrictions, and the semantics of `scatter`/`gather` operations, along with documentation clarity.
4. In-place operation support and proper usage of Python hooks and autograd behavior, especially concerning nested backward passes, hook invocation, and their compatibility with distributed training.
5. The importance of robust CI/CD testing for various configurations, including correct rebase procedures, addressing test failures due to external dependencies, and ensuring stability across multiple platforms and environments."
2020-11-20,pytorch/pytorch,"The discussions primarily revolve around the nuances of PyTorch's tensor shape and attribute behaviors, such as the correct use of `.size()` and `.size` attributes, and the importance of explicit broadcasting support in `gather`/`scatter`. There are concerns about the accuracy and proper documentation of functions like `grid_sample`, especially for 3D/5D cases and the impact of different coordinate conventions. Some issues highlight compatibility problems with CUDA, cuDNN, and driver versions, as well as build failures related to outdated compiler/tool versions and environment configurations. Several discussions also address the challenges of maintaining cross-platform code, especially for backend integrations like MPI, CUDA, ROCm, and external libraries like libuv, and how to best test those in CI. Lastly, there are questions about the proper support for non-deterministic GPU operations, on namespace and device management, and how to improve error handling, testing, and documentation for these complex scenarios."
2020-11-21,pytorch/pytorch,"The discussions highlight several key technical concerns, including ensuring proper installation and compatibility of dependencies like Pillow, handling of sparse tensor formats such as GCS and COO, and the support for advanced interpolation methods like trilinear sampling within `grid_sample`. There are questions about the implementation details of sparse tensor classes, merge conflicts related to submodule commits, and compatibility issues with specific hardware and software configurations, especially concerning CUDA and driver versions. Additionally, multiple discussions emphasize verifying and optimizing performance, particularly on newer GPU architectures like the RTX 30 series, and ensuring that features such as autograd, broadcasting, and specific operators (e.g., RMSProp, BatchNorm) support intended functionalities. Unresolved questions involve maintaining backward compatibility, managing build system limitations, and providing clear documentation for less common formats like GCS."
2020-11-22,pytorch/pytorch,"The discussions highlight issues related to download speed and accessibility of PyTorch, with suggestions to use older versions or alternatives to mitigate slow download times in various regions. There are technical concerns around CUDA compatibility, specifically verifying the compute capability (e.g., SM_86) when building from source and ensuring proper configuration during installation. Several reports mention build errors caused by missing modules or misconfigurations, such as the absence of 'tools.nnwrap' and the need to rebase or update source branches for compatibility. Additionally, there's attention to test failures and CI pipeline issues, some attributed to upstream or external dependencies, with suggestions to update or modify tests for better coverage and stability. Overall, the discussions involve improving installation robustness, ensuring proper CUDA configuration, and maintaining stable CI testing."
2020-11-23,pytorch/pytorch,"The comments highlight issues related to the capabilities and limitations of MAGMA's `gels` function on rank-deficient matrices, performance concerns with SVD computation, and discrepancies in GPU versus CPU speeds. There are discussions about extracting intermediate outputs from JIT-compiled models, especially for quantized models, and API requests for features like subgraph extraction similar to XLA feed/fetch. Several issues regarding compatibility, error debugging, and build failures are also mentioned, with specific attention to driver and library versions, especially on MacOS and Linux environments. Additionally, some comments address potential improvements to PyTorch's API (e.g., adding `lstsq`), handling of sequences in functions like `copy_`, and troubleshooting warnings or errors in specific modules such as `grid_sample`, `hardswish`, and `stft`. Unresolved questions include when updates for certain features will be released, drivers' impact on performance, and the development status of new functionalities like `lstsq`."
2020-11-24,pytorch/pytorch,"The comments highlight several ongoing technical issues and discussions in the PyTorch community. Key concerns include difficulties with installing and importing PyTorch, particularly in different environments and configurations (e.g., source compilation, pip, and Python versions), and challenges related to converting ONNX models to Caffe2, especially with operator schema mismatches requiring attribute fixes like 'is_test'. There are also reports of performance variations with different CUDA and cuDNN versions on GPUs like the RTX 3090, as well as specific bugs such as errors with 'sum_to_size' in autograd engine code and operator support for new layers (e.g., Hardswish). Additionally, users mention issues with CI/CD failures due to upstream incompatibilities, build conflicts, and platform-specific compilation challenges, such as Windows build complications with MSVC and device architecture errors. Finally, some unresolved questions involve support for advanced features (e.g., dynamic typing in TorchScript with Union types), the need for community-included operations like Mish, and issues with environment setup on macOS and installation with conflicting dependencies."
2020-11-25,pytorch/pytorch,"The discussions highlight ongoing challenges and efforts related to multi-processing DataLoader behavior on Windows, including the use of `__name__ == '__main__'` fixes and issues with file naming conventions affecting multiprocessing. There are concerns about the stability and correctness of models when exporting to formats like ONNX, particularly dealing with padding, tensor shape mismatches, and support for various tensor types and devices, along with suggested workarounds for compatibility. Debugging PyTorch thread and tensor string representations is also discussed, with questions about thread management and potential crashes. Additionally, issues related to CUDA driver support, environment setup, and version compatibility are frequently mentioned, especially in context of GPU driver versions, CMake build failures, and specific hardware environments. Overall, unresolved technical questions revolve around improving cross-platform multiprocessing support, model export robustness, and environment consistency for large-scale deployment and debugging."
2020-11-26,pytorch/pytorch,"The discussions cover a range of issues including performance bottlenecks, compatibility, and implementation details. Notably, there are technical concerns about optimizing multi-GPU training, ensuring correct NCCL communicator initialization, and resolving inefficiencies in CUDA operations. Several bugs and failures related to build environments, library support, and runtime behavior are highlighted, with suggestions for fixing specific code paths or reworking configurations. There is also interest in improving user experience through better documentation, platform support (e.g., Windows), and APIs (e.g., C++ DDP). Unresolved questions include ensuring proper support for various hardware architectures, maintaining backward compatibility, and addressing performance variability due to nondeterministic behaviors."
2020-11-27,pytorch/pytorch,"The comments reflect ongoing discussions about specific technical issues in the PyTorch codebase, including warnings related to tensor overlaps during in-place operations, inconsistencies in behavior and return types of functions like `svd`, and support for complex number operations and autograd. Several discussions involve refining error messages, supporting additional data types (e.g., complex doubles), and aligning function behaviors with NumPy standards, with some proposals for backward-incompatible changes. There's interest in improving build and testing infrastructure, especially for Windows, C++ API support for distributed training, and documentation clarity, particularly around sparse tensor formats and hardware-specific features like TF32. Many comments also highlight the need for better CI stability, test coverage, and resource management, with suggestions for more targeted testing and handling of platform-specific discrepancies. Unresolved questions include the compatibility impact of certain API changes, detailed design and documentation for new features, and the timeline for supporting advanced hardware capabilities."
2020-11-28,pytorch/pytorch,"The discussions primarily revolve around handling NaN propagation and device-specific consistency in PyTorch, with considerations about performance trade-offs and operator behavior across CPU and GPU. Several issues address build and installation challenges, including compatibility with CUDA versions, driver support, and ABI issues, particularly when building from source or replacing binaries. There are feature requests for extending NumPy compatibility (e.g., numpy.diff, quantile interpolation methods) and API clarifications, such as the distinction between `torch.diag` and `torch.diagflat`. Some discussions focus on experimental features like TF32, with suggestions to improve reproducibility notes and documentation. Additionally, several CI failures and resource considerations are highlighted, emphasizing the need for more targeted testing and clearer deprecation strategies."
2020-11-29,pytorch/pytorch,"The discussions highlight several issues, including the support for `tensor.unfold` on tensors with more than 4 dimensions, with suggested workarounds such as merging dimensions. There is concern about CUDA device support, specifically with older CUDA compute capabilities (e.g., 3.0 vs 3.5), and the need to build PyTorch from source for certain hardware. Memory management issues are discussed, particularly relating to DataLoader behavior, dataset object copying, and the impact of `shuffle=True`. Several CI failures and flaky tests are present, often due to build conflicts, environment inconsistencies, or specific test cases not handling edge cases like zero-sized batches. Additionally, feature requests and API design questions are raised, such as managing default device/dtype configurations and the support for `broadcast_shapes`."
2020-11-30,pytorch/pytorch,"The discussions highlight several technical concerns: inconsistencies and design issues in learning rate schedulers for optimizers with multiple parameter groups; limitations in the `torch.nn.unfold` function for tensors with more than 4 dimensions and the possibility of obtaining maximum values over splits; the deprecation of `torch.Tensor.new()` in favor of more specific methods like `new_empty`; challenges in exporting and ONNX compatibility, especially related to models like LSTM; and complexities in distributed training, such as DDP's support for double backward, multi-device initialization, and handling NCCL process hangs and errors during multi-GPU operations. Several proposals aim to improve API consistency, testing robustness—including for complex and sparse tensors—and to address bugs in functions like `torch.linalg.norm`, `torch.inverse`, and `torch.norm`. Unresolved questions include how to refactor or overhaul internal APIs for device creation, and how to better handle failures due to upstream issues or platform-specific limitations (e.g., macOS wheel compatibility or compiler issues with MSVC)."
2020-12-01,pytorch/pytorch,"The comments highlight several recurring themes: the need for better native support and native extensions, especially around custom functions and missing features (e.g., RSample for mixture models, support for certain functions like fill_ and zero_ with batching, handling of frozen models), and the desire for improved CI and testing strategies, such as more targeted tests for distributed features and handling of edge cases (e.g., reproducibility across different hardware). There are concerns about the stability and correctness of specific functionalities, such as autograd behavior with views, numerical stability issues with certain operators (like min on NaN values), and compatibility issues on various OS/environments (macOS, Windows, M1). Many discussions involve better infrastructure support, including build improvements, environment setup, and tooling, to streamline development and testing workflows. Unresolved questions include the best approaches for native feature integration, handling of edge cases in autograd and distributed training, and how to improve testing fidelity without incurring excessive overhead."
2020-12-02,pytorch/pytorch,"The discussions reveal several recurring themes: first, there are persistent issues related to the interaction between multi-process CUDA contexts, NCCL communication, and stream synchronization which cause deadlocks, prompting considerations for pre-initializing contexts or restricting device activation; second, numerous bugs caused by changes to default behaviors—such as how tensor views and parameters are handled, or differences in Python AST parsing across versions—necessitate careful refactoring, API clarifications, or more robust error checking; third, there is ongoing work on enhancing the compiler infrastructure (e.g., for `TensorIterator` and codegen), which requires addressing compatibility, code robustness, and testing strategies; and finally, circuitous CI failures influenced by upstream issues, environment setup, or flaky tests are frequent, demanding improved testing resilience and clearer reproduction instructions."
2020-12-03,pytorch/pytorch,"The discussions highlight issues related to environment management and package installation, such as ensuring the correct CUDA/cuDNN versions are used, and handling environment activation for Python imports. There are technical concerns about specific PyTorch functions and their compatibility, such as the behavior of tensor operations (e.g., `min` with NaNs), and how modifications might impact existing features like views, views preservation, and caching mechanisms. Several questions around implementation details surface, including the differences in behavior across compilers (MSVC, GCC), handling of views and tensor sharing (shared storage, `Tensor`, `Parameter`, pointers), and the impact on reproducibility and deterministic outputs. Many discussions also involve dealing with CI/build failures possibly caused by external dependencies or configuration issues, and the need for rebase or cleanup, as well as the importance of test coverage and regression detection. Unresolved issues include environment dependency management, performance regressions, compatibility with various hardware (e.g., ROCm, Apple M1), and handling complex features like scripting with nested or special tensor types."
2020-12-04,pytorch/pytorch,"The discussions primarily concern the development and integration of advanced features and optimizations in PyTorch, such as support for list operations, loops, and symbolic differentiation, with ongoing questions about their design considerations and compatibility, especially in higher-order differentiation. Several threads also address technical challenges related to memory management (OOM issues), build environment configurations (e.g., compiler, linker flags, static library linking), and hardware-specific problems (GPU overclocking effects, CUDA compatibility, ROCm issues). There are recurring concerns regarding testing and CI stability, with discussions about fixing flaky tests, incorporating new APIs, and improving build and deployment processes, including handling of external dependencies and third-party build tools. Additionally, some discussions delve into compatibility issues with different software versions (e.g., ABI configurations, Python versions) and platforms (e.g., Windows, Android, iOS), often requiring patches or manual interventions. Unresolved questions remain around optimizing performance trade-offs, acquisition of lower bounds for communication in distributed training, and the design of internal APIs and code generation mechanisms to better support extensibility and robustness."
2020-12-05,pytorch/pytorch,"The discussions mainly revolve around improving and refactoring PyTorch's internal implementations, such as deduplicating search result processing and enhancing C++ API support for distributed training like DDP, with particular interest in support for multi-GPU training in C++. There are concerns about the complexity and reliability of current build and compilation workflows, especially for Windows, CUDA, and various environments, leading to issues like build failures, conflicting dependencies, and platform-specific constraints. Additionally, several discussions highlight the need for better documentation, testing robustness, and consistent behavior across dtype and device variations, including handling of floating point precision issues. Upstream failures and ongoing CI stability challenges are also noted, complicating progress on new features and refactorings. Overall, the key focus is on making the codebase more modular, robust, and performance-oriented, while addressing infrastructural and tooling challenges, with unresolved questions about the best approach to API design, testing, and build reproducibility."
2020-12-06,pytorch/pytorch,"The discussions cover a range of technical topics including the optional integration of external serialization libraries like dill and cloudpickle, with suggestions to make this configurable. There are ongoing efforts to improve functionalities such as implementing Python-like numpy.unravel_index in torch, optimizing sparse matrix operations, and fixing performance or correctness issues such as autograd support with sparse tensors and backward algorithms. Several reports highlight build failures, merge conflicts, and CI issues, emphasizing the need for careful maintenance and compatibility checks across different platforms and configurations. Concerns are also raised about ensuring reproducibility, performance improvements in convolutions, and handling platform-specific bugs or limitations, especially related to Windows, ROCm, and CUDA. Unresolved questions include testing for dtype combinations in norm functions and the impact of certain in-place modifications on memory consumption and backward correctness."
2020-12-07,pytorch/pytorch,"The comments reflect ongoing efforts to troubleshoot issues related to PyTorch installation (especially within conda environments), compatibility (e.g., CUDA, cuDNN versions, Python versions), and build failures across various platforms (Linux, MacOS, Windows). Several discussions focus on optimizing performance with channels-last memory formats and addressing bugs in tensor operations like `view` or `adaptive_avg_pool2d`, often involving vectorization or support for different memory layouts. There is also significant attention to runtime and API design concerns, such as correct behavior of `torch.tensor`, consistent documentation between `torch.svd` and `torch.linalg.svd`, and handling of bugs related to just-in-time compilation and module serialization. Many issues involve failures due to upstream dependency mismatches, build system conflicts, or incompatibilities, and some suggest splitting large PRs, improving test coverage, or adjusting API design for better consistency and usability. Unresolved questions mostly revolve around platform-specific support (e.g., MacOS GPU/Metal support), backward compatibility, and performance regressions in edge cases like extreme values or sparse tensors."
2020-12-08,pytorch/pytorch,"The discussions cover a variety of technical issues including runtime errors in CUDA related to label ranges and data types, such as 'Assertion `t >= 0 && t < n_classes` failed' during loss computations, and memory management or compatibility issues with different hardware architectures and software versions (e.g., OpenCV, CUDA, ROCm, neural engine support on macOS). Several threads address failure modes in testing and build environments, often due to version mismatches, incomplete feature support, or specific bugs in third-party libraries like numpy, opencv, and CUDA/MAGMA. There are suggestions for improving API usability and correctness, such as explicit support for rank-deficient matrices, clear documentation for in-place operations, and better error messages for unsupported features or incompatible data types. Some discussions recommend delaying certain changes or patching existing issues temporarily (like disabling flaky tests or error-prone features) pending more robust long-term solutions, and there’s ongoing work to align functionalities like `torch.linalg.svd` with numpy/scipy standards. Overall, unresolved questions include how to implement support for new hardware features, improve testing coverage and reliability, and enhance cross-platform compatibility with minimal regressions."
2020-12-09,pytorch/pytorch,"The comments span a variety of issues, mainly focusing on build failures, test failures, and features or API design suggestions. Several failures are related to upstream incompatibilities, missing dependencies, or environment-specific issues, such as CUDA, compiler, or library support, particularly on ARM architectures or with specific CUDA versions. There are discussions about API enhancements, such as faster tensor unwrapping, the introduction of broadcast_to, and better error messaging for autograd-related features. Some concerns revolve around testing strategies and the stability of certain features across different environments and PyTorch versions. Unresolved questions include the support for certain device contexts, the handling of NamedTuple in TorchScript, and whether to support specific modes or configurations with fallback options."
2020-12-10,pytorch/pytorch,"The document contains numerous GitHub issue comments discussing a wide range of technical topics in PyTorch development, including memory management (ulimit settings, memory usage), sparse tensor autograd support, support for specific operations (e.g., SVD, QR, pack_padded_sequence), and performance optimizations. Several issues involve build environment challenges, such as compatibility with hardware, CUDA versions, and compilation errors on various platforms, with particular emphasis on handling deadlocks, device context management, and backend support (MAGMA vs cusolver). There are ongoing discussions regarding API design, such as the support for ""device"" contexts, module wrapping, and testing infrastructure, including codegen, type annotations, and test coverage. Overall, many unresolved questions revolve around ensuring compatibility, performance, and correct semantics across diverse hardware and software configurations, with some concerns about build reproducibility, CI failures, and proper testing of new features."
2020-12-11,pytorch/pytorch,"The discussions encompass multiple topics including the development and release status of features like static linking of libtorch, support for context managers in TorchScript, and improvements to API consistency such as `torch.broadcast_to`. Many comments address build and compatibility issues, especially related to CUDA versions, legacy build configurations, and platform-specific concerns like QNX cross-compilation and Apple's M1 architecture. There are recurring questions about error handling and debug features, such as proper warnings, in-place operations, and sticky errors in CUDA. Several discussions focus on the future of the API, including handling of sparse tensors, user-defined modules, and more robust support for rank-deficient linear algebra operations. Unresolved issues include test failures due to upstream regressions or platform-specific bugs, as well as questions on implementation details for new features like remote tensor management and device context handling."
2020-12-12,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA memory management, emphasizing that low free memory space can still trigger OOM errors, and suggesting reducing batch sizes or input dimensions as potential remedies. There are concerns about supporting non-standard RNN modes like CUDNN_SKIP_INPUT, which require compatibility support in AutogradRNN and testing adjustments, especially regarding cudnn version-specific fixes. Several threads address build and installation difficulties on M1 Macs, including configuring environment variables, supporting various BLAS backends, and ensuring compatibility with Python versions and libraries like protobuf, OpenMP, and vecLib. Additionally, there are technical questions about warning mechanisms (e.g., TORCH_WARN_ONCE), function signature changes impacting CI testing (notably with XLA), and type annotation practices within documentation for better clarity and usability. Unresolved issues include ensuring robust memory handling, library support across different hardware, and maintaining consistency in documentation and warning behaviors."
2020-12-13,pytorch/pytorch,"The discussion covers several key areas: (1) handling threading and asynchronous request completion in distributed training, with suggestions to use `req.wait()` and thread status checks to avoid blocking issues; (2) clarifying type annotations in function signatures, proposing more expressive or standardized types for parameters like dimensions, and concerns about their documentation and compatibility across Sphinx versions; (3) improving warning mechanisms, specifically controlling `TORCH_WARN_ONCE` via runtime flags or environment variables to better test warning behavior; (4) addressing compatibility and error issues in C++ extension and JIT, especially related to VS versions and type signature expectations; and (5) discussing potential enhancements to the API, such as adding new parameters to but with considerations on usability and implementation complexity, alongside some ongoing CI and coverage concerns. Overall, unresolved questions include the best approach for warning control, the handling of async requests in distributed contexts, and compatibility or implementation of API changes."
2020-12-14,pytorch/pytorch,"The discussion covers multiple technical concerns including the behavior of embedding operations with padding, the support and implementation details of ONNX reshape, and the handling of complex tensors in autograd. There are questions about the support for different device types such as Intel's oneAPI, the potential for extending PyTorch with new device abstractions like `xpu` or `nulldevice`, and how to properly support tensor layout, memory formats, and device interoperability with minimal performance overhead. Additionally, developers discuss the implications of BC-breaking changes, the need for better error messages, test coverage updates, and configuration options for advanced features like workspace size in cuDNN. Finally, multiple discussions focus on improving accessibility and usability of features, potential backward compatibility issues, and the integration of new hardware support, notably Apple Silicon and Intel GPUs."
2020-12-15,pytorch/pytorch,"The discussions mainly revolve around the challenges of implementing and testing advanced features in PyTorch, such as support for sparse tensors and their unary operations, handling rank deficiencies in linear algebra routines, and the complexities of differentiable graph optimization, especially in the context of profiling and type guarding. There are concerns about the impact of certain autograd and in-place operation behaviors on correctness and performance, and suggestions to improve documentation, error messages, and API consistency—particularly around deterministic algorithms, out semantics, and quantization. Several threads also cover build issues, environment configuration, and compatibility with external dependencies like protobuf, ONNX, and CUDA, with some discussions on CI failures and build reproducibility. Overall, unresolved questions about the proper handling of tensor types, guard mechanisms, and interoperability with external libraries remain, alongside ongoing efforts to refine testing and documentation."
2020-12-16,pytorch/pytorch,"The comments highlight ongoing challenges and proposals related to PyTorch's operations and infrastructure. Key concerns include the performance implications of `torch.flip()` and support for negative strides, with suggestions to utilize views or alternative methods to improve efficiency; support for label smoothing in loss functions, and whether to embed it within loss modules or allow user-defined smoothing; and compatibility and build issues, particularly around CUDA versions, protobuf versions for ONNX, and proper handling of operator dispatching and inlining in different environments. Additionally, there are discussions on improving API design, such as enabling `broadcast_to()` akin to NumPy, and on testing, including ensuring proper coverage and reproducing failures. Questions remain about appropriate mechanisms for error reporting, backward compatibility, and ensuring efficient and correct implementation of new features within the complex build and runtime environment."
2020-12-17,pytorch/pytorch,"The discussions highlight significant concerns about the clarity and consistency of PyTorch's API and documentation, especially regarding tensor formats, loss functions with label smoothing, and naming conventions like 'swap_batch_and_timestep.' There are technical questions about improving and standardizing these features, such as adding new loss variants, support for sparse gradients, or handling different device types like 'XPU' and 'remote' tensors, with questions on implementation details and interoperability. Several issues involve debugging or fixing internal errors, performance regressions, or build failures, including memory issues, compiler errors, and incompatible CUDA versions. Contributors suggest better testing strategies, rebase practices, and handling of third-party dependencies, with an emphasis on robustness, reproducibility, and clarity in the codebase. Unresolved questions about future plans for core features, support for new hardware backends, and detailed work on batching rules for complex tensor transformations remain, indicating ongoing development and coordination efforts."
2020-12-18,pytorch/pytorch,"The comments showcase various technical challenges and discussions related to PyTorch, including issues with native library loading conflicts when importing multiple packages like onnx and dlib, leading to segmentation faults. Several bugs are associated with hardware and software incompatibilities, particularly on Windows, Mac, and Linux systems with specific GPU configurations, driver updates, or OS versions, often affecting CUDA, ROCm, or CPU-only setups. There are also detailed debates about internal implementation practices, such as the new dispatching style for operators, batch sharding strategies for distributed datasets, and the support for Unicode in PyTorch's string handling, as well as concerns about code coverage, testing frameworks, and proper documentation. Issues related to build processes, dependency management, and CI failures are frequent, indicating ongoing efforts to improve stability, correctness, and usability, especially across different hardware architectures and OS environments. Many discussions entail proposals for enhancements, bug fixes, and best practices to handle edge cases or improve developer experience, some awaiting formal PRs or further review."
2020-12-19,pytorch/pytorch,"The discussions primarily focus on the hardware and software support for quantization on various GPUs, notably the Tesla T4 and newer RTX series, and the compatibility of PyTorch with different CUDA versions, including nightly builds for CUDA 11.1 and 11.0. Several comments inquire about improvements in memory management, such as reducing memory overhead and handling uneven data sharding for distributed training, alongside clarifications on recommended installation procedures for nightly builds. There are concerns about performance optimization, instruction count stability, and compatibility issues, especially related to AVX, Neon, and GPU capabilities like sm_86 support. Additionally, a recurring theme involves clarifications and troubleshooting around specific PyTorch features like `torch.fft`, `irfft`, and `SyncBatchNorm`, including documentation clarifications and API behavior nuances. Unresolved questions include the status of support for new GPU architectures, the integration of community-requested features, and CI/test failures unrelated to code changes."
2020-12-20,pytorch/pytorch,"The discussions primarily address performance optimization, particularly around efficient interpolation methods, FFT computations, and gradient handling, with emphasis on leveraging GPU and multi-threaded CPU capabilities. Several issues concern improving documentation, including clarifying operator signatures, supporting sparse tensor operations, and ensuring proper rendering in generated docs. Conflicts in codebase management are noted, especially regarding operator dispatching strategies, legacy interface support, and compatibility with different build environments, including mobile and M1 Macs, highlighting the need for robust build instructions and environment setup. Some discussions focus on testing strategies, including enhancing test coverage with OpInfos, and handling CI failures related to missing overrides or incompatible dependencies. Overall, questions revolve around code maintainability, extending operator support, and optimizing build and runtime environments across hardware and software configurations."
2020-12-21,pytorch/pytorch,"The comments largely revolve around the support and implementation challenges of sparse tensor formats (CSR/CSC) in PyTorch, including the need for new kernels and how expanding sparse tensors outside of math operations introduces complexity. Performance benchmarking of FFT operations against other libraries (e.g., MKL, cupy) indicates PyTorch's FFT is competitive, yet some issues with memory management, kernel support, and compatibility with older hardware or configurations (like gcc7 or ARM NEON) are discussed. Bug reports and CI failures highlight ongoing integration and compatibility issues, especially around complex autograd, distributed training, and hardware-specific bugs. Several suggestions aim to improve testing, documentation clarity, and code consistency, but unresolved questions remain regarding support for legacy functions (like torch.fft), sparse gradient handling, and build environment stability."
2020-12-22,pytorch/pytorch,"The comments highlight a range of technical issues, including compatibility of PyTorch with different Python versions and support for specific CUDA drivers and GPU models, often requiring driver updates or environment adjustments. Several discussions focus on performance and memory management challenges in DataLoader operations, especially regarding multi-worker prefetching, virtual environments, and memory leaks, with deep dives into the internal logic of worker shutdowns and data preprocessing crashes. There are multiple mentions of the complexity of maintaining backward compatibility in quantization and quantized tensors, as well as the difficulty of flattening parameters for optimization, which is often hindered by corner cases and device-type mismatches. Specific issues also relate to build failures caused by compile errors in native code (e.g., neon support, out-of-range errors, code deprecations), indicating ongoing stability and portability concerns across hardware and compiler environments. Lastly, some discussions suggest improving documentation, testing frameworks, and the handling of operator dispatch keys to better support new features like `torch.fft` and extended device types, though some questions remain about the best practices for deprecation and transition strategies."
2020-12-23,pytorch/pytorch,"The discussions highlight several key technical issues: the handling of padding zeros in Embedding and EmbeddingBag (with proposals to exclude padding from mean calculations), and the need for better support for sparse and pruned matrices in custom convolution operations for efficiency and autograd compatibility. There's a recurring concern about the standardization and support of new features, such as operator autograd support, custom build tooling, and expanding operator coverage in core PyTorch. Several discussions revolve around ensuring reproducibility and deterministic behavior, especially with operations like AdaptiveAvgPool2d, and addressing runtime or memory overhead issues linked to CUDA and cuDNN, with suggestions for documentation updates and better error reporting. The community also questions the integration of third-party extensions, like parameter flattening, and the support for advanced functionalities like torch.fx IR optimization and custom operator overrides, seeking clarity, robustness, and proper documentation."
2020-12-24,pytorch/pytorch,"The discussions highlight several technical concerns: discrepancies in GPU memory usage during multi-GPU training, particularly related to the FastDataLoader's `__iter__()` method and pin_memory compatibility; challenges with converting models (e.g., WaveGlow, OCR) to ONNX due to operator support, data type mismatches, and device limitations; build and compatibility issues on macOS and older hardware, especially regarding CUDA support, missing symbols, and bitcode compatibility with QNNPACK/XNNPACK; and various CI failures stemming from system dependencies, compiler flags, and code regressions, some of which appear upstream or due to environment mismatches. There are also discussions about adding new features (e.g., AdaBelief optimizer, supporting new operators) and improving testing infrastructure, but many questions remain about system support, performance impacts, and integration processes. Overall, the main themes are system compatibility, model conversion and operator support, and improving stability and performance across diverse platforms."
2020-12-25,pytorch/pytorch,"The discussions involve a variety of technical issues related to PyTorch, including dimension mismatch errors in loss functions due to incorrect input shapes, often stemming from dataset misalignment or data preprocessing errors. Several contributors highlight the importance of ensuring datasets are correctly formatted and correspond in size, especially when combining image and text data. Common errors such as ""Dimension out of range"" in `log_softmax`, ""CUDA error: invalid configuration argument,"" and deadlocks in distributed training are frequently addressed, with suggestions including verifying dataset consistency, avoiding conditional operations that restrict all processes from executing necessary functions, and checking hardware stability and driver compatibility. Additionally, several discussions touch on the importance of proper API usage (e.g., `torch.distributed.reduce` placement), code update considerations (e.g., changes in PyTorch API versions), and troubleshooting environment-specific issues like CUDA or hardware overclocking. Overall, many issues are resolved through code corrections, dataset validation, or environment adjustments, but some technical questions remain unanswered, especially regarding the handling of specific operations in serialization, export, or hardware contexts."
2020-12-26,pytorch/pytorch,"The discussions encompass several key technical concerns: the resolution of merge conflicts and implementation of features such as separation of Dataset processing into dedicated processes for efficient data loading; performance and compatibility issues related to hardware (e.g., NVIDIA RTX 3090, CUDA versions, and driver updates affecting NCCL operations); challenges in building and deploying PyTorch on various platforms including macOS with MLCompute and ARM processors; the need for improved distributed training support in libtorch, particularly for large models and RL applications; and ongoing CI failures highlighting the importance of updated test pipelines, compatibility, and environment configurations. Questions frequently raised include specifics of gradient synchronization in distributed setups, proper CUDA compute architecture flags, and how community contributions can better support platform-specific backends. Lastly, there are suggestions to streamline workflows by using libtorch directly, and discussions on improving hardware compatibility and software infrastructure."
2020-12-27,pytorch/pytorch,"The discussions encompass a range of technical issues including model compatibility across PyTorch versions, with concerns about backward and forward compatibility and model training cost. There are troubleshooting guides for building libtorch with CMake, addressing path specification issues on Ubuntu, and handling non-contiguous tensor operations, with suggestions to improve out-of-place tensor handling and validation behavior. Additionally, challenges with distributed training on GPUs—such as NCCL-related errors and environment-specific configurations—are noted. Other topics involve performance profiling of tensor indexing compared to NumPy, plugin support for FFT operations on ARM architectures, and code contributions requiring Contributor License Agreements. Central unresolved questions include improving model serialization compatibility, refining tensor operation behaviors, and expanding hardware-optimized libraries to open-source standards."
2020-12-28,pytorch/pytorch,"The discussions highlight several key concerns: challenges in supporting alternative data formats in PyTorch due to kernel and API complexities; ambiguities and inconsistencies in tensor format flags like `swap_batch_and_timestep` and `batch_first`, warranting clearer documentation or redesign; efforts to improve interoperability between PyTorch and OpenCV, especially in image tensor conversions; questions around supporting newer CUDA versions such as CUDA 11.1 or 11.2, and the proper setup of environment variables; and broader discussions on the architectural and API evolution of features like Smooth L1 vs. Huber loss, complex number support, and the handling of deterministic operations. Many issues involve clarifications, performance impacts, and ensuring backward compatibility, with several proposals for enhancements, refactoring, or documentation improvements still under consideration."
2020-12-29,pytorch/pytorch,"The discussions highlight several core issues: (1) a proposed feature toggle to exclude padding zeros during embedding reductions, with a consensus to default to ignoring padded elements; (2) persistent build and environment compatibility challenges across CUDA, hardware, and OS versions, often requiring user intervention or system configuration adjustments; (3) enhancements in debugging, profiling, and testing, such as better process hangs handling, stream recording, and test coverage validation; (4) API and implementation details around tensor operations, like the proper handling of tensor resizing with memory formats, and the support for various data types and precisions, including float16, bfloat16, and integer GEMMs; and (5) ongoing feature requests and discussions for new functionalities (e.g., ranking distributions, custom ONNX operators) and fixing of bugs related to autograd, CUDA support, and dynamic graph tracing, often with unresolved questions about integration, performance, or compatibility."
2020-12-30,pytorch/pytorch,"The comments cover a wide range of issues encountered in the PyTorch repository, including specific bugs, build and compatibility problems, and feature requests. Many discussions focus on compatibility with newer hardware (e.g., RTX 3090, Apple M1), software build issues (e.g., C++ compiler compatibility, CUDA compute capability), and Python API behaviors (e.g., in-place operations, gradient handling, CUDA environment variables). There are also suggestions for improving code stability, testing, and documentation, such as clarifying expected behaviors for in-place updates, out arguments, and onnx export quirks. Some concerns address the handling of non-contiguous tensors, deprecation plans, and the support of new data types like bfloat16. Unresolved questions include how to better support distributed training, model parallelism, and the maintenance of consistent APIs and behaviors across various platforms and hardware configurations."
2020-12-31,pytorch/pytorch,"The discussions primarily revolve around profiling and debugging PyTorch, with concerns about support for deep copying of tensor gradients (`.grad`) especially for leaf tensors, and the need for improved API hooks for backward operations to monitor statistics across multiple models or threads. There is also a recurring theme about build and environment issues, such as compatibility of compilers with Windows and CUDA versions, and the complexity of hardware-specific optimizations like pinned memory registration and tensor transfers across devices. Several issues point out inconsistencies or potential mismatches between Python, NumPy, and C++ behaviors, such as with negative strides, `flatten()` deprecation, and `cmath.phase()` handling of `-0`. Lastly, some discussions touch on CI failures, upstream build failures, and the importance of verifying that code annotations or refactoring don't introduce regressions, especially concerning JIT compatibility and performance."
2021-01-01,pytorch/pytorch,"The discussions highlight issues related to CUDA and hardware stability, such as illegal memory access errors potentially caused by GPU overclocking or power settings, with suggestions to adjust GPU clocks and power levels. There are concerns about the accuracy and clarity of PyTorch’s documentation, specifically for functions like `torch.bucketize` and `torch.searchsorted`, with users requesting corrections to behavior descriptions to match implementation. Compatibility and performance questions are raised regarding torchvision models with TorchScript, especially involving custom C++/CUDA ops like NMS, emphasizing whether such models can run standalone without Python. Additionally, some posts address design debates over the behavior of tensor view operations, deprecation policies for functions like `flatten`, and the consistency of behavior compared to NumPy, aiming for clearer UX and documentation. Lastly, questions about performance optimizations, such as PCA algorithms in PyTorch and multi-threading behaviors in functions like `torch.eye`, suggest ongoing efforts to improve efficiency and debugging strategies."
2021-01-02,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA and hardware compatibility, notably issues arising from incompatible or unsupported GPU driver and compiler versions, which cause build failures and runtime errors such as illegal memory access and assertion failures. There are concerns about the efficiency and clarity of core functions like `multi_head_attention_forward`, with suggestions to refactor for better readability and modifiability. Several users suggest hardware-related causes for runtime errors (e.g., overclocking, insufficient power, hardware failure) rather than software bugs alone, emphasizing the importance of hardware stability and configuration. Additionally, there are questions about proper installation and build configurations, such as enabling CUDA support, managing dependencies like MAGMA and NCCL, and ensuring consistent reproducibility across environments. Overall, unresolved issues include build failures on newer CUDA versions, clarity of certain function behaviors, and hardware stability impacting software reliability."
2021-01-03,pytorch/pytorch,"The discussions highlight ongoing challenges with dtype promotion, particularly aligning PyTorch's behavior with NumPy and handling scalar categories to avoid unnecessary precision up-promotions. There are concerns related to subclassing tensors like `Parameter`, and how promotion rules interact with subclassed types and the numpy-like promotion logic. Performance and compatibility issues are discussed, especially regarding CUDA-related features such as pinned memory management, `cudaHostRegister`, and potential migration of LAPACK functions (e.g., `gels*`) to CUDA for efficiency. Additionally, inconsistencies and bugs related to distributed training, autograd, and GPU memory management, as well as the impact of version changes on reproducibility, are raised. Overall, the conversations focus on improving functional correctness, API consistency, performance optimizations, and providing better support for advanced use cases like model parallelism."
2021-01-04,pytorch/pytorch,"The comments reflect ongoing efforts to improve PyTorch's functionality, compatibility, and performance, including adding features like `torch.isin`, `torch.linalg.norm` with flattening, and support for sparse matrix operations. There are discussions about fixing existing bugs (e.g., in backward or data parallelism), updating internal implementations (like deep copying or handling tensor shapes/metadata), and refactoring complex functions (such as multi-head attention or onnx exporters) for better maintainability. Questions about hardware support (like CUDA on MacOS, ARM backends, and Neural Engine) and platform compatibility also appear, along with considerations for adding support for new or experimental features (e.g., support for orthogonal constraints or various distributed process group operations). Some issues remain unresolved or are under active development, with community contributions and testing needed to finalize enhancements and fixes."
2021-01-05,pytorch/pytorch,"The comments reveal ongoing efforts and challenges around several core topics in PyTorch development. Notably, there are concerns with backward compatibility and testing, such as ensuring JIT support for new symbolic functions, and accurately measuring gradient-related operations like `torch.isin` and `linalg.norm` with performance benchmarks. There are issues with specific operator behaviors, including proper handling of `deepcopy` for tensors with `.grad`, and differentiability when overriding `out` arguments, especially for functions like `torch.linalg.norm`. Several discussions point to the need for better test coverage, refactoring large test files, and defining clear API behaviors, especially in the context of quantization, distributed training, and model parallelism. Unresolved questions include handling of CUDA-specific behaviors, performance implications of new implementations, and how to robustly manage experimental features or non-standard hardware support."
2021-01-06,pytorch/pytorch,"The discussions encompass a variety of technical concerns, primarily focusing on adding new functionalities and improving existing features in PyTorch. Key issues include implementing `torch.isin` and optimizing `nonzero()` performance on GPU, as well as enhancing deterministic behavior in operations like interpolation and upsampling. There are questions about porting linear algebra functions such as `orgqr` with support for complex types, and concerns regarding the support of distributed training, especially model parallelism and DDP on different hardware platforms. Several discussions also involve improving code quality and consistency, such as transitioning from `OrderedDict` to native `dict`, fixing test failures, and addressing API support and backward compatibility. Unresolved questions include the timing for new feature merges, handling of certain bugs, and ensuring stable, deterministic behavior across various systems and operations."
2021-01-07,pytorch/pytorch,"The discussions primarily revolve around handling edge cases and improving clarity in PyTorch's implementation and API. Key concerns include properly managing zero-length sequences in RNN/embedding contexts, clarifying tensor device and data type semantics (especially for complex and quantized tensors), and enhancing error messages for better usability. Several discussions question the current design choices, such as the behavior of backward functions with interval gradients and the potential for changing internal mechanisms like `__getattr__` handling or using `__getattribute__`. There are also recommendations to refactor certain functions (e.g., `repeat` vs. `tile`) and to ensure consistency across various modules and documentation. Unresolved questions involve performance impacts of new features, handling nondeterminism, and how to best expose or abstract distributed and linear algebra operations."
2021-01-08,pytorch/pytorch,"The discussions cover a range of technical issues within the PyTorch repository, such as handling label ranges in classification tasks, device-side CUDA errors triggered by label out-of-range assertions, and the importance of proper validation and error messaging for edge cases like fully padded embeddings or empty tensors. There are concerns about the validity and efficiency of certain implementation choices, like prime number generation for random seed management, and about ensuring backward compatibility and correctness of distributed training modules like DDP. Other topics include the need for better documentation, clarifications on support for certain backends or features (e.g., XPU, wavelet transforms, ONNX export, use of `torch.jit` and `__torch_function__`), and performance benchmarking of new features (e.g., channels-last support, `cholesky_inverse`). Several discussions involve bug fixes, test coverage, build system issues, and contributor licensing, emphasizing the ongoing maintenance, testing, and validation challenges in large-scale deep learning frameworks."
2021-01-09,pytorch/pytorch,"The comments reveal recurring challenges related to environment setup and package compatibility in PyTorch, including installation issues, CUDA/cuDNN version mismatches, and dependencies for distributed training and extensions. Several discussions focus on runtime errors stemming from multiprocessing and Windows-specific constraints, such as the need for 'if __name__ == ""__main__""' guards and freeze support, especially when training models with multiprocessing. There are also concerns about build configuration and reproducibility issues, often requiring environment modifications like debug builds, updating submodules, or specific compiler flags. Additionally, questions around the internal workings of JIT, constants management, and certain APIs (e.g., torch.as_tensor and complex convolutions) suggest ongoing efforts to improve stability, performance, and usability. Unresolved topics include handling multi-node distributed setups with qsub, fixing bugs that cause memory leaks or crashes, and refining build and deployment workflows across platforms."
2021-01-10,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's features and compatibility, such as adding NumPy-like functions (e.g., torch.corrcoef, torch.searchsorted) and implementing missing tensor operations (e.g., torch.mean for ByteTensor). Several issues concern performance and correctness, including memory leaks linked to torch.as_tensor(), GPU-related errors like CUBLAS_STATUS_EXECUTION_FAILED, and inconsistencies in operations like torch.norm and torch.squeeze. Attention is given to enhancing testing, error messaging, and documentation to ensure robustness, with discussions on handling tensor shape inference in ONNX export and ensuring correct device and dtype management. Some unresolved questions involve support for higher-dimensional SSIM, tensor broadcasting semantics, and handling multi-device/multi-process contexts, alongside platform-specific bugs and build system reliability."
2021-01-11,pytorch/pytorch,"The discussed comments primarily revolve around PyTorch's development issues, including feature implementation status, performance regressions, and API behavior inconsistencies. Several queries concern the support and behavior of specific functionalities such as `PixelUnshuffle`, `PixelUnShuffle`, `torch.norm`, `torch.linalg.norm`, and `einsum`, as well as GPU memory management and multi-threading. There are technical considerations about optimizing certain operations (e.g., `cholesky_inverse`, `torch.vmap`), addressing numerical stability, and ensuring backward compatibility. Some comments relate to build and integration issues, including disambiguation of operator signatures, onnx export behavior, and code correctness in both CPU and CUDA environments. Overall, unresolved questions pertain to improving performance, enhancing API consistency, and fixing bugs that affect model training, inference, and deployment."
2021-01-12,pytorch/pytorch,"The discussions encompass issues related to tensor manipulation, particularly extending or clarifying the behavior of `unsqueeze`-like operations and `torch.flip`, with suggestions to improve documentation and consistency across CPU and CUDA implementations. Several reports concern performance regressions of linear algebra routines like `cholesky_inverse` and `svd`, with proposals for specialized kernels or code restructuring to optimize speed on GPUs and CPUs. Compatibility and build issues are frequent, including challenges with CUDA versions, cuDNN support, and the build system's configuration, as well as concerns about the portability and stability of certain APIs and modules across different hardware and software environments. There are questions about the future support for additional data types, improved model saving strategies for distributed settings, and better tooling or APIs to facilitate debugging, testing, and deployment. Unresolved topics include balancing performance with API clarity, extending support for more input types in model export, and handling platform-specific quirks in low-level implementation."
2021-01-13,pytorch/pytorch,"The comments reveal multiple recurring themes and concerns in the GitHub discussions. Several issues relate to performance and compatibility, such as CUDA out-of-memory errors, slow execution times, and optimizing GPU kernels (e.g., cuSOLVER vs MAGMA, tensor layouts). Others focus on code maintainability and correctness, including the handling of complex number gradients, type promotion, and ensuring proper registration and support for different tensor types and data formats (e.g., channels_last, support for string inputs in ONNX). Additionally, there are operational concerns like test coverage, CI failures, and build configurations, especially related to compiler support, environment setup, and cross-platform issues (Linux, Windows, Apple M1). Unresolved questions include how to properly extend support for new data types, improve automatic code generation, and optimize distributed sharding and runtime behavior."
2021-01-14,pytorch/pytorch,"The comments primarily reflect discussions around feature additions, bug fixes, and build configuration issues in the PyTorch repository. Key topics include the proposal for adding a SpaceToDepth module and the handling of its onnx export support, especially related to shape inference and shape support. Several discussions address build and linking complexities across platforms (e.g., ROCm, mobile, Windows), as well as dependency management issues such as multiple tensorboard versions and CMake configuration for libtorch. There are also technical debates on automatic output shape inference for neural modules, enhancements to the lazy module initialization, and support for specific features like Huber loss or quantization behaviors. Unresolved questions include ensuring backward compatibility, the impact of build environment differences, and integrating user-friendly debugging and validation tools—highlighting ongoing efforts to improve robustness, usability, and portability of the PyTorch codebase."
2021-01-15,pytorch/pytorch,"The discussions encompass various technical concerns in PyTorch development, including the need to implement new behaviors for sparse tensor multiplication to always produce sparse output, and addressing issues with batch normalization layers when batch size equals one. Several posts point out performance inconsistencies between CUDA versions and hardware, especially regarding kernel launch configurations and their impact on speed and stability. There are also recurring questions about enhancing onnx export robustness, improving autograd support with complex numbers, and reducing memory leaks linked to tensor management and CUDA operations. Additionally, discussions highlight the importance of improving testing practices, such as better coverage, debugging, and ensuring proper handling of edge cases like custom hooks, complex tensor operations, and compatibility issues across different environments and hardware."
2021-01-16,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA out-of-memory errors in PyTorch, often caused by large batch sizes, inefficient tensor operations, or persistent attribute accumulation during forward passes, and suggest solutions such as reducing batch size, input dimensions, or avoiding in-place list/array appends. Several issues relate to the complexity of ensuring performance and correctness across different hardware (e.g., ARM, PowerPC, ROCm) and software configurations, including thread management, compiler compatibility, and kernel efficiency. There are also concerns about proper testing (including Python and distributed tests) when integrating new features or backends, emphasizing the importance of comprehensive validation. Specific technical fixes include adjusting GRAIN_SIZE for OpenMP parallelism, patching kernel code for compiler compatibility, and rewriting code to avoid NaNs or allocation errors, with some unresolved questions about effective debugging and benchmarking strategies. Overall, the discussions underscore the need for careful performance tuning, cross-hardware support, and thorough testing to tackle memory and stability issues in PyTorch."
2021-01-17,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch’s extensibility, such as implementing open standards like OpenCL support and refactoring modules like multi-head attention, with questions about integrating new components into existing frameworks. Several concerns pertain to ensuring reproducibility and performance optimization across different hardware, including issues with determinism settings, environment configurations, and benchmark results, especially on ARM and GPU architectures. Other topics involve improving code maintainability and clarity, such as consolidating repeated code, refactoring complex functions, and handling versioning for model initialization, while also addressing workflow challenges like test discovery, CI failures, and proper module exports for static type checkers. Additionally, questions about API consistency, better error messages, and build artifacts (e.g., M1 wheels, ONNX exporters) reflect the drive for more robust, transparent development and deployment processes. Unresolved questions remain around integrating these improvements seamlessly, ensuring cross-platform compatibility, and maintaining code quality through testing and documentation."
2021-01-18,pytorch/pytorch,"The comments highlight several ongoing issues and proposals concerning PyTorch development. Major concerns include the lack of open standards support such as OpenCL in favor of proprietary frameworks, with suggestions for funding campaigns to encourage open alternatives. There are technical discussions about hardware compatibility, especially for GPU support and topology issues, which affect reproducibility and performance. Additionally, there are proposals for improving the robustness, stability, and testing of features, such as ensuring consistent initialization, better error handling for complex operations, and explicit device management to prevent memory and deadlock problems. Overall, the conversations reflect both strategic interests in open, hardware-agnostic solutions and detailed technical challenges needing future fixes or enhancements."
2021-01-19,pytorch/pytorch,"The discussions reveal several recurring technical issues in the PyTorch repository, such as the semantics of sparse tensor operations (e.g., whether sparse * dense should produce sparse output), and the handling of label smoothing in loss functions, with proposed solutions involving new loss classes or functions accepting arbitrary smoothing parameters. Concerns around API design and compatibility are evident in discussions on adding support for complex sparse tensors, implementing additional functions like `cov`, `average`, or `einsum`, and ensuring operation consistency with NumPy, SciPy, or other libraries. There are also numerous build, CUDA, and performance-related issues, including out-of-memory errors, driver compatibility, and performance regressions, alongside questions about profiling tools and optimization strategies. Finally, some discussions address code management practices like merge conflicts, test coverage, and the need for clearer documentation or API adjustments to facilitate future development and stability."
2021-01-20,pytorch/pytorch,"The discussions reveal several core issues: there is significant interest in adding or improving OpenCL support and backend flexibility in PyTorch, with some contributors suggesting funding or contribution efforts; batch normalization and input handling in models, especially for time series segmentation, require careful batching strategies and proper batching/processing techniques; issues with distributed training, especially related to multiprocessing, process startup timeouts, and CUDA device management, need better handling or configuration, such as adjusting timeouts, environment variables, or process start methods; there are concerns regarding static lists of supported data types for operations like `amax`, `amin`, and `count_nonzero`, which could benefit from more dynamic or comprehensive support, and the need for robust fallback or validation strategies; finally, there are compatibility issues, reproducibility concerns, and build failures linked to dependencies such as CUDA versions, GPU environments, or external libraries like Magma, which require careful environment management, driver support, or code refactoring to ensure stability across platforms and use cases."
2021-01-21,pytorch/pytorch,"The discussions cover a broad range of topics related to PyTorch development, including support for heterogeneous backends and hardware accelerators (like OpenCL, ROCm, TPUs), and concerns about framework monopolies influenced by hardware vendors. There are technical issues regarding CUDA memory management, tensor conversion between OpenCV and PyTorch, and the complexities of exporting models to ONNX, especially with unsupported operators or improper graph lowering. Several questions also address performance optimizations, such as better multi-streaming, caching strategies, and improving the speed of operations across different CUDA versions. Additionally, there are support questions for tools like profiling, issues with versioning, and efforts to expand testing, bug fixing, and API enhancements (like support for unions, quantization, and complex types). Many unresolved or ongoing concerns relate to build stability, cross-platform compatibility, and ensuring consistent behavior across diverse hardware and software configurations."
2021-01-22,pytorch/pytorch,"The comments cover a wide range of technical topics related to PyTorch development: requests for new features like torch.corrcoef and Spearman's correlation, model analysis and visualization improvements, semantics of sparse operations, enhancing training API (adding grads to step, schedulers), and various bug fixes and performance optimizations. Several issues discuss ensuring correctness and safety (e.g., in backward passes, synchronization, device management), often proposing API changes or new utility functions. There are concerns about build dependencies, compatibility, and ensuring codebase consistency (like union types, common configs). Some comments highlight ongoing or unresolved problems such as CUDA initialization crashes, slow download speeds, and complex nested data handling, often seeking better usability and robustness. Overall, the discussions suggest a push towards more flexible, performant, and maintainable PyTorch features, with careful attention needed for correctness, compatibility, and ease of use."
2021-01-23,pytorch/pytorch,"The discussions encompass a range of technical concerns, including handling iterable datasets in distributed data parallel scenarios, with suggestions such as WebDataset as an alternative. There are issues related to CUDA compatibility and NCCL network errors when using multiple GPUs, especially on older or different hardware configurations. Several comments address code quality, including plans for deprecating legacy tensor creation ops, and fixing specific bugs like the `fmod` issue in PyTorch 1.7.*. Additionally, the discussions touch upon internal infrastructure topics such as test coverage, CI failures, and configuration management. Many unresolved questions focus on improving robustness of multi-GPU operations, supporting complex nested structures in pipeline models, and simplifying API design for broader user scenarios."
2021-01-24,pytorch/pytorch,"The discussions highlight various technical issues with PyTorch, including build failures caused by missing Docker images and unresolved external symbols, particularly on Windows and during linking stages. There are questions about the default use of XNNPACK for FP32 models, the support of `torch.distributed.optim` on Windows, and limitations in exporting scripted models containing LSTM or handling certain data types like booleans in ONNX graph lowering. Some discussions focus on memory leaks in the RPC backend and the impact of multiprocessing strategies and shared memory on stability and resource limits. Additionally, updates on platform-specific support, such as M1 Mac wheels and build configurations, are mentioned, along with efforts to improve test coverage and fix CI failures."
2021-01-25,pytorch/pytorch,"The discussions primarily revolve around CUDA and PyTorch internal errors, such as device-side asserts, memory management issues, and support for specific hardware and software configurations (e.g., CUDA 11.1, M1 architectures). Several comments address solidifying correct tensor behaviors, especially regarding complex number support in functions like `torch.svd` and `torch.norm`, and ensuring proper MPI/multiprocessing handling across different platforms and Python versions. There are ongoing concerns about API stability, including the deprecation of certain arguments, API design choices, and compatibility for features like `torch.nn.Module` annotations and symbolic tensor padding. Additionally, discussions mention fixing internal bugs, updating dependencies (BLAS/LAPACK), and building binary support for various environments, with particular attention to testing, benchmarking, and CI stability. Unresolved questions include support for newer hardware, ensuring correctness across diverse configurations, and streamlining multi-platform build processes."
2021-01-26,pytorch/pytorch,"The comments encompass various issues including CUDA and library compatibility problems, such as unsupported architectures (e.g., sm_86 with CUDA 9), and the need for improved error messages for device mismatches in functions like `stft`. Several discussions focus on project maintenance and testing, such as handling stale branches, test failures related to PyTorch's distributed or JIT components, and the support for different hardware platforms (e.g., ppc64le, Windows). There are technical proposals like refactoring the dispatcher API to streamline the removal of functions like `callWithDispatchKey()` and `redispatch()`, and adding new profiling or debugging tools for performance diagnostics of pipelines and GPU utilization. Additionally, questions about compatibility (e.g., ONNX backward support), build configurations (e.g., ABI issues), and support for features like tensor de-duping indicate ongoing efforts to enhance robustness, usability, and platform coverage."
2021-01-27,pytorch/pytorch,"The discussions reflect ongoing challenges related to PyTorch's development, including interface and API changes such as modifications to native functions, new API proposals like parametrizations and tensor support, and issues with backward compatibility. Several issues address build and integration hurdles, especially on macOS, Windows, and with specific hardware configurations, as well as correctness concerns like proper handling of numerical stability, data type conversions, and device management. There are also questions about testing practices, code maintenance (e.g., cleaning stale branches, ensuring code coverage, and managing external dependencies), and performance regressions or inefficiencies introduced by certain changes. Unresolved topics include specific implementation details for new features (like memory pools for CUDA graphs), testing for new functions (e.g., `corrcoef`, `cov`), and handling of discontinuities or exceptional cases in code (e.g., in automatic differentiation and quantization)."
2021-01-28,pytorch/pytorch,"The comments mostly revolve around debugging and optimizing PyTorch functionalities, including addressing installation issues on Windows and Mac, such as package compatibility and linker errors, and improving stability of features like the new onnx export, onnx opset support, and runtime profiling. There are concerns about the performance overhead of certain operations (e.g., tensor device access, expand/upsample routines) especially when using AMP or in backward passes, with suggestions to benchmark and tune these. Several discussions target enhancing user experience and maintainability, such as introducing recommended VSCode configurations, handling deprecated internal APIs, and ensuring backward compatibility for state_dict serialization, as well as managing large repositories and branches with stale PRs. Unresolved questions include whether to treat certain functionalities as global or stream-specific (e.g., CUDA mempools), how to best expose and support custom or private objects in TorchScript, and the proper way to handle incremental API changes across versions to avoid breakage in serialized models."
2021-01-29,pytorch/pytorch,"The comments reveal several recurring technical concerns: (1) Managing shared memory and optimizing DataLoader worker settings in PyTorch, especially on HPC systems with large shared memory configurations, (2) Performance regressions and slowdowns in tensor operations such as upsampling and interpolation across different PyTorch versions, particularly between 1.7 and master, and the impact of autocast and casting strategies, (3) Compatibility and build issues related to specific environments such as macOS Xcode configurations, Windows, and ROCm, including missing dependencies and compilation errors, (4) API and support improvements like better type annotations, JIT scripting support, and ensuring stability across different backends, and (5) CI stability and upstream build failures due to environment misconfigurations, outdated dependencies, or incompatible compiler/linker issues. Several suggestions involve detailed profiling, reworking or adding tests for correctness, and strategic upgrades of build tools or dependencies to resolve ongoing issues."
2021-01-30,pytorch/pytorch,"The discussions highlight several key technical concerns, including the challenges of effectively quantizing transformer-based models like Albert-xlarge-v2, with the suspicion that some layers such as `torch.nn.Bilinear` are not being quantized properly, resulting in unchanged model sizes and inference times. There are ongoing issues with the `ParameterList` container, especially when used with `DataParallel`, where it appears empty on multiple GPUs, accompanied by warnings about unsupported attribute setting. Several CI-related failures persist due to network or infrastructure problems, such as failed git clone operations or disk space shortages, impeding reliable testing and validation. Other discussions delve into the future API design for functions returning multiple tensors, debating NumPy-compatible options versus namedtuples, to enhance clarity and extensibility. Additional topics include the slow performance of large models on new GPUs like the RTX 3090, bugs related to TorchScript compilation with updated pybind11 versions, and the need to update build scripts and hardware driver support for CUDA/CuDNN compatibility across different environments."
2021-01-31,pytorch/pytorch,"The discussions primarily revolve around debugging and optimizing PyTorch's functionalities, including handling exceptions in DataLoader, addressing GPU hardware issues, and diagnosing CUDA errors such as illegal memory access and runtime CUDA errors. Several questions concern improving debugging tools, such as enabling detailed logging with cudnn/cublas, and exploring whether ltrace could help trace errors despite static linking. There are inquiries about PyTorch's behavior with multiprocessing, especially in Jupyter notebooks and with num_workers > 0, highlighting issues with reproducibility, OOM errors, and compatibility. Additionally, some discussions focus on internal code maintenance, such as the transition to upstream pybind11, and the impact of recent code changes on build stability and testing coverage. Unresolved questions include how to handle custom trace handlers effectively, fixing GPU-related runtime errors, and ensuring compatibility with third-party libraries like timm under recent build environments."
2021-02-01,pytorch/pytorch,"The comments encompass a range of technical concerns including compatibility and correctness issues with recent PyTorch versions, especially related to multi-GPU training, in-place operations, and API support for complex numbers, quantization, and new padding modes like 'symmetric.' Several discussions focus on improving API consistency, such as adding support for `__array__`, `H`, or `adjoint` attributes for matrices, and refining function signatures, backward compatibility, and serialization. There are recurring questions about the development timeline, testing strategies (particularly for deterministic behavior and GPU stress tests), and the need for better documentation or error handling for features like `ParameterList` in distributed settings. Unresolved questions include the implementation specifics for new features (e.g., 'same' padding, reparametrization frameworks), and compatibility issues with hardware or software versions, emphasizing ongoing efforts but no definitive resolutions yet."
2021-02-02,pytorch/pytorch,"The discussions encompass several technical issues related to PyTorch, including handling of import and environment conflicts (particularly Python version and package compatibility), subtle bugs in CUDA kernel behaviors (notably with thrust and cuDNN's static linking and version regressions), and the need for improved testing and documentation standards (such as for `OpInfo` and dtype support). Concerns are raised about specific build failures on architectures like ppc64le, the importance of supporting mixed MPI and NCCL usage within a single process, and adapting internal CUDA code for better stability and compatibility (e.g., in `masked_scatter`). Several suggestions point to refactoring or workarounds: enhancing error messages, patching certain kernels, clarifying API behaviors, and improving build scripts. Lastly, ongoing maintenance issues, merge conflicts, CI failures, and the need for clearer documentation and better test coverage are also highlighted, with unresolved questions mainly about fixing build regressions and dependency compatibility."
2021-02-03,pytorch/pytorch,"The discussions highlight several technical concerns: the behavior of non-differentiable operations (like dtype casting) affecting gradient flow, and the need for clearer documentation and testing around these operations. There are issues related to the integration and proper management of internal APIs, such as handling complex number operations, eigenvalue computations, and tensor memory layouts, especially on different hardware and CUDA versions. Multiple discussions also address performance benchmarking of fused kernel operations versus native implementations, and the impact of certain API or implementation changes on correctness and stability, including multi-GPU and CUDA-specific behaviors. Additionally, there is interest in improving the static typing, code quality, CI reliability, and compatibility with legacy systems or external libraries like PyBind and NumPy. Unresolved questions include how best to balance backward compatibility, documentation, testing rigor, and performance optimization in these complex scenarios."
2021-02-04,pytorch/pytorch,"The comments cover a range of technical issues, including environment setup and build problems with CUDA and PyTorch on different platforms (e.g., CentOS, Windows, macOS), with specific concerns about CUDA libraries, environment variables, and compatibility. There are recurring discussions about the handling of environment variables such as `CPLUS_INCLUDE_PATH`, issues with CI/CD failures, and the need for better testing, debugging tools, and API stability, especially around distributed training, synchronization, and JIT compilation. Some comments suggest that certain bugs are upstream or external (e.g., thrust, thrust::exclusive_scan, third-party libraries), while others highlight the need for better documentation, user guidance, and API design (e.g., `torch.tensor`, `WeightedNorm`, `SparseCSR`). Several discussions involve refactoring, bug fixes, or feature additions (e.g., environment reporting, deterministic operations, custom gradient reduction), often with questions about best practices, compatibility, or proper usage. Overall, the discussions reflect ongoing efforts to improve stability, performance, and usability of PyTorch across diverse environments and use cases, with some unresolved issues related to build reproducibility, environment handling, and advanced distributed features."
2021-02-05,pytorch/pytorch,"The comments reflect ongoing technical discussions around compatibility issues with PyTorch and related libraries, including dependency conflicts caused by glibc versions, CUDA driver incompatibilities, and build failures related to specific hardware like RTX 3090 and V100 GPUs. Several threads highlight the need for improved or new features, such as support for higher-dimensional tensor operations like support for dimensions in `torch.histc`, or enhanced functionalities like `autocast` with a `cast_inputs=""promote_to_match""` option. There are also recurring issues with pre-built binary compatibility, build failures in CI pipelines, and the importance of proper testing and documentation, especially for experimental or less common configurations such as sparse tensors, complex number support, or custom autograd functions. Additionally, some discussions suggest structural changes, like adjusting the API signatures for functions like `sort()` to accommodate default arguments or handling deprecated or unsupported features more gracefully. Unresolved questions mainly involve compatibility fixes for new hardware, maintaining backward compatibility while introducing new features, and improving debugging and diagnostic tools in complex build environments."
2021-02-06,pytorch/pytorch,"The comments reflect various technical concerns such as limitations in `torch.scatter` when indices are not unique, compatibility issues with newer GPUs like RTX 3090 and CUDA versions, and performance regressions in distributed training scenarios. Several discussions address support for large input sizes, especially in batch normalization and convolutional layers, with suggestions to fallback to native implementations or modify existing kernel thresholds. There are also questions about test stability, code deprecations, and the impact of recent updates on existing functionalities, such as quantization, model serialization, and TorchScript compatibility. Unresolved issues include the support for sophisticated features like positional encoding in `nn.Transformer`, handling large inputs gracefully, and improving the robustness of kernel implementations across different hardware and software configurations."
2021-02-07,pytorch/pytorch,"The discussions highlight several key concerns in the PyTorch repository, including challenges with documentation versioning and searchability, especially in relation to stable versus master docs, and search engine indexing issues. There are technical debates about implementing a ""same"" padding mode, with considerations on its stability and symmetry, emphasizing the complexity of padding strategies across different frameworks. CUDA memory management remains a recurring issue, with suggestions to improve memory handling and potential fallbacks for large tensor operations, alongside ongoing CI and compatibility testing. Additionally, user questions address experimental features, such as large batch norm handling, CUDA version compatibility, and detailed environment setup diagnostics. Overall, unresolved questions include the stability of padding modes, memory management strategies, and ensuring consistent documentation and framework behavior across versions."
2021-02-08,pytorch/pytorch,"The discussions highlight various technical concerns including the lack of `torch.int32` support affecting compatibility with ONNX and TensorRT, with suggestions to support casting from int64 to int32 more effectively. There are recurring issues with data loading—particularly with `num_workers` causing hangs on different operating systems—and the need for better debugging communication or workarounds. Several issues pertain to deep learning model features such as making padding modes like `same` more stable, support for advanced quantization and backward compatibility, and the implementation of specific operations like GLU, softmax, and layer normalization, often requesting enhancements or fixes in their respective code paths. Some discussions emphasize the importance of version consistency (e.g., mypy, numpy), and the need to carefully manage API deprecations, especially around dispatch and call mechanisms. Overall, unresolved questions involve integrating new features (e.g., MHA fixes, webdataset improvements) while maintaining stability and performance, with many concerns about build compatibility, test coverage, and proper API evolution."
2021-02-09,pytorch/pytorch,"The comments highlight various technical discussions related to PyTorch, including search result de-duplication, performance issues on Windows and Linux, and symbol exposure in libtorch. Several investigators are addressing specific bugs, such as memory allocation, CUDA compatibility, and function support (e.g., quantization, softmax safety). There are suggestions for API improvements, documentation clarifications, and potential deprecations (e.g., `chain_matmul` vs `multi_dot`). Some discussions involve CI failures, build environment setups, and possible backwards compatibility concerns. Overall, the discussions revolve around bug fixes, performance optimizations, API usability, and ongoing CI challenges."
2021-02-10,pytorch/pytorch,"The discussions highlight several key points: First, there's an ongoing effort to standardize PyTorch documentation, especially for `nn.functional` APIs, including creating templates and better consistency between functions like `linear` and `bilinear`. Second, there's interest in expanding support for distribution operations such as matrix logarithm and square root, with particular focus on CUDA implementations and alternative iterative algorithms due to SciPy compatibility. Third, issues around implementation details and testing for complex support, sparse tensor behaviors, and operator consistency (e.g., in `autograd`, in-place, and out variants) are prominent. Fourth, several technical challenges are discussed, like handling shared parameters in pipeline models, Low GPU utilization diagnostics, and CUDA-specific bugs related to device ordering and IPC. Finally, there's a recurring theme around fixing build/compatibility issues across environments (Xcode on Mac, Anaconda restrictions, ROCm hardware bugs), and improvements in robustness (warnings suppression, error messages, better profiling tools)."
2021-02-11,pytorch/pytorch,"The discussions highlight several technical concerns, notably around GPU memory management and system compatibility issues (e.g., shared memory limits, driver versions, and specific hardware like RTX 3090). There are recurring questions about handling non-differentiable operations, inconsistencies in operator support for export and JIT, and challenges in expanding or modifying existing APIs (e.g., `torch.linalg`, `torch.distributions`, and support for optional output tensors). Several proposals suggest improving tooling, error messages, testing strategies (especially for edge cases or flaky tests), and potential API redesigns to better handle side effects, in-place updates, and multi-return functions. Unresolved questions include how to best design user-friendly function signatures for complex operations, managing backward compatibility, and handling system-specific bugs or limitations (e.g., CUDA or ROCm issues). Overall, there is a need for clearer documentation, better error handling, more robust testing, and architectural adjustments to accommodate new features and system variations."
2021-02-12,pytorch/pytorch,"The discussions largely revolve around resolving technical build and runtime issues related to PyTorch's compilation, GPU memory management, and operator behavior. Key issues include memory errors during CUDA operations, particularly out-of-memory errors and system freezes, which are mitigated by strategies like reducing batch size, detaching variables, or clearing caches. There are concerns about build failures stemming from compiler incompatibilities (e.g., GCC versions) and platform-specific problems like Windows DLL linkage errors. Additionally, discussions touch on enhancing PyTorch features, such as adding label smoothing functions, implementing new operators (e.g., symmetric padding, tensor indexing), and improving debugging/benchmarking tools, with some solutions requiring code reorganization, API changes, or additional testing. Unresolved questions include how best to ensure compatibility for new operator features across different backends and platforms, and handling edge cases that cause subtle bugs or performance regressions."
2021-02-13,pytorch/pytorch,"The comments encompass a range of technical issues including the implementation of grouped convolutions on V100 GPUs, CUDA tensor sharing bugs in multiprocessing, and the transition from TH to ATen for index operations, highlighting ongoing porting efforts and related failures. There are concerns about specific bugs, such as incorrect behavior with certain tensor types, performance regressions, and CUDA memory errors, with suggestions for improvements like adding warnings, error checking for structured kernels, and updating licensing for MKL dependencies. Multiple discussions also address build and environment challenges, such as Windows linking errors, driver issues, and the need for better documentation and testing utilities, including stress tests or basic GPU checks. Several PRs aim at porting and optimizing functions, porting complexities, and improving build configurations, with some unresolved questions about handling specific tensor operations, ensuring backward compatibility, and better test coverage or error messaging. Overall, the conversations reflect active development, debugging, and planning for enhanced functionality, performance, compatibility, and usability."
2021-02-14,pytorch/pytorch,"The discussions include various technical topics such as the support for computing SVD of complex matrices (Issue #13505.0), potential enhancements in interpretability and diagnostics within PyTorch, and the support for specific hardware backends like x86 environments (Issue #37817.0). There are also multiple reports of CI failures across different issues, often related to build environment misconfigurations, dependency issues, or incomplete support for certain features (e.g., sparse matrix support in MKL in Issue #50937.0). Some discussions focus on improving data loading pipelines, especially distributed sharding and high-performance I/O solutions, with references to external tools and libraries such as WebDataset, AIStore, and Tensorcom. Additional concerns involve code coverage, API design decisions (e.g., static dispatch keys), and correctness or performance implications of specific implementations (e.g., ONNX optimizations and argument types)."
2021-02-15,pytorch/pytorch,"The discussions primarily revolve around ongoing and future development efforts in PyTorch, such as porting and optimizing code for ATen, ensuring reproducibility across different hardware, and improving the robustness of operations like interpolation and tensor indexing. Several issues highlight challenges with deterministic behavior, multi-threading, and backend compatibility, including CUDA kernel inconsistencies, thread management in functions like `torch.eye()`, and device-specific faults. There are also concerns about integration with external tools and libraries—such as MKL, ONNX, TVM, and Bazel—and their impact on build stability, performance, and cross-platform compatibility. Many discussions seek to clarify implementation details, propose performance improvements, and troubleshoot high CPU loads or GPU faults. Overall, unresolved questions focus on optimizing kernel dispatch, managing hardware-specific issues, ensuring accurate and efficient autograd and scripting behavior, and refining build and dependency management workflows."
2021-02-16,pytorch/pytorch,"The discussions highlight several recurring themes: difficulties in implementing or extending PyTorch functionalities such as computing Spearman's correlation with Torch, and enabling lazy evaluated parameters; challenges related to multi-GPU and multiprocessing compatibility, especially with CUDA tensors; ongoing performance regressions or bugs in core operations like SVD, eigen, and tensor reshaping, often tied to device compatibility or implementation details; infrastructure concerns such as build issues on Apple M1, dependencies, and CI failures; and proposals for improving API design, documentation, and internal mechanisms like cache management, module modifications, and function organization. Many comments suggest that simplifying or making certain features ""first-class"" (e.g., computed parameters, reparametrizations) would significantly improve usability, but some proposals are hindered by constraints like not modifying core classes. Unresolved questions include optimal ways to handle multi-device tensor sharing, fixing performance regressions, and automating build processes on new hardware platforms."
2021-02-17,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's modularity and flexibility, particularly concerning the handling of parameters and modules—such as proposing to make Tensors callable or to support parametrizations that map multiple tensors, with an emphasis on backward compatibility and minimal interface disruption. There are concerns about automatic serialization/deserialization of scripted modules, especially regarding complex data structures, complex SVD support in FFT-based methods, and the automatic management of device-specific optimizations like MKL and CUDA behaviors. Additionally, the conversations include suggestions for improving compiler and build system behaviors, debugging and testing strategies, and ensuring consistent API behavior across different hardware and software configurations. Unresolved questions involve how to automatically support multi-tensor parametrizations, automatic inverse computations for parametrizations, and incorporation of new features into existing APIs without breaking compatibility."
2021-02-18,pytorch/pytorch,"The discussions primarily focus on the implementation and behavior of padding modes in convolutional layers, especially the challenges with 'same' padding in PyTorch versus TensorFlow, including its instability with variable input sizes and potential asymmetry issues affecting learned filters. There's interest in providing a more flexible reparametrization framework with support for invertible modules and transformations, highlighting design questions such as supporting `right_inverse` and `inverse` methods, and how to incorporate such mechanisms into `nn.Module`. Concerns are raised about the correctness and stability of normalization layers like BatchNorm under small batch sizes or specific configurations, along with performance considerations in distributed training, kernel fusion, and sparse matrix operations. Additionally, logistical topics such as source code distribution, test coverage, build configurations (e.g., MKL and LLVM support), and CLI or documentation improvements are also discussed. Many questions remain about ensuring API stability, backward compatibility, and standardized interface design for features like transforms, padding, and backend-specific execution hints."
2021-02-19,pytorch/pytorch,"The comments reveal ongoing discussions surrounding PyTorch features and development practices, including the implementation of ""Same"" padding support, issues related to compatibility with C++ serialization and loading model weights, and workarounds for specific hardware and software environment issues (e.g., CUDA versions, NCCL configurations). Several technical concerns include ensuring the correct implementation of convolution algorithms and the configuration of distributed training, particularly in multi-GPU setups, as well as discrepancies caused by environment variables like `CUDA_VISIBLE_DEVICES`. There are suggestions for improving user experience, such as better default behaviors, error handling, and documentation clarity to mitigate common pitfalls. Additionally, discussions highlight the need for better interface consistency, especially around custom functions, data types, and backend-specific options, along with upstream enhancements to logging and debugging capabilities. Unresolved questions include how to systematically expose backend algorithm choices, manage environment-dependent behaviors, and improve test reliability across different architectures and setups."
2021-02-20,pytorch/pytorch,"The discussions highlight several key technical themes: 

1. There is debate over the implementation and support of ""SAME"" padding in PyTorch, with concerns that it is unstable relative to input size and that omitting certain padding functionalities could be problematic for models requiring it.
2. Several issues pertain to distributed training, including shared memory limitations, NCCL configuration, and multi-node synchronization, with suggested troubleshooting steps like adjusting `MASTER_ADDR` and `MASTER_PORT`.
3. There is interest in developing a robust abstraction for invertible modules and transforms, with comparisons drawn to Pyro's `Transform` class, and considerations around making such transforms compatible with `nn.Module`.
4. Several CI failures, build conflicts, and environment inconsistencies are discussed, emphasizing the need for better testing, shape tracking, and dependency management.
5. Numerous bug reports involve dtype mismatches, shape inconsistencies, and reproducibility issues across platforms, often with suggestions to enhance error clarity, add benchmarking, and improve developer tooling for debugging."
2021-02-21,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's architecture and usability, including clarifications on the design of invertible modules and the distinction between `inverse` and `right_inverse`, with suggestions to standardize naming conventions. There are concerns about the current API's structure, such as why `Transform` objects are not `nn.Module`s and how to better support differentiable inverse functions, especially for flow models. Several issues address technical problems like CUDA out-of-memory errors in evaluation mode, differences between Torchtext's MultiheadAttention and PyTorch's, and runtime errors related to JIT scripting and gradient computations. Additionally, there are project improvements proposed for data loading scalability, integrating distributed sharding, and resolving platform-specific installation issues. Overall, these discussions highlight ongoing efforts to improve API consistency, performance, debugging tools, and extended functionalities in PyTorch."
2021-02-22,pytorch/pytorch,"The discussions highlight a series of common themes and technical concerns within the PyTorch repository. Many comments focus on code correctness, such as fixing typos, ensuring consistent parameter handling, and improving error messages or input validations, especially in functions like `torch.norm`, `torch.nn.functional`, and custom operator implementations. There is also a recurring emphasis on performance optimization, including reducing overhead in modules like `Autograd` and `DataParallel`, and ensuring GPU kernels are correctly dispatched and optimized, such as for weight normalization or upsampling operations. Several comments address stability and compatibility issues, including handling device mismatches, deterministic behavior, and cross-machine reproducibility, often suggesting ways to enhance user-facing warnings or error reporting. Unresolved questions mostly concern whether certain enhancements (e.g., added `dim` support, new operator APIs, or support for alternative data formats) are necessary or whether existing solutions (like deprecation or workarounds) are sufficient, with some discussions about future refactoring or bug fixes pending review or further investigation."
2021-02-23,pytorch/pytorch,"The comments span a wide range of issues in the PyTorch repository, including debugging CUDA errors related to tensor operations, inconsistencies in tensor and module behaviors across device types and data types, and concerns around proper code API design—especially for features like parametrizations, custom distributions, and default behaviors for certain operations. Several discussions propose solutions such as refining the compilation process, improving documentation, adding new utility functions and APIs (e.g., `is_integer`, support for multi-output kernels), and specific code changes (like support for `start` and `end` in `torch.linalg.vector_norm`). There are ongoing concerns about test coverage and CI stability, especially with hardware variability and newer CUDA versions. Some issues are due to external dependencies, build environment complexities, or outdated driver versions, requiring careful handling. Overall, the main concerns focus on robustness, usability, API clarity, and compatibility improvements within the PyTorch codebase."
2021-02-24,pytorch/pytorch,"The comments cover a range of technical topics related to PyTorch development, including the deprecation and replacement of legacy methods like `.new`, handling batch normalization updates in eval mode, and issues with NaN values in BatchNorm layers, often involving modifications to default parameters or setting modes explicitly. Several discussions focus on enhancing functionalities via parametrizations, distribution handling, and API consistency, such as the introduction of `logits`/`probs` parametrizations or supporting `out` variants. Performance concerns are also prominent, including optimizing FFT implementations, tensor dispatch, and CPU/GPU backends, with benchmarks and profiling results showing significant improvements or regressions. Additional questions involve issues with CUDA compatibility, ABI compatibility, build processes, and the proper use of distributed or RPC frameworks like TensorPipe, sometimes highlighting the need for better documentation or alternative approaches. Overall, unresolved questions relate to API stability, backward compatibility, build environments, and performance optimizations."
2021-02-25,pytorch/pytorch,"The discussions primarily revolve around enhancements and modifications to PyTorch loss functions and APIs, such as integrating label smoothing into `CrossEntropyLoss`, supporting smoothed labels, and handling different target tensor types. There are considerations about refactoring existing components, like the TensorPipe agent and the `frexp` implementation for GPU, to improve clarity, maintainability, or performance, with some suggestions for adding new utilities or fixing bugs. Several issues concern compatibility, performance regressions, and correctness in distributed training, ONNX export, or tensor behavior, including specific concerns about int64_t multiplications, CUDA kernel crashes, and tensor sequence properties. Questions about documentation updates, test coverage, and the impact of certain API changes or deprecations are also prominent. Overall, many discussions focus on improving robustness, performance, and usability of PyTorch's core features while managing regression risks and ensuring backward compatibility."
2021-02-26,pytorch/pytorch,"The comments reflect a range of technical concerns, primarily revolving around the behavior and implementation of functions, stream synchronization, and debugging in the PyTorch framework. Notable issues include clarifications on the behavior of the `.size()` method versus `.size()`, stream synchronization nuances during autograd, and the handling of zero-dim tensors in reduction functions like `mode` and `max`. There are also discussions about build and environment configurations affecting CUDA NCCL errors, device communication, and the impact of system topology and hyperthreading on performance. Additionally, several questions seek guidance on code modifications, testing strategies, and the implications of proposed changes, with some unresolved issues related to CI failures, rebase conflicts, and specific bug fixes awaiting review or further investigation."
2021-02-27,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting efforts within the PyTorch repository, focusing on enhancements like batch affine transformations support, shape annotation via type hints, and thread management for OpenMP to prevent resource thrashing and memory leaks. Several issues address compatibility and environment configuration challenges, including CUDA version mismatches, library dependencies, and build reproducibility. There are concerns about the robustness of pattern-matching optimizations in the JIT compiler and the need for better testing and validation of generated C++ code. Additionally, issues involve CI failures, performance regressions, and system-specific behaviors, reflecting the complexity of maintaining stable, high-performance deep learning infrastructure. Many questions remain about how to improve runtime concurrency, environment setup, and testing strategies for various hardware and software configurations."
2021-02-28,pytorch/pytorch,"The discussions primarily revolve around managing system resources and stability, notably the ""too many open files"" error on Mac OSX and the impact of DataLoader sharing strategies. Strategies such as switching the multiprocessing sharing strategy to 'file_system' are proposed, with questions about proper placement in multi-process contexts. There are also ongoing efforts to improve testing frameworks by integrating pytest, addressing issues with test coverage, and ensuring generated C++ code is properly tested. Additionally, there are concerns regarding CUDA kernel compatibility with newer GPUs like the RTX 3090, especially post-OS updates or driver changes. Lastly, discussions include issues related to package distribution on PyPI, Python serialization formats (notably the security implications of pickle files), and plans for distributed data loading and sharding enhancements."
2021-03-01,pytorch/pytorch,"The comments reflect a broad set of technical concerns, including the need for consistent input validation checks and the implementation of input parameter checks across CPU and CUDA. Recurrent issues involve debugging and fixing memory-related errors, such as invalid free errors, thread leaks, and nondeterministic behavior in operations involving MKL, OpenMP, and NCCL, often on specific hardware like AMD or GPU architectures. There are discussions about enhancing the API, such as improving `torch.Tensor` support and `__torch_function__` dispatch, as well as updating the library's internal design to better handle complex data types, especially support for complex tensors and their associated operations. Unresolved questions include how to best incorporate simplification passes in the compilation pipeline, how to handle serialization and model safety concerns, and how to improve test coverage and debugging tools, especially for multi-threaded and hardware-specific issues."
2021-03-02,pytorch/pytorch,"The comments reflect a range of technical topics related to PyTorch, including model weight loading practices (e.g., `load_state_dict` with `DataParallel`), distribution and source code dissemination issues (e.g., source tarballs and submodules), compatibility and version-related bugs (e.g., `requires_grad_` in TorchScript, OpenMP thread behavior on different architectures), and implementation details for specific features like CUDA support, ONNX export, and new operators. Several discussions concern the stability, correctness, and performance improvements of core functionalities, such as NCCL communications, autograd support, tensor shape behaviors, and convolution implementations. There are also suggestions for API enhancements, user workflows, and testing strategies to better support edge cases and advanced functionalities (e.g., complex number support, graph verification, device handling). Unresolved questions include how to safely introduce new features without breaking backward compatibility, how to clarify or simplify API behaviors (e.g., `Tensor` constructors, `ModuleList`), and how to manage build or runtime issues across diverse hardware and software environments."
2021-03-03,pytorch/pytorch,"The discussions primarily revolve around handling device and data management in PyTorch, including device transfers, especially with CUDA, and the implications of moving models and optimizers between devices. There are concerns about proper device synchronization, device-specific behaviors (like `cuda.runtime_error`), and ensuring compatibility when loading models across different environments or hardware (e.g., ROCm, CUDA versions). Several issues highlight challenges in managing device contexts, like automatic support for ""meta"" tensors, lazy device assignment, and the impact on training speed and correctness. Additionally, questions about testing practices, CI stability, and the best ways to enhance PyTorch's API support and documentation are also prominent. Overall, key issues focus on robustness, consistency, and usability of device and data handling in deployment and development workflows."
2021-03-04,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's testing and debugging infrastructure, such as transitioning `assertEqual` methods to functional forms to adopt pytest features, and improving test parameterization and fixture use. There are concerns about maintaining backward compatibility when making API changes, particularly for site-specific or deprecated behaviors, and the need to introduce new arguments (e.g., for gradient clipping) with toggles to prevent breaking existing code. Several issues address system-level or hardware compatibility challenges, for example, NCCL interface and multi-GPU communication problems, CUDA version mismatches, and device-specific performance regressions. There's also interest in expanding the use of ""meta"" tensors for lazy initialization, delaying parameter setup, and standardizing argument naming conventions (like `dim` vs `axis`). Unresolved questions include synchronization of test behaviors across backends, handling of certain tensor conversions, and handling more complex or experimental features like PyTorch Mobile or new operator behaviors, often considering their impact on backward compatibility and test stability."
2021-03-05,pytorch/pytorch,"The comments cover a range of technical issues and discussions related to the PyTorch repository, including network communication problems in distributed setups (e.g., reachability and port conflicts), support for complex number operations, and the behavior of specific functions like `InstanceNorm2d` with small inputs. Several discussions address build reproducibility, build system nuances (such as third-party dependencies and environment configurations), and potential breaking changes or BC considerations for new features. There are also questions about feature support (e.g., `torch.linalg.norm`, ONNX export issues), code correctness (e.g., numerical precision and determinism), and infrastructure updates like CI failures, test behaviors, and environment setup. Many unresolved questions relate to performance implications, compatibility, and how to better document or systematize these complex behaviors for users."
2021-03-06,pytorch/pytorch,"The discussions primarily revolve around build and compatibility issues with specific release tarballs in the PyTorch repository, such as problems building source releases (e.g., version 1.8.0) due to mismatched third-party dependencies, especially fbgemm. There are concerns about ensuring the correctness and performance of CUDA kernels, particularly regarding environment variables controlling CUDA memory behavior, and how certain commits impact training speed or stability on different GPU architectures. Several issues highlight the need for better documentation, tooling, and test robustness, such as handling of tensor interfaces (`__array__`, `__cuda_array_interface__`), and ensuring reproducible, deterministic behavior in distributed and multi-GPU setups. Additionally, there are suggestions for improving overall testing infrastructure, including more informative pytest output and better handling of specific warnings or errors during build and runtime. Unresolved questions include compatibility across hardware and driver versions, and how to maintain correctness amid evolving features and optimizations."
2021-03-07,pytorch/pytorch,"The discussions primarily revolve around improvements and design considerations in PyTorch's reparameterization and distribution APIs, including handling cache flushing, multiple parametrizations for the same parameter (e.g., logits and probabilities in Bernoulli distributions), and the implementation of isomorphisms between constrained and unconstrained spaces. There is also concern about the support for inverse functions and their interaction with autograd, notably in invertible modules and flows, with suggestions for explicit `right_inverse` methods and caching strategies. Some discussions address performance issues, such as optimizing reduction operations like `topK` via `TensorIteratorBase`, and also address system and dependency problems, including CUDA version compatibility, onnx support, and library dependencies on `libncurses`. Overall, the comments highlight ongoing efforts to improve PyTorch’s API flexibility, numerical stability, performance, and compatibility, alongside addressing technical challenges and user feedback."
2021-03-08,pytorch/pytorch,"The discussions encompass a variety of technical topics related to PyTorch development, such as implementing explicit parametrizations and caching mechanisms, supporting multiple parametrizations per parameter (e.g., logits and probabilities), and the impact of different API designs on usability and stability. Key concerns include API consistency (especially around the `parameters`, `register_parametrization`, and `right_inverse`), handling edge cases like zero-length sequences in RNNs, and ensuring support for various hardware configurations, including CUDA, ROCm, and newer architectures like Apple Silicon. Several discussions highlight the challenges of maintaining backwards compatibility, drop-in functionality, and correctness across diverse backends and usage patterns, with specific issues such as the support of `bfloat16` on CPUs, the performance of libtorch binaries, and the proper integration of new features like hooks or linear algebra routines. Unresolved questions involve API choices (e.g., new protocols over `__torch_function__`), the support of dynamic input shapes in ONNX exports, and how to best handle device synchronization, deterministic behavior, and support for complex number gradients."
2021-03-09,pytorch/pytorch,"The discussions highlight concerns about replicating PyTorch's internal convolution logic for compatibility with frameworks like XLA, emphasizing the potential need to replicate specific view transformations and internal functions such as `_convolution()`. There are questions about the dispatch mechanism for `aten::convolution` and whether it goes through the c10 dispatcher, affecting how backend implementations are accessed. Several issues involve implementing or improving features like positional encoding in `nn.Transformer`, handling `padding_idx` semantics correctly, and supporting complex math operations in TorchScript, with debates on the intended semantics versus current implementations. Additionally, there are frequent CI/test failures, often related to environment configurations, device mismatches, or unsupported features, raising questions about build reproducibility and proper testing strategies. Overall, the discussions center on ensuring compatibility, correct semantics, and robust testing across diverse hardware and framework extensions."
2021-03-10,pytorch/pytorch,"The discussions primarily revolve around runtime errors, particularly CUDA device-side asserts triggered during model training, often due to label range issues or incorrect ignore_index settings. Several comments highlight the need for proper validation of input data, such as ensuring label values are within valid ranges, and suggest debugging steps like restarting kernels or adjusting ignore labels. There are concerns about CI failures, test coverage, and how to efficiently incorporate pytest or optimize test execution, especially for meta and slow tests. Additionally, questions arise regarding compatibility adjustments, code reordering, and performance benchmarking across different configurations. Overall, the key focuses are on debugging runtime errors, improving test infrastructure, and maintaining compatibility and robustness of the codebase."
2021-03-11,pytorch/pytorch,"The comments primarily discuss various technical issues and proposed improvements within the PyTorch codebase, such as addressing runtime CUDA errors (device assert triggers, large tensor reduction bugs), enhancing API support (support for custom `grid_sample`, `one_hot` with dtype, `forward` mode AD), and improving build and testing workflows (fixing build errors on old/unsupported CUDA versions, modifying test decorators for meta-architecture, reducing CI flakiness by replacing fork with spawn, addressing slow meta test runs). Several questions revolve around specific bug reproductions (e.g., large tensor reductions, inconsistent `sum` behavior), desired API changes (e.g., making `TensorImpl` more transparent, introducing `dtype` arguments), and project plans (e.g., forward-mode AD development, linear algebra namespace organization). Some comments suggest practical fixes or express curiosity about ongoing work (e.g., fixing the unsupported `adaptive_avg_pool3d_backward`, build solutions for ARM, handling `ZeroDivisionError`, or clarifying CUDA's context memory management). Overall, unresolved issues include build failures (notably on old CUDA versions), runtime errors on large tensors, and API evolution questions; many of these are being addressed through bug reports, PR reviews, and planned discussions."
2021-03-12,pytorch/pytorch,"The comments mainly involve troubleshooting and bug fixing for PyTorch's development and testing infrastructure, including addressing CI failures, merge conflicts, and test coverage issues. Several discussions focus on improving backward compatibility, especially around the `out=` parameter handling, model serialization, and operator behavior, with suggestions for better error messaging and API design. Specific technical concerns include CUDA and ROCm compatibility, the need for more comprehensive testing (especially with large tensors, large models, and on different hardware architectures), and potential improvements in modularity and code organization (e.g., test decorators, internal API fixes). There are also questions about supporting specialized features such as orthogonalization in linear algebra, support for arm64/M1, and the integration of new functions into existing namespaces like `torch.linalg`. Unresolved questions include the proper migration path for internal/external support, handling of concurrency issues in testing, and how to extend testing and documentation strategies efficiently."
2021-03-13,pytorch/pytorch,"The discussions primarily revolve around dimension mismatch errors in datasets and model inputs, specifically when datasets contain varying or mismatched sizes, leading to out-of-range or shape errors during training, especially in classification and loss computations like cross-entropy. Several issues highlight challenges with multi-GPU and distributed training, such as CUDA runtime errors, CUDA tensor sharing across processes, and the need for proper handling of unused parameters in DistributedDataParallel, indicating complex debugging in multi-GPU environments. There are also recurring concerns about dataset preparation consistency, dataset size matching, and how to properly align inputs (e.g., text and image data) for training, as well as handling of edge cases like zero-sized tensors or mismatched data structures. Additionally, some discussions focus on tool support for advanced features like custom bucketing, versioning, and maintaining compatibility with evolving library and hardware stacks, with unresolved questions about best practices for robust, reproducible training pipelines across different platforms and software versions."
2021-03-14,pytorch/pytorch,"The discussions highlight ongoing challenges and inquiries related to optimizing depthwise convolutions in PyTorch, where current implementations with the `groups` parameter are slow despite being less computationally intensive, with recent cuDNN updates promising improved performance. Several issues involve errors in dataset downloading on platforms like Colab and Kaggle due to HTTP 403 and 503 errors, and workarounds involving manual dataset downloads. Other topics address API and framework limitations, such as missing batching rules in autograd, the deprecation of `Tensor.new_tensor` for JIT support, and the need for improved documentation and build configuration clarity, especially regarding BLAS libraries. Some discussions focus on CI reliability, code coverage, and the significance of supporting various backends like ROCm and NCCL, alongside questions about initial contribution permissions, platform support, and integration with new features like `vmap`. Overall, unresolved issues relate to performance optimizations, better error handling, clearer documentation, and expanding framework compatibility."
2021-03-15,pytorch/pytorch,"The diverse comments largely revolve around persistent dimension mismatch and index out-of-range errors in various PyTorch training and custom op scenarios, indicating potential bugs or misalignments in tensor shape handling, especially with views, slices, and backward passes. Several discussions address the need for better debugging, testing, and documentation, particularly around complex autograd, sparse tensor support, and ONNX export issues. Performance and compatibility concerns, such as support for CUDA 11.2, ROCm, and multi-GPU setups, are also prominent, with suggestions for infrastructure improvements like better CI testing, more explicit error messages, and refined operator registration strategies. A recurring theme is the need for clearer, more robust APIs—such as in overload resolution, initialization, and custom tensor types—while maintaining BC and robust error handling. Overall, unresolved questions focus on fixing shape and dimension errors, enhancing multi-GPU and backend support, and establishing better testing and debugging practices."
2021-03-16,pytorch/pytorch,"The discussions cover multiple technical issues related to PyTorch development, including bug reports, feature requests, and API design considerations. There are concerns about correctness and stability in various modules like eigen-decomposition, distributed training, and model serialization, with suggestions for improved error handling and compatibility. Several discussions address performance optimizations, such as memory management, kernel support, and efficient implementation of mathematical operations (e.g., least squares, LU decomposition). Questions also arise about API usability, including the placement of functions in the namespace, clarity in documentation formatting, and interface design for distributed subgroup management. Unresolved questions include handling degenerate cases in eigenvalue derivatives, improving test coverage for large inputs, and ensuring cross-platform/build compatibility, especially on Windows and 32-bit architectures."
2021-03-17,pytorch/pytorch,"The comments primarily address several technical issues involving PyTorch's implementation and edge cases, such as the lack of `torch.int32` support impacting ONNX export and TensorRT compatibility, and the need for fixed or consistent behaviors in operations like `view()`, `scatter()`, and dynamic shape handling. Additionally, there are discussions on improving documentation formatting (e.g., use of TeX and code highlighting), the handling of non-deterministic behaviors in algorithms like `pad()` with reflection or replicate modes, and enhancements to the C++/CUDA kernel implementations (e.g., quantized operators, convolution padding, collective operations). Several comments focus on debugging runtime errors such as GPU hangs, autograd bugs tied to specific APIs, and build issues related to OpenMP linking or version compatibility. Overall, these discussions reflect ongoing efforts to improve robustness, compatibility, performance, and clarity of PyTorch across different platforms and use cases."
2021-03-18,pytorch/pytorch,"The discussions primarily revolve around several technical issues and feature considerations in PyTorch, including runtime errors like CUDA assertions and device-side asserts triggered by index out-of-bounds, often caused by mismatched tensor sizes or out-of-range indices, especially when handling ground-truth labels with ignore indices. There are concerns about the reproducibility of certain errors in different environments and the impact of CUDA version mismatches, particularly on cloud platforms like Google Colab and AWS, with suggested solutions such as environment reconfiguration or specific library installations. Additionally, issues around distributed training—such as synchronization, process groups, and potential deadlocks—are addressed, with suggestions to improve code robustness and debugging tools (e.g., use of `--tb=native`). Other topics include API design questions, like handling tensor batching in pipeline parallelism (`Pipe`), and considerations for backward compatibility, error messaging, and test coverage. Overall, the discussions highlight ongoing troubleshooting, API refinement, environment management, and development workflows in PyTorch."
2021-03-19,pytorch/pytorch,"The comments cover a broad range of technical discussions related to PyTorch, including feature requests (e.g., support for scale factors less than 1 in pixel shuffle, HDFS support for tensorboardX), bug reports and fixes (e.g., CUDA errors, operator export problems, numerical inconsistencies), and infrastructure-related issues (e.g., build failures, CI flakiness, environment setup with CUDA and MKL). Several discussions revolve around improving PyTorch's extensibility and compatibility, such as better support for different backends, more generic API designs, and proper handling of device-specific operations. There are inquiries about existing features, testing frameworks, and usage patterns (e.g., OpInfo tests, timing utilities, reproducing bugs), as well as workflow and build process questions involving CI/CD and dependencies. Many comments seek clarifications, propose code fixes, or request feature additions, with some unresolved issues related to flaky tests, environment configurations, or missing operator support."
2021-03-20,pytorch/pytorch,"The discussions primarily revolve around CUDA and PyTorch internals, with key concerns about stream dependencies, backward pass errors, and ensuring compatibility between different hardware and driver versions. Several questions are raised about debugging and optimizing CUDA kernels, especially related to kernel performance discrepancies across GPU architectures like RTX 2080, 3080, and A100. There are suggestions to improve user experience and framework robustness, such as handling non-tensor inputs, deprecating certain functions like `torch.Tensor()`, and creating more flexible APIs for model parallelism and stream management. Issues linked to build configurations, external dependencies like OpenMP, and framework features such as `DistributedDataParallel` and `TorchScript` are also discussed, with some open questions about best practices and future directions. Overall, the conversations indicate ongoing efforts to resolve performance, compatibility, and API consistency challenges within the PyTorch ecosystem."
2021-03-21,pytorch/pytorch,"The discussions highlight several key technical concerns: issues with memory management and segmentation faults during CPU training, possibly linked to library conflicts or data preprocessing errors; DLL load failures and missing dependencies on Windows, notably related to VC redistributables and OpenMP support across various build environments; device and environment compatibility problems, especially regarding GPU support for non-NVIDIA hardware like Intel GPUs and the impact of system configurations involving shared memory, containerization, or specific compiler and library versions; the need for better debugging tools and practices, such as use of `decord` crash detection or `collect_env.py`, and understanding how specific implementation decisions (e.g., non-contiguous outputs, differences between scalar Tensors and sequences) affect performance and correctness; and ongoing efforts to improve build robustness, compatibility, and feature support in diverse environments, including considerations for future hardware architectures and API consistency."
2021-03-22,pytorch/pytorch,"The discussions cover numerous technical issues related to PyTorch, including shared memory errors in multiprocessing, memory allocation failures when loading large models, and performance regressions with specific GPU configurations (e.g., NVIDIA RTX 30 series). Several comments suggest that misconfigurations or environmental factors (like thread counts or driver versions) may cause hangs or slowdowns, with particular attention to CUDA versions and library linkages. There are questions about updating or replacing internal functions and headers, e.g., compatibility headers for CUDA math functions, to improve portability on standalone systems and newer compilers. Additionally, some conversations address testing strategies, code consistency, API design considerations, and documentation formatting, indicating ongoing efforts to improve robustness, usability, and clarity across the PyTorch project."
2021-03-23,pytorch/pytorch,"The comments highlight recurring issues related to numerical precision and hardware compatibility in PyTorch, such as deviations in floating-point operations (e.g., `torch.linalg.matrix_power` and `torch.norm`), and inconsistencies in GPU kernel support for certain dtype like bfloat16 across different hardware and CUDA versions. Several discussions suggest improving error messaging and robustness, including handling unsupported operators, creating more generic device-agnostic APIs, and clarifying default argument behaviors (e.g., in loss functions and `broadcast_tensors`). There are also concerns about build environment challenges, especially with CUDA support, and the necessity for better testing, documentation, and CI coverage for various configurations. Overall, the community emphasizes refining precision handling, machine support, and user experience in both software and build systems."
2021-03-24,pytorch/pytorch,"The comments span a wide range of topics in the PyTorch repository, including discussions on numerical stability issues (e.g., NaN handling in functions), deep learning optimization techniques (like in-place batchnorm and invertible modules), and hardware-related bugs (GPU and CUDA support, NCCL errors). Several issues focus on improving usability and compatibility, such as standardizing API conventions (e.g., tensor naming, support for new types like bfloat16, integration with the array API standard), and enhancing debugging tools (profilers, monitored barriers). There are also ongoing infrastructural and testing concerns, including build system adjustments, test coverage, and proper handling of environment inconsistencies (driver versions, device support). Unresolved questions include how to best introduce new features without breaking backward compatibility, how to unify support for diverse hardware, and the best strategies for documentation and user guidance in a rapidly evolving ecosystem."
2021-03-25,pytorch/pytorch,"The comments cover a wide range of topics related to PyTorch development, including requests for new features like sparse matrix operations, invertible modules, and better documentation, as well as discussions on existing issues such as NCCL errors, NaN loss propagation, and CUDA compatibility problems. Several comments suggest improvements or bug fixes, such as adding support for torch.LongTensor, handling edge cases in gradient computations, and avoiding silent changes in function behavior across versions. There are concerns about performance regressions, build failures on different systems, and the accuracy of operator behaviors, especially regarding type support and binary incompatibilities. Overall, the discussions reflect ongoing efforts to improve PyTorch's reliability, usability, and documentation, with some unresolved technical questions and workarounds still under investigation."
2021-03-26,pytorch/pytorch,"The comments span various technical topics related to PyTorch, including the ongoing development and support of mathematical special functions (such as spherical Bessel functions, hypergeometric functions, and Cephes library wrappers), and the support for matrix multiplication and data types on CUDA devices. Several discussions address issues with memory management, CUDA compatibility, and performance regressions, especially on different GPU architectures and environments. There is an emphasis on improving API consistency, backward compatibility, and developer experience, including the design of new APIs, testing strategies, and documentation standards. Additionally, warnings, CI failures, and build system configurations reveal ongoing efforts to ensure stability and correctness across platforms, with unresolved questions about support for complex tensors, multi-dimensional reductions, and cross-library compatibility with standards like the array API."
2021-03-27,pytorch/pytorch,"The discussions primarily revolve around handling and improving low-level backend operations in PyTorch, such as CUDA and NCCL integrations, device abstraction, and distributed training. Several issues address specific bugs—like CUDA errors (CUBLAS_STATUS_EXECUTION_FAILED), inconsistencies across hardware architectures (AVX/AVX2 differences), and build stability on different platforms—and suggest solutions such as explicit device setting, custom CUDA installations, or code refactoring. There are also concerns related to PyTorch’s internal API design, including the need for more idiomatic, flexible runtime API functions and better debugging support (e.g., core dumps, stack traces). Additionally, some threads discuss testing practices, error reporting, and handling legacy components like LongTensor, indicating ongoing efforts to enhance code robustness and compatibility. Unresolved questions include verifying the impact of hardware differences on numerical results, improving CI testing and diagnostics, and refining API design for heterogenous and distributed systems."
2021-03-28,pytorch/pytorch,"The comments reveal ongoing issues with PyTorch's build configurations, particularly regarding support for the C++11 ABI and the availability of wheels with the new ABI by default. There are concerns about the size and performance implications of static linking, especially with cuDNN, which may impact container performance differences between official distributions and source builds. Several discussions focus on hardware compatibility and kernel linking issues, such as inconsistencies in CUDA versions, driver compatibility, and architecture-specific bugs affecting functions like `randperm` and tensor reduction operations. Additionally, there are technical questions about signature ambiguity in operator registration, handling of meta tensors in testing, and plans for features like `ldl` decomposition or cumulative trapezoid functions. Overall, these discussions highlight the complexities of managing build systems, hardware/software compatibility, and API consistency in PyTorch development."
2021-03-29,pytorch/pytorch,"The comments reflect a range of technical discussions and issues related to the PyTorch repository, including code readability and API design (e.g., handling of tensor shape manipulations and constants), build and platform-specific issues (SSL certificates, macOS and Linux build discrepancies, and hardware compatibility on AMD and ARM architectures), and specific feature requests or bug reports such as support for complex numbers, benchmark clarifications, and new operator support. Several discussions focus on correctness, error handling, and performance (e.g., inference mode, FFT support, and batch operations), as well as infrastructure and tooling concerns like CI failures, documentation improvements, and test extensions. Moving forward, unresolved questions include how to better manage ambiguous function signatures, incorporate new features like LDL decomposition, and improve user guidance and robustness in cross-platform environments. The overall suggestions center on improving code consistency, testing coverage, release support for multiple hardware and software configurations, and user-facing documentation."
2021-03-30,pytorch/pytorch,"The comments span multiple technical discussion points, primarily focusing on PyTorch's data loading shuffling behavior, the design and modularity of weight pruning mechanisms, and serialization challenges with hook-based weight modifications. Key questions include why `next(iter(data_loader))` isn't inherently an iterator, the utility of hookable weights for modular pruning, and serialization concerns related to hooks not being serializable. There are discussions about addressing specific bugs—such as CUDA out-of-memory issues, non-deterministic behavior, and `det` backward stability—by changing API designs, adding support for `out` parameters, or adjusting algorithms. Unresolved questions involve how best to handle class invariants in custom implementations, the impact of threading and process management in DataLoader, and how to improve test coverage and build reproducibility across hardware and software configurations."
2021-03-31,pytorch/pytorch,"The discussions highlight ongoing challenges with multi-processing and data loading on Windows, particularly with multiprocessing strategies, seeding, and dataset file naming issues affecting reproducibility and quality. Several reports concern the correct configuration and stability of CUDA contexts, streams, and memory, especially in multi-GPU and distributed environments, with suggestions to manage CUDA streams and avoid automatic stream changes that disrupt performance. There are multiple mentions of the need to improve autograd's handling of subclassed tensors and the importance of proper documentation, particularly regarding SPMD mode incompatibilities, quantization API changes, and compatibility with different hardware architectures like AMD or Apple M1. Additionally, current CI failures, build issues, and the necessity for better test coverage and bug reproducibility procedures are recurring themes, emphasizing the need for clearer workflows, versioning, and robustness in the codebase and testing infrastructure."
2021-04-01,pytorch/pytorch,"The comments highlight persistent issues related to multiprocessing with PyTorch tensors, including serialization errors and connection resets, especially with CUDA and multiprocessing start methods. Several users note that reducing batch sizes, restarting systems, or switching to CPU can mitigate memory errors or hangs, indicating possible system resource or driver-related causes. There is discussion about improving tensor construction from external data, with suggestions to better handle device and storage management. Additionally, some comments point to CI flakiness, build difficulties, and the need for more robust testing, documentation, and code structure improvements—particularly around tensor contiguity, optional types in TorchScript, and structured kernel support. Overall, unresolved questions include optimizing multiprocessing communication, ensuring compatibility across different hardware and software configurations, and simplifying tensor handling in external and distributed contexts."
2021-04-02,pytorch/pytorch,"The discussions highlight several core issues within PyTorch development, including the implementation and behavior of data loaders and iterators, serialization and pickling of tensors and data structures, and the handling of low-rank matrix factorizations across autograd and numerical differentiation. Key concerns include ensuring that `iter()` returns shuffled data in a consistent manner, improving serialization support for torch Tensors in a fast and user-friendly way, and correctly managing the non-differentiability at low-rank matrices, especially in the context of QR and eigendecomposition. There are also discussions about API designs for model versioning, such as exporting models with different schema versions, and the importance of supporting various hardware backends and environments (e.g., ROCm, CUDA, AMD). Several unresolved questions involve the precise behavior at low-rank or boundary points, as well as the architectural implications of sequencing and sharding in distributed modules."
2021-04-03,pytorch/pytorch,"The discussions highlight performance regressions and potential bugs related to specific operations across different hardware and software configurations, such as the use of flip in cumprod, and inconsistencies in behavior of functions like `torch.save` vs. `pickle`. There are observations about non-determinism and speed issues when using `channels_last` format with half-precision on GPUs, suggesting that CUDA version, autocast, and hardware influence performance. Some issues concern build failures and compatibility problems with MSVC, Visual Studio, and cross-compilation, often linked to specific dependencies like QNNPACK or thrust/cub. Several CI failures are noted, some tied to changes in PRs affecting the build system, or related to specific functions like `pow`, `log`, or `randperm`, indicating underlying bugs in those implementations. Overall, unresolved questions include ensuring compatibility, fixing performance regressions, and improving CI robustness in the face of diverse environments."
2021-04-04,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's capabilities, such as implementing invertible modules, complex neural networks, and support for ROCm GPUs, with concerns around compatibility, performance, and correct functionality. Several comments focus on improving distributed training using techniques like DDP with checkpointing, manual gradient synchronization, and simplifying the implementation of these features. Issues related to hardware support, especially for AMD GPUs and ROCm compatibility, reveal challenges in building and deploying PyTorch across different environments, often requiring custom patches or workarounds. There are technical inquiries about specific features, such as softmax safety improvements, and the need for contributions and testing scripts to validate new functionalities. Overall, the discussions emphasize balancing development progress with stability, performance, and broader hardware support."
2021-04-05,pytorch/pytorch,"The discussions highlight a recurring emphasis on improving modularity, serialization, and extensibility in PyTorch. Key concerns include the desire for hookable, serializable, and more modular weight manipulation mechanisms (e.g., weight hooks, hookable weights), the challenge of maintaining backward compatibility and versioning in model serialization (e.g., for TorchScript models), and the need for clearer APIs in distributed training (e.g., deferred placement, static graph support). There is also ongoing work to refactor large codebases for better organization, as well as suggestions to enhance the framework’s compatibility with standards like NumPy and onnx. Additionally, addressing performance regressions and internal build dependencies (e.g., CUDA, MAGMA) remains a concern, alongside ensuring that new features like automatic differentiation for complex or custom data types are incorporated in a way that supports future extensibility."
2021-04-06,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's robustness and usability in various areas, including error handling in embedding and padding scenarios, batch device management, and distribution strategies like sharding and pipelining. Several proposals suggest more explicit error handling (e.g., when tensors are fully padded or empty) and more flexible API designs for model sharding, such as leveraging torch.fx or introducing virtual devices, to support complex architectures and device placement. There is also interest in refining the internal testing infrastructure, including better support for stacked PRs, more granular test decomposition, and improved test configs, especially around CUDA performance, ONNX compatibility, and complex dtype support. Some concerns address deep internal implementation details, such as handling of `__torch_function__` overrides, CUDA memory management, and compatibility issues across different platforms and compiler environments. Overall, the conversations highlight ongoing efforts to enhance PyTorch's reliability, extensibility, and performance, with a focus on better API design, testing, and distributed training support."
2021-04-07,pytorch/pytorch,"The comments encompass a diverse set of discussions related to PyTorch's development: advanced data loading strategies for multi-configuration datasets, implementation details for covariance and correlation computations, and enhancements to the testing and op registration infrastructure, including OpInfo pattern improvements. Several issues involve ensuring compatibility and correctness across hardware (especially CUDA, ROCm, and quantized tensors), with questions about kernel selection, deterministic behavior, and precision stability. There are operational concerns around build systems, external dependencies like MAGMA, and runtime behavior, such as handling inconsistent operator outputs, stream management, and distributed training API improvements. Many suggestions aim to improve code maintainability, performance diagnostics, and extensibility, often seeking better API clarity, test robustness, and integration with external libraries or standards. Overall, unresolved questions primarily concern operational stability, cross-platform consistency, and feature completeness in complex, high-performance, and distributed settings."
2021-04-08,pytorch/pytorch,"The discussions cover a range of technical issues in the PyTorch repository, including data loading shuffling behavior, compatibility and installation problems for earlier versions, and serialization/deserialization inconsistencies between C++ and Python. Key concerns involve understanding the behavior of dataloader iterators, resolving compatibility conflicts with different Python and package versions, and handling model weights and state dictionaries across C++ and Python boundaries. There are questions about improving the robustness of tensor and module serialization, especially related to version management, and about performance optimizations for specific GPU architectures and operations. Additionally, some discussions address test efficiency, codebase organization, and the handling of complex data types like complex or half-precision floating points."
2021-04-09,pytorch/pytorch,"The comments collectively highlight several core issues: the handling of empty or padding sentences in embedding operations, with a debate on whether to default to including or excluding `padding_idx` in reduction calculations; complexities in ensuring consistent behavior across related loss functions and embedding layers, especially with regards to `padding_idx` semantics. There are considerations about default behaviors versus configurable options, aiming for predictability and minimizing surprise for users. Discussions also touch on implementation details, such as supporting multi-dimensional reductions, the need for backward pass support in JIT, and handling device specificity during tensor creation. Unresolved questions include default settings for `padding_idx` handling, default behavior consistency with `Embedding`, and how internal representations (like JIT's backward graphs) should be accessed or exposed."
2021-04-10,pytorch/pytorch,"The comments highlight several recurring issues and discussions within the 'pytorch/pytorch' repository. Key concerns include the challenges of random number generator (RNG) state sharing in multi-process data loading, especially with the spawn method, which affects reproducibility; and the support for complex number operations, including developing complex modules and ensuring their compatibility and gradient support. There are also technical debates on build configurations, such as supporting BF16, handling compiler and platform-specific issues, and ensuring correct versioning and dependency management. Additional topics involve improving testing robustness, addressing intermittent CI failures, clarifying API documentation, and ensuring backward compatibility and proper error handling in core components like TensorIterator. Overall, these discussions aim to enhance PyTorch's robustness, flexibility, and usability across diverse hardware and software environments."
2021-04-11,pytorch/pytorch,"The discussions highlight ongoing challenges with reproducibility and randomness in multi-process data loading with PyTorch and NumPy, emphasizing the need for proper seed management and recommended patterns for parallel PRNG use. Several comments concern the export and serialization of models, specifically how to efficiently go from traced or scripted models to C++ modules, and vice versa, including potential API enhancements for creating models directly in C++ and exporting to TorchScript. There are also technical notes on improving test robustness, handling NaN values in gradient checks, and aligning PyTorch's `searchsorted` with NumPy for better compatibility. Additionally, issues with CUDA runtime errors, especially related to matrix multiplication performance and internal errors, have been observed, prompting questions about setup and performance optimizations. Finally, some discussions focus on system design considerations, such as managing tensor deallocations, referencing refcount behaviors, and ensuring API consistency between Python and C++ frontends."
2021-04-12,pytorch/pytorch,"The comments predominantly discuss concerns around reproducibility, randomness patterns, and API design choices in PyTorch. Several contributors raise questions about ensuring deterministic behavior with multi-processing data loading, especially regarding the use of numpy's random number generators and seeding strategies, with suggestions to adopt `SeedSequence` for better parallel PRNG management. Others address the challenges of serialization and deployment in C++ vs Python, with debates on modeling quantized operations and the complexity of backend dependencies. Additionally, there are discussions on testing practices, such as splitting large test files, handling optional outputs in modules, and incorporating new operator features, with an emphasis on balancing maintainability, correctness, and compatibility. Overall, unresolved issues include improving reproducibility guarantees, API clarity, and ensuring consistent behavior across different hardware and software environments."
2021-04-13,pytorch/pytorch,"The comments from these GitHub issues reflect ongoing discussions about PyTorch's handling of randomness seeding in multi-process data loading, with suggestions to improve seed derivation using `SeedSequence` and better seed management per worker, as well as concerns regarding reproducibility across versions. Several issues address performance bottlenecks or bugs—such as CUDA memory out-of-memory errors during training, performance regressions in double-precision reduction, or differences between Python and C++ inference speeds—highlighting the need for profiling, bug fixes, or API modifications. Others focus on build system challenges, platform compatibility (e.g., support for specific GPU compute capabilities), and code refactoring, with solutions including environment-specific CI testing, build fixes, and API redesigns like the potential removal of the `Tensor` cache cache or better type safety. Additionally, there's discussion about code correctness in distributed and serialization scenarios, including ensuring proper deallocation and autograph support, and regarding internal development workflows like linting, testing, and documentation improvements. Many unresolved questions pertain to compatibility, performance optimization, correctness guarantees, and infrastructure changes necessary for future features or bug fixes."
2021-04-14,pytorch/pytorch,"The comments primarily revolve around enhancements and issues related to PyTorch's functionalities, including the implementation of advanced mathematical functions like the multivariate gamma and beta functions, as well as support for new operations and data types (e.g., int8, low-bit types, and support for boolean tensors). Several discussions address the optimization and correctness of existing features, such as depthwise convolutions, RAM usage, and kernel performance regressions, with some proposals to patch or revert certain changes for stability. There are concerns about inter-platform compatibility, build system configurations, and testing practices (e.g., CI setup, cross-architecture support, and incremental testing). Additional topics include bug fixes (like CUDA memory handling, operator handling in TorchScript, and specific kernel behaviors), as well as suggestions for tooling improvements and usability (e.g., support for lazy tensors, better support for Vulkan, and default behaviors for tensor creation)."
2021-04-15,pytorch/pytorch,"The comments cover several technical topics: improvements in seeding and random number generation methods involving `np.random.SeedSequence` and `torch.initial_seed()`, with considerations of reproducibility and API changes; handling of global seed reseeding within DataLoader workers to maintain reproducibility in multi-processing; issues and potential improvements to `torch.jit.ScriptModule` inheritance, especially regarding deprecated API practices and supporting non-forward methods; challenges with CUDA and cuDNN performance, especially related to depthwise convolutions and mixed-precision, including hardware-specific variations and validation of speedups in newer CUDA versions; and the complexity of implementing reusable or dynamic embeddings, with discussions on API design, maintainability, and integration with existing PyTorch components, alongside various CI and build stability issues across different architectures and software versions."
2021-04-16,pytorch/pytorch,"The comments cover a range of technical issues related to PyTorch development, including challenges with variable-length sequence handling in RNNs, the speed and optimization of depthwise convolutions with cuDNN, and the integration of distributed training features like NCCL and CUDA streams, often highlighting performance regressions or potential deadlocks. There are concerns about proper support and testing for newer features such as scatter of tensor subclass objects, mixed precision usage with cuDNN algorithms, and the correct handling of custom modules or backporting APIs to ensure compatibility and correctness. Several discussions focus on improving profiling support, reducing package size, and debugging complex internal issues like GPU deadlocks or bug manifests in autograd functions. Additionally, there is a recurring emphasis on testing robustness across hardware variations, such as different GPU architectures and driver versions, as well as maintaining accurate, informative CI/CD workflows. Overall, unresolved questions revolve around optimizing performance, ensuring API stability, and handling system-specific discrepancies."
2021-04-17,pytorch/pytorch,"The discussions highlight ongoing issues with the slow download speeds of the PyTorch package, particularly when downloaded from regions with limited bandwidth or from specific mirrors, suggesting a concern with distribution infrastructure. Additionally, there are technical problems reported with specific functionalities such as `max_pool1d` input shape requirements and certain `torch.jit` serialization failures potentially linked to environment or version mismatches. Several bug reports involve core C++/CUDA components, such as regex errors, model serialization crashes, or discrepancies in behavior depending on tensor properties (e.g., requires_grad), indicating deep underlying implementation challenges. Some discussions also focus on CI/CD stability, code coverage, and the integration of patches from external sources like Open3D or changes in distributed training. Overall, unresolved questions pertain to fixing environment-specific bugs, improving packaging and download speeds, and ensuring compatibility across different hardware and software configurations."
2021-04-18,pytorch/pytorch,"The discussions highlight several technical issues within the PyTorch project, including a resource limit in mini-batch processing tied to tensor element counts, which varies depending on reduction mode and tensor size, with a suggested workaround of refactoring code to use reduced loss computations. There's an ongoing effort to improve the clarity and organization of linear algebra operations within the documentation, proposing categorization by functionality and notation standards, such as preferring backticks over LaTeX and avoiding Unicode characters. Several pull requests involve bug fixes, enhancements, and refactoring, such as implementing a linear-time `logcumsumexp`, refining CI pipelines, and addressing bugs in tensor operations (like `tensordot` support for integer types, shape handling in `jit.trace`, and edge cases in various functions). Some discussions focus on testing frameworks, internal tool compatibility, and ensuring correctness in distributed and JIT contexts. Overall, unresolved questions include understanding internal constraints causing tensor size limits, clarifications on input types for comparison functions, and ensuring new features or fixes are accurately tested and integrated into the codebase."
2021-04-19,pytorch/pytorch,"The discussions primarily revolve around GPU support in PyTorch, highlighting issues with CUDA-enabled tensors on macOS, handling multi-GPU training efficiency, and the deprecation of GPU-specific functions, suggesting migration towards more structured, general implementations. Several posts address debugging and performance concerns, such as the need for better error messages, handling of gradient synchronization, and kernel stability across platforms, including Windows and Linux. There are also extensive considerations about API design, particularly around tensor comparison functions, device and dtype checks, and the addition of user-friendly debugging tools like model summaries. Additionally, issues are raised about build system stability, test timeouts, and potential platform-specific failures impacting internal and external workflows. Overall, the main themes include improving GPU compatibility, robustness, and usability of the PyTorch framework."
2021-04-20,pytorch/pytorch,"The comments reflect ongoing discussions about improving PyTorch core functionalities, including enhanced support for padding strategies in convolutions (Issue #3867), migrating MAGMA routines to cuSOLVER for better GPU performance (Issue #47953), and the challenges of porting algorithms like LU and SVD to CUDA with performance considerations. There are concerns about the CUDA build infrastructure, package size, and compatibility—particularly with different GPU architectures, system dependencies, and binary packaging (Issues #56063, #56086)—along with proposals for splitting wheels and improving dynamic linking. Additional questions address API enhancements such as better tensor comparison semantics, device-agnostic autocast, handling of advanced indexing in TorchScript, and the need for robustness in distributed and JIT compilation contexts. Some discussions highlight potential discrepancies in gradients, consistency, and handling of special tensor types, with a focus on maintaining performance while increasing flexibility and correctness across different hardware and software configurations. Unresolved questions remain about maintaining backward compatibility, improving error messaging, and optimizing build and packaging strategies for larger, multi-architecture CUDA support."
2021-04-21,pytorch/pytorch,"The comments cover a wide range of technical topics including performance optimization in data loading (sharing tensors in shared memory, zero-copy approaches with PyArrow), hardware-specific code generation and compatibility issues (supporting various CUDA architectures, checking for available features like sm_37), extending PyTorch functionalities (adding support for new operations in ONNX or TorchScript, implementing assert_equal for tensors, adding new API parameters and ensuring backward compatibility), and debugging/test failures across different systems (unit tests, CI failures, CUDA kernel issues, unsupported operators). Several discussions focus on improving the integration of backend libraries (MLK-DNN, FBGEMM, MKL, ONNX), kernel implementation details for operations like determinant, cross product, and convolution, as well as addressing issues in distributed training and runtime behavior (thread-local CUDA device, MPI or NCCL dependencies). Unresolved questions involve supporting multi-device or mixed backend configurations, managing deterministic behaviors during testing, and ensuring compatibility with evolving standards (ONNX opset, array API). Overall, the conversations highlight ongoing efforts to enhance PyTorch’s performance, extensibility, and robustness across diverse hardware and software environments."
2021-04-22,pytorch/pytorch,"The comments reveal several recurring technical concerns: 

1. Deadlock and deadlock evasion in DataLoader when using multiple workers, with some fixes (like setting sharing strategy and re-commenting `cancel_join_thread`) proven effective, while others involve adjusting multiprocessing start methods. 

2. Compatibility issues with protobuf versions, especially for building extensions like caffe2, which are addressed by changing protobuf dependencies or upgrading to recent versions. 

3. Enhancements to PyTorch features, such as incorporating MKL-DNN int8 support, stable sort, and API improvements for autograd and serialization, with ongoing discussions about design choices (e.g., serialization APIs, device support for autocast). 

4. Debugging and profiling difficulties, related to NCCL errors in multi-GPU/multi-node setups, and issues with internal CI failures possibly due to hardware or environment inconsistencies. 

5. Requests for documentation clarifications, feature additions (like positional encodings in transformers), and usability improvements (like tensor serialization, reproducibility, and multi-backend process groups), with some solutions already integrated or planned."
2021-04-23,pytorch/pytorch,"The comments from the GitHub issues encompass a diverse set of topics, including the deprecation of the `only_inputs` argument in `grad()`, and the impact of various runtime and build issues such as NaNs during training, device compatibility (especially with ROCm and specific GPU architectures), and compilation errors related to specific hardware or software configurations. Several discussions focus on enhancing or fixing core robustness and performance, such as improving covariance matrix calculations, enabling support for new hardware capabilities (e.g., AVX512), and making error handling and debugging more informative and reliable. There are also questions around API design choices, backward compatibility, and the proper handling of distributed and parallel training workflows, with suggestions to simplify or improve usability and correctness, especially for advanced use cases like nested tensors and custom graph serialization. Overall, the issues highlight ongoing efforts to improve stability, performance, and API consistency, while also addressing environment-specific bugs and compatibility challenges."
2021-04-24,pytorch/pytorch,"The discussions primarily address challenges with multi-process data loading from H5 files using h5py, highlighting limitations and recent solutions involving MPI4py support. There are ongoing efforts to improve neural network performance, particularly regarding CUDA cuDNN algorithm setting interfaces, with suggestions for per-layer configurations and API design considerations. Several issues relate to build and linking problems on Windows with static libraries, involving compiler version constraints and RPATH management, alongside suggestions to improve build robustness. There is also interest in integrating positional encodings into Transformer modules, with debates on default inclusion, batching, and API exposure. Lastly, multiple CI failure reports and efforts to enhance test coverage, especially related to CUDA/ROCm support and various bug fixes, are recurrent themes, alongside discussions on API consistency, code refactoring, and experimental features."
2021-04-25,pytorch/pytorch,"The discussions cover a range of technical topics including the implementation and support of covariance and correlation functions in PyTorch, with emphasis on handling complex tensors, special cases like 0D tensors, and performance optimization complexities. Several issues involve challenges with building and linking PyTorch from source, particularly on Windows and Mac, due to DLL inconsistencies, architecture mismatches, and build size limitations, with some suggestions pointing to reinstallation or environment configuration adjustments. There are concerns about improving CI processes, test coverage, and community engagement, especially regarding documentation and starter issues for less-explored modules. Additionally, various discussions highlight debugging difficulties, performance concerns, and the importance of proper licensing and contribution procedures. Unresolved questions remain about optimizing tensor operations like SVD, handling off-support data types, and configuration of build environments for efficiency and compatibility."
2021-04-26,pytorch/pytorch,"The comments reflect several recurring technical issues in the PyTorch repository. Key concerns include debugging runtime CUDA errors such as device-side asserts and kernel failures, often related to out-of-bounds indices or unsupported kernel functions in ROCm. There are questions about proper environment setup, such as CUDA version mismatches, incorrect library dependencies, and conflicts caused by mixed or stale installations. Developers are also discussing API support for functions like `triu`, `unsqueeze`, and behavior consistency across hardware and software environments, including support for older kernels and different device capabilities. Additionally, some comments address testing and debugging practices, environment reproducibility, and ensuring backend compatibility, especially for older or deprecated configurations."
2021-04-27,pytorch/pytorch,"The comments reflect widespread issues related to PyTorch's functionality and performance, particularly with support for operations like scalar broadcasting in `index_add_` and `scatter_add_`, handling of input tensor sizes in Conv/Deconv layers, and compatibility with various hardware architectures. Several discussions address the need for better deterministic testing, improved API design (such as supporting `assert_equal()` for integer tensors and more explicit error handling for functions like `cholesky` on different backends), and efforts to optimize binary size by modularizing PyTorch packages or supporting dynamic linking. There’s also emphasis on fixing backend-specific bugs for CUDA, NCCL, and ROCm, and on enhancing distributed training stability and compatibility across diverse hardware setups. Some questions relate to how to effectively implement tensor conversion without unnecessary copies, handle deprecated or unsupported features, and improve the testing infrastructure for better coverage and clarity. Overall, unresolved issues include ensuring correct operation across new hardware, minimizing binary size, and establishing clear, user-friendly APIs and testing standards."
2021-04-28,pytorch/pytorch,"The comments reveal multiple ongoing technical concerns in the PyTorch repository, including issues with hardware failures (e.g., NCCL NCCL errors and GPU hardware problems), performance regressions in specific operations like CNNs, RNNs, and tensor reductions, and problems related to certain functionalities like `vmap` and `torch.jit`. Several discussions focus on build and compatibility issues, particularly on older hardware, OS versions, or specific configurations like ROCm/HIP, CUDA, and static linking. There are also reports of intermittent CI failures possibly due to system resource misconfigurations or build environment inconsistencies, as well as ongoing efforts to improve binary package size, security (model serialization formats), and API stability (such as BC-breaking for `linalg` functions). Many comments suggest improvements in error handling, documentation, and feature support, but unresolved questions remain on whether to deprecate certain APIs, how to handle special cases in the implementation, and how to streamline build processes for better compatibility and performance."
2021-04-29,pytorch/pytorch,"The comments reflect several main concerns: (1) the optional dependencies (NLTK and SpaCy) are not clearly documented, leading to confusion about their installation; (2) issues regarding CUDA, cuDNN, and system RAM often cause performance slowdowns, memory errors, or inconsistencies across environments like Colab, local machines, and clusters, with suggested workarounds including system restarts and power cycles; (3) support for distributed training on Windows, WSL, and the integration of features like `torch.distributed` is ongoing, with updates and improvements in progress; (4) there are multiple compatibility and build issues, including VS2019 support, code generation for TensorOptions, and HIP/ROCM support, which require careful version and configuration management; (5) several bug reports, especially around CUDA kernel efficiency, ONNX export errors, and autograd support, indicate ongoing efforts to improve performance, correctness, and security, with solutions ranging from refactoring, codegen fixes, to community feedback requesting clearer APIs and better diagnostics."
2021-04-30,pytorch/pytorch,"The comments primarily revolve around ongoing developments and issues in PyTorch, such as improvements to tensor reduction operations support (e.g., `argmax`, `argmin`, `softmax`), and the intricacies of supporting multiple functionalities like `softmax` and `log_softmax` with tensor iterators. There are concerns about CI failures due to specific commit errors, build environment configurations, and library linkage (notably CUDA and compiler toolchains), indicating ongoing maintenance challenges. Several discussions highlight the need for better API design, such as supporting multiple backward passes, supporting double backward in distributed data parallel (DDP), and safe serialization/loading of untrusted models — with some proposed API patterns and technical solutions. Additionally, there's mention of system and architecture support issues, including CPU instruction set support (AVX-512), build system configurations, and codebase refactoring plans for cleaner namespace management. Unresolved questions include ensuring compatibility and correctness across diverse environments, and addressing CI failures and build issues."
2021-05-01,pytorch/pytorch,"The discussions highlight persistent challenges with PyTorch's handling of buffers with non-None values when loading state dictionaries, emphasizing the need for functionality to load checkpoints with buffers of unknown size, which current solutions (like simply removing `if v is not None`) do not address adequately. Several issues pertain to improving the robustness and consistency of API features, such as adding complex number support for functions like `einsum` on CUDA, and supporting `at::nan_to_num` for complex types. Performance and compatibility concerns are recurrent, including slow downloads of pre-built wheels, platform-specific build failures, and discrepancies in LU decomposition results on different APIs, with ongoing discussions about optimal use of cuSOLVER, MAGMA, and cuBLAS for small matrices. Additionally, there are discussions about internal code organization, such as refactoring serialization code for mobile, and ensuring CI tests and benchmarks accurately reflect platform capabilities, especially regarding AVX512 support on various architectures. Overall, the conversations underscore ongoing efforts to enhance PyTorch's flexibility, stability, and platform compatibility amidst complex internal development and external user feedback."
2021-05-02,pytorch/pytorch,"The discussions primarily focus on performance and correctness issues in PyTorch, including GPU slowdown of operations like torch.nonzero, which is mitigated by ensuring proper synchronization before timing. Several issues concern the performance implications of tensor operations such as matmul, where unnecessary copies and memory layout considerations affect speed, especially with transposed or non-contiguous tensors. There are questions about improving numerical consistency across compilers (clang vs gcc) by relaxing tolerances, and ensuring tests cover diverse hardware platforms like AMD CPUs and GPUs. Some threads address code organization, API design, and the need for more detailed, user-friendly testing and documentation, alongside internal CI and build environment stability concerns. Unresolved questions include how to handle tensor structure comparisons, the impact of kernel modifications, and managing global variable conflicts in CUDA libraries."
2021-05-03,pytorch/pytorch,"The comments cover a broad range of technical topics, including challenges with memory management during package installation, the utility of meta tensors and `to_empty` for initializing models without weights, and methods for validating positive definiteness through fallbacks like SVD. Several discussions address improving documentation clarity, standardizing practices such as weight initialization and weight loading in models, and enhancing support for device placement and interoperability, such as using `device='meta'` and supporting different backends. There are also questions around build systems, compiler compatibility, and platform-specific issues—especially for Windows, MacOS, and CUDA-related failures. Unresolved questions include the correct handling of multi-backend process groups, the use of `reset_parameters()`, and how to effectively prevent overflow errors or optimize model size and performance, all with an emphasis on robust testing and documentation updates."
2021-05-04,pytorch/pytorch,"The discussions highlight several technical issues in PyTorch development, including the limitation of `torch.multinomial` where the number of categories cannot exceed 2^24, and a custom sampler workaround to bypass this. There is ongoing debate about the design and documentation of support for initializing models on meta devices and skipping weight initialization, with plans for helper functions and standardized practices. Performance concerns are raised around CUDA/cuBLAS/cuSOLVER routines, especially regarding large matrix sizes, batch processing heuristics, and ensuring consistent backend behavior. Serialization stability and model hashing consistency are also questioned, especially with regard to deterministic outputs and reproducibility across runs. Additionally, multiple discussions address build system challenges, CI reliability, and integration issues on different hardware and software environments."
2021-05-05,pytorch/pytorch,"The comments reflect a variety of ongoing issues and discussions in the PyTorch repository, including performance scaling challenges related to CPU affinity and threading (issue #3021), difficulties with dataset loading and num_workers defaults (issue #5301), and complex build/ABI mismatches especially with libtorch and CUDA/cuDNN versions (issues #13541, #57284). Certain feature requests like torch.onnx support for exporting/importing models, a native lazy embedding for dynamic vocabularies, and better profiling and device offloading APIs are also debated. Problems with testing flaky or slow tests, especially related to distributed, GPU, and tensor operations, are frequent, alongside infrastructure concerns like CI failures, build environments, and specific bugs on different platforms (e.g., AMD, ROCm). Many unresolved questions revolve around ensuring robust, portable, and correct behavior of advanced features such as vectorization, mixed precision, custom ops, and JIT compilation, with some discussions about potential refactoring, API design improvements, and the need for more comprehensive testing and documentation."
2021-05-06,pytorch/pytorch,"The discussions highlight several key issues with PyTorch, including dtype inconsistencies—particularly the persistent `RuntimeError: Found dtype Double but expected Float` during backward passes—even after converting tensors to float32, indicating a potential mismatch in internal tensor types or operations. Many threads discuss the importance of proper dtype handling, conversion, and explicitly ensuring tensor types match the expectations of loss functions and backward computations. Additionally, there are concerns about low-level implementation details such as kernel dispatch macros (`AT_DISPATCH_INTEGRAL_TYPES`) that lack runtime dtype checks, which may lead to silent failures or incorrect results, especially in meta or non-supported data types. Some discussions suggest introducing `check` variants or macros to enforce dtype support at runtime, along with adding tests or OpInfo support for new dtypes like bfloat16. The unresolved questions focus on how to reliably detect, enforce, and document dtype compatibility in complex scenarios involving CUDA, meta, and low-level kernel dispatches, while maintaining backward compatibility and performance."
2021-05-07,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's stability, performance, and usability. Key technical concerns include enabling stable sorting on GPU, addressing deadlocks in multiprocessing, and supporting additional operations like `max_unpool2d` in ONNX conversion. There are questions about the proper design and implementation of features such as lazy embeddings and operator testing with OpInfo, as well as handling type and aliasing issues in JIT and autograd. Some discussions address CI failures, build reliability across platforms, and maintaining backward compatibility with attribute naming changes. Overall, the focus is on ensuring robustness, expanding supported functionalities, and improving development workflows."
2021-05-09,pytorch/pytorch,"The discussions highlight issues related to managing large tensors with host-mapped memory, questioning the support for mmap-backed tensors and the complications of reflecting changes to disk. Several comments address the stability and accuracy of determinant-based gradient computations, advocating for thresholds to determine singularity and suggesting improvements like using `solve` over `inverse` for better numerical stability. There are mentions of CI/test failures, especially related to changes affecting text-based test comparisons, with suggestions to revert problematic commits and document underlying issues. Additionally, concerns about CUDA RPC and distributed autograd behavior during micro-batch processing point to potential gradient update inconsistencies. Overall, unresolved questions involve proper support for certain memory mappings, numerical stability enhancements, and handling of distributed autograd in micro-batched contexts."
2021-05-12,pytorch/pytorch,"The comments cover a broad range of issues including potential code improvements, feature requests, and bug reports. Key technical concerns include optimizing existing functionalities (e.g., reducing code size, improving hashing for tensors), extending support for new data types or operations (e.g., complex types, betainc, custom datatypes like posit), and fixing specific bugs such as inconsistencies between CPU and GPU behavior, build failures across environments, and incorrect or inefficient implementation of certain functions. Several suggestions involve refactoring approaches (e.g., improving module handling, dispatch strategies, and interface consistency), along with discussions about testing strategies and maintenance challenges. Unresolved questions include the support timeline for new features (e.g., MKL-DNN backend integration, Neural Engine support), and clarifications on implementation details (e.g., handling of shape information, data type support, and backward computation correctness)."
2021-05-13,pytorch/pytorch,"The discussions predominantly revolve around enhancements and bug fixes in PyTorch, including memory formats, tensor data access speed on CPU and GPU, and new features such as a CUDA-capable truncated normal distribution and support for various data types like bfloat16 and posits. Several issues concern correctness and stability, such as ensuring garbage collection behavior matches expectations, addressing compilation and runtime errors, and fixing specific operator behaviors that are inconsistent (like mathematical functions with zero in the domain). There are also concerns about maintaining reproducibility, proper testing, documentation, and handling backward compatibility issues, especially when introducing new functionalities or data types. Overall, unresolved questions include the correct handling of strides with non-contiguous tensors, error propagation in complex distributed or CUDA operations, and how to best extend the PyTorch core with new numerical types without compromising existing features."
2021-05-14,pytorch/pytorch,"The discussions primarily revolve around issues related to GPU and CPU memory management, with many users experiencing out-of-memory errors or performance regressions due to hardware limitations, CUDA/cuDNN support issues, or incompatibilities between tensor devices. Several comments mention workarounds such as reducing batch sizes, clearing cache, or downgrading PyTorch versions, indicating instability in memory handling across different hardware and software configurations. Others highlight the need for better documentation, testing, and support for specific operators, data types, or hardware features (e.g., structured operators, complex numbers, vmap). Some threads involve technical refactoring suggestions for code generation and operator registration, as well as solutions for profiling and build system issues. Overall, unresolved questions include how to improve memory stability, operator support, and test coverage across diverse environments, alongside questions about build and testing workflows."
2021-05-15,pytorch/pytorch,"The discussions primarily revolve around the internal implementation details of PyTorch functions, especially `NLLLoss`, which is implemented in C in `ClassNLLCriterion.c`. Several comments address API inconsistencies and the need for documentation, such as clarifying the behavior of `torch.nn.NLLLoss` and related functional APIs. There are recurring concerns about CI testing failures, including model export to ONNX with different opsets, kernel size padding strategies, and compatibility with hardware features like AVX512 and DLA cores. Some comments point out issues with floating-point types, mixed-precision (AMP), and support for complex numbers, with suggestions to improve testing and documentation. Unresolved questions include how to handle BC-breaking changes, ensure backward compatibility, and properly document and test new or modified features."
2021-05-16,pytorch/pytorch,"The discussions primarily revolve around the need for enhancements and bug fixes in PyTorch's core functionalities, including the addition of an `align_corners` option to `grid_sample`, fixing size-agnostic issues, and addressing bugs related to `MaxPool`, `avg_pool2d_channels_last`, and the `CTC loss` gradient computation. Several issues concern ensuring compatibility and correctness across different hardware capabilities (e.g., AVX, AVX512), and improving error messages and user guidance for better usability. There are ongoing efforts to improve test coverage, resolve CI failures, and unify vectorization code, with community contributions and permission requests for collaborative development. Additionally, some discussions highlight the challenges of reproducing CI failures across different systems and the need for better debugging permissions."
2021-05-17,pytorch/pytorch,"The comments span a broad range of technical issues mainly related to PyTorch's internal development, including dtype inconsistencies (e.g., `torch.float64` vs `torch.float32`), exceptions during autograd backward passes, and improvements to debugging and testing frameworks. Several discussions highlight the need for better error messages, especially for CUDA or distributed operations, and enhancements in test coverage, such as adding reference implementations or handling special cases in JIT. Implicit concerns involve compatibility issues with third-party libraries (like protobuf versions, MKL, or ROCm support), as well as infrastructural improvements like build configurations, pipeline optimizations, and CI reliability. Unresolved questions include how to improve API discoverability, error diagnostics, and handling of complex or custom user workflows, especially around distributed or fused kernels. Overall, the discussions suggest ongoing efforts to improve robustness, performance, and user experience in PyTorch's development ecosystem."
2021-05-18,pytorch/pytorch,"The discussions reveal several recurring technical concerns: (1) Handling data type mismatches in PyTorch, especially the RuntimeError ""Found dtype Double but expected Float"" during loss computation, with suggestions to set default dtypes globally or convert tensors explicitly. (2) Challenges in exporting models with dynamic control flow (like pack_padded_sequence) to TorchScript or ONNX, including limitations of tracing and support for tuple outputs, raising questions about handling such constructs. (3) Difficulties with GPU memory management, particularly around CUDA 11, specific to different hardware (RTX 3090, A100), and issues with building or linking CUDA environments, often addressed by downgrading drivers or environments. (4) Complications with PyTorch's model serialization, especially when creating modules inside forward functions or dynamically generating submodules, leading to suggestions to properly register modules or adjust scripting approaches. (5) Infrastructure and build system concerns such as CI failures, support for new hardware or software versions, and code refactoring plans to unify dispatching mechanisms and improve maintainability, including discussions about the scope of changes and testing strategies."
2021-05-19,pytorch/pytorch,"The discussions reveal multiple ongoing technical concerns, primarily centered around improving PyTorch's data loading efficiency (e.g., tackling slow `num_workers > 0` in Jupyter), handling tensor shape and type mismatches during `load_state_dict`, and issues with CUDA memory management and performance regressions across various hardware and library versions. There are also considerations related to extending or clarifying PyTorch's type promotion rules for 0-D tensors and categories, as well as enhancing the robustness and consistency of the JIT and ONNX export mechanisms (addressing symbol redefinitions, subgraph exclusions, and symbolic function support). Additionally, some threads explore refactoring efforts in the codegen and operator registration pathways, with questions about their complexity and futureproofing. Several discussions highlight the importance of correct handling of distributed autograd, Tensor device consistency, and safety regarding CUDA stream reuse. Unresolved questions include how to properly support custom operators with complex or tuple outputs, and whether to incorporate new features like mish activation or optional custom OOM handling, often balancing BC concerns and user experience."
2021-05-20,pytorch/pytorch,"The comments reflect a range of technical concerns and questions, including performance regressions between Python and C++ execution with TorchScript (issues #19106, #25032), compatibility issues with GPU hardware and CUDA versions (e.g., RTX 3080 support in #45028, #58442), and potential improvements in testing and debugging infrastructure like `assert_close`, `tensor_view`, and ensuring proper handling of optional and reference types in the codebase (#56625, #58390). There are also discussions on optimizing CUDA memory management (#58458, #58625), handling autograd threading in multi-threaded contexts (#58503), and maintaining compatibility and stability across different platform builds and dependencies (#58418, #58442, #58483). Several issues highlight build failures, version mismatches, and CI failures, often leading to requests for review, patch submission, or code alterations to address these problems. Unresolved questions remain regarding performance tuning for specific ops, correct serialization and testing practices for new functionalities, and platform-specific handling of GPU and CUDA configurations."
2021-05-21,pytorch/pytorch,"The comments reflect a variety of technical concerns in the 'pytorch/pytorch' repository, including inaccuracies in custom implementations of statistical measures (e.g., Spearman's correlation), requests for better error reporting (e.g., verbose load failures), and issues with tensor and operator support in ONNX export (e.g., unsupported operators, operator inconsistencies, and conversion precision). Several discussions highlight the need for improved debugging, such as better verbose output for model loading or explicit error messages when unsupported features are used. There are questions about the correctness of backward implementations (e.g., for functions like cummax), requesting bug fixes and precise gradient computations. Additionally, operational concerns such as package download speeds, threading issues, and CUDA context management are raised, alongside discussions on feature proposals like adding no_train context managers or refining API consistency to align with NumPy."
2021-05-22,pytorch/pytorch,"The discussions highlight concerns about the memory overhead associated with the `.contiguous()` operation in PyTorch, particularly in broadcasting scenarios involving tensors with higher dimensions (ndim >= 3). There is a question regarding the internal implementation of broadcasting in matrix operations, specifically why batch dimensions are broadcasted internally and whether this should affect the original tensor dimensions. Several issues involve documented limitations of ONNX export support for certain operators and opset versions, with requests for enhancements and bug fixes, especially for operators like `chunk`. Other discussions address performance regressions in LU decomposition, the design of tensor and stream representations for efficiency, compatibility issues with newer GPUs (e.g., RTX 3080, A40) and CUDA versions, and efforts to improve debugging and serialization features. Overall, unresolved questions include compatibility fixes for new hardware, improvements in memory performance, and more transparent, stable operator support across frameworks."
2021-05-23,pytorch/pytorch,"The discussions highlight several key technical concerns in PyTorch development, including challenges with implementing universal functions like `result_type` supporting variadic inputs and complex data types, which require intricate handling in both Python and C++ layers with specializations for JIT and eager modes. There are ongoing efforts to improve test robustness and coverage, such as introducing `assert_close` with configurable precision, better sample input representations, and integrating reference implementations for OpInfos to ensure consistency across CPU and CUDA, as well as cross-library comparisons. Concerns about CI efficiency are raised, emphasizing the need for categorizing slow tests, managing build times, and support for testing operators with diverse variants (function, method, in-place). Discussions also touch on improving discoverability and usability of testing features, handling BC-breaking changes, and ensuring compatibility with evolving hardware platforms like AMD and Apple's M1. Unresolved questions include balancing test coverage with CI costs and designing extensible, user-friendly testing infrastructure."
2021-05-24,pytorch/pytorch,"The discussions revolve around various issues and proposed improvements related to the PyTorch codebase. Notable concerns include the handling of `Tensor.grad` history and potential memory leaks, CUDA out-of-memory errors during training, and the need for better user-facing documentation and testing for specific features like `assert_close`, `softmax` masking, and model serialization. Several tickets highlight the importance of backward compatibility when changing APIs, such as `torch.unique` and `torch.one_hot`. There are also technical questions about implementation details, such as how to improve the structure of generating operator functions (`ComputeFunctions`), managing device contexts for multi-GPU training, and fixing platform-specific build issues like `pybind11` version detection. Unresolved questions include the best approach for incorporating certain internal API changes, handling device-specific bugs (e.g., cufft), and improving test robustness while maintaining backward compatibility."
2021-05-25,pytorch/pytorch,"The discussions cover a range of technical concerns related to PyTorch's design and behavior. Notably, users highlight that the `batch_first=True` flag in RNNs does not impact the `h_n` and `c_n` outputs, suggesting documentation improvements; and there are issues with memory leaks or growth linked to MKLDNN and profiling in JIT, which may relate to profiling nodes or profiling mode settings. Others point out discrepancies in CPU and CUDA tensor operations, such as the correctness and performance of `torch.lobpcg` versus `eigh`, and potential bugs in magnitude, normalization, and serialization across different data types and backends. Some discussions involve infrastructure problems like NCCL, CUDA plan caching, or environment misconfigurations affecting distributed training, as well as internal development topics like code refactoring, deprecation of constructors, or API extensions. Unresolved questions remain about compatibility, correctness, and performance for certain features, especially around advanced backends, distributed execution, and custom autograd functions."
2021-05-26,pytorch/pytorch,"The collected comments cover several topics, including stability issues when training on CPU versus GPU, with one suggesting importing `cv2` before `torch` to resolve a memory error; attempts to improve PyTorch's docs SEO and search snippet display; upcoming support for ""same"" padding on convolutions in the next release; efforts to enable multi-sampler/distributed training; and ongoing efforts for features like optimizer reset functionality, custom serialization, and support for certain operations like `special.betainc`. Many technical concerns relate to backward compatibility, build environment configurations (notably CUDA and macOS threading), and test reliability. There are proposals for API improvements, such as allowing custom sampler usage with DDP, more flexible `result_type` functions, and better management of model and tensor states for functional programming paradigms. Unresolved questions include version scheduling, compatibility fixes, and ensuring feature support is aligned with user needs and infrastructure constraints."
2021-05-27,pytorch/pytorch,"The comments cover a wide range of technical concerns in the PyTorch codebase, including issues with ONNX export support for operators (e.g., `chunk`, `as_tensor`), casting and dtype promotion compatibility, and serialization BC considerations, particularly regarding tensor storage and `__reduce__` mechanisms. Several discussions focus on performance optimization, such as enabling AVX512 kernels, benchmarking quantized operations, and parallelizing build processes for faster compilation. There are also questions about model correctness versus optimization in certain PRs, handling of dynamic shapes, and device-specific support (e.g., ROCm, CUDA, and specific hardware like Cascade Lake). Additionally, there are concerns regarding stability and correctness of functions like `detach()`, `shallow_copy_and_detach`, and their implications on autograd and memory management. Overall, unresolved issues include improving operator support in ONNX, maintaining backward compatibility, optimizing kernel performance, and enhancing safety and reliability in Autograd and serialization mechanisms."
2021-05-28,pytorch/pytorch,"The comments reflect ongoing efforts to address various technical issues in PyTorch, including build compatibility, CUDA support, and operator support for new types such as Posit. Several discussions focus on fixing environment setup, CUDA and cuSPARSE support, and improving documentation for installations across different platforms. There are also multiple discussions on improving internal code robustness, including operator registration, type support, and source code highlighting behavior. Some issues relate to CI testing, hardware-specific bugs, and the development of new features like Java bindings and support for custom scalar types, with suggestions for contributions and improvements. Many unresolved questions concern compatibility, performance optimization, and the proper integration of new features into existing infrastructure."
2021-05-29,pytorch/pytorch,"The discussions encompass several key topics: addressing hardware stability issues in training (e.g., BIOS updates, power supply upgrades, CPU turbo boost deactivation); clarifying and improving documentation accuracy, particularly regarding the implementation and implications of Adam and AdamW optimizers; enhancing testing practices including the capability for precise and flexible assertions (such as comparing tensors to scalars and controlling stride checks); resolving compatibility issues with older GPUs and CUDA versions, especially for legacy hardware; and improving the usability and maintainability of the testing framework (e.g., managing sample input reporting, test organization, and error messaging). Unresolved questions include the best approach for flexible assertion defaults, the update of documentation referencing Decoupled Weight Decay, and the potential for better Java API bindings integration."
2021-05-30,pytorch/pytorch,"The discussions primarily revolve around optimizing CUDA synchronization modes in PyTorch to reduce CPU core spinning, with solutions involving setting device flags and synchronization modes such as 'block' and 'yield'. Several issues address build and compatibility challenges across platforms, especially ARM64 and mobile, highlighting the need for better support, potentially through libraries like pocketfft. There are ongoing efforts to improve type annotation consistency, error message clarity, and handling of numerical tolerances in testing functions like `assert_close`. Some concerns involve build system intricacies and fixing internal bugs, such as SourceRange misalignments and API compatibility, with contributions ranging from code fixes to documentation enhancements. Additionally, there are discussions about PyTorch's CUDA extension build process, environment setup, and the management of development versus runtime CUDA packages, emphasizing the complexity and need for more user-friendly tooling."
2021-05-31,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's documentation SEO, especially search snippets and organization, with proposals to exclude code examples and function pages from search results, and suggestions to enhance the page structure via `autofunction` and `autosummary` annotations. Several issues concern support and debugging for sparse tensor autograd and related operations like `mm` and `abs.backward`, including potential workarounds for unsupported gradients and support for elementwise multiplication errors. There are technical considerations about `torch.jit.script`'s handling of variable argument functions, the implementation of `result_type` with variadic and mixed inputs, and necessary changes to function signatures and metadata for OpInfos to support NNC testing and avoid brittle test failures. Other discussions include addressing CI failures caused by hardware dependencies, build configuration issues, and compatibility concerns with CUDA, ROCm, and compiler versions; along with standard questions about code deprecation, test design, and sign-off procedures for contributions. Overall, unresolved questions focus on support for variable arguments in JIT-compiled functions, improving test infrastructure, and ensuring cross-platform compatibility for advanced tensor operations."
2021-06-01,pytorch/pytorch,"The discussions highlight a variety of technical concerns including the desire for model visualization tools like torchviz or pytorch-summary, the implementation details of new features such as soft-label support in loss functions, and the challenges of supporting new tensor data types like posits or bfloat16 within PyTorch's internal dispatch and serialization systems. There are questions regarding the best practices for extending PyTorch's kernel dispatching (e.g., adding new data types, support for AVX512), handling device-specific behaviors (CUDA, CPU), and maintaining backward compatibility in serialization. Additionally, users express concerns about build system issues, particularly related to CUDA environment setup, and the potential for simplifying API design and testing strategies to improve usability and reliability. Overall, unresolved issues include how to support new data types seamlessly, improve build robustness, and refine internal mechanisms for in-place operations, autograd, and device handling."
2021-06-02,pytorch/pytorch,"The discussions encompass a variety of technical topics related to PyTorch, including proposals for extending TransformedDistribution with a Logistic distribution class, addressing sparse tensor autograd support, and handling complex number serialization. Key concerns involve making PyTorch more user-friendly and extendable, such as adding examples for distributions, improving external backend support (e.g., ONNX, ORT), and refining gradient support for sparse tensors. Several threads highlight issues with backward compatibility, code optimization (e.g., for CPU kernels, large tensor operations, and AVX512 support), and build system challenges across platforms. There is also ongoing interest in integrating PyTorch with Java via JNI, and clarifications around control flow, dispatch mechanisms, and tensor options handling. Many discussions remain open or in progress, reflecting efforts to enhance robustness, performance, and extensibility of PyTorch's core and ecosystem."
2021-06-03,pytorch/pytorch,"The comments encompass a range of topics, primarily centered around PyTorch distribution implementations, custom extensions, and performance optimization. Several discussions address how to extend `TransformedDistribution` to easily support common distributions like Logistic, and the challenges involved in shape misalignments or errors—particularly with flexible mixture models and soft targets. There are also questions about enhancing ONNX export support, improving CUDA and hardware compatibility, and optimizing low-level kernel performance, especially relating to AVX512 versus AVX2. Troubleshooting runtime errors, such as out-of-memory issues and device support mismatches, are recurrent, along with suggestions for better build diagnostics and compatibility handling. A notable concern is maintaining correct subclass behaviors, especially with meta tensors and custom tensor options, which impacts both debugging and model correctness."
2021-06-04,pytorch/pytorch,"The comments highlight multiple technical issues and feature considerations within the PyTorch ecosystem. Key topics include the lack of support for soft targets in core loss functions, with suggested alternatives like KLDivLoss, and the potential benefit of a dedicated, user-friendly label smoothing wrapper that could unify approaches across domains like vision and NLP. Several discussions address build and deployment challenges, such as slow download speeds, environment incompatibilities, and extension build errors due to library mismatches or missing support in ONNX exports (e.g., RoiAlign). There are ongoing efforts to improve hardware support, notably for Apple Silicon M1, and addressing execution bugs related to CUDA, NCCL, and native kernel complexities. Overall, the conversations reveal a combination of feature proposals, usability improvements, and troubleshooting of system incompatibilities."
2021-06-05,pytorch/pytorch,"The discussions highlight several technical concerns, notably the incomplete support for certain data types such as ""Half"" (float16) in convolution operations, which leads to runtime errors like ""unfolded2d_copy not implemented for 'Half'."" There are questions about integrating non-PyTorch frameworks and bindings, such as Numba and JavaCPP, to enhance flexibility and potential support for training on different platforms. Issues with package compatibility, especially regarding Python versions (e.g., Python 3.9 with PyTorch 1.8.1), are recurring, often requiring environment resets or specific channel configurations. Additionally, there are concerns around proper cleanup of resources like TCP stores, inter-process communication stability, and CI pipeline failures across different operating systems and hardware backends (CUDA, ROCm), with some regressions reverted or blocked for further fixes. Overall, areas needing attention include expanding data type support, improving cross-language bindings, resolving environment dependency conflicts, and stabilizing distributed training and resource management in CI environments."
2021-06-06,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's CUDA support, including issues with GPU compatibility (e.g., older cards like GeForce GT 710), and performance considerations of asynchronous kernels and multi-stream execution. Several threads address the need for enhanced error checking, clean API design (such as support for insertions in `Sequential` and lazy initialization for optimizers), and proper handling of tensor serialization and sharing (e.g., `from_blob()` behavior, memory address reuse). There are questions about porting or deprecating functions like `lstsq`, improvements in distributed communication functions (`all_gather` order guarantees), and support for Vulkan on Windows. Some discussions seek clarifications or propose features like lazy attributes in modules, while others address CI failures and build configuration issues across environments. Overall, unresolved questions remain around stability, feature completeness, and performance optimization in these areas."
2021-06-07,pytorch/pytorch,"The comments reveal ongoing discussions around improving PyTorch's usability, including fixing checkpoint loading behavior for buffers with `None` values, implementing label smoothing support via a dedicated loss function or wrapper, and enhancing the `load_state_dict()` handling of uninitialized buffers. There is also suggestion to develop a user-friendly soft cross-entropy that directly supports soft targets via KLDivLoss, addressing community confusion. Further concerns involve ensuring compatibility and stability across different hardware (e.g., CUDA, ROCm, ARM), clear documentation of behaviors (like `register_buffer(None)` and `torch.finfo()`), and refining the internal API and serialization mechanics to support more flexible, structured model representations. Some discussions also touch on the importance of correctness for distributed training, code provenance, and maintaining deterministic instruction counts for performance monitoring. Overall, the focus is on making PyTorch more robust, transparent, and easier to extend for diverse use cases."
2021-06-08,pytorch/pytorch,"The comments encompass diverse topics, primarily focusing on device compatibility and performance improvements in PyTorch, such as default device settings to CUDA for BC reasons, and support for old GPU architectures. Several discussions highlight performance regressions and benchmarking for functions like einsum and matrix multiplication, as well as the integration of libraries like cuTENSOR and cutensor, with considerations for supported hardware. Other concerns involve CI build failures, test stability, and reproducibility, along with proposals for API and internal design changes, including support for functional modules, better device management, and test infrastructure. Several questions are raised about specific implementation details, such as handling variable batch sizes during tracing, the impact of multithreading in BLAS on mobile, and maintaining backward compatibility with quantization and autograd operations. Unresolved issues include performance regressions, CI failures, and the need for further validation of proposed code modifications."
2021-06-09,pytorch/pytorch,"The discussions cover various technical concerns including the implementation details of `torch.view` in C++ for efficiency, ensuring spatial order preservation during tensor reshaping, and the support and behavior of ONNX export for models with dynamic or optional inputs, especially for sequence models and complex architectures. Several issues highlight incompatibilities or missing features in current PyTorch and ONNX versions (e.g., support for `repeat_interleave`, certain operators like `unfolded2d_copy` for specific dtypes, and shape inference issues) as well as challenges with CPU/GPU modes and distributed training, such as debugging distributed processes and the impact of CUDA memory leak checks on test performance. Others suggest improvements in testing infrastructure—like early failure detection, better sample input reporting, and removal of redundant leak checks—to streamline CI processes. Additionally, concerns around compatibility with hardware (e.g., M1 Macs, PowerPC CPU, ROCm) and the integration of C++ APIs and features like `Kineto` profiling are discussed, along with proposals for modular, extensible testing and reference implementation strategies."
2021-06-10,pytorch/pytorch,"The discussions primarily focus on best practices for loading checkpoints onto GPU devices in PyTorch, emphasizing the importance of loading the model state before the optimizer's state to ensure both are on the same device, and suggesting using ""map_location"" in torch.load for direct GPU loading. Several issues involve limitations in TorchScript support, especially with dicts of mixed types, where static typing restrictions prevent support for dicts with varying value types. Users also express concerns about performance bottlenecks, such as delays caused by CUDA context initialization, tensor allocation overheads, and the overhead of certain gradient computations in large batch or network scenarios. Additionally, some discussions address CI pipeline limitations, like test time slowdowns due to gc.collect() and potential improvements in build infrastructure. There are ongoing proposals for new features, including handling optional inputs in ONNX, quantization range safety checks, and better support for specific tensor operations across different hardware and software environments."
2021-06-11,pytorch/pytorch,"The comments cover a wide range of technical topics related to PyTorch, including the behavior of BatchNorm with zero learning rate and the impact of momentum on running mean/std; support for soft targets in loss functions, with suggestions to use KLDivLoss; improvements to export and quantization features such as range handling and device support; issues with CUDA kernel behavior and numerical stability, especially in mixed precision training; proposal of new native approximations for GELU to optimize performance; and various CI/test stability concerns, including flaky tests, internal build processes, and deprecation of certain APIs. There are suggestions for API design improvements, like a unified soft cross-entropy loss, and discussions on test infrastructure, parametrization, and documentation to enhance usability. Some comments also highlight community and usability issues, like handling of unsupported hardware or model size variability. Overall, the discussions focus on improving performance, stability, usability, and correctness across the PyTorch codebase and its ecosystem."
2021-06-12,pytorch/pytorch,"The discussions reveal ongoing concerns about specific bugs in PyTorch, such as issues in particular versions (e.g., 1.6, 1.7.0) and how they are managed or unresolved despite being reported. There are questions about the proper inspection of tensor storage pointers and data access, especially for view and reshape operations, to understand underlying memory semantics. Troubleshooting and build issues are prominent, including problems with CUDA extension compilation, environment mismatches, and ensuring compatibility across CUDA and library versions, with suggested workarounds like trusted hosts and version matching checks. Debugging complexities in distributed autograd failures, CUDA driver errors, and performance considerations in batched gradient computations are also discussed, along with ongoing infrastructure and CI/CD pipeline improvements. Uncertain or unresolved points include CUDA version compatibility nuances and the need for more detailed error diagnostics during extension builds."
2021-06-13,pytorch/pytorch,"The discussions primarily revolve around the need for a function to compute entropy directly from raw data in PyTorch, which involves calculating probabilities from data without prior probability inputs, highlighting concerns about differentiability and gradient computation. Several issues also touch on device compatibility and CUDA-related build errors, such as GPU memory leaks, DLL loading conflicts, and compilation failures due to compiler or environment mismatches, especially on Windows with Visual Studio. There are ongoing considerations about type promotion rules in the array API standard, emphasizing the complexity and potential inconsistencies in dtype handling for tensor operations and scalars. Additionally, some discussions highlight CI infrastructure challenges, including experimental efforts to optimize testing times by disabling memory leak checks and refining test coverage. Overall, key unresolved questions include implementing and validating entropy functions that support differentiability, resolving cross-platform build issues, clarifying type promotion semantics, and optimizing CI workflows."
2021-06-14,pytorch/pytorch,"The discussions mainly revolve around handling multiprocessing and DataLoader issues in PyTorch, especially on Windows and in notebook environments; solutions include proper use of `if __name__ == '__main__':`, `freeze_support()`, and setting `num_workers=0`. There are concerns about reproducibility and numerical stability in tensor operations, particularly with CUDA, floating point precision, and operator support for custom or specialized data types (e.g., integers, complex, bfloat16). Several discussions also focus on API design choices, such as breaking backward compatibility for functions like `tensor.min()` with `dim` arguments, and how to implement support for new features like forward-mode autodiff or type promotion consistent with standards like the Array API. Unresolved questions include how to improve error reporting, whether to refactor large source files into smaller modules, and how to handle unification of data types and disallowed operations when using new tensor view features or conjugate views."
2021-06-15,pytorch/pytorch,"The comments reveal ongoing discussions about enhancing PyTorch's user interface and functionality, such as adding a wrapper for label smoothing loss support, or providing a dedicated soft cross-entropy loss that supports label smoothing directly. There is interest in improving the usability and API clarity for distributed training, especially regarding the `torch.distributed.run` script and compatibility with older PyTorch versions. Several technical concerns involve ensuring backward compatibility (e.g., with conjugate tensor views, sparse tensor conversions, and model serialization), optimizing performance (e.g., for CUDA kernel fusion, multi-versioned TorchScript), and refining internal code organization (e.g., splitting large test files, restructuring OpInfo definitions). Unresolved questions include how to handle complex number conjugation consistently across frameworks, how to properly support legacy CUDA versions, and how to manage long-term maintenance for large codebases while implementing new features."
2021-06-16,pytorch/pytorch,"The comments reflect a mixture of technical issues, feature proposals, and infrastructure discussions within the PyTorch ecosystem. Key concerns include guidance on how to set `num_workers=0` for DataLoader, complexities around implementing scatter reduction modes, and nuances of device device flags, especially on Windows and CUDA contexts. Several discussions address the behavior and design of tensor operations such as `scatter_`, `max_pool`, and `autograd.grad`, often seeking clarity on expected semantics, API consistency, and performance impacts. Enhancement suggestions include better profiling support, improved error handling, and more expressive debugging tools like tailored logging or source code tracking. Unresolved questions largely revolve around optimizing device and memory management, improving test frameworks, and refining API specifications to align with standards like the Python Array API and NumPy."
2021-06-17,pytorch/pytorch,"The comments reveal ongoing discussions about improving error handling consistency in PyTorch, especially suggesting transitioning to ValueError for specific errors, and the need for better APIs to support diverse backends and configurations, notably selecting algorithms for different hardware or modules. Several issues involve extending support for complex tensors in distributed operations, and the difficulty of adding flexible scatter reduction modes that integrate element-wise operations and reductions. There are technical challenges related to debugging build failures, such as linker issues with static libraries, and platform-specific problems like Windows path length limitations. Some discussions focus on improving testing infrastructure, code compatibility across Python versions, and maintaining backward compatibility during API updates. Overall, the conversations highlight efforts to enhance robustness, flexibility, and usability, while unresolved issues include error message standardization, API design for new features, and platform-specific build stability."
2021-06-18,pytorch/pytorch,"The discussions highlight several technical concerns: (1) the need for better handling of module and parameter replication in the context of functional programming and compatibility with optimizers, lazy modules, and distributed training, including considerations for in-place operations and hooks; (2) questions about how to support external CUDA streams via a PyTorch-specific API, including whether to expose private or public interfaces; (3) challenges related to ONNX export and tracing, especially with dynamic control flow, operator support, and graph optimization, including issues with profiling and graph invariants; (4) performance and memory issues in various operations such as `einsum`, `bmm`, and memory bandwidth benchmarking, along with suggestions for custom kernels and code refactoring; and (5) ongoing CI failures, reproducibility problems, and environment discrepancies, emphasizing the importance of test robustness, deterministic algorithms, and accurate profiling, with questions about dependencies, serialization, and backward compatibility."
2021-06-19,pytorch/pytorch,"The discussions highlight ongoing development concerns including the timing of feature releases, especially those involving NumPy compatibility issues and potential numerical problems, with recommendations to use `torch.multinomial` as an alternative. Several reports of bugs and bugs' root causes are mentioned, such as segmentation faults with DataLoader or CUDA interactions, with requests for minimal reproducible scripts to assist debugging. There is an emphasis on improving code organization, notably splitting large files like `common_methods_invocations.py` into smaller, more manageable modules, and reworking the test infrastructure to better support subtests, parametrization, and output capturing—possibly via pytest. Additionally, issues related to determinism, serialization, and performance benchmarking are discussed, alongside situational questions about the behavior of specific functions and configurations, as well as CI stability and test flakiness. Overall, the dialogue reflects a focus on improving robustness, usability, maintainability, and testing practices within the PyTorch project."
2021-06-20,pytorch/pytorch,"The discussions highlight several technical issues and feature proposals, including the need for better handling of tensor parameters as real scalars in PennyLane operators, and supporting custom autograd functions in Python and C++. There is ongoing debate about refactoring the large `common_methods_invocations.py` into smaller, more manageable modules—favoring modular design with separate files for different operator categories—and implementing a registry-based approach for easier expansion and clearer organization. Several issues relate to performance and compatibility concerns, such as optimizing `ilp_factor` for vector addition, addressing NumPy and PyTorch discrepancies in data type promotion, and improving padding modes consistency with NumPy standards. Additional discussions focus on resolving CI/CD reliability, especially with large test files, and on clarifying behavior differences across platforms or versions, such as byte overflow in `torch.add` and warning suppression. Overall, many suggestions aim at improving modularity, maintainability, and correctness of the PyTorch codebase and its ecosystem integrations."
2021-06-21,pytorch/pytorch,"The discussions primarily revolve around addressing technical issues and potential enhancements in PyTorch, including: managing error messages and debugging practices, especially for custom or complex scenarios; improving reproducibility and testing, such as handling NaN propagation bugs, verifying reproducibility of operator outputs, and dealing with GPU memory and performance bottlenecks; potential architectural changes, like refactoring the API for Tensor operations, expanding support for more precise or safe memory handling, and integrating or replacing existing APIs for serialization, padding, or distributed training; and infrastructure or CI pipeline improvements to reduce flaky tests and better document dependency/version requirements. Unresolved questions include the compatibility and behavior of certain features across different hardware/software environments, the effort involved in supporting new features (like multi-dimensional padding), and how to best organize or automate test and build processes for future stability and maintainability."
2021-06-22,pytorch/pytorch,"The discussions predominantly revolve around ensuring correctness, performance, and usability improvements in PyTorch. Several issues concern the proper implementation and behavior of specific functions, such as cross-entropy calculations, soft label support, and the handling of softmax/soft labels, with proposals for aligning definitions (e.g., KL-divergence vs. cross-entropy) and clarifying naming conventions. Other concerns include addressing platform-specific test failures, environment and build configuration inconsistencies, and improving internal infrastructure practices such as code organization, test stability, and documentation updates. Several threads also discuss GPU-related performance variations, correctness in numerical reproducibility, and system-specific issues (e.g., macOS, ROCm, Windows), prompting suggestions for further testing, bug fixes, or feature clarifications. Unresolved questions remain about how to best support certain new features (e.g., custom ops, CUDA RPC, TensorList support), test management, and ensuring stability across different hardware and software environments."
2021-06-23,pytorch/pytorch,"The discussions cover several technical issues in PyTorch, including challenges with multi-processing data loading with h5py files, device management in DataLoader collate_fn, and debugging in-place autograd errors, with suggestions like locking or dynamic device transfer solutions. There are questions about supporting in-place operations with in-place operation profiling and the implications for autograd and in-place operation detection. Several discussions concern compatibility problems, such as combining CUDA RPC with other features, resolving version conflicts for Android (PyTorch Android vs. Lite), and ensuring proper support for sparse formats with duplicate indices in sparse CSR matrices. Other points involve ensuring the correctness and performance of features like channels-last memory support, proper handling of tensor types in shape inference and autograd, and considerations around backward compatibility, serialization, and API changes. Unresolved questions include how to debug specific errors, performance implications of migration strategies, and adjusting tests or configurations to prevent flaky failures."
2021-06-24,pytorch/pytorch,"The discussions reveal ongoing efforts and challenges related to enhancing PyTorch’s features and performance. Notable topics include the integration of block-sparse operations with Triton and support in cuSPARSE, with some progress linked to CUDA 11.4 updates; proposals for a LinearOperator abstraction for sparse matrix operations, referencing external projects like SciPy and PyLops; and the development of specialized functions such as `nanmean` and support for quantized modules. There are also concerns about correctness and stability, for instance, in BatchNorm behavior, deterministic numerical differences between implementations, and handling of in-place operations. Additionally, infrastructure issues like build reproducibility, CI failures, and packaging complexities—especially with CUDA and GPU driver setups—are frequent, alongside questions about best practices, testing, and documentation improvements."
2021-06-25,pytorch/pytorch,"The comments cover a range of technical concerns including persistent and unresolved dimension out-of-range errors in PyTorch data loading and dataset handling, as well as issues related to CPU capabilities, environment configurations, and compatibility with different CUDA architectures and versions. Several discussions focus on improving or debugging the behavior of autograd, softmax, and operator dispatch mechanisms, including the handling of soft labels, softmax in mixed precisions, and kernel optimizations for various hardware (AVX2, AVX512, ROCm). There are also recurring questions about bugs introduced by recent changes, CI test failures, and the need for better test coverage, documentation, and user guidance especially regarding new features, backend differences, and internal implementation details. Some suggestions involve refactoring API, adding new parameters for flexibility, or disabling/revisiting certain features (e.g., redundant layer norms, in-place operations, operator fusion). Overall, unresolved questions include how to effectively address performance issues, ensure backward compatibility, and improve error handling and environment reproducibility across diverse hardware and software setups."
2021-06-26,pytorch/pytorch,"The comments highlight issues related to PyTorch's API features and stability, such as the absence of early stopping in PyTorch and ongoing progress on related pull requests. Several discussions concern reproducibility, such as performance with different integer types and how system settings (e.g., IPV6_V6ONLY) affect socket behavior in distributed setups. Some issues pertain to debugging and improving user experience, including handling NaNs in mixed precision training, proper logging levels, and accurate documentation for torch.distributed.launch vs. torch.distributed.run. Additionally, future improvements are discussed for API consistency, including support for C++ aliases, better handling of in-place tensor operations, and clarifications on model exporting procedures. Unresolved questions involve performance impacts of data types, proper configuration of system network parameters, and ensuring backward compatibility and robust testing for distributed training tools."
2021-06-27,pytorch/pytorch,"The discussions primarily revolve around optimizing DataLoader performance and memory management, with suggestions including running Dataset in separate processes via IPC or using zero-copy methods with PyArrow to prevent memory leaks when loading large datasets. There are ongoing efforts to integrate such solutions into the DataLoader API, but domain-specific considerations remain. Several issues concern compatibility and linking problems with different compiler versions and PyTorch build artifacts, especially regarding custom extension compilation with libtorch, highlighting challenges in dependency management and dynamic symbol resolution across compiler/linker configurations. Additionally, issues like proper freezing of Batch Normalization layers, version support on platforms like macOS, and CUDA compatibility are raised, along with proposed improvements in testing, documentation clarity, and support for specific hardware/software configurations. Unresolved questions include how to best standardize dataset separation, ensure correct symbol linking, and maintain backward compatibility amidst evolving APIs and platform differences."
2021-06-28,pytorch/pytorch,"The comments reflect a broad range of technical discussions and feature requests in the PyTorch repository, including enhancements to existing operators (e.g., `grid_sample`, `cross_entropy` with soft labels, rank-deficient QR decompositions), improvements to distributed training and CPU kernels (e.g., CSR support, broadcasting objects, multi-GPU process management), and infrastructure updates (e.g., support for `PackedTensorAccessor`, future release planning, and CI stability). Several issues involve compatibility and correctness concerns, such as handling rank-deficient matrices in autograd, ensuring proper device and memory guard settings in multi-GPU and distributed contexts, and fixing build issues related to architecture-specific assembly code. There are also discussions on improving developer experience through better documentation, API stability, and version tracking, as well as addressing performance, correctness, and usability of features like label smoothing, AMP, and onnx export support. Unresolved questions include the best approach for tensor representations in custom functions, how to automate tracking API changes across releases, and how to ensure CI robustness across different environments and hardware."
2021-06-29,pytorch/pytorch,"The discussions encompass various PyTorch issues, primarily focused on debugging, performance optimization, and feature support. Key concerns include handling multiple tensor outputs from traced functions, input handling for models with complex nested structures (like dictionaries or lists), and proper management of CUDA device contexts to prevent errors or inefficient execution. Several comments suggest simplifying APIs, such as integrating parameters into a mode argument instead of multiple flags, and improving documentation for functions like pooling or dropout. Unresolved questions involve ensuring compatibility for certain operators with specific backend keys in autograd, handling complex input validation for distributed or CUDA operations, and addressing performance regressions or bugs introduced in recent updates. Overall, the discussions emphasize debugging, correctness, usability, and performance improvements in PyTorch."
2021-06-30,pytorch/pytorch,"The discussions reveal ongoing challenges related to the proper handling of input tensors and distribution keys in PyTorch, especially concerning complex tensors in DataParallel, uninitialized or mismatched CUDA device types, and dispatch keys for linear algebra functions. Several suggestions include using `torch.view_as_real`, moving from DataParallel to DistributedDataParallel, and ensuring correct device guard and dispatch key annotations for operations like `linalg_eigh` and `lu_solve`. There are also issues with compatibility and validation, such as undefined behaviors with empty tensors, softmax stability, and the need for additional tests or deprecation warnings. Overall, the key concerns involve correct, efficient tensor handling across devices and data types, proper API support for new features (like complex tensors), and maintaining the robustness and clarity of the code and testing infrastructure."
2021-07-01,pytorch/pytorch,"The discussions highlight several ongoing technical concerns: 1) Compatibility issues with protobuf versions and build configurations in PyTorch, with suggestions for custom protobuf builds; 2) Challenges in extending PyTorch's linear algebra functionalities to include LAPACK routines, including implementation details and API design; 3) Problems related to distributed communication backends like NCCL and Gloo, especially network configuration and connectivity issues on GPUs; 4) Compatibility and performance issues with features like autocast, CUDA RPC, and complex tensor support, including in-place operation handling and device synchronization; 5) CI build failures and test inconsistencies due to various reasons such as unsupported ops, missing annotations, and environment misconfigurations. Suggestions involve adding thorough tests, improving API consistency, addressing compatibility and performance regressions, and refining build and test infrastructure."
2021-07-02,pytorch/pytorch,"The discussions mainly focus on the development and API design of a ConvLSTM layer in PyTorch, with considerations for API simplicity versus flexibility, and whether to support hyperparameter flexibility for input and hidden states. There are various technical issues raised, including hardware compatibility and performance concerns (e.g., power supply sufficiency for GPUs), environment setup problems, and build failures related to compiler and platform specifics. Additional concerns include implementation details in complex internal systems, such as support for new data types (e.g., posit, bfloat16), and the handling of model tracing, storage, and serialization issues. Several discussions also address CI/CD pipeline efficiency, testing coverage, and compatibility with external libraries or hardware (like ONNX, NNAPI, and InfiniBand drivers). Unresolved questions include how to properly handle special argument values like `None`, and whether certain features should be supported directly in core APIs or via forks or extensions."
2021-07-03,pytorch/pytorch,"The discussions highlight ongoing efforts to improve hardware support and performance for PyTorch on Apple Silicon, emphasizing the need for better GPU acceleration leveraging Metal, and noting TensorFlow's support via tensorflow-metal as a potential model. There are technical queries about integrating similar acceleration in PyTorch, especially regarding the transition from MLCompute to Metal on macOS. Other comments address compatibility issues with CUDA, ONNX export constraints (notably statelessness and handling of state-modifying modules), and support for NNAPI on Android, including backend linkage limitations. Additionally, several issues pertain to CI build failures and test stability, alongside API and runtime errors such as ""miopenStatusInternalError"" and profiling-related hangs, indicating active debugging and infrastructural improvements. Unresolved questions include how to effectively support current and future Apple Silicon machines, and how to bridge compatibility gaps across diverse hardware and software configurations."
2021-07-04,pytorch/pytorch,"The discussions mainly revolve around serialization and internal layout preservation of mkldnn tensors, querying whether all opaque tensors can be serialized similarly, and how this affects model inference optimizations in torch.save and torch.load. There are concerns about potential deadlocks or hangs in distributed data parallel (DDP) training, especially when using tensorboard or multiple nodes, possibly related to TensorFlow integration or threading issues. Several issues address performance bottlenecks on specific hardware configurations, such as slow training on 16-core machines, with suggestions to improve CPU-based COO to CSR conversion using multi-threaded implementations or CUDA support. Other topics include build configuration problems, environment mismatches, and CI failures, with some questions about reproducibility and environment setup, as well as potential renaming from ""differential"" to ""differentiable"" for clarity. Overall, unresolved questions include ensuring correct environment setup, avoiding deadlocks in distributed settings, and improving tensor serialization and conversion performance."
2021-07-05,pytorch/pytorch,"The discussions primarily revolve around ongoing development and integration of advanced features in PyTorch, such as merging BatchNorm into convolution layers for inference, expanding padding options in RNNs, and enhancing low-level LAPACK bindings for linear algebra routines. Several issues address hardware compatibility and stability, including NCCL CUDA errors and segmentation faults related to DataLoader on specific GPU setups, with requests for minimal reproducing scripts. There is also discussion on the process for adding research-based optimization algorithms (like QHM and QHAdam) into core PyTorch, with a focus on community adoption and citation evidence, leading to PR closures. Additionally, some questions concern serialization of MKLDNN tensors, ONNX sequence input support, and build configuration compatibility across different compiler and system environments. Overall, unresolved technical challenges include hardware-specific errors, ensuring correctness and performance of low-level linear algebra routines, and establishing clear contribution pathways for new features."
2021-07-06,pytorch/pytorch,"The comments reflect a range of technical discussions, including deprecated APIs, proposals for new or generalized APIs for conditional normalization layers, and considerations for merging batch norm parameters into preceding layers for inference speedup. Several issues involve handling of specific operations like `torch.norm`, `scatter_add`, and `torch.mm`, often with performance implications or platform-specific behaviors. There are questions about supporting soft labels in loss functions (`CrossEntropyLossWithSoftLabels`) and the best ways to extend or refactor existing class hierarchies (e.g., Dataset types or dispatch key enums). Additionally, many discussions concern system-specific problems such as CUDA memory management, compiler support, build failures, and CI pass rates, alongside a need for clearer coding standards, proper test coverage, and internal workflow clarifications."
2021-07-07,pytorch/pytorch,"The comments reveal issues related to process management in PyTorch, such as the importance of wrapping code within `if __name__ == '__main__':` when using multiprocessing with `spawn` or `fork`, especially on Windows and in environments like Google Colab. Several discussions focus on the challenges of exporting and training TorchScript modules, highlighting the need to support mode switching (`train()`/`eval()`) and the compatibility with torch.jit.script vs torch.jit.trace, including how to properly handle complex model modules and device placement, particularly on Windows and with different runtime configurations. Other issues concern serialization/deserialization nuances across various backends (e.g., ONNX, XLA, Metal), and the importance of API stability and backward compatibility when modifying models, as well as correctness validation with tools like `collect_env.py`. Overall, the key unresolved questions involve ensuring proper environment setup for multiprocessing, effective serialization mechanisms for different deployment scenarios, and the development of robust testing and validation procedures for model export/import workflows across diverse hardware and backend targets."
2021-07-08,pytorch/pytorch,"The comments reveal ongoing discussions and concerns about multiple topics: (1) challenges in supporting soft labels in loss functions such as `CrossEntropyLoss`, with suggestions to support it either via a new loss class or extending existing ones, emphasizing clarity and usability, (2) issues with ONNX export support for operators like `broadcast_tensors`, where some operators are unsupported or require operator rewriting, (3) the need for better testing and validation of distributed operations like `all_reduce`, especially to ensure correct gradient aggregation and avoid performance pitfalls or incorrect behaviors, and (4) practical considerations around API design, including the use of in-place operations, optional tensor arguments, and function signatures, as well as infrastructure and CI/test management concerns. Several comments suggest that some current features or behaviors are suboptimal, with proposals for API improvements, better documentation, or workarounds, but many unresolved questions remain about implementation details, compatibility, and best practices."
2021-07-09,pytorch/pytorch,"The discussions primarily revolve around handling multiprocessing and data loading issues in PyTorch across different environments, especially Windows and distributed settings, emphasizing the importance of proper `if __name__ == '__main__'` guards and `freeze_support()`. Several contributors highlight the need for better, prebuilt data loader support for production use, especially in multi-GPU distributed training, contrasting PyTorch's simplicity with more complex solutions like Uber's Petastorm. There are ongoing concerns about the slow download and installation of PyTorch, especially on older hardware or via conda, with suggestions to improve installer efficiency and better handle dependencies and compatibility issues. Multiple issues address internal code quality, refactoring large monolithic files into smaller, modular components for maintainability, and improving APIs to match evolving standards like the NumPy API, especially for functions like `min`, `max`, and their variants. Unresolved questions remain about CUDA version support, performance regressions on specific hardware like RTX 3090, and the best approaches to maintain backward compatibility and feature enhancements without breaking existing workflows."
2021-07-10,pytorch/pytorch,"The discussions chiefly revolve around addressing CUDA out-of-memory errors in PyTorch training, with suggestions to reduce batch sizes or modify model inputs to mitigate memory consumption. Several comments indicate attempts to resolve these issues by adjusting batch size, input scaling, or model components, but persistent errors suggest these measures are insufficient. There is also mention of the importance of efficient model recomputation and move operations, with questions about the utility and implementation of parallel modules, and concerns about performance impacts of using lower-precision BF16 calculations. Additional issues involve dependency management with `poetry`, compatibility of Torchvision with system environments, and general CI failures, complicating stable development and deployment. Unresolved questions remain about optimal approaches to memory optimization, the practicality of certain API changes, and ensuring compatibility across various hardware and software configurations."
2021-07-11,pytorch/pytorch,"The discussions primarily address issues related to resource management and compatibility in PyTorch, notably the ""Too many open files"" error on Mac OSX, which can be mitigated by changing the multiprocessing sharing strategy to 'file_system'. There are concerns about proper implementation of subclassing support, especially regarding pickling of `torch.Tensor` subclasses, with questions on how to correctly implement `__reduce_ex__` and `__torch_function__` for compatibility and serialization. Several issues also highlight operational challenges with GPU memory balancing, especially after checkpointing or changing batch sizes, and the impact of different hardware (e.g., M1, K80, P4) on PyTorch performance and support. Additionally, some discussions touch on managing CI failures, code merging processes, and the integration of external functions (e.g., SciPy special functions) into PyTorch. Overall, key themes involve resource handling, subclassing mechanics, hardware support, and code maintenance."
2021-07-12,pytorch/pytorch,"The comments highlight ongoing efforts to extend PyTorch's internal operator definitions, such as overriding existing operators, and support for custom scalar types like bfloat16, with native GPU acceleration considered preferable. Several discussions focus on the implementation and documentation of tensor operations, including slicing methods like `narrow` and the behavior of `Tensor` constructors versus `tensor`, especially concerning deprecation and type promotion issues. There is significant attention to distributed training concerns, including debugging collective operations, process group initialization, and proper barriers, especially around DDP and elastic training workflows. Additionally, numerous requests and suggestions involve improving APIs for tensor sharding, parallel iteration, and model rewriting for distributed and pipeline parallelism, as well as addressing CI and build issues related to compiler support and internal infrastructure. Unresolved questions include compatibility considerations for sparse and strided tensors, and how to best implement features like transpose as views for sparse layouts."
2021-07-13,pytorch/pytorch,"The comments highlight several key issues: (1) Build and linking errors related to ABI compatibility when compiling PyTorch with specific compiler and environment configurations, especially involving C++ ABI flags and dependencies like Gloo, NCCL, and gloo-specific functions. (2) Challenges with multiprocessing and distributed training—particularly models getting stuck or hanging when loading models in parent versus child processes, with suggested workarounds involving model loading order or restructuring to avoid shared states. (3) Compatibility and performance issues with CUDA versions, driver versions, and specific GPU architectures, leading to hangs, slower performance, or build failures, especially concerning newer GPUs or CUDA 11.x support. (4) Internal development concerns such as integrating features like nan-checking in C++, ensuring proper documentation, and handling codegen or fuse-related bugs. (5) Ongoing issues with testing infrastructure, environment setup, and internal CI failures, alongside suggestions for better API design and handling of complex data types or graph transformations."
2021-07-14,pytorch/pytorch,"The discussions mainly revolve around enhancing PyTorch's features and resilience, with some emphasizing technical limitations and implementation details. Key concerns include: the need for supporting ""padding='same'"" and ""padding='same_minimal'"" for strided convolutions, especially in relation to TensorFlow compatibility; improving label smoothing loss implementations for canonical and canonical-like distributions; addressing numerical instability and behavior in matrix multiplication and CUDA operations, such as nondeterministic errors and precision discrepancies; and supporting new data types like Posit and bfloat16 within PyTorch’s internal architecture, including operator support, serialization, and build requirements. Unresolved questions involve how to best integrate and test these features (e.g., proper handling of out-of-place parameters, enabling deterministic algorithms) and how to manage internal code constraints (such as C++ standard version compatibility and build system modifications)."
2021-07-15,pytorch/pytorch,"The discussions mainly revolve around handling complex challenges in PyTorch's development: ensuring reproducibility with deterministic settings across various operations (e.g., interpolation, upsampling), managing non-deterministic behaviors introduced by certain operations or hardware (GPU architectures, CUDA versions, tensor shape inference), and addressing issues with data handling such as batching None values or dealing with sparse tensor multiplication inconsistencies. Several suggestions are proposed, including modifying API design (e.g., adding `copy` parameters, redefining tensor constructors), implementing custom backends or wrappers to catch illegal operations in composite modes, and updating documentation to clarify behaviors and limitations. Additionally, some discussions focus on build and environment issues, like CUDA compatibility, compiler flags, and build system configurations, which influence runtime stability and performance. Unresolved questions include how to reliably detect illegal calls in custom/autograd code, how to reconcile NumPy’s and PyTorch’s differing behaviors over empty reductions, and how to best integrate new operators or features (e.g., support for special functions, improved deterministic algorithms) into existing infrastructure."
2021-07-16,pytorch/pytorch,"The discussions mainly revolve around issues related to ensuring correct and efficient implementation of tensor reshaping and view semantics in PyTorch. Key concerns include proper handling of gradient propagation through views, especially when delegating to internal operators or custom implementations for performance gains. There is debate over whether to extend existing APIs or create new internal operators, with an emphasis on maintaining autograd correctness and performance benchmarks. Some discussions address the potential for inadvertent backend or device-specific incompatibilities, such as with CUDA, XLA, or nested tensors, and how to test and prevent incorrect behavior. Unresolved questions include how to efficiently catch all invalid tensor operations in user-defined or composite autograd functions, and how to best structure API and internal code to balance maintainability, correctness, and speed."
2021-07-17,pytorch/pytorch,"The discussions primarily focus on clarifying the API design for the `scatter` function, highlighting the conceptual inconsistency between `reduce=` options that imply element-wise versus reduction operations, and debating whether to provide separate functions like `scatter_add` or to extend `scatter` with mode parameters such as `elementwise-op` and `reduction-op`. There's also mention of maintaining backward compatibility while exploring a more consistent and extensible API, possibly deprecating current usage. Several issues address specific bugs or implementation details, such as handling tensor dtype mismatches, improvements to `index_select`, and supporting complex number convolutions via FFT, with suggestions ranging from code refactors to backend API support. Additionally, operational edge cases, such as handling zero-sized tensors or device-specific concerns in distributed or hardware-accelerated contexts, are discussed, along with CI failure summaries and stability considerations for certain environments."
2021-07-18,pytorch/pytorch,"The discussions highlight ongoing limitations and inconsistencies in PyTorch's support for specific operations, such as `index_add_` not supporting scalar sources and broadcasting, and the default behavior of `OneCycleLR` not aligning with the original paper's three-phase cycle, leading to confusion. There are concerns about documentation accuracy, particularly regarding stable sorting on GPU, which has been addressed via PR updates. Some issues involve performance degradations, like significant slowdown when converting NumPy arrays to tensors, and compatibility challenges with different compute backends such as Vulkan and CUDA. Additional questions focus on error handling, like runtime errors during SVD on GPU versus CPU, and ensuring proper input validation to avoid NANs in embeddings. Overall, the discussions emphasize the need for clearer API semantics, better documentation updates, and resolving backend compatibility and stability issues."
2021-07-19,pytorch/pytorch,"The discussions encompass a variety of technical concerns and suggestions, including: (1) improving documentation and search engine snippets, such as rearranging docs structure and microformats; (2) refactoring internal APIs for distributed primitives like `allreduce_coalesced`, considering their implementation and backend performance optimizations (e.g., NCCL group calls); (3) addressing specific bugs and behavior inconsistencies in functions like `torch.arange`, `tensor()`, `mean`, and `result_type`, especially regarding dtype promotion and support across different hardware and data types; (4) handling API changes and backward compatibility, particularly with optional arguments like `out`, and their impact on existing models; (5) dealing with CI failures, build environment issues, and integration of external tools, including deprecation warnings, compiler flags, and build configuration nuances. Many unresolved questions remain about API design choices, performance trade-offs, and precise scope of backward compatibility in the face of ongoing development changes."
2021-07-20,pytorch/pytorch,"The comments highlight several recurring technical concerns: (1) Difficulties with proper setup and compatibility of PyTorch with virtual environments, Jupyter, and specific libraries like MKL, especially under different operating systems and environments such as WSL. (2) Challenges with exporting models to ONNX, especially in complex models like Tacotron2, which involve tracing, scripting, and ensuring correct dynamic behavior. (3) Implementation challenges related to internal PyTorch features such as `view` semantics, autograd correctness, and performance optimizations, including shape propagation, type promotion, and API BC-breaking issues. (4) Debugging and CI-related failures often caused by environment inconsistencies, missing dependencies, or specific hardware/software configurations, with discussions on how to better document or fix these issues. (5) Broader questions about future API designs, such as async process group initialization, multi-device support, and how to adapt complex features like `vmap`, `nan_policy`, or distributed communication primitives for efficiency and usability."
2021-07-21,pytorch/pytorch,"The discussion covers multiple topics related to PyTorch, including the proper placement of `if __name__ == '__main__'` blocks in scripts to prevent errors like `BrokenPipeError` when using multiprocessing, and the potential need for a `nan_policy` argument to handle NaNs in reduction operations, aligned with SciPy/Numpy conventions. There are questions about model exporting support, especially for complex RNN modules like `LSTMCell` to ONNX format, and the handling of model serialization in C++. Additionally, some discussions focus on improving API consistency for functions like `scatter_()` and the implementation of features like `allgather` with coalescing, as well as addressing various CI failure reports, build breakages, and code refactoring proposals for better code organization and performance, such as template-based device dispatch. The unresolved questions include whether certain features (e.g., `LSTMCell` export support) are planned, how to properly implement NaN policies, and how to handle large tensor operations or custom kernels efficiently across different platforms."
2021-07-22,pytorch/pytorch,"The comments span a wide range of issues in the 'pytorch/pytorch' repository, including memory management problems, performance bottlenecks, and feature support gaps. Several discussions focus on debugging and fixing errors related to DataLoader configurations, distributed training (DDP), and GPU utilization, often suggesting code modifications or workarounds such as setting environment variables or adjusting code structure. Issues with ONNX export compatibility, library dependencies, and build system intricacies are also highlighted, with some concerns about CI failures and reproducibility. There is ongoing interest in enhancing usability features, such as better error messages, support for complex tensors, and improvements in model serialization and tracing. Unresolved questions include compatibility with specific hardware, the impact of certain fallbacks on performance, and how best to integrate new functionalities like auto differentiation or tensor typing in the graph."
2021-07-23,pytorch/pytorch,"The discussions predominantly revolve around performance and support limitations in PyTorch, such as issues with mixed-precision floating point operations on certain backends (e.g., MAGMA, LAPACK not supporting float16), and the challenge of implementing support for sparse matrix inverse and custom autodiff operations. There are concerns about numerical precision loss when downcasting in fp16, as well as questions on interfacing and API design, such as how to handle user-defined backward functions, and improving user experience with test logging and CI reporting. Several threads address compatibility issues with CUDA versions, driver support, and the need for better documentation and robust error handling in distributed and CUDA-related contexts. Additionally, there are discussions about modifying internal APIs, error handling in distributed training, and clarifications in documentation, especially regarding types like torch.Size and tensor initialization. Unresolved questions include when certain features (e.g., custom autograd support, allreduce coalescing) will be available, and how to balance API stability with flexibility and performance."
2021-07-24,pytorch/pytorch,"The discussions primarily address limitations in PyTorch's support for sparse matrix operations, such as the unsupported sparse-sparse matrix multiplication (WIP in PR #39526), and the need for enhanced functionalities like `expm1()` on sparse tensors (PR #46914). There are technical challenges in building and linking the library, notably the ambiguous multiple implementations of `cusparseGetErrorString` across different CUDA versions and the visibility issues of functions like `torch::jit::mobile::Function::qualname()` due to missing `TORCH_API` annotations, affecting internal vs external linkage. Additionally, users experience hardware compatibility issues, such as CUDA kernel errors on older GPUs like GTX 730, and environment-specific problems, including rate-limiting of Torch Hub downloads and CI build failures related to missing function definitions or platform support. Suggestions include improving support for new hardware architectures, clarifying API impacts on custom classes, and interoperation with iOS/M1 devices using Metal or CoreML, indicating ongoing efforts to expand functionality and ensure compatibility."
2021-07-25,pytorch/pytorch,"The discussions primarily revolve around CUDA compatibility and support issues, especially concerning older GPU architectures like Kepler (compute capability 3.5), with users experiencing runtime errors such as ""no kernel image is available"" when running newer PyTorch builds against outdated hardware. There are suggestions to modify build configurations, such as setting `TORCH_CUDA_ARCH_LIST`, and rebuilding PyTorch with custom patches or specific versions to support these older devices. Several users express concerns about the limitations of current implementations, particularly with inhomogeneous `total_count` in `torch.distributions.Multinomial` and how weights affect loss computations and training stability. Additionally, some discussions focus on code maintenance, such as exposing internal functions like `torch._aminmax` for broader use, and addressing performance improvements and CI failures through updates and merges. Overall, unresolved questions include ensuring adequate support for legacy hardware, optimizing gradient and loss calculations with weights, and maintaining API consistency and usability."
2021-07-26,pytorch/pytorch,"The comments reveal several recurring themes: (1) Users frequently encounter environment and compatibility issues, especially on macOS concerning compiler support, CUDA, Xcode, and proper configuration, with solutions often involving environment resets, specific command invocations, or system upgrades. (2) Many discussions involve intricacies of PyTorch's build and installation processes, including handling different versions, dependencies like NCCL or cuDNN, and custom extension mechanisms, with some proposing or noting the need for better, more maintainable build tooling. (3) There are unresolved questions about PyTorch functionalities such as backward compatibility in tensor operations, handling multiple outputs in C++ API, and extending autograd for custom operations, with suggestions to update or introduce new APIs and mechanisms. (4) Several comments raise issues around efficiency and resource management, including memory leaks during distributed training, CUDA-related compilation errors due to unsupported GPU architectures, and potential divergence in default behaviors for functions like loss reduction. (5) Overall, the discussions suggest ongoing efforts to improve developer experience, system compatibility, and feature robustness, yet many core technical questions, particularly about build environment support, custom extensibility, and numerical stability, remain open."
2021-07-27,pytorch/pytorch,"The comments highlight ongoing issues related to DataLoader workers crashing with invalid pointer/free errors, often linked to multi-processing or threading concerns such as improper sharing strategies, num_workers settings, and environment configurations (e.g., in Docker or Jupyter). Several users suggest setting `num_workers=0` as a workaround, with others exploring strategies like sharing memory settings (`set_sharing_strategy`) and installing dependencies differently (e.g., using pip instead of conda). There is also mention of potential improvements in the internal PyTorch APIs, such as supports for `*_coalesced` operations, and discussions about system-specific compatibility issues such as GPU compute capability and build environment constraints. While some issues are resolved or mitigated through configuration changes or updates, unresolved questions remain about underlying bugs (like memory leaks in distributed RPC or threading), compatibility with newer hardware or software versions, and feature implementations (e.g., sparse tensor handling, ONNX operator support)."
2021-07-28,pytorch/pytorch,"The comments span a wide range of issues, primarily related to PyTorch's internal API, debugging, and compatibility challenges. Many discuss specific bugs and their workarounds, such as CUDA/CPU memory leaks, tensor subclassing, and operator registration issues, often highlighting the complexities of PyTorch's autograd, JIT, and distributed systems. Some threads focus on improving build performance, code generation, and serialization consistency, with suggestions for API revisions, deprecation strategies, and better tooling like ASAN or static analysis. Several entries mention environment-related problems, like CUDA version mismatches or inter-process communication inefficiencies, as well as ongoing efforts to improve documentation and test robustness. Overall, the discussions reflect active troubleshooting, ongoing API evolution, and the community’s efforts to enhance stability, performance, and usability."
2021-07-29,pytorch/pytorch,"The discussions highlight several technical concerns, including the semantics and behavior of sparse tensor operations (e.g., element-wise multiplication and addition) and their compatibility with different layouts such as COO or strided tensors. Several issues revolve around the efficiency and correctness of tensor serialization (`tobytes`, `memoryview`) and index performance, especially scalar indexing across CPU and GPU tensors. There are questions about extending type annotations for tensor shapes and adding shape information in type hints to optimize code and aid in static analysis, with ongoing work on symbolic shape support. Debugging and error handling concerns are raised, notably around CUDA initialization, backend compatibility (like with TensorRT and ONNX), and the impact of reference cycles or non-deterministic behaviors on garbage collection and tensor subclassing. Lastly, numerous CI pipeline failures, build system questions, and potential regressions related to specific operator support (e.g., `GatherElements`, `BatchNorm`) and build configurations (like NCCL, CUDA, and linking) are discussed, reflecting ongoing stability and compatibility challenges."
2021-07-30,pytorch/pytorch,"The discussions highlight several key concerns, notably issues arising from different ways tensors, especially custom subclasses and views, propagate and support autograd and forward AD, with specific attention to correct gradient and storage handling. There's ongoing work to improve reproducibility and debugging in distributed training, such as better handling of interleaved logs, leak detection, and stable test behaviors. Compatibility and performance across different hardware and compiler environments (e.g., CUDA, ROCm, MSVC, GCC, Windows) are also a significant focus, including fixes for build errors, device-specific optimizations, and support for new tensor types like bfloat16. Additionally, there's interest in refining the design of operator and API behaviors, e.g., handling of optional parameters, consistent behavior for scalars/tensors, and better community integration via TorchHub and model/documentation support. Unresolved questions remain around optimizing distributed memory management, supporting advanced features like view support in autograd, and enhancing testing frameworks for robustness."
2021-07-31,pytorch/pytorch,"The discussions highlight ongoing challenges with achieving high performance for deep learning kernels, particularly GEMM and convolution operations, on opencl and non-CUDA platforms, with efforts to improve kernel efficiency through custom implementations and backend extensions. There is concern about the correct and safe use of tensor in-place modifications and autograd, especially regarding the use of `.data` versus `detach()`, emphasizing that `.data` can lead to incorrect gradient calculations if tensors are modified without proper version checks. Environment configuration issues, such as setting CPU types for MKL on AMD/Intel systems, and build system nuances for hardware like ROCm on various architectures, are also prominent. Furthermore, there is interest in refining code structure, including const-correctness and move semantics, to optimize performance and code safety, alongside discussions about integrating features like checkpointing and ensuring compatibility across hardware and software environments."
2021-08-01,pytorch/pytorch,"The discussions primarily focus on technical challenges related to PyTorch and its ecosystem, including environment setup issues such as module not found errors with torchvision in conda environments, and dependency management complexities. Several comments highlight the need for better API consistency, such as deprecating or adjusting the `return_complex` argument in `stft`, to ensure compatibility across versions. There are ongoing efforts to enhance features like automatic differentiation, with concerns about performance, correctness, and integration, suggesting potential refactoring, API revisions, or the need for new utilities. Some discussions involve debugging CI failures, compatibility issues across different hardware and software configurations, and the necessity for clearer testing and validation strategies. The overarching theme is improving robustness, user experience, and maintainability in PyTorch's development process."
2021-08-02,pytorch/pytorch,"The discussions highlight several recurring issues and areas of ongoing development within PyTorch. Key concerns include compatibility and exportability of models with custom C++ extensions and features like spectral normalization, which are sometimes unsupported in ONNX or require specific operation support (e.g., `dot`). There are challenges related to model tracing and scripting performance, especially for large models or complex control flow, where specific optimizations or new APIs are desired. In addition, there's interest in improving the user experience around model graph visualization, debugging, and test infrastructure, along with infrastructure updates for CI, build processes, and support for different hardware targets (e.g., iOS, CUDA). Unresolved questions involve refining the API for new features, handling backward compatibility, and expanding support for complex data types (like `return_complex` in STFT) while maintaining stability and usability."
2021-08-03,pytorch/pytorch,"The discussions highlight several key technical concerns: implementing matrix functions such as sqrt and logm on GPU is challenging due to the lack of direct support for decompositions like Schur; there is ongoing work to adapt iterative algorithms for GPUs to approximate these functions efficiently. There's an emphasis on improving tensor operations, such as constructing block diagonal sparse matrices more efficiently, with suggestions to optimize indexing methods for performance gains. Additionally, issues with PyTorch's integration and build systems are raised, including dependencies on external libraries (like libtorch_python), and concerns about improving build configuration and documentation clarity for features like distributed training, autograd, and operator behavior. Finally, there are specific questions about the implementation details, best practices for experimental features, and ways to make documentation and user debugging easier, such as through better error messages, test logging, and API support for custom classes and control flow in JIT and FX."
2021-08-04,pytorch/pytorch,"The discussions revolve around several technical issues: the support and implementation of model export and ONNX compatibility, especially for models containing dictionaries or non-supported data types; challenges with multi-GPU parallelism involving batch size restrictions and data scattering strategies in `DataParallel`; and performance considerations for custom CUDA kernels such as `sort`, `addmm`, and `adaptive_avg_pool2d`, including how to handle unstructured memory layouts and optimize kernel behavior for small tensors. Additionally, there are questions about backend integration (e.g., ONNX Runtime, Triton), and managing the complexity of extending sparse tensor operations and distributed training infrastructure on Windows and with various hardware backends. Proposed solutions include modifying scatter logic, adding error handling, and deferring structural changes to future versions, with ongoing investigations into kernel performance and API design challenges. Unresolved questions include how to best support large-span uniform sampling, manage non-structured tensor outputs while preserving performance, and streamline multi-backend/distrubuted environment configurations."
2021-08-05,pytorch/pytorch,"The discussions highlight persistent issues with the `ModuleNotFoundError` for `torch._C`, often stemming from environment misconfigurations, incorrect package installations, or path-related conflicts. Several contributors are troubleshooting compatibility problems on Windows, macOS, and Linux, emphasizing the importance of ensuring the correct virtual environment is active during installation. There are suggestions to modify or bypass certain build files (e.g., `Dependencies.cmake` and `FindMKL.cmake`) to support alternative libraries like PocketFFT, especially for mobile builds, but challenges remain due to internal build system restrictions or missing dependencies. The community also raises questions about API design choices, such as handling multiple signatures for `torch.max` and supporting multi-dimensional reduction operations, with some proposing clearer, more consistent approaches. Unresolved issues include reproducibility variability on different hardware, build configuration intricacies, and how to properly expose advanced features without breaking backward compatibility."
2021-08-06,pytorch/pytorch,"The comments highlight various technical challenges and proposed solutions within the PyTorch ecosystem. Key concerns include handling memory leaks during data loading, particularly with multi-worker DataLoader configurations, and methods for efficient data loading like zero-copy approaches with PyArrow. Several issues address compilation/build problems on specific hardware (like CUDA architectures and cuDNN versions) and compatibility in different environments (e.g., Windows, WSL). There are ongoing discussions around API design improvements, such as supporting keyword-only arguments, unifying tensor types, and adding features like `nn.Buffer`. Unresolved questions involve fixing internal build errors, ensuring backward compatibility, and improving CI robustness to prevent flaky tests."
2021-08-07,pytorch/pytorch,"The discussions primarily focus on simplifying device management in PyTorch data loading and training loops, with suggestions to automatically load data onto a designated device, such as via a `device` argument in `DataLoader` or customizing the `collate_fn`. There are concerns about the safety and efficiency of moving data to GPUs in multiprocessing contexts, with warnings about potential issues when returning CUDA tensors in subprocesses. Several users seek official or recommended approaches to seamlessly transfer data to devices without manual `.to(device)` calls, with workarounds involving subclassing `DataLoader` or modifying `collate_fn`. Additional topics include compatibility issues with newer GPUs like RTX 3090, specific bugs in JIT compilation, and troubleshooting installation errors, especially related to CUDA and environment configurations. Unresolved questions remain about the best official practices for device placement in data pipelines and handling of multi-GPU, multi-process loading scenarios."
2021-08-08,pytorch/pytorch,"The discussions primarily revolve around issues of reproducibility, device management, and API consistency in PyTorch. There is a proposal to introduce a `torch.set_default_device` function to unify device selection semantics. Concerns are raised about the BC-breaking implications of fixing argument order discrepancies between module and functional forms, with suggestions to either modify documentation or implement warnings. There is also mention of build configuration challenges for mobile deployment, particularly regarding missing or misconfigured flags like `USE_LAPACK` and `BUILD_LITE_INTERPRETER`. Overall, the discussions highlight the need for systematic, user-friendly solutions to device handling, API consistency, and build reproducibility concerns."
2021-08-09,pytorch/pytorch,"The comments highlight ongoing issues related to gradient computation near zeros or infinities, especially in the context of masking, masking strategies, and numerical stability, with discussions on how to better distinguish between ""zero gradient"" and ""no gradient"" at element-wise levels. Several discussions focus on CUDA, GPU support, and environment configuration issues, such as sharing strategies, driver/library compatibility, and performance optimization, including the behavior of specific operators (e.g., `adaptive_avg_pool2d`, `matmul`) across different hardware and software versions. There are also various technical questions about internal PyTorch features, such as support for complex numbers in optimizers, tracing and scriptability, and specific operator support (e.g., `max_unpool2d`, `roll`). Many comments concern build, export, and deployment issues, including errors with dependencies, build scripts, and environment setup, as well as integration with internal systems like CI/CD, Phabricator, and internal dependencies. Unresolved questions persist around support for features like asynchronous CPU execution, variadic functions, and improving user diagnostics for failed operations."
2021-08-10,pytorch/pytorch,"The discussions predominantly revolve around hardware and software bug diagnostics, including issues with NCCL and CUDA (e.g., NCCL errors with NCCL 2.7.8, NCCL_P2P_LEVEL configurations, and NVIDIA GPU interconnects), as well as stability concerns with PyTorch distributed training, JIT tracing hangs, and potential memory leaks. There are multiple concerns about API consistency, especially with PyTorch scatter operations, scatter APIs, and API design for new features like sharded tensors, as well as discussions on API stability and backward compatibility for API changes (e.g., argument order in max pooling, distributed autograd cleanup, and module state dict handling). Some comments address experimental features, e.g., support for Metal API on M1, TP/sharding API designs, and support for asynchronous CPU ops, along with integration with external tools and platform-specific issues (like ROCm support and compiler issues). Several discussions provide workarounds, bug fixes, and proposals for API improvements or new features, often with an emphasis on correctness, performance, and user experience. Finally, questions regarding CI/test failure diagnostics, environment setup, and the proper procedures for contributions, such as licensing and repository management, are also raised."
2021-08-11,pytorch/pytorch,"The discussions highlight several technical concerns, including the desire to enhance support for lists, loops, and derivatives within autograd, with questions about symbolic differentiation for these constructs. Environment setup and compatibility issues are prominent, such as building PyTorch from source with specific hardware (e.g., ROCm, GPUs) and resolving build errors related to dependencies and platform-specific configurations. Users inquire about the treatment of complex tensors in optimizers, debating between treating complex parameters as tuples of real numbers versus using `z * z.conj()`, with considerations about preserving properties like basis invariance and optimizer behavior. There are also recurring issues with runtime errors, build failures, and the need for better testing, documentation, and diagnostics to ensure correctness and stability across diverse environments and features. Overall, unresolved questions include handling of special cases in numerical kernels, the evolution of tensor serialization, and the integration of new features like custom autograd, TorchScript extensions, and advanced differentiation support."
2021-08-12,pytorch/pytorch,"The comments span a variety of technical topics including: efforts to improve code architecture and maintainability (e.g., managing conditional module attributes, reworking function behaviors to be more consistent with standards like Array API or NumPy), performance optimization (e.g., fixing JIT compilation regressions, reducing kernel launch overhead, and benchmarking different implementations), and compatibility/builder issues (e.g., building ROCm support for gfx803 cards, which requires specific build configuration adjustments). There are recurring questions about deprecated features, backward compatibility, and feature support extensions (e.g., ONNX export support, metal GPU support on M1, and handling of custom types). Several discussions focus on debugging and diagnosing failures (e.g., distributed RPC memory leaks, CUDA/NCCL errors, shape inference in TorchScript, and potential runtime/backend bugs), often with suggestions for profiling, benchmarking, or code refactoring. Overall, unresolved issues include the need for better support for certain hardware configurations (ROCm, Metal), improved testing strategies (reducing CI load, enhancing BC/FC tests), and proper documentation of behavior changes or limitations."
2021-08-13,pytorch/pytorch,"The discussions highlight several key technical concerns, including the ambiguity in the semantics of sparse tensor functionality and the need for clearer criteria when prioritizing feature development (Issue #3158). There are multiple questions around the proper handling of specific APIs and behaviors, such as the correct implementation and aggregation of `GradBucket` in distributed training (Issue #63212), or the support for complex numbers in functions like `einsum` (Issue #63152). Several issues involve refining or fixing existing features—such as the `ignore_index` behavior in `CrossEntropyLoss` (Issue #63074), or correctness and performance improvements in tensor operations and backends—along with implementation considerations like BC (Issue #63142). Additionally, there are ongoing challenges with test failures, environment dependencies, backend selection, and version compatibility that remain unresolved. Overall, the discussions reflect active development, debugging, and planning to improve robustness, clarity, and functionality in PyTorch's core features."
2021-08-14,pytorch/pytorch,"The discussions highlight challenges with dtype compatibility in PyTorch's `grid_sampler`, specifically when input and grid tensors have mismatched data types, with workarounds involving dtype conversion suggested. There are ongoing efforts to improve support for certain operations, such as `pixel_unshuffle`, with community contributions needing CLA agreements. Several CI failures and internal build issues are frequent concerns, often linked to specific commits, requiring attention to ensure stability. Questions about release versions and branch fixes, like whether the latest code is included in v1.9, indicate uncertainties about release management. Additionally, some technical features like oneDNN LayerNorm support are noted for limited functionality, requiring careful runtime handling."
2021-08-15,pytorch/pytorch,"The discussions highlight issues with supporting complex numbers in PyTorch, particularly in functions like `einsum`, where current support is limited, and in operations like eigenvalue calculations, which lack backward differentiation support for complex eigenvalues. There are concerns about CUDA support for complex number operations and whether recent changes (e.g., in sort and linear algebra routines) have introduced regressions or require additional checks, such as handling negative inputs in functions like `polar`. Several discussions address the need for better documentation, code comments, and user guidance for new features or known limitations, especially regarding hardware support like Apple’s Neural Engine and Metal backend on Mac M1. Additionally, issues related to library support (e.g., MAGMA initialization, bfloat16 support) and build configurations suggest ongoing challenges in extending PyTorch's hardware acceleration and precision support. Overall, key unresolved questions involve enhancing complex number support in core functions, ensuring hardware-specific optimizations, and clarifying API behaviors and limitations."
2021-08-16,pytorch/pytorch,"The discussions highlight issues with PyTorch installation and build support, particularly for specific hardware like ROCm (gfx803 support), and the need for better documentation or fixes in the build process. There are technical debates about the implementation of GLUs in understanding the input splitting and gating mechanisms, with clarifications about their mathematical operations and efficiency. Concerns regarding the correctness of gradient computations and the handling of data types (float32 vs float64) that can lead to NaNs, as well as the proper use of `is_last()` and `is_first()` in distributed training, are raised. Additionally, there are questions about the stability, testing, and CI coverage, especially related to new features, regressions, and internal workflows, suggesting a need for better documentation, comprehensive testing, and alignment with internal standards. Unresolved issues include build failures, compatibility with hardware/SDK versions, and ensuring consistent behavior across different environments."
2021-08-17,pytorch/pytorch,"The comments encompass a range of technical concerns primarily centered around PyTorch's serialization and data handling. Notably, there is discussion about low-bit quantization support, serialization compatibility between C++ and Python, and issues with loading model weights saved via `torch.save()`—highlighting challenges in deserialization workflows and backward compatibility. There are questions about the behavior of functions like `result_type`, differences in data type handling between Python and C++, and compatibility issues with TorchScript and ONNX exporters. Additionally, participants mention documentation clarity, the need for better API consistency (e.g., properties vs. functions), and CI/test reliability concerns. Proposed solutions include adding new properties, clearer documentation, and serialization workarounds, with some questions remaining about the optimal implementation strategies for these features."
2021-08-18,pytorch/pytorch,"The comments highlight ongoing development issues and proposals related to PyTorch features. Key concerns include the lack of low-bit (2-bit or 4-bit) quantized dtypes and the potential for low bit-width support, including adding a property for item width in bits. There are discussions around improving serialization support for JIT and custom modules, notably through `__reduce__` and `__getstate__`, to enable better multiprocessing and checkpointing. Performance improvements are also a focus, with proposals to optimize functions like `torch.stack` and `torch.cat`, and addressing kernel launch overheads. Additional questions involve correct implementation of certain operators, bug fixes for specific PyTorch functions, and better tooling for testing, skipping, and custom operator handling in external projects like functorch."
2021-08-19,pytorch/pytorch,"The discussions center around robust handling of corrupted data, optimizer behavior, and API design considerations. Key concerns include how to avoid training disruptions when encountering all-None batches—suggesting filtering None values before default collate or implementing batch-level filtering, especially for small batch sizes. There are questions about the sign convention in momentum terms for optimizers, referencing literature indicating potential sign inconsistencies in PyTorch's implementation. Issues related to tensor memory formats, especially in out-of-place tensor operations like `cat` and `resize_`, highlight the need for consistent, BC-breaking fixes and proper testing. Unresolved questions involve the best abstractions for custom kernels in lazy mode, the correct handling of sparse tensors with private methods, and the impact of API changes on reproducibility and performance."
2021-08-20,pytorch/pytorch,"The comments primarily revolve around clarifying implementation details and ensuring correctness in several areas. Notably, issues about the sign convention for momentum parameters in optimizers highlight the importance of consistent theoretical and practical behavior. Several discussions address the support for custom autograd functions, the behavior and naming of transposing and Hermitian operators (`H` vs `T`), and handling in-place tensor modifications to avoid layout contract violations. Other recurring themes include code stability and compatibility (e.g., reproducibility, backward compatibility, and correct tensor resizing), debugging and profiling results, and test validation, especially concerning precision modes like float16 and bfloat16. While some proposals suggest enhancements like new operator support or API adjustments, unresolved questions remain about implementation details, performance impacts, and usage semantics."
2021-08-21,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's tensor manipulation and autograd systems, especially around the `expand` function sharing storage and potential aliasing issues, and the need for warnings when pointwise kernels modify tensors with zero strides. There's a strong interest in porting SciPy's special functions, like `betainc` and `kv`, into PyTorch, including considerations for autograd support and differentiability with respect to parameters. Several questions concern the performance implications of new implementations versus existing functions like `constant_pad_nd` and how to optimize tensor operations, such as padding and `pad`, on different hardware (CPU, GPU, iOS). Issues with the integration of JIT, CUDA build configurations, and static initializers for backends (like XLA, Lazy) raise concerns about reproducibility and correctness, especially regarding cache invalidation and graph equivalence. Overall, there's a focus on improving tensor operation safety, extending functional support (special functions), and ensuring robust, efficient backend integration."
2021-08-22,pytorch/pytorch,"The discussions highlight several technical concerns around memory management and potential leaks in PyTorch, especially involving autograd, DataLoader, and GPU memory. Users report memory leaks when using create_graph=True, with some solutions involving shared memory strategies like PyArrow or numpy arrays, but these are not universally effective. There are ongoing issues with CUDA errors, including illegal memory accesses and initialization errors, often related to setup, driver, or hardware configuration. Additionally, there are feature requests for improved API support for large model handling (LMS) and invertible functions, aiming for better modularity and system compatibility. Several unresolved questions remain about the effectiveness of current fixes, platform-specific issues, and API developments to improve debugging, resource management, and user experience."
2021-08-23,pytorch/pytorch,"The discussions highlight concerns about memory management and debugging in PyTorch, such as memory leaks when using create_graph=True with autograd.grad, and the impact of anomaly detection mode on memory usage. Several questions also address improvements to the autograd API, particularly around support for invertible functions, memory format propagation, and easier implementation of invertible neural networks. There are technical details about potential API changes, including adding inverse() methods, better handling of custom backward functions, and introducing auxiliary functions like `slice_embed` and `select_embed`. Additionally, discussions mention issues with distributed training, CUDA error handling, and optimization passes like autocast and weight caching, with suggestions to improve robustness and usability. Unresolved questions include how to better integrate optional features like weight caching with JIT tracing and how to handle memory and performance trade-offs for data type support (e.g., bfloat16)."
2021-08-24,pytorch/pytorch,"The discussions highlight several technical concerns: support for interpolating integer (especially uint8 and bool) tensors is incomplete, with runtime errors for unsupported dtypes; there are ongoing issues with distributed training and process group cleanup, especially regarding higher-order gradients and GPU resource leaks; and there are performance implications of certain kernel implementations and automatic mixed precision (AMP) features, with debates on whether to disable cache or incorporate explicit knobs. Additionally, there are questions about handling CUDA streams for concurrency, as current testing indicates operations serialize rather than run concurrently. Some issues involve build environment compatibility, like CUDA version support and CMake configurations, particularly with new CUDA versions and dependencies like cuDNN. Lastly, the conversations include questions on PR workflows, code maintainability (e.g., using dataclasses.replace vs explicit method definitions), and the need for clearer documentation or test coverage for features like onnx export, distributed gradients, and custom operation support."
2021-08-27,pytorch/pytorch,"The discussions highlight significant challenges in implementing Matrix logarithm (`logm`) and square root (`sqrtm`) functions on GPU, primarily due to reliance on iterative algorithms and absence of direct GPU-friendly implementations like SciPy's. There is interest in developing iterative algorithms that can efficiently work on both CPU and GPU, with some contributors suggesting starting points using existing CPU-based SciPy functions wrapped in custom autograd functions. Several comments address the difficulties in ensuring consistent environment setups for nightly builds, emphasizing the importance of proper environment management for package versions. Additionally, there are concerns about certain deprecated or unused constructors in core libraries, leading to warnings, and a need for potential code cleanup or refactoring. Overall, key unresolved issues include creating GPU-compatible matrix functions and maintaining environment consistency across development setups."
2021-08-28,pytorch/pytorch,"The collected comments highlight several ongoing issues and concerns in the PyTorch ecosystem. Key points include recurring CUDA runtime errors such as illegal memory access, which may be hardware or driver related; discussions about spectral normalization and in-place operation issues affecting backpropagation; challenges with building PyTorch from source due to complex dependencies and inadequate documentation, especially for ROCm support; uncertainties regarding the behavior and API design of `torch.shape()` versus `torch.size()`; and questions about integrating AMP with JIT tracing, with proposed solutions involving API modifications and user control. Overall, the discussions emphasize stability, usability, and clarity improvements in the framework."
2021-08-29,pytorch/pytorch,"The discussions revolve around various technical challenges and feature requests in PyTorch, including addressing shared memory errors in multiprocessing and the impact of system limits, improving the robustness of autograd and AMP interactions, and adding support for specifying output shapes and dtypes in torch.bernoulli for consistency and usability. There are ongoing efforts to improve the performance of linear algebra operations, especially QR decomposition, and to optimize CUDA graph support for better efficiency. Several issues concern the correctness and stability of FP16/AMP training, with suggestions to control caching behavior and to document precision limitations of floating-point representations. Additionally, there are requests for contributions to new features like `chunk` behavior, linear algebra routines, and CUDA-based implementations, with some discussions on test coverage, performance, and merging priorities. Overall, unresolved questions include balancing system-level constraints, ensuring backward compatibility, and choosing design approaches for features like autocast behavior during JIT tracing."
2021-08-30,pytorch/pytorch,"The comments primarily address issues related to PyTorch's API and implementation inconsistencies, such as the need for explicit `alpha` and `beta` parameters in `addmm`, and the lack of documentation for `is_grad_enabled`. Several discussions concern data loading pitfalls, like shared memory strategies in DataLoader, CUDA graph support, and specific bug fixes in operators (e.g., `torch.linalg.svd`, `torch.matmul`) involving differentiability and numerical stability. Cryptic error messages and failures (e.g., segmentation faults, mismatched gradients, unsupported types) point to underlying bugs or API limitations that require better error reporting or more robust support, especially for complex and sparse tensors, FP16/BF16 mixed precision, and low-rank matrix operations. There are ongoing discussions about code cleanups, API deprecations, and improvements in testing practices, such as harnessing OpInfos or better CI workflows. Unresolved questions include how to improve or expose certain features (e.g., `native::is_grad_enabled`), and how to properly handle low-rank or low-precision computations to ensure correctness and stability."
2021-08-31,pytorch/pytorch,"The comments cover various topics, primarily focusing on enhancing PyTorch's functionality and addressing ongoing bugs or performance issues. Notable concerns include the potential addition of NumPy-like functions for efficient pooling over unordered data, device mismatches and memory leaks in distributed training, and the support for new operators or improved ONNX export capabilities. Several discussions also involve code maintenance challenges such as refactoring, API design considerations, and ensuring compatibility with different hardware (e.g., AMD CPUs, NVIDIA GPUs with NVLink, ROCm). Questions about test coverage, proper handling of non-tensor values in JIT, and establishing clear policies for warnings are also prevalent. Unresolved issues include performance degradation at large tensor sizes, complex shape-tracking in symbolic tracing, and ensuring correctness of functions like softmax in large dimensions, with proposed solutions like fallback algorithms, better documentation, and more robust testing."
2021-09-01,pytorch/pytorch,"The comments reflect a range of technical concerns in the PyTorch repository, including the handling of backward hooks in distributed training, support for new tensor formats like meta tensors, and issues with hardware failure detection (e.g., GPU faults). Several discussions involve improving or adding features, such as `__torch_function__` dispatch support for `torch.nn.init.*`, robust handling of tensor copying and metadata during serialization, and API stability around custom extensions, especially in the context of structured kernels and mobile model optimization. There's also attention to build system and CI configuration issues, such as requiring newer CMake versions or fixing internal build failures. Some questions remain about the best practices for integrating with external tools, testing hardware robustness, and ensuring concurrency correctness in memory management."
2021-09-02,pytorch/pytorch,"The discussions highlight several technical concerns, notably the recurring CUDA runtime error ""device-side assert triggered,"" which is often linked to out-of-bounds indexing or label mismatch issues, especially when working with custom datasets or models on different CUDA versions. There are also concerns about compatibility and performance regressions when porting kernels (e.g., `sort`, `autograd`, `inplace/view` operations) between different device types and versions, with suggestions to adopt boxed kernels or handle stride/contiguous allocation strategies more consistently. Additionally, questions arise about CUDA version management in Colab environments, the impact of model modifications like adding sigmoid or reshaping layers, and the necessity of infrastructure improvements like better test logging, CI reliability, and handling of unstructured tensor allocations. Unresolved questions include how best to support different memory allocation strategies without degrading performance and whether runtime or compile-time checks can prevent out-of-bounds errors."
2021-09-03,pytorch/pytorch,"The comments reveal ongoing technical challenges and discussions around GPU hardware reliability, CUDA version compatibility, and specific operator support in ONNX export (notably the 'col2im' operator). Several issues concern kernel performance and implementation details, such as the advantages of specialized transpose kernels versus regular copy, and handling of tensor views to ensure contiguous memory layouts for correct gradient calculations. There is also a focus on ensuring robust testing and documentation, especially for shape inference logic and dynamic/static tensor shapes. Questions remain about proper integration of double backward operations with DDP, handling of asynchronous tensor transfers, and the need for additional support flags for gradient checking, as well as some external library BC issues. Overall, unresolved questions include operator support in ONNX, performance trade-offs in tensor views, and ensuring comprehensive test coverage for new features."
2021-09-04,pytorch/pytorch,"The discussions primarily focus on the differentiability and mathematical validity of entropy calculations in PyTorch, emphasizing that entropy based on count distributions (e.g., using torch.unique) isn't inherently differentiable, unlike entropy of probability distributions. Several issues relate to CUDA and GPU performance optimizations, including memory management, data transfer, and benchmarking of tensor operations like transposes and copies, with some highlighting significant speed disparities for large tensor sizes. There are also maintenance concerns, such as merging code patches, handling multiple PyTorch installations, and ensuring reproducibility and determinism in autograd. Additionally, questions arise about tensor representation improvements (like including shape info in print outputs) and package version management. Overall, unresolved questions involve how to handle non-differentiable entropy computations in gradient-based optimization, GPU performance optimizations, and proper build and environment setup."
2021-09-05,pytorch/pytorch,"The discussions primarily revolve around numerical precision and behavior consistency between PyTorch and NumPy (Issue #36571), especially concerning division and out-parameter handling, with some debate over whether current PyTorch behavior aligns with C++ standards. The handling of uninitialized tensors is clarified, emphasizing that the observed ""random"" values stem from uninitialized memory rather than intentional randomization (Issue #64512). Several issues involve debugging runtime errors such as segmentation faults in CUDA samples (Issue #21819), in-place operation checks, and improper use of unused parameters in distributed models, which may be due to parameter scope or usage assumptions (Issues #43259, #64018). There is concern over tensor representation and printing, with suggestions to standardize shape/size display via `size=(...)` across all tensor types for better consistency and debugging convenience (Issues #64043, #64455). Lastly, ongoing efforts include improving code contributions, documentation, and testing practices in PyTorch’s development workflow, with some questions about handling specific error cases and consistency in tensor behaviors."
2021-09-06,pytorch/pytorch,"The comments highlight several recurring technical concerns: issues related to CUDA errors such as device-side asserts and segmentation faults, often tied to specific hardware or software configurations; problems with installing or updating dependencies like Pillow/PIL, and ensuring compatibility with GPU driver and CUDA versions; challenges when exporting models to ONNX, particularly unsupported operators like `col2im` and protobuf version mismatches; and inconsistencies or BC-breaking considerations in tensor representations, such as the naming of tensor shape attributes (`shape` vs. `size`) and their impact on debugging and user experience. Additionally, there are discussions about improving support for GPU-based linear algebra operations, tensor memory formats, and backward compatibility in the JIT and autograd systems. Overall, the discussions reflect ongoing efforts to debug, optimize, and clarify deep learning framework functionalities across diverse hardware and software environments."
2021-09-07,pytorch/pytorch,"The comments highlight various technical concerns including the challenge of dynamically managing module imports, especially for model repositories with different versions (e.g., YOLOv5), and the difficulty in avoiding hardcoded paths. Several discussions focus on improving PyTorch's internal API and testing infrastructure, such as handling overloaded methods, enabling forward automatic differentiation, and better test parametrization. Issues around memory management, GPU communication (e.g., multi-GPU training, CUDA IPC, and GPU memory leaks), and compatibility problems with different library versions and hardware configurations are also prominent. Additional concerns involve the interpretation of terms like ""uninitialized"" in documentation, and the implications of changes on backward compatibility, model serialization, and performance benchmarking. Overall, the discussions reflect ongoing efforts to improve flexibility, robustness, and usability across PyTorch's core, testing, and deployment layers."
2021-09-08,pytorch/pytorch,"The comments reflect ongoing discussions and issues related to PyTorch's development, notably around implementation details, performance optimizations, and API behaviors. Key concerns include the proper handling of tensor shape representations (size vs shape), and how to implement lazy loading or stream tensor loading from zip checkpoint formats to reduce memory overhead. There are questions about the implications of using `async_execution` in RPC for memory and thread management, and deliberations on whether to support complex numbers in optimizers, and how to generalize certain functions (e.g., `replace_`) for efficiency. Some unresolved questions involve backward compatibility, instrumentation of error behaviors, and the integration details with external systems like ONNX, TensorPipe, and internal build environments. Overall, the discussions encompass refinements to functionality, correctness, performance, and API usability in PyTorch's evolving codebase."
2021-09-09,pytorch/pytorch,"The comments reflect ongoing efforts to improve PyTorch's features and stability, including issues with JIT tracing, ONNX export, and multi-GPU training. Notable concerns involve the handling of shape inference and constant folding in FX graph transformations, especially when models contain dynamic shape operations like `tensor.shape`. There are discussions about enhancing the `@rpc.functions.async_execution` to reduce thread contention and optimize distributed RPC workflows. Some PRs aim to address device-agnostic state loading for distributed training, while others focus on performance optimizations such as custom kernel implementations and performance boosts for specific operators like `log_softmax`. Unresolved questions include how to unify error handling and backward compatibility across different operators, and how to better support model conversion (e.g., ONNX, TensorRT) for complex models with custom ops or dynamic shapes."
2021-09-10,pytorch/pytorch,"The discussions revolve around various technical issues encountered in PyTorch development, such as runtime errors in DataLoader workers potentially caused by shared memory limitations or multiprocessing bugs, floating point inaccuracies affecting functions like cosine similarity, and shape incompatibilities especially with sparse or variable-sized input tensors. Several comments highlight implementation details or proposed fixes, such as reordering floating point operations for numerical stability, adding missing clamps to prevent floating point errors, or adjusting shape handling to accommodate dynamic axes in JIT or ONNX exports. Other concerns include correctness and backward compatibility (e.g., different behaviors of resize methods like ""nearest"" vs. ""nearest-exact""), and the impact of parallelism and threading configurations on performance and stability in distributed training environments. Some suggestions involve improving testing, logging, and error messaging, as well as addressing specific issues with CUDA, ROCm, or onnx support, while questions remain about re-entrant behavior, handling device placement during state load, and whether certain API changes are safe for external users."
2021-09-11,pytorch/pytorch,"The comments reflect ongoing efforts to improve PyTorch's support for edge cases such as zero-sized tensors, support for CUDA graph APIs, and deterministic behavior, highlighting specific operator support issues and plans for fixing segfaulting operators like `InstanceNorm`. There are discussions on how to handle model device placement (CPU vs GPU), especially regarding model export and state dict loading across devices, with suggestions to improve robustness in distributed settings. Some comments address internal infrastructure concerns such as CI failures, dependency management, and internal checks, indicating a focus on testing and build stability. Notably, there are questions about the support for lazy modules, the function of internal call mechanisms (`__call__`, hooks, etc.), and potential compatibility with JAX/TensorFlow approaches, suggesting an interest in API extensibility and simplification. Overall, the discussions revolve around operator support, deterministic workflows, device handling during export/load, and CI stability, with some unresolved questions about future API paradigms and low-level CUDA features."
2021-09-12,pytorch/pytorch,"The discussions highlight multiple build failures in the PyTorch repository, primarily caused by undefined identifiers such as 'local_sample_count' and 'size_' in the distributed data sampler code, indicating potential regressions or incomplete code updates. Additionally, there are concerns about the deprecation of certain logging statements (e.g., VLOG) and how to properly enable or view these logs during runtime. Some pull requests involve merging or reverting changes that impact test coverage and performance, with questions about the effects of certain modifications (e.g., adding module info, runtime impact). The conversation also mentions issues related to sparse tensor support in TorchScript and the handling of dynamic loss scaling with CUDA graphs, emphasizing the need for careful coordination and potential workarounds. Overall, unresolved questions include fixing build errors, understanding logging configurations, and ensuring backward compatibility and feature support."
2021-09-13,pytorch/pytorch,"The comments reveal ongoing efforts to enhance PyTorch's functionality, robustness, and performance across various operators and features. Several discussions focus on improving support for edge cases, such as empty tensors, zero-dimensional batch processing, and non-standard tensor representations, emphasizing the need for rigorous testing and backward compatibility. There are also concerns about technical limitations and infrastructure, including CUDA stream concurrency, shared memory management, and build system behaviors, often coupled with proposals for refactoring or new APIs. Some issues highlight debugging challenges, such as race conditions, ffi limitations, and runtime errors, indicating areas where better tooling or clearer error messaging could help. Overall, the community is balancing the introduction of new capabilities with maintaining stability and performance, often seeking clarity on implementation details and backward compatibility implications."
2021-09-14,pytorch/pytorch,"The comments reveal ongoing challenges and discussions regarding PyTorch's internal implementation details and user-facing features. Certain topics focus on the behavior and consistency of tensor representations, especially regarding shape/size print options, and the compatibility issues faced when extending support for new data types such as posit or bfloat16, often entailing BC-breaking concerns or complex code changes. There are multiple reports of bugs and crashes associated with CUDA, NCCL, or distributed training (e.g., illegal memory access, runtime errors in DDP, RPC, or kernel launches), with suggested troubleshooting steps or potential workarounds. Discussions also highlight structural and API considerations for features like remote profiling, tensor locking, and model sharding, reflecting a balance between flexibility and safety. Network infrastructure issues and environment-specific compilation issues are mentioned, along with proposals for better diagnostics, testing frameworks, and code refactoring to improve robustness and maintainability."
2021-09-15,pytorch/pytorch,"The comments primarily raise concerns about low-level optimization and hardware-specific behavior, such as vectorized comparison operations on small tensor dimensions, enabling and optimizing tensor comparison in vector registers, especially when dimensions are not aligned with register sizes. There is discussion around supporting BF16 operations with vectorized accumulation, proposing API extensions for Vec256/Vec512 to handle different data types and higher precision reductions, aiming to improve numerical accuracy in large reductions like softmax. In addition, some issues relate to the behavior and implementation details of PyTorch's JIT tracing with named tuples, serialization of models with namedtuples, and the correctness and stability of softmax/matrix operations under various configurations. Several internal CI and infra issues are also mentioned, including build failures, test duplications, and logging truncation problems, with some suggestions on handling multi-process RPC errors and environment configurations. Overall, the discussions span from low-level hardware optimization to higher-level API consistency, debugging, and CI stability concerns."
2021-09-16,pytorch/pytorch,"The comments cover a range of topics, including the optimization of tensor operations and potential vectorization issues, particularly with dimensions not aligning with register sizes, and the need to add more tests for small tensor sizes. Several discussions address the implementation of label smoothing for loss functions, and the lack of documentation for soft-label support in CrossEntropyLoss. There are multiple instances of problems with model conversion to ONNX, especially involving custom operators and ensuring compatibility with the latest PyTorch and ONNX versions. Additionally, there are concerns about build issues—such as CUDA and ROCm compatibility, MSVC bugs, and memory allocation efficiency—and the importance of better test coverage, clearer error messages, and proper handling of custom tensor subclasses and store abstractions. Some discussions also include modifications for functional APIs, test infrastructure, and CI management, highlighting the ongoing efforts to improve performance, compatibility, and usability."
2021-09-17,pytorch/pytorch,"The discussions mainly revolve around efforts to improve PyTorch's functionality, stability, and usability, including handling of specialized cases like `Parameter` in custom modules, and performance improvements such as efficient memory management and kernel optimization. Several comments highlight issues with bugs or unexpected behavior in features like `torch.hub.load()`, `gradcheck`, and gradient computations in certain layers, with proposed fixes or workarounds such as code restructuring and additional tests. There are also ongoing questions about backward compatibility, the semantics of `__torch_function__` overrides, and the design of experimental features, as well as logistical concerns like CI failures and build environment configuration. Some discussions focus on refining API consistency, documentation, and community support processes, with a few technical debates about implementation details for specific operators, like `torch.batch_norm`. Overall, many comments indicate active troubleshooting, incremental improvements, and open questions for future development."
2021-09-18,pytorch/pytorch,"The comments highlight several technical concerns regarding PyTorch's compatibility with CUDA, particularly issues with dynamic parallelism support, linking errors during extension compilation, and differences in exported ONNX graphs between PyTorch versions. There are questions about whether PyTorch officially supports CUDA dynamic parallelism, and suggestions to modify the build system's ninja rules to handle such features, potentially through a new boolean flag in `CUDAExtension`. Additionally, some users report runtime errors and differences in model export results when switching between PyTorch versions and on different hardware setups. There are also discussions about handling hooks during `gradcheck` in `torch.nn.Module`, performance optimizations in quantile computations, and proper ways to shut down DataLoader subprocesses, indicating broader concerns about stability and usability across different scenarios. Unresolved questions include the precise support level for CUDA dynamic parallelism in PyTorch and how to adapt extension build processes accordingly."
2021-09-19,pytorch/pytorch,"The discussions highlight ongoing efforts to implement and improve utility functions like `isin`/`in1d` in PyTorch, with suggestions for more memory-efficient versions and plans to match NumPy's behavior. There is interest in supporting custom samplers for distributed training, with Catalyst's solution cited as a reference, and questions about future native support in PyTorch. Several issues involve exporting operations like `pixel_unshuffle` to ONNX, with notes on discrepancies in tensor reordering and color artifacts, alongside proposed alternative implementations. Multiple discussions address runtime errors, including gradient modification detection and nan values in backward passes post-movement from CUDA to CPU, often linked to complex model changes or batch size adjustments. Additionally, some comments request proper CLA signing and mention ongoing CI/CD build checks, indicating active development and validation processes."
2021-09-20,pytorch/pytorch,"The discussions highlight ongoing advances and issues with PyTorch's development, including the implementation of functions like `isin`, improvements to ONNX export, support for distributed training in C++, and handling of specific bugs like gradient discrepancies or memory leaks. Several questions pertain to hardware compatibility, particularly with AVX/AVX512 and external CUDA allocators, as well as concerns about test coverage, performance regressions, and correctness in edge cases such as sparse tensors or specific operator behaviors. Significant emphasis is placed on fixing bugs related to autograd hooks, deterministic behavior, and memory management, alongside considerations for expanding distributed features and improving user experience in error handling and documentation. Some discussions also involve maintaining compatibility and performance across various platforms and dependencies, with open questions about long-term support and infrastructure changes like the integration of third-party libraries. Overall, the focus is on refining functionality, robustness, and usability while navigating complex internal dependencies and external hardware/software constraints."
2021-09-21,pytorch/pytorch,"The discussions mainly revolve around CUDA out-of-memory errors during model training, often attributed to batch size being too large, with suggestions to reduce batch size or modify memory allocators. Several entries mention the need to ensure proper device state resets, especially when errors like illegal memory access occur after exceptions, and question how to handle re-runs or retries without restarting the process. There are concerns about compatibility issues, such as linking dynamically vs. statically with NVIDIA libraries like CUPTI and the impact on exception handling, referencing recent patches addressing these issues. Some discussions also touch on the support for specific hardware (e.g., Apple M1, NVIDIA A100, ROCm devices), the need for better API design (e.g., controlling memory allocators via environment variables), and ensuring that debugging, profiling, and testing accurately reproduce errors for effective troubleshooting. Unresolved questions include how to systematically identify and clear the GPU state after errors, and whether recent driver/SDK updates have addressed these memory-related issues."
2021-09-22,pytorch/pytorch,"The comments reflect a wide array of issues and discussions within the `pytorch/pytorch` repository, including bug fixes, feature proposals, and maintenance tasks. Notable topics concern CUDA-related errors such as device asserts, out-of-memory issues, and symbol missing problems; the development of new tensor or operator features like support for complex numbers, polar representations, and lazy initializations; and maintenance activities like CI failures, serialization support, and build system updates. Several discussions highlight debugging processes, including reproducing crashes, testing in different environments, and reviewing code changes against expected behaviors. There are also proposals for API improvements, documentation updates, and testing enhancements, often coupled with questions about support status or compatibility. Overall, the correspondence indicates active troubleshooting, ongoing feature development, and coordination efforts aimed at refining PyTorch's robustness and functionality."
2021-09-23,pytorch/pytorch,"The comments span various topics—including the implementation details and benefits of weight normalization, support for label smoothing, issues with CUDA errors and binary dependencies, JIT compilation, async operation handling, and CUDA support for different architectures, among others. Key concerns involve ensuring correctness and compatibility (e.g., in backward formulas, datatype promotion, operator support, and binary linkage), as well as improving usability (e.g., documentation, testing, and user-facing APIs). Several discussions highlight ongoing efforts or planned fixes—such as support for CUDA with RRef, async JIT compilation, and robust error handling—while some issues remain unresolved or require further investigation, especially around internal build environments, dependency management, and compatibility with different hardware and software setups. Overall, these conversations reflect active development, debugging, and planning toward feature improvements, bug fixes, and better integration, with important questions about long-term design and immediate technical fixes."
2021-09-24,pytorch/pytorch,"The discussions highlight ongoing efforts to extend PyTorch's numerical and functional capabilities, including implementing advanced interpolation methods, adding SciPy-like functions (such as special.kv), and improving support for various numerical precisions (bf16, float128). There are concerns about ensuring consistency and correctness in operations like loss functions, handling edge cases such as empty tensors or all ignored labels, and managing the auto-grad autograd behavior with scripted vs. traced modules. Issues related to build configurations and compatibility—such as symbol visibility, dependency management, and platform-specific compilation problems—are also prominent. Additionally, questions remain about optimal design choices like async compilation, backend interface abstractions, and integration of custom test bases, especially for closed-source subprojects like HPU support."
2021-09-25,pytorch/pytorch,"The discussions highlight various technical concerns including unpredictable gradient behaviors when masking infinities and NaNs in PyTorch, and the potential need for updates or fixes in gradient computation with masked or masked tensor operations. There are questions about the support and implementation details for new hardware features, like BF16 operations on CPUs, suggesting enhancements for vectorized reduction APIs and accumulator support, as well as integration with new hardware classes like Vec512. Compatibility and installation issues are frequently raised, especially concerning different PyTorch versions, system environments, and package managers (pip, conda), including fixing bugs related to serialization, CUDA support, and package publishing. There are ongoing discussions about API improvements, such as decorators for scripting modules and handling of specific functions like softmax or matrix inverse, and concerns about correctness and stability of numerical operations in low-precision data types. Unresolved questions remain about supporting new hardware features, fixing existing bugs, and maintaining compatibility across environments."
2021-09-26,pytorch/pytorch,"The discussions highlight several technical issues: a build failure in the Android environment caused by missing or misinterpreted Gradle scripts, with suggestions to mitigate project cleaning issues by renaming the 'gradle' folder; persistent failures in loading models via torch hub due to branch name discrepancies in 'torch/hub.py,' with solutions involving editing the source code to handle 'main' vs 'master' branches; ongoing concerns about the behavior and freezing of BatchNorm layers during training and evaluation, including how to modify parameters and attributes to achieve desired fixed statistics; and a repository of various build, environment, and memory leak issues, emphasizing the need for better documentation, handling of environment-specific problems, and awareness of internal file descriptor sharing that can lead to resource leaks, especially in data loaders. There's also discussion about the sensitivity of linear algebra operations to floating point precision, with suggestions to recommend explicit data type conversions for stability. Unresolved questions remain about proper handling of BatchNorm freezing semantics, and improvements in build and installation procedures are ongoing."
2021-09-27,pytorch/pytorch,"The discussions primarily revolve around configuration and usage issues in PyTorch, including correct CMake syntax for including Torch directories, and compatibility of libtorch with environment paths and configs such as Torch_DIR. Several users highlight challenges with version mismatches (CUDA, PyTorch, torchvision), leading to runtime errors, and procedural details for exporting and converting models (e.g., JIT, ONNX). There is also notable concern about how batch normalization should behave during training and evaluation—specifically, the need for more straightforward methods to freeze BN layers akin to FrozenBatchNorm, and discussions on how different BN modes impact training stability. Additionally, there are queries about memory leaks, operator support, and build system issues across platforms, as well as the desire for clearer testing, documentation, and API consistency. Several unresolved questions remain about the best practices for freezing BN, handling complex numbers, and improving code reliability and performance across varied environments."
2021-09-28,pytorch/pytorch,"The discussions predominantly revolve around improving PyTorch's multiprocessing robustness, especially regarding DataLoader with `persistent_workers=True`, and handling of complex numbers in autograd, where issues with LSTMCell export to ONNX and tensor data types are noted. Several comments suggest structural code improvements, such as moving promotion logic into `where`, refactoring operator support (e.g., `PackedSequence`), and fixing inconsistencies in loss function behaviors like `mean()` with empty inputs. Concerns about build errors on Windows and ROCm, as well as the need for better caching, are also expressed. Unresolved questions include how to better support non-sorted sequences in `pack_padded_sequence`, enhancing ONNX operator support (e.g., `triu`, `tril`), and ensuring backward compatibility with new kernel IR generation strategies."
2021-09-29,pytorch/pytorch,"The discussions primarily revolve around the stability and correctness of the PyTorch codebase, with concerns about specific issues such as potential deadlocks in distributed training using NCCL (notably when calls to `torch.distributed.reduce()` are made inside conditionals), and the need for explicit shutdown of multiprocessing workers to prevent memory leaks and zombie processes. Several comments highlight the importance of proper shape inference, support for optional arguments (like `out` variants in functions), and the handling of non-resizable storage to prevent runtime errors. Some discussions query the implementation and testing strategies for features such as autograd support in custom dispatch, the handling of NaNs in SVD, and the behavior of certain functions (e.g., `gradcheck` with `fast_mode=False`). Additionally, there's emphasis on ensuring compatibility across different platforms, compilers, and build tools, along with considerations of how to best document or test these edge cases and new features. Unresolved questions include how to reliably test certain failure conditions and how to integrate partial fixes into the broader codebase effectively."
2021-09-30,pytorch/pytorch,"The comments reflect ongoing discussions and concerns about various PyTorch development issues, including the implementation and integration of custom modules (e.g., local connected layers, CUDA extensions), correctness and performance of certain operations (e.g., matrix inverse, matmul routines, quantization, and distributed training), and API design choices (e.g., support for `out=` parameters, tensor subclassing, and dispatch mechanisms). There are questions about maintaining backward compatibility, better error handling, and test coverage, as well as considerations for improving build and CI processes, especially regarding environment setup, dependency management, and regression testing. Some technical challenges include handling edge cases in linear algebra functions, managing shared memory in multi-processing environments, and ensuring proper registration of extensions across different build configurations. Overall, unresolved issues involve refining the API ergonomics, addressing specific bugs (e.g., in tensor serialization, distributed initialization, and memory leaks), and coordinating upcoming new features like structured tensor support and advanced dispatch strategies."
2021-10-01,pytorch/pytorch,"The discussions focus on several key areas: the development and support of custom autograd functions and derivatives, with plans to enhance support for custom functions in future PyTorch releases; challenges with handling complex numbers, especially in `einsum`, where current limitations prevent full complex support; performance and backward compatibility concerns related to shape and layout management in JIT and tensor resizing, including potential issues with `resize_output` and `as_strided`; maintenance and organization of API functions such as `tensor_storage_type` and the placement of `masked_*` functions, with suggestions to better categorize or document their scope; and ongoing development efforts for features like sparse tensor support, distributed training, and automation of testing and CI, with some unresolved bugs and implementation challenges noted."
2021-10-02,pytorch/pytorch,"The discussions highlight several core issues: the first pertains to the compatibility and correctness of ONNX export, especially around padding and layer shape computations in models like InceptionV3, which may lead to shape mismatches or unexpected behavior in downstream frameworks like Caffe2; the second involves the support for complex numbers (e.g., in `torch.einsum` and autograd functionalities), where current limitations or inconsistencies (like expected scalar types) hinder proper usage; a third concern is about build and deployment configurations, such as missing system or environment settings (e.g., port ranges for distributed communication, removing dependencies on specific channels like conda-forge) that affect reproducibility and stability; additionally, some discussions focus on bug fixing or feature addition, such as code generation for ops with tuple outputs, or patching model state dicts for correct loading/exporting; unresolved questions include confirming the persistence of certain issues with latest nightly/experimental builds and the best practices for handling model compatibility across different framework versions."
2021-10-03,pytorch/pytorch,"The discussions raise several technical concerns, notably: handling corrupted images in DataLoader by filtering out `None` results in collate_fn, with the risk of empty batches causing index errors; updating deprecated `addcmul_` method signatures by passing 'value' as a keyword argument; addressing NaN issues in attention mechanisms potentially by replacing `-inf` values directly or using `torch.nan_to_num()` to stabilize softmax computations; and managing GPU-specific errors like CUDNN INTERNAL_ERROR, often mitigated by disabling `cudnn.benchmark`. Additionally, there are questions around ensuring backward compatibility for differentiable `lstsq` outputs, and requests for bug fixes or feature enhancements such as timeout functionalities in DataLoader. Unresolved issues include handling all-bad-sample batches, softmax stability, and hardware-specific runtime errors."
2021-10-04,pytorch/pytorch,"The discussions highlight several technical concerns, notably the need for improved API design for model manipulation (e.g., adding `out` variants and handling `alpha` parameters for `index_add`) and the complexity of supporting multiple Shape types across different backends without polluting core Node classes. There is debate over the best approach to extend support for operations involving tuple outputs, with some experts preferring long-term solutions like codegen and specialized lowering, rather than immediate modifications. The difficulty of maintaining compatibility, especially with tools like ONNX, XLA, and JIT, is recurrent, as well as challenges related to threading, dispatch order, and safe serialization. Additionally, questions about duration of development, merging strategies, and testing approaches, including compatibility tests and CI pipeline adjustments, remain unresolved."
2021-10-05,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of model deepcopy with weight normalization removal, support for complex numbers in functions like `einsum`, and ensuring backward compatibility when changing function signatures such as `index_add` to include an `alpha` parameter. There are ongoing efforts to improve plugin support (e.g., ONNX, XLA, Vulkan), port features like `out=` variants, and integrate better abstraction mechanisms for backend-specific types (shapes and tensor metadata). Additionally, some issues relate to CI infrastructure and build process reliability, as well as the need for more comprehensive tests and documentation, especially for new APIs or ported functionality. Questions remain on long-term plans for features like logging tensors, handling dynamic shapes, and the implications of API changes on existing extensions and tools."
2021-10-06,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for clarifying use cases for `training=TrainingMode.PRESERVE` in inspection scenarios, and suggestions to consolidate refactoring efforts into existing codegen utility functions, specifically `translate`, for managing type conversions between JIT and value types. There are questions about the appropriate handling of weight-only quantization formats like embedding/embeddingbag, and whether to modify the eager mode API for consistency with fx graph mode using multiple dtype parameters for activation, weight, and compute. Several discussions also touch on CI testing failures, the appropriateness of including additional DataPipes in core, and the implementation details of type and dtype handling in various contexts, indicating ongoing work on code correctness, API consistency, and test stability. Some unresolved questions involve proper access controls, aligning API interfaces, and managing the impact of internal branches and CI workflows on development and review processes."
2021-10-07,pytorch/pytorch,"The discussions cover multiple areas: support and API development for CUDA graphs in PyTorch, especially post-1.10 release; limitations and future directions for tensor type casting and view operations, including considerations for non-contiguous tensors and complex number views; performance variability in TorchScript with newer executor profiling, suggesting potential fallback to NVFuser; and improvements in sparse tensor sum operation behavior, advocating for consistent dtype handling. Additional points include communicating stability and feature status effectively, handling CI/test failures, and clarifying support for hardware like Apple M1 GPUs and Intel TBB for parallelism. Unresolved questions involve how to formalize the API for dtype casting, ensuring robust testing across backends, and strategies for pinning optimized models for production deployment. Overall, these discussions highlight ongoing development, compatibility, and performance optimization challenges."
2021-10-08,pytorch/pytorch,"The discussions primarily revolve around memory leaks and efficiency issues associated with Python datasets and DataLoader usage in PyTorch, especially when using multiple workers or large datasets. Several contributors suggest solutions such as using zero-copy techniques with PyArrow or numpy arrays to reduce memory leaks, but some users report ongoing problems despite these approaches. There are requests for added warnings or safeguards within the DataLoader code to alert users about potential memory issues, as well as considerations for reusing CUDA contexts across datasets or dataloaders to optimize memory usage. Some discussions explore system-level improvements like increasing shared memory or file descriptors, though these are only partial remedies. Additionally, there are peripheral technical concerns about ONNX support, the correct placement of utility functions, and the handling of various tensor operations in the context of profiling, quantization, or code generation."
2021-10-09,pytorch/pytorch,"The discussions primarily focus on hardware and software support challenges in PyTorch, such as the limited GPU support on Apple's M1 chips and performance issues with certain operations like `torch.randperm`, where specific code modifications (e.g., moving computations to CPU before GPU transfer) can prevent overflow errors. There are concerns about integrating new features without breaking existing functionality, as seen with the complex number support API and optimizers that selectively update parameters. Additionally, there are build system issues and configuration tasks related to compiling with different environments, such as Windows, Linux, and various CUDA versions, often addressed through detailed CI workflows and documentation. Some discussions also highlight ongoing efforts to improve documentation, handle multi-GPU setups, and fix regressions introduced by recent code changes. Unresolved questions include better support for non-tensor variables in distributed pipelines and supporting DataParallel within Jupyter notebooks."
2021-10-10,pytorch/pytorch,"The discussions primarily revolve around ensuring reproducibility and deterministic behavior in PyTorch, highlighting the importance of setting proper seeds and configuring `cudnn.benchmark` and `deterministic` flags, especially in multi-GPU or complex models like DETR. Several threads address distributed training and NCCL errors, where issues like environment setup, correct port sharing, and version compatibility are critical. There are also concerns about code organization, such as module placement for functions like `masked_*`, with suggestions to place them under `torch._masked` for clearer separation from sparse tensors. Additionally, build and platform support, particularly on Windows with different compiler options (VS vs. MinGW), and ensuring Jupyter notebook compatibility, are notable topics. Unresolved questions include best practices for multi-GPU validation, handling of NCCL hangs, and the ongoing development of support for build environments across different systems."
2021-10-11,pytorch/pytorch,"The discussions highlight ongoing concerns with memory leaks in DataLoader when using Python lists or dictionaries within custom datasets, which can be mitigated by converting data to numpy arrays, but lack a fully reliable solution. There is a call for adding warnings within the DataLoader to alert users about potential memory issues, especially when using multiple workers or shuffle, and suggestions for workarounds such as increasing shared memory or setting `num_workers=0`. Additionally, there's interest in improving and standardizing the CUDA host-to-device copy semantics, profiling tools, and CUDA graph support, along with discussions about ensuring correct behavior during model tracing, autograd, and support for new features like CUDA graphs and CUDA graphs APIs. Several issues relate to CI failures, build reproducibility, and ensuring proper testing across different environments, with some specific concerns about the behavior of pinned memory, support for data types like bfloat16, and compatibility across CUDA and operating systems. Overall, these discussions suggest a focus on stability, memory management, profiling, and expanding feature support in PyTorch's core and API layers."
2021-10-12,pytorch/pytorch,"The discussions predominantly focus on enhancements and issues related to PyTorch features and testing. Key topics include proposing a generic Maxout layer as a drop-in replacement for ReLU, handling BFloat16/float16 sparse matrix-vector multiplication bugs, and improving distributed training APIs, especially support for DDP in C++. There are concerns about test stability and coverage, such as adding support for various ops across supported, fallback, and full-codegen categories, and ensuring that tests account for platform differences (e.g., Windows vs Linux). Additionally, discussions address CI/CD pipeline failures, build environment issues, and test reproducibility, with suggestions for isolating tests, better logging, and fixing specific bugs like device mismatches and intermittent failures. Overall, the main challenges involve improving feature support, test robustness, and CI reliability."
2021-10-13,pytorch/pytorch,"The discussions mainly focus on improving PyTorch's serialization via `get_extra_state()` / `set_extra_state()` for modular extensions, especially for complex objects like ShardedTensor, instead of relying on hooks or custom methods. There is concern about how to handle extra state for submodules and whether recursion is necessary, with suggestions to simplify the API and make it more user-friendly. Additionally, questions arise about the proper mechanism to group overloaded operations (e.g., `bitwise_and`) during code generation, avoiding redundant function definitions, and how to manage dispatching for overloaded ops. Some discussions also touch on CI failures, performance optimizations, and the impact of platform-specific issues like DLL loading errors. Overall, unresolved issues include designing a flexible, clean API for custom state, and handling operation grouping during codegen."
2021-10-14,pytorch/pytorch,"The discussions highlight several recurring issues in the PyTorch repository, including handling heterogenous types in list constant retention, and ensuring compatibility with different package versions and build configurations, especially regarding CUDA, ONNX, and external dependencies. There are concerns about backward compatibility, particularly when changing internal behavior like parameter exposure in DataParallel and masked reduction functions, where changes can disrupt existing user code or autograd support. Multiple issues stem from platform-specific problems, such as Windows DLL loading errors or ROCm build failures, often necessitating environment adjustments or explicit build configurations. Certain code generation and API organization questions are also raised, such as whether to modularize masked operations into separate namespaces or modules, and how to group overloaded functions during codegen. Unresolved questions include the proper placement of new functions, handling autograd for masked reductions, and ensuring build/test robustness across diverse environments."
2021-10-15,pytorch/pytorch,"The discussions highlight confusion about whether models can now be saved in TorchScript from C++ front-end, with clarification that loading and re-saving a TorchScript module is possible, but converting C++ models directly is not. Several issues concern the support for exporting models to ONNX, noting limitations with certain operations and opset versions, especially support for quantization operators like `quantize_per_tensor`. There are questions about API deprecations versus removals, especially regarding `torch::jit::Module` and related functions, with suggestions for better separation of code and formatting changes in PR workflows. Additionally, some discussions address differences in behavior between Python and C++ dispatching, support for distributed training setups, and the need for clearer documentation or handling of custom types and their versioning. Overall, unresolved questions include expanding ONNX operator support, managing API compatibilities, and ensuring correct source attribution for error messages."
2021-10-16,pytorch/pytorch,"The discussions highlight several technical concerns, including the proper handling of deprecated APIs such as `F.upsample` and the need for migration to `torch.nn.functional.interpolate`, especially in ONNX export workflows. Several issues relate to model export stability, notably handling control flow with scripting vs. tracing, and ensuring unsupported operations are addressed or clearly documented. Compatibility across environments remains a concern, exemplified by SSL verification errors during model downloads and dependency management for building with PocketFFT and LAPACK on iOS. Additionally, there is ongoing interest in improving ONNX support, such as exposing certain ONNX Runtime APIs and fixing specific operator bugs (e.g., gather, index_add). Many discussions call for clearer repro steps and test coverage to resolve bugs effectively, with some noting that fixes are pending review or automation updates."
2021-10-17,pytorch/pytorch,"The discussions highlight several technical issues: (1) the incorrect maximum tensor dimensions (MAX_DIMS=128) in tensor_new.cpp, which needs revision due to support limitations; (2) a casting concern in memory allocation checks, where casting nbytes to ptrdiff_t could cause overflow issues on 64-bit systems, with questions about whether overflow results in zero and if that triggers error handling correctly; (3) the behavior and implementation details of PyTorch's LayerNorm compared to TensorFlow, particularly regarding input dimensions, normalization axes, and shape handling; (4) a bug in TorchScript's source code range calculation leading to incomplete or mis-highlighted error messages, with ongoing investigation into SourceRange initialization and source attribution; and (5) various ongoing or reverted pull requests addressing build failures, size optimizations, and API support enhancements, with suggestions for cleaner code changes and proper handling of optional features."
2021-10-18,pytorch/pytorch,"The discussions raise several key points: first, there's ongoing consideration about whether storage classes should be made untyped, with recent efforts (e.g., `_to()` method) addressing this. Second, API design for inverse functions, particularly supporting arbitrary graph inversion and branching, is debated, with suggestions to use `inverse()`, `inplace()`, or checkpoint-like approaches. Third, questions about the behavior and documentation of `nn.LayerNorm`—whether it normalizes over full shapes or just specific dimensions—highlight the need for clearer explanations or pseudo-code. Fourth, there's an emphasis on proper deprecation versus removal practices, and ensuring compatibility and BC-breaking notices in PR titles and descriptions. Lastly, the discussions also mention build issues, such as failure to initialize MAGMA on certain systems, and the organization of PyPI wheels for different CUDA versions."
2021-10-19,pytorch/pytorch,"The discussions primarily revolve around various technical challenges in PyTorch related to CUDA and ONNX export errors, specifically issues with operator support at different opset versions, and compatibility problems with specific hardware and driver configurations (e.g., ROCm and ROCm libraries). There are concerns about existing bugs and unsupported operators in certain ONNX opset versions (e.g., quantize_per_tensor at opset 10 or 13) which impede model export, with suggestions for requesting support or adding custom support. Additionally, memory consumption during model tracing and export, as well as hardware-specific build issues (like ROCm runtime linking problems), are noted, with recommendations to improve stability, error handling, and package completeness. Some discussions also touch on code maintenance issues such as adding proper type annotations in TorchScript, integrating automatic tests, and managing framework updates to ensure compatibility. Unresolved questions include handling of complex data types, support for specific custom operators, and ensuring backward compatibility across different hardware and software environments."
2021-10-20,pytorch/pytorch,"The discussions predominantly revolve around enhancing ONNX export support in PyTorch, such as customizing Python operator names and attributes to improve ONNX compatibility. Several conversations address the challenges of saving and loading module state, especially for user-defined or sharded tensors, with suggestions to support buffers that contain custom or distributed tensors. There are questions regarding testing completeness, code consistency, and ensuring that certain functions (e.g., `linalg_solve_triangular`) are correctly overridden or exposed. Additionally, multiple discussions involve CI testing and validation, including managing test failures, reverts, and build configurations, with a focus on improving robustness and usability of model export, serialization, and distributed state management. Unresolved issues include how to properly handle extra state at submodule and instance levels, and how to customize operator attributes for ONNX export."
2021-10-21,pytorch/pytorch,"The comments highlight ongoing development efforts and feature requests related to PyTorch, including support for sparse matrix operations like `.max` with custom zero or empty value semantics, sparse layout support, and support for masked reductions like `amax`. Several discussions also focus on improving debugging and performance monitoring tools, such as the proposed ""vitals"" mode to provide actionable performance warnings and debugging information. There are also various issues surrounding compatibility, such as ensuring proper import statements, supporting different hardware capabilities (e.g., VSX, CUDA versions), and handling different data shapes (like batch vs. non-batch). Unresolved questions include the timeline for feature support, potential API changes for backward compatibility, and how to properly collect and utilize signals from monitoring jobs. Overall, these discussions reflect active development, bug fixing, and feature planning to enhance PyTorch's performance, usability, and hardware compatibility."
2021-10-22,pytorch/pytorch,"The discussions highlight compatibility issues involving multi-GPU communication, particularly with NCCL and AMD IOMMU, affecting distributed training and testing, with suggestions to verify NCCL functionality via tests. A recurring concern is GPU driver installation and the need for proper environment setup to avoid deadlocks and out-of-date behavior. Several comments address the importance of passing specific parameters like `find_unused_parameters=True` in `DistributedDataParallel` to resolve runtime errors. CI consistency and build stability are also emphasized, with potential issues stemming from environment discrepancies, package versions, or infrastructure changes. Overall, key unresolved questions relate to environment configuration, driver support, and ensuring robustness of distributed training workflows across different hardware setups."
2021-10-23,pytorch/pytorch,"The discussions highlight ongoing issues with CUDA and build environment compatibility, particularly regarding the presence of 'cub/version.cuh' in CUDA 11.0, and concerns about build failures and reverts, suggesting the need for clearer environment configurations or dependencies. Several comments reference CI failures and reruns, indicating unstable or flaky tests that may require improved test environment management. Questions about the presence of specific files in CUDA toolkits suggest the need to verify and possibly update build scripts or dependencies accordingly. Reverts and refactoring notes point to instability in PR workflows, emphasizing the importance of stable, tested changes before merging. Overall, unresolved questions about CUDA toolkit contents and CI stability remain, requiring further investigation and coordination."
2021-10-24,pytorch/pytorch,"The discussions highlight a recent change that was reverted due to breaking the `test_profiler` during CI testing, indicating instability or compatibility issues with that modification. Multiple comments relate to ongoing CI/CD process statuses, including triggered workflows and skipped tests, emphasizing the importance of robust testing and validation before merging impactful changes. There is a suggestion to carefully manage re-landing the reverted change, following internal workflow steps, to ensure stability. The CI summaries suggest active monitoring and troubleshooting to resolve build failures and test issues. Overall, the key concern revolves around maintaining stability after code changes, with attention to CI feedback and proper re-implementation procedures."
2021-10-25,pytorch/pytorch,"The comments primarily provide detailed reports on continuous integration (CI) test results for multiple pull requests in the PyTorch repository. They include CI flow statuses, triggered workflows, skipped tests, and links to detailed artifacts, highlighting the extensive testing matrix across different environments and configurations. There are no explicit technical concerns, questions, or suggestions raised; the content mainly documents build statuses and encourages rerunning failed workflows via specific commands. Overall, the discussion emphasizes monitoring CI health and ensuring successful builds across various platforms but does not address specific code or implementation issues."
2021-10-29,pytorch/pytorch,"The discussions mainly revolve around managing and removing hooks in PyTorch, with a suggested implementation for removing hooks by storing handles and calling `remove()` on them, which needs confirmation. There are concerns about ensuring deterministic behavior, especially regarding `index_put_` with duplicate indices and potential nondeterminism when using `gpuAtomicAdd`. Some issues address the correct aggregation of tensor accumulation results in ONNX, suggesting that the return value might not reflect the accumulated state. Additional questions include the support and testing for operations like `conv1d`, potential improvements in test infrastructure (e.g., handling `ir_version=3` failures), and best practices for documenting workarounds and compatibility issues. Overall, the conversations highlight ongoing work on hook management, deterministic operations, test stability, and clear documentation."
2021-10-30,pytorch/pytorch,"The discussions highlight concerns about test coverage and behavior with the meta device in PyTorch, noting that current test assertions like `assertEqual` are fragile for meta tensors because they rely on operations (e.g., `torch.isnan`) unsupported by the meta backend, leading to skipped tests. There is debate over whether existing tests should be modified or skipped explicitly via `skipMeta`, or whether meta support should be expanded to make tests more meaningful in this context. Several issues involve compatibility and correctness, such as how operations like `kl_div` and `pack_sequence` should handle meta tensors and nested data structures. Additionally, there are ongoing maintenance and stability concerns, especially with recent CI failures, flaky tests, or regressions caused by recent code changes. The overall theme is to improve test robustness and clarity regarding support and expected behaviors across different tensor backends."
2021-10-31,pytorch/pytorch,"The discussions highlight recurring issues related to slow or unreliable downloads of PyTorch and related libraries (like torch and torchvision), often addressed by environment adjustments or driver updates. There are technical inquiries about correct tensor operations, such as handling duplicate indices during accumulation or ensuring proper output in ONNX models, with some suggestions to improve documentation or fix bugs. Performance enhancements, such as optimizing index_select, have been successfully implemented and integrated. Several posts involve troubleshooting hardware compatibility, driver versions, or environment setup, emphasizing the need for clearer guidance and better tooling for debugging problems like segmentation faults or build failures. Overall, unresolved questions remain about reproducibility of certain errors, optimal configuration for multi-GPU or distributed training, and transparency of build and commit visibility for users."
2021-11-01,pytorch/pytorch,"The discussions mainly revolve around ongoing and unresolved issues in PyTorch, such as support for performing reduction operations like variance over multiple axes, with numpy-like behavior, and related PRs or features in development. Several reports concern potential memory leaks, especially with DataLoader workers, SHM exhaustion, and multi-GPU distributed training, with workarounds and debugging suggestions provided. There are technical questions about specific functionalities, such as the impact of support for PyPy, the behavior of `torch.all` in JIT tracing, and the stability of rendezvous in elastic training, including proposed API or implementation adjustments. Additionally, issues include build failures, platform-specific bugs, performance considerations for fusers, and maintainability improvements like symbolic functions and autodiff handling. Overall, unresolved questions pertain to bug fixes, feature support consistency, and better tooling or testing practices."
2021-11-02,pytorch/pytorch,"The comments cover a range of technical topics including content indexing and SEO strategies, specifically de-indexing pages for better snippet control; proposals and considerations for implementing padding schemes in convolutional layers, especially for strided ""same"" padding and its impact on support and validation; challenges with model conversion and tracing, notably handling variable-length sequences and ensuring correct behavior across different model operations like `adaptive_max_pool2d`; improvements to autograd and in-place operation detection, especially regarding multi-threaded and distributed contexts; and issues related to testing, environment setup, and compatibility with evolving hardware and software environments (e.g., Python 3.10, ROCm, device memory management). Several discussions also suggest restructuring, API consistency, or better tooling to enhance maintainability, performance, and user experience."
2021-11-03,pytorch/pytorch,"The discussions mainly revolve around implementing support for strided (particularly ""same"" padding with stride > 1) convolutions in PyTorch, questioning the current support status and error messages for such cases, and proposing formulas aligned with TensorFlow's padding schemes. There is also concern about the correctness and consistency of padding logic, especially in transposed convolutions, and the implication of different padding implementations (input-dependent vs. non-input-dependent). Additionally, some discussions touch on improving testing strategies, handling de-duplication of aliasing information, enhancing the TorchScript compiler to better handle Python-based operators, and addressing performance and memory optimization, especially in specific hardware contexts like ROCm and CUDA. Trade-offs between extending existing classes versus creating new abstractions (e.g., for shapes or optional tensors) are also considered, alongside the verification of bug fixes or feature support across multiple environments and hardware configurations."
2021-11-04,pytorch/pytorch,"The comments highlight multiple ongoing issues and discussions related to the PyTorch repository. Key concerns include the organization and discoverability of distributed optimizer APIs, with proposals to better link or integrate the documentation between the main `torch.distributed` page and the RPC framework docs. Several troubleshooting reports indicate persistent bugs, unexpected behaviors, or performance regressions—such as linear algebra backend replacements (MAGMA vs cuSOLVER), complex ONNX export issues, and CUDA/FP32 denormal handling—often addressed by temporary workarounds or future planned fixes. Additionally, there's discussion on improving testing, code organization (like moving classes into relevant pages), and stability of testing frameworks (spawn-based tests). Unresolved questions include the optimal placement of API documentation, handling of specific bug regressions, and how organizational changes can streamline discoverability and maintenance."
2021-11-05,pytorch/pytorch,"The discussions primarily revolve around the implementation and documentation of trilinear (and higher dimensional) grid sampling in PyTorch, with users questioning whether `grid_sample` correctly implements the expected interpolation, and suggesting the addition of more complex kernels (like sinc or cubic) to reduce blur, especially in medical imaging contexts. There are concerns about the behavior of the sampling coordinates relative to input dimensions (e.g., whether XYZ coordinates map properly to WHD, and how to clarify this in the docs). Several issues also address software stability and correctness, such as ensuring reproducibility, handling of arbitrary data types (e.g., boolean tensors), and maintaining consistent behavior with various hardware backends (CPU, CUDA, Metal). Unresolved questions include whether `grid_sample` can be extended to support new kernels, how to improve documentation clarity (coordinate conventions), and ensuring backward compatibility and correctness in edge cases like singular matrices or specialized hardware scenarios."
2021-11-06,pytorch/pytorch,"The discussions highlight ongoing challenges with adding new mathematical functions, such as Bessel functions and inverse gamma functions, to PyTorch's tensor operations, emphasizing the need for proper autograd support and integration into torch.special. There are concerns about the performance and accuracy implications of precision-reduction techniques in CUDA GEMM operations, with suggestions to document these nuances more clearly in the specifications. Compatibility issues with various environments, kernels, and GPU setups, particularly in multi-process or multi-threaded contexts, are also recurrent, raising questions about stability and correctness. Several discussions reference the process of extending PyTorch with custom operations or backend support, underscoring the importance of clear guidelines, test coverage, and documentation to facilitate contributions. Unresolved questions mainly revolve around the proper implementation of advanced special functions, autograd support, and best practices for integration and performance tuning."
2021-11-07,pytorch/pytorch,"The discussions highlight ongoing concerns about hardware and software support for AI workloads on Apple M1/M1 Max chips, with users comparing M1 performance to NVIDIA GPUs and requesting updates from the PyTorch team on compatibility and optimization efforts. There are technical questions about implementing custom operations such as FFT in Metal, handling auto-fallback mechanisms for unsupported operators, and ensuring proper integration with frameworks like ONNX and ONNX Runtime, especially regarding data types like BFloat16 and boolean tensors in DLPack. Additionally, issues related to build system reliability, dependency management, and documentation clarity are raised, alongside efforts to improve CI stability and reproducibility. Some discussions suggest modifying or extending existing APIs for kernel dispatch, tracing overrides, and synchronization primitives, indicating active development and debugging in the framework's backend support."
2021-11-08,pytorch/pytorch,"The discussions raise several key concerns, primarily regarding framework support and API design decisions. Some conversations question the stability and compatibility of different CUDA versions and hardware architectures (e.g., RTX 2080, Tesla T4, Maxwell) with PyTorch, as well as support for specific features like `sum_to_size`, `TensorPipe`, or operator fusion patterns, especially for mobile and specialized backends like MKLDNN or ROCm. There are also questions about API improvements, such as substituting `None` for undefined tensors in autograd, and the need for clearer documentation or API changes to manage performance, backward compatibility, and user experience. Additionally, some technical debates focus on the implementation details of support for container types, the importance of supporting GPU/CPU concurrency, and the impact of framework refactoring on build and runtime behavior. Unresolved issues include support for older CUDA versions, multi-backend configuration, and explicit handling of automatic mixed precision or dtype support."
2021-11-09,pytorch/pytorch,"The comments highlight ongoing issues with slow or unreliable downloads for PyTorch on various platforms, as well as difficulties in exporting or converting models to ONNX format, especially with TorchScript models. There is discussion about the API design for custom message types and operators, emphasizing the need for simplified, user-friendly interfaces that abstract away internal message type classes. Some threads address the correctness and numerical stability of low-precision computations, such as TF32 and BF16, and whether current default settings should be adjusted for better trade-offs. Additionally, the conversations touch on test organization, backend support, and ensuring robustness across hardware (e.g., ROCm, CUDA, and different hardware generations). Many unresolved questions involve improving user experience, API consistency, and maintaining correctness while evolving the internal infrastructure."
2021-11-10,pytorch/pytorch,"The discussions highlight several key technical issues: the proper use of the `if __name__ == '__main__':` idiom, especially when implementing multiprocessing in Windows and different environments; handling of deprecated or internal APIs such as the `resize_` function, with suggestions to better control memory reuse without unintended reallocations; the importance of test reproducibility and proper validation, particularly around GPU-based operations and floating-point precision concerns; the need for clearer documentation and support for complex number inputs and broadcasting behaviors; and addressing internal build and dependency issues, such as module imports, package structuring, and cross-platform compatibility, particularly for CUDA, ROCm, and ONNX export functionalities. Many follow-up actions involve code refactoring, improved error messaging, environment-specific adjustments, and establishing best practices for contribution and testing."
2021-11-11,pytorch/pytorch,"The discussions highlight issues related to CUDA and GPU device memory management, with concerns about the accuracy and stability of certain operations (e.g., interpolation, convolution, automatic memory clearing). There are questions about correct implementation strategies for CUDA stream synchronization, efficient memory reuse, and proper handling of sparse tensors. Several discussions suggest the need for more precise control over platform-specific tests, caching behaviors, and proper propagation of data types and device contexts in complex models. Unresolved concerns include ensuring deterministic behaviors, correct automatic cache clearance on CPU-only environments, and managing platform-specific limitations or bugs (e.g., in cudnn, ROCm, or specific hardware like M60 GPUs)."
2021-11-12,pytorch/pytorch,"The discussions highlight several technical concerns: the proper configuration of MKL threads in PyTorch, especially in multi-process or distributed settings; the need for better support for sparse matrix operations and efficient row-slicing on GPU; issues with DataLoader memory corruption, particularly in multiprocessing contexts within Docker or certain environments; and the importance of robust error handling and proper API design for modules like `torch.distributions` and module export mechanisms like `@torch.jit.export`. Many questions revolve around improving performance (e.g., optimized sparse convolution, CUDA kernels for integer matmul), ensuring compatibility (e.g., device-agnostic code, proper serialization of distributed states), and clarifying best practices for interfacing with low-level components, while unresolved issues remain about memory management in multi-GPU/multi-process setups, the long-term API structure, and ensuring correctness in quantization and tensor shape inference."
2021-11-13,pytorch/pytorch,"The discussions highlight efforts to improve consistency between PyTorch and TensorFlow resizing implementations by adjusting parameters such as `align_corners` and `half_pixel_centers`, noting that aligning these settings can make behaviors similar. There is interest in leveraging `torch.nn.functional.grid_sample()` to replicate or approximate resizing behavior for better compatibility. Several issues pertain to platform-specific support, especially on Apple Silicon (M1) with Metal, and challenges in ensuring reproducibility, correctness, and performance across diverse hardware. Support for additional data types like `uint16` in `torchvision`, along with CUDA and cuDNN compatibility issues (notably on older GPUs like M60), are also discussed. Furthermore, some active development relates to backend support, memory planning, and ensuring models (such as RNNs) remain scriptable, alongside general infrastructural and CI stability concerns."
2021-11-14,pytorch/pytorch,"The discussions primarily focus on improving the usability and functionality of PyTorch's APIs, such as implementing a proper `reset_parameters` method for optimizers to handle scenarios like freezing layers, and adding a `symmetric` constraint for matrices in `torch.distributions.constraints`. There are concerns about manual tracking of nested OpenMP (OMP) and memory management issues, including handling the memory overhead caused by certain operations like `adaptive_avg_pool2d` during ONNX export and the need for a CPU caching allocator to optimize RAM usage. Additionally, users inquire about modifying `grid_sample` implementations for better gradient support and question the support and compilation process of custom C++/CUDA extensions, especially on different hardware architectures. Unresolved questions involve API design choices for optimizer state resets, API support for symmetric constraints, and improving memory allocation strategies to prevent out-of-memory errors."
2021-11-15,pytorch/pytorch,"The discussions highlight challenges in extracting input shape information from loaded or traced models, particularly for deployment systems like LibTorch, as PyTorch layers don't natively store input shapes. There are multiple attempts and suggestions, such as embedding shape info as module attributes or making inference from graph constants, but no built-in or consistent method currently exists. Several comments note workarounds—like manually recording input shapes, modifying model code, or exporting models with fixed sizes—that may not generalize well. There are also issues related to ONNX export compatibility, runtime behavior differences across hardware (e.g., ROCm vs CUDA), and the need for more robust tooling or API support for shape inference and metadata extraction. Overall, proposals aim to improve ease of shape retrieval for deployment, but a comprehensive, standardized solution remains unresolved."
2021-11-16,pytorch/pytorch,"The discussions largely revolve around issues related to model exporting, ONNX compatibility, and static or symbolic graph representations in PyTorch, often highlighting specific problems like runtime errors, mismatched Jacobians during gradient checks, and limitations in exporting certain layers or models (e.g., HRNet, custom ops). Several comments suggest that some errors may stem from outdated APIs, incorrect usage, or internal limitations such as 32-bit memory constraints (e.g., SVD workspace size). There are repeated concerns about ensuring proper support for features like masks, broadcasting, and parameters' data types, as well as the need for more robust testing, more informative error messages, and clearer documentation—especially around the relationship between eager, static, and symbolic modes. Some proposals highlight the potential benefits of adding fallback mechanisms, improving shape inference, and surfacing more descriptive errors or checks to facilitate debugging. Unresolved questions include how to handle large models given hardware interfaces (e.g., workspace size limits) and whether certain features like mask broadcasting or parameter type changes could be safely integrated or require further API adjustments."
2021-11-17,pytorch/pytorch,"The discussions highlight concerns about ensuring correctness and compatibility in complex PyTorch functionalities, especially related to ONNX export support for operators like adaptive_avg_pool2d and resize with strides, where errors often occur due to unsupported or undefined behaviors in certain operators or device-specific issues. There are questions about handling large tensor sizes and indexing, especially in distributed or GPU contexts, with suggestions for safer, more robust approaches such as using double precision or introducing wrapper store classes to prevent race conditions. Some discussions address the stability and reproducibility of tests, particularly on different hardware platforms like ROCm and Mac M1, with indications that certain failures are due to environment-specific or implementation details rather than code defects. Additionally, there's a recurring theme of improving API design and user convenience, such as consolidating labels or introducing new store wrappers, while maintaining backward compatibility and avoiding breaking existing use cases. Unresolved questions include best practices for failure handling, store design, and test robustness across diverse hardware and software environments."
2021-11-18,pytorch/pytorch,"The discussion encompasses multiple technical issues in the PyTorch repository, including challenges with linting and IDE configurations (e.g., `pylint` issues with PyTorch, and customizing VS Code linters), and advanced data loading strategies for multi-cluster datasets, exemplified by the `ClusterRandomSampler` implementation. There are concerns about CI failures and environment setup, especially on multi-machine and distributed training (e.g., GLOO initialization on Windows, shared memory limitations, and variables like `ROCM_VERSION`). Several discussions focus on the design and semantics of tensor operations, such as `resize_` with strides, restriding complexities, and the consistency of internal vs. representation equality in COO tensors. Additionally, there are questions regarding the compatibility of DLPack headers across different versions and ABI stability, and considerations for merging or deprecating features, such as `autocast` for BF16, `scatter_reduce` APIs, and store/mutex usage for concurrency control. Unresolved questions include ensuring test coverage for new features, handling CI environment inconsistencies, and proper documentation for complex internal behaviors."
2021-11-19,pytorch/pytorch,"The comments reveal that several implementations of ""same"" padding with `stride > 1` (notably in `conv.py`) and transposed convolutions do not handle ""same"" padding correctly in PyTorch, leading to discrepancies in expected output shape and size, especially when combined with `padding=""same""`. This inconsistency stems from a fundamental difference between how PyTorch and TensorFlow define ""same"" padding, as well as limitations in current APIs, such as restrictions on `resize_output` for strided convolutions, and the lack of a general `Pad` operator in Caffe2 for non-2D padding. There is ongoing debate about whether to fix the semantics of `resize_` to support restriding with different storage sizes, or to always return contiguous tensors for certain operations like `sort` and `topk`. Additionally, several discussions concern how to properly support and test sparse tensor operations, especially in regard to autograd, saving/loading, and support for constraints like positive-definite matrices. Some issues are resolved in newer PyTorch versions or through workarounds, but core API inconsistencies and incomplete support for certain layer behaviors remain unresolved."
2021-11-20,pytorch/pytorch,"The comments highlight various issues in the PyTorch repository, including compatibility and support for sparse CSR matrices, autograd behavior, and ONNX export limitations. Several users report deadlocks, unsupported features (like CSR transpose and second-order autograd), and bugs when using certain hardware configurations or specific functions, suggesting that some features (e.g., transpose of CSR) are under development or require workarounds. There are ongoing discussions about better handling of certain tensor operations, support for different data types (like bool and complex), and ensuring BC compatibility, with proposals such as making specific functions like normalized or masked normalization more consistent and publicly accessible. Additionally, issues around build environment configurations and integration with external systems (e.g., AWS Lambda, ONNX) are mentioned, along with plans for incremental improvements like adding support for more sparse algebra operations and autograd support for sparse tensors. Unresolved questions include how to extend support for transposing sparse matrices efficiently, manage BC breaking changes, and improve test coverage for new features."
2021-11-21,pytorch/pytorch,"The discussions primarily focus on memory management inefficiencies in PyTorch, particularly regarding intermediate tensor storage during computations like reductions, autograd, and sparse matrix transpositions. Contributors express concern over increased GPU memory usage in specific operations and the lack of optimization shortcuts within autograd to reduce memory footprint. Some suggest implementing features like COO transpose as a view to improve efficiency, while others identify issues with quantized operators and threading behavior that impact performance. There are ongoing efforts to improve CUDA, QNNPACK, and backend support, with community contributions and upcoming support plans for hardware like Apple M1 GPUs. Open questions remain about better memory optimization strategies, the behavior of certain tensor operations, and improvements to geometrical and sparse matrix handling."
2021-11-22,pytorch/pytorch,"The discussions reflect ongoing efforts to improve and standardize PyTorch's features including JIT compilation for custom RNNs, distributed training stability, and sparse tensor support, with emphasis on ensuring usability and robustness. Several issues pertain to CUDA/NCCL memory and communication bugs, especially when scaling across multiple GPUs or nodes, some related to driver incompatibilities or specific hardware (e.g., NVIDIA vs AMD, Jetson, Mac M1). There are concerns about the consistency and correctness of tensor views, especially around sparse matrices and layout sharing, which impact autograd and serialization. Enhancements are suggested for documentation, testing, and interface consistency (e.g., making transpose operations truly Views, handling sparse tensor properties, or adding convenience functions like `eye_like`). Some unresolved questions involve security implications of GPU access, the support for newer hardware features (e.g., Apple Neural Engine, AMD CPUs), and the need for accurate, comprehensive tests for recent backend changes or bug fixes."
2021-11-23,pytorch/pytorch,"The discussions highlight several issues: (1) inefficiencies in tensor data access, with suggestions to support the buffer interface or use `memoryview` instead of `tobytes`; (2) performance irregularities with `topk` on small tensors and kernel strategies, prompting considerations for heuristics; (3) compatibility and installation concerns, especially related to CUDA versions, GPU types, and environment setup, with workarounds like downgrading or manual wheel installation; (4) specific bug reports and fixes, including handling multi-machine distributed training over filesystems or TCP, and issues with specific hardware like Jetson or AMD ROCm; and (5) ongoing CI failures, flaky tests, and build system inconsistencies that need addressing, often accompanied by suggestions for code fixes or better testing practices. The unresolved questions include how to improve tensor buffer support natively, resolve performance regressions, and handle multi-node training reliably."
2021-11-24,pytorch/pytorch,"The discussions highlight various technical challenges and considerations, including the impact of software versions (e.g., PyTorch, CUDA, CMake, dlpack headers) on stability and compatibility, especially across different hardware (GPUs, CPUs, ROCm). Several issues relate to numerical stability, such as NaNs in loss calculations, imprecise distance computations with float64, and the need for safer softmax implementations. Memory management concerns also feature prominently, with approaches like reducing batch size, using `torch.no_grad()`, and handling variable-length sequence padding to avoid NaNs during attention or training. Additionally, there are unresolved questions about the behavior of operators like `maxunpool` with mismatch output sizes, and compatibility or ABI concerns with external libraries like dlpack and thrust. Overall, many discussions revolve around improving robustness, correctness, and hardware compatibility in PyTorch's core and deployment pipelines."
2021-11-25,pytorch/pytorch,"The discussions highlight several key concerns: (1) Compatibility issues when converting CPU tensors with unsupported dtypes like bfloat16 to numpy arrays, with proposed fallback solutions such as using `dtype=""V2""` but risking loss of dtype information; (2) Reproducibility and stability of bugs such as deadlocks, GPU memory issues, or NaNs in attention mechanisms, with suggestions for safer softmax implementations and handling of mask values; (3) Hardware and driver compatibility challenges, especially with newer GPUs like RTX 3090 or AMD ROCM, and ways to build or install compatible PyTorch binary distributions; (4) API design questions, including the possibility of supporting non-module classes with `__call__` for flexibility, and implications of changes for code reuse; (5) Ongoing efforts to improve internal infrastructure, including build workflows, CI/CD pipelines, and handling of dynamic shapes or tensor format support."
2021-11-26,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's API and infrastructure, including adding features like `torch.searchsorted`, interpolation functions, and metadata support for ONNX exports, with community contributions and PR reviews. There is concern about edge-case correctness for custom interpolation implementations, with suggestions to adopt reliable methods such as `torch.searchsorted` combined with precise interpolation formulas. Compatibility issues are discussed, particularly with package management and environment conflicts involving tensorboard, TensorFlow, and dependencies like `pyyaml` and `tensorboardX`. Support for new Python versions (e.g., 3.10) is being planned, considering dependency updates and CI build loads. Additionally, performance optimization efforts, such as specialized kernels for quantized CPUs and parallelization strategies, are actively pursued, along with addressing CI pipeline failures and infrastructure constraints."
2021-11-27,pytorch/pytorch,"The discussions highlight several technical concerns, including the difficulty of supporting UTF-8 characters in tensor operations like einsum, and the potential methods for simplifying implementation such as mapping characters to numbers within a limited range. There are questions about how to extend support to more characters, including Greek letters, without external dependencies like libicu, and considerations about response limitations when handling large numbers of tensor dimensions. Some comments propose modifications to existing code (e.g., in Linear.cpp and functional.py) to support broader character sets by using number-based labeling, but testing reveals issues like support for tensors with more than 64 dimensions. Additionally, issues related to compatibility, especially with different CUDA versions and PyTorch releases, as well as the need for rebase and proper testing, are recurrent, alongside ongoing discussions about features like model size optimization and distribution constraints."
2021-11-28,pytorch/pytorch,"The comments highlight several technical concerns related to PyTorch, including the need for clearer error messages for out-of-memory issues during data loading, and potential improvements in distribution implementations such as Wishart distributions and matrix shape handling. There are ongoing discussions about compatibility and ABI stability regarding DLPack, as well as ensuring consistent behavior in tensor shape reductions and shape mismatches in specific functions like `_PositiveDefinite`. Some comments address build and environment issues, such as CUDA version dependencies, CMake version requirements, and distributed mode bugs. Additionally, there are suggestions to clean up or move related changes into unified PRs and considerations about test coverage and integration testing."
2021-11-29,pytorch/pytorch,"The discussions primarily revolve around bug fixes, feature enhancements, and maintenance concerns within the PyTorch ecosystem. Several issues address core functionality such as improving kernel performance (e.g., embedding backward timing), enhancing ONNX export metadata (e.g., scope, training info), and fixing shape/inference bugs (e.g., in fold shape checks, sparse tensor serialization). There are ongoing discussions about adding support for new hardware backends (like Apple Silicon GPU/Neural Engine) and clarifying behaviors in operators (like `unpool` output sizes, `sort` stride preservation). Contributors also raise questions about test robustness, deprecation policies, and compatibility issues with previous Python or CUDA versions. Throughout, there is a focus on stabilizing existing features, extending operator support, and ensuring accurate, future-proofed behaviors in the PyTorch framework."
2021-11-30,pytorch/pytorch,"The discussions mainly revolve around addressing out-of-memory errors, performance issues, and compatibility concerns in PyTorch's codebase. Several comments suggest reducing batch sizes, using alternatives like auto-adjusting batch size libraries, or applying specific workarounds such as explicit synchronization on CUDA or disabling mkldnn to resolve memory bugs. There are also questions about the support for certain operations (like in-place `ldexp`, `matmul`), differences in data types with newer libraries, and compatibility of specific features (like quantization schemes and ONNX export) across versions. Some comments highlight the need for clearer documentation, more robust error handling, and the importance of tests and proper ventilation for regressions, especially for internal CI and external user workflows. Unresolved issues include performance impacts of certain implementation changes, dependencies on specific hardware/software configurations, and ensuring consistent support across platforms and versions."
2021-12-01,pytorch/pytorch,"The discussions cover a range of technical issues and enhancements related to PyTorch, including device placement and CUDA tensor handling, sparse matrix operations support, performance optimizations with Triton, and ONNX export support for operations like `triu`. Several questions revolve around compatibility and support of newer features, such as sparse maximum support, complex number operations, and operator support for export to ONNX, especially for dynamic or variable parameters like diagonals. Efforts are also discussed around improving data loading architectures, autograd handling of undefined tensors, and better modularity and extensibility in operator registration and fusion passes. Several unresolved questions include how to handle certain operator export limitations, support for custom convolution/backends like ROCm and QNNPACK, and ensuring reproducibility and determinism in tests involving randomized operations."
2021-12-02,pytorch/pytorch,"The discussions largely revolve around optimizing CPU threading in PyTorch, specifically enabling a ""smart switch"" or unified API to restrict all threading for OpenMP and BLAS, which is crucial for resource sharing in constrained environments like academic clusters. Several users highlight that `torch.set_num_threads(1)` and related functions help, but inconsistencies remain, especially regarding thread counts and affinity, and the effectiveness varies across different configurations. Issues also include the need for better environment management, accurate thread counting, and cross-platform or backend detection, particularly with issues related to OpenMP fork resilience and interference from external libraries (like OpenCV or Open3D). Some discussions suggest enhancing API support, like adding out variants or new functions for more flexible control (e.g., masking or broadcasting support). The ongoing unresolved questions include how best to enforce thread limits comprehensively and detect threading issues dynamically, especially with diverse backend fallbacks or external interactions."
2021-12-03,pytorch/pytorch,"The discussions highlight concerns about integrating hardware acceleration on newer platforms, such as Apple Silicon and M1 Macs, with questions about support for GPU and Neural Engine resources, and whether that would leverage multiple cores or rely on limited features like Metal kernels. Several comments address ongoing efforts to support M1 GPUs, with updates indicating a re-implementation using new approaches, and questions about whether training could utilize available cores or neural units effectively. Other issues involve the complexity of critical-path code churn (e.g., profiling, autograd, and distributed training) and whether upstream changes or backports are feasible, alongside questions about compatibilities with different CUDA versions, compilers, or Python environments. Additionally, some discussions query why specific testing or environment configurations (e.g., Python 3.8 images, CUDA contexts, or logics around optional tensors) are set in certain ways, reflecting larger concerns about build stability and feature correctness. Lastly, unresolved questions about supporting dynamic features like label smoothing in segmentation and large-model training on emerging hardware remain open, indicating ongoing development and integration challenges."
2021-12-04,pytorch/pytorch,"The discussions highlight ongoing efforts to improve hardware support and compatibility within PyTorch, such as integrating DML support via tensor.to(""dml""), addressing CUDA-related errors (e.g., system not yet initialized and out-of-memory issues), and exploring backend extensibility for diverse hardware including Windows ML/DirectML and third-party backends like DirectML and ROCm. There are concerns about CUDA memory management, especially related to detecting available memory using `cudaMemGetInfo()` to prevent memory allocation failures during tests. Some issues involve compatibility with macOS ARM (Apple M1) and ensuring proper package installation, with suggestions to update pip and PyTorch wheels. Unresolved questions include plans for future direct integration of DML support for tensor operations, and the need for more robust, generalized support for masked operations and hardware-specific acceleration."
2021-12-05,pytorch/pytorch,"The discussions predominantly revolve around extending PyTorch's support for advanced autograd features, such as custom functions and automatic differentiation (forward and backward modes), with ongoing efforts to improve usability, performance, and compatibility across hardware like M1 GPUs and Android devices. Several comments highlight current limitations, such as incomplete or unimplemented support for certain autograd operators (e.g., linalg_vector_norm, softplus), and challenges in correctly computing higher-order derivatives like Hessian-vector products, especially in non-linear networks. There are also issues related to hardware and environment-specific problems, such as GPU memory exhaustion on certain systems and discrepancies in image processing between PIL and other libraries, which impact model deployment and inference. Additionally, the repository is actively developing new features (e.g., support for Apple Silicon GPUs, Neural Engine) and addressing code contributions, with questions about timelines and functionality. Overall, the core concerns involve enhancing autograd capabilities, improving cross-platform support, and resolving hardware-specific bugs, with some features still under development or awaiting fixes."
2021-12-06,pytorch/pytorch,"The discussions highlight concerns about clarity and maintainability in PyTorch, such as the need for more precise error messaging during DataLoader worker failures, the potential to simplify type systems by removing OptionalType, and improving support for newer hardware like M1 GPUs and complex input types. There is interest in enhancing automation and robustness in build and CI processes, including better tracking of flaky tests, support for new operators (e.g., `scatter_reduce`), and the integration of features like deterministic operations, multi-process data loading, and expanded support for custom CUDA kernels. Some participants emphasize the importance of backporting upstream improvements, avoiding unnecessary skipping of tests, and providing clearer documentation and error reporting. Unresolved questions include how to improve the support for convolution operators, handle scientific reproducibility with deterministic ops, and streamline the code generation and testing of new features like `__torch_dispatch__`."
2021-12-07,pytorch/pytorch,"The comments reveal ongoing issues and discussions around PyTorch's implementation details, including support for ""same"" padding with stride > 1, and the need for clearer documentation and support for negative dimensions in sparse tensors. Several users also raise concerns regarding compatibility and build dependencies, especially for older PyTorch versions or specific hardware (e.g., ROCm, CUDA, Windows). Additionally, there are questions about the proper design of custom functions, custom backpropagation, and the handling of non-contiguous tensors, indicating interest in flexible, reliable, and backward-compatible extensions. Some discussions hint at experimental features, refactoring strategies (e.g., for `TensorIterator`, lazy kernels, and custom dispatch), and performance profiling, reflecting a focus on correctness, usability, and diagnostics. Overall, unresolved questions involve how to improve support for various tensor operations, maintain backward compatibility, and enhance documentation and tooling for both internal development and external users."
2021-12-08,pytorch/pytorch,"The discussions primarily revolve around challenges in customizing and extending PyTorch's infrastructure, including issues with virtual environment detection of protoc, implementing differentiation support for complex operators, and managing memory, especially with CUDA contexts and shared resources. Several suggestions advocate for simplifying code pathways, such as switching from `exponential_family` inheritance, consolidating `__module__` information, or creating helper functions like `torch.to` and `__torch_function__` modes for better customization. Concerns are also raised regarding build system complexities, compatibility with different CUDA versions, and handling system dependencies like cmake and compiler versions. Many unresolved questions pertain to ensuring the correctness of custom kernel implementations, compatibility of static/dynamic configurations, and effective testing strategies to cover new extensions and modifications."
2021-12-09,pytorch/pytorch,"The discussions mainly revolve around the migration from `.cuda()` to `.to(device)` in PyTorch, emphasizing the need for a flexible environment where tensors can be automatically migrated across devices, proposing context managers like `torch.cpu()`. There are technical challenges noted, such as the difficulty of creating dual-tensors for Jacobian computations, and questions about supporting batch Jacobians for per-sample derivatives, especially in the context of higher-order derivatives and custom functions. Some concerns involve ensuring consistent support for operations that involve in-place modifications, batching across layers of models, and handling specific function signatures for operators like `normal.float_float`. There are also ongoing efforts to refactor code organization (e.g., introducing utility subdirs) and platform/build system improvements, while unresolved issues include ensuring test robustness, performance regressions, and compatibility with features like TorchScript and ONNX. Overall, the key questions concern improving device-agnostic tensor management, flexible Jacobian computation, and ensuring that operator and API support support both performance and generality."
2021-12-10,pytorch/pytorch,"The comments highlight ongoing issues related to CUDA memory management, backpropagation correctness, and testing framework improvements. Several discussions emphasize the importance of implementing proper Jacobian computations for functions like `cummax` and ensuring backward functions support sparse and tensor operations correctly. There are concerns about environment-specific problems, such as driver compatibility and build configurations, particularly with recent PyTorch and CUDA versions, which impact reproducibility and stability. Additionally, there is interest in refining documentation for function version tracing, error handling for input validation, and simplifying debugging and testing processes (e.g., saving graphs or verifying outputs). Overall, unresolved questions include improving backward function correctness, environment setup clarity, and ensuring consistency with older PyTorch versions."
2021-12-11,pytorch/pytorch,"The discussions mainly revolve around improving type annotation correctness and testing in PyTorch, with proposals including integrating tools like Numba and Typeguard for more effective validation, and adopting NumPy's compact testing syntax with `# E:` comments. There is also ongoing work on refining function implementations, such as `unravel_index`, to optimize performance and static typing testing frameworks, and resolving issues related to CI setup, dependency management, and error messaging. Many comments suggest enhancing user experience for package installation, ensuring accurate error reports, and streamlining workflows for different hardware and environment configurations. Unresolved questions include specific code modifications needed for certain persistent errors, dependency integration strategies with Poetry, and better automation or tooling for managing test artifact outputs."
2021-12-12,pytorch/pytorch,"The discussions primarily address challenges with enabling gradient computation for integer or quantized tensors in PyTorch, including handling custom autograd modules with integer outputs and passing gradients through STE estimators. There are concerns about compatibility and errors when working with specific operator supports, such as `F.interpolate` on BFloat16, and issues with backend-specific build configurations like `aten::equal` for sparse tensors. Several conversations explore advanced gradient and Jacobian computation techniques, such as using `autograd.functional.jacobian` and FX graph capturing with `make_fx`, aiming to optimize and verify gradients, especially in dynamic or shape-variable models. Additionally, there's discussion on performance measurement and accurate FLOP modeling for backward passes, highlighting potential bugs or misconceptions. Unresolved questions include handling dynamic shapes, ensuring cross-backend compatibility, and integrating these advanced features into PyTorch's core toolchain."
2021-12-13,pytorch/pytorch,"The discussions encompass several technical topics, notably the need to implement recursive parameter retrieval in C++ frontend modules (as suggested in #28478), the handling of model saving, loading, and permission issues (#43196), and performance optimizations in operations like `torch.unfold` (#60466). Furthermore, there are ongoing developments around enhancing support for hardware features (e.g., low-end GPUs, BFloat16, and CUDA/CUDNN versions) and ensuring cross-platform consistency, sometimes revealing build or environment-related bugs that require fixes or reconfiguration (e.g., #68689, #68687). Several threads also address API design considerations, such as exposing functions without complete docstrings (#69852), and the adequacy of existing testing protocols and coverage for backward formulas and sparse tensor operations (#69867, #69862). Overall, unresolved questions include how to extend recursive parameters in C++, improve model serialization practices, optimize particular kernels, and ensure comprehensive testing coverage, especially for more complex or sparse tensor functionalities."
2021-12-14,pytorch/pytorch,"The discussions highlight issues with distributed training and gradient handling when using Apex's DistributedDataParallel (DDP), especially in scenarios involving freezing layers or handling None gradients, and the need to verify whether Apex DDP supports these use cases in current implementations. Multiple experimental problems are associated with exporting models to ONNX, notably discrepancies in layer behavior (e.g., average pooling padding, setting of 'pads') and limitations in support for certain operators (e.g., `torch.roll`, `quantize_per_tensor`), often requiring workarounds like manually adjusting padding or replacing operators. Several concerns also address compatibility and robustness, such as ensuring backward compatibility for newly added functions, dealing with device assertions and out-of-bounds indices, and fixing build issues related to linking external libraries (like cuRAND) in custom extensions. The necessity of improving documentation, error messaging, and supporting frameworks like CoreML or integrating with hardware-specific accelerators (e.g., Apple's M1 GPU, TensorRT) are also recurring themes. Lastly, there are ongoing questions about proper dependency management, version compatibility, and migration strategies for version-specific features or deprecations, with some unresolved issues awaiting broader support or review."
2021-12-15,pytorch/pytorch,"The discussions reveal ongoing challenges with multi-GPU training, particularly with distributed data parallelization methods like DDP and apexDDP, especially when dealing with models that involve non-standard output layers such as adaptive-softmax. Users report issues with gradient handling, parameter freezing, and the compatibility of certain modules (e.g., BatchNorm variants, softmax output layers) with specific distributed wrappers. There are also concerns about CUDA-related performance bottlenecks, device compatibility, and the need to support features like gradient segmentation or custom backends, which are sometimes hindered by API constraints or lack of proper testing. Additionally, logistical issues such as environment setup (CUDA, PyTorch versions), build configurations, and CI pipeline stability are prevalent, alongside questions about integrating new features like implicit quantization or batchnorm conversion utilities. The unresolved questions center around improving debugging support, ensuring backward compatibility, and clarifying API behavior across various configuration scenarios."
2021-12-16,pytorch/pytorch,"The discussions highlight several technical concerns: the persistence of performance issues (e.g., slow `topk` or matrix operations) until specific fixes are merged; challenges in exporting models with operations like `unfold` that may lack direct ONNX support; the complexity of correctly handling in-place operations in gradient computations and the need to fix or update derivative formulas; inconsistencies in using `like`-style tensor creation—some suggest replacing it with options argument to avoid confusion; and the importance of properly skipping or fixing problematic test cases involving custom subclasses or certain hardware-specific issues (like ROCm or CUDA). Unresolved questions include how to ensure runtime behavior (especially for error handling and op support) is robust, and the best ways to refactor tensor creation APIs for clarity and correctness."
2021-12-17,pytorch/pytorch,"The comments primarily address various technical challenges and considerations in PyTorch development, such as the necessity of precise error handling in tensor operations (e.g., ensuring proper exceptions for zero-dimensional tensors in functions like `swapaxes` and `permute`), and the importance of clarifying default behaviors for quantization to match NumPy and other standards, especially regarding tolerances and module support. Additionally, there are discussions on build system issues, like linking to the correct `ld` in conda environments, and the need for clear documentation and test coverage for features such as implicit quantization, CUDA memory management, and custom backend support. Some comments highlight ongoing CI failures or environment-specific bugs, and there is interest in updating docs and error messages to improve user experience and consistency. Unresolved questions include the best approach to handle certain edge cases in tensor shape operations, whether to deprecate no-op behaviors, and if certain features like implicit quantization should be further developed or deprecated."
2021-12-18,pytorch/pytorch,"The discussions primarily revolve around optimizing pairwise cosine similarity computations, emphasizing the trade-offs between batch processing, memory usage, and parallelization across CPU and GPU, with suggestions to use tensor broadcasting for efficient calculations. Additionally, there are concerns about performance regressions when updating PyTorch versions, potentially due to backend differences or environment settings (e.g., cudnn deterministic mode). Several issues related to training speed, reproducibility, and hardware utilization are raised, alongside questions about environment variables like `OMP_NUM_THREADS` and `CUDA_LAUNCH_BLOCKING`. Some discussions suggest that certain features, such as quantized ops and profiling, need better static analysis tools or explicit support to ensure compatibility and performance optimization. Unresolved questions include the exact causes of performance degradation between versions and the best practices for ensuring deterministic results while maintaining speed."
2021-12-19,pytorch/pytorch,"The discussions highlight several technical concerns, including the appropriate handling of sparse tensor operations and the use of ""*_like"" factory functions, with suggestions to treat these as memory allocators due to their storage-specific semantics. There is a recurring debate on whether to throw errors or perform no-ops for functions like `moveaxis` and `transpose` when applied to 0-dimensional tensors, with a lean towards raising errors for future NumPy compatibility. Regarding distribution modules, a decorator-based approach to ""modulify"" distributions is discussed, emphasizing the need to register parameters as buffers and manage lazy properties efficiently. Compatibility with JIT tracing, particularly avoiding data-dependent control flow in `rsample` methods, is a noted concern, with proposed solutions involving `torch._C.get_tracing_state()` checks. Lastly, questions about the `torch.linalg.LinAlgError` incorporation from C++ are raised, suggesting the need for deeper integration between C++ and Python error handling to maintain consistency and error semantics."
2021-12-20,pytorch/pytorch,"The discussions highlight several technical concerns and proposals, including the desire to enhance PyTorch's Python API with functions like `torch.deep_to()` and a more general `Modulify` class for handling distribution classes, emphasizing the separation of concerns to avoid interface bloat. There are recurring issues related to exporting TorchScript models to ONNX, with questions about supporting conversion from TorchScript and differences in handling intermediate nodes and graph transformations. Additionally, some comments address the challenges of maintaining flexible, extensible code structures—such as the handling of tensors and modules across different backends, and meta-programming concerns like avoiding circular dependencies in core tensor classes. A notable unresolved question is whether changing internal implementations, such as enabling custom `pin_memory()` or supporting heterogeneous device loading, can be achieved safely without impacting existing code. Overall, the discussions suggest a push for more modular, flexible APIs, better cross-platform/export support, and clearer documentation around these advanced features."
2021-12-21,pytorch/pytorch,"The discussions mainly revolve around two key technical issues: (1) the inefficiency of a proposed `Modulify` class for caching distribution objects in PyTorch, which fails when distributions internally create tensors unrelated to initialization arguments; and (2) complications with dynamic linking and dependency resolution of `libtorch.so` and its related libraries, especially on newer gcc versions, leading to undefined symbols and build failures. There are concerns about how to properly handle tensor subclassing and the inheritance structure for LazyTensor and its backends (e.g., XLA), with suggestions to move towards pointer-based polymorphism for flexibility. Additional questions include timing and platform support for upcoming ONNX features and specific failures in quantization and NVIDIA CUDA builds, some of which appear hardware or driver related. Overall, unresolved issues relate to build system dependencies, internal API design, and improving clarity and efficiency in distribution and hardware abstraction layers."
2021-12-22,pytorch/pytorch,"The collected GitHub comments highlight ongoing challenges with DataLoader multiprocessing on Windows, especially issues related to `num_workers > 0`, support for `persistent_workers`, and the inability to bypass CPU bottlenecks in favor of GPU processing on Windows, with suggestions like increasing `num_workers` or using `persistent_workers`. Several discussions mention the support and exportability of models like Mask R-CNN, `torch.unbind`, and other operators to ONNX, noting that certain operators (e.g., `Resize`, `NonMaxSuppression`) are not well supported or have been under review. Compatibility issues between PyTorch versions, ONNX opset versions, and target frameworks (like CoreML or TensorRT) are also a concern, especially with model-related errors and version mismatches. Moreover, discussions include concerns about supporting latest Python versions (like 3.10), maintaining backward compatibility, and the need for testing and documentation improvements for various operators and distributions. Overall, unresolved questions include improving multi-GPU and CPU-bypass support in DataLoader on Windows, enhancing ONNX operator support for complex models (like Mask R-CNN), and ensuring broad version support and robustness across platforms."
2021-12-23,pytorch/pytorch,"The comments highlight several core issues encountered in PyTorch's integration with ONNX, Caffe2, and runtime backends. Notably, support for dynamic sequences and variable-length inputs remains limited or unsupported, leading to export failures or incorrect behavior, as seen with packed sequences and control flow constructs. Compatibility problems between different ONNX operator versions and datatype mismatches, particularly in loop and tensor operations, cause validation errors and hinder deployment. Multiple reports suggest that certain operations, such as inplace modifications, deprecated or unsupported operators, and complex control flows, require stricter handling or new support to improve portability and correctness across frameworks. Additionally, ongoing issues with hardware-specific bugs, version incompatibilities, and CI infrastructure inconsistencies indicate unresolved technical and infrastructural challenges in ensuring robust, portable model export and execution."
2021-12-24,pytorch/pytorch,"The discussions highlight challenges with GPU memory management in PyTorch, particularly with processes lingering after manual kills, and suggest using system utilities like `lsof` to identify GPU-using processes. There is concern about performance bottlenecks in DataLoader, especially on Windows, with solutions such as setting `persistent_workers=True` and alternative custom DataLoader implementations shown to improve speed via high `num_workers`. Questions are raised about effectively utilizing multiple GPU cores, including bypassing CPU bottlenecks by loading data directly onto GPU memory, with proposed custom loaders requiring full dataset in memory. Several issues address debugging and compatibility, including support for scripting distributed models with `DistributedDataParallel`, and varying system-specific CUDA installations, emphasizing the need for proper environment configuration and error handling. Unresolved questions include efficient multi-process data loading on Windows, handling complex dataset structures, and ensuring compatibility of advanced features like JIT tracing with distributed and multi-GPU setups."
2021-12-25,pytorch/pytorch,"The discussions highlight concerns about the current behavior of `torch.save`, with suggestions to deprecate it in favor of APIs that prevent overwriting files, such as `save_once` or using file objects with the ""x"" flag for failure on overwrite. There is a recurring issue with CUDA illegal memory access errors during prediction or model saving, which sometimes resolve unpredictably, indicating possible driver or environment-related problems. Several issues relate to code maintenance and internal dependencies, including outdated or missing include directives and internal branch management, as well as discussions about optimizing parallel processing by replacing DataParallel with DistributedDataParallel. Some conversations involve reviewing or importing pull requests, with specific focus on internal dependency management, compilation issues, and the need for internal sign-offs before external contribution. Overall, unresolved questions include how to best enforce safe save behaviors, handle CUDA memory errors reliably, and manage internal dependencies and code merge workflows effectively."
2021-12-26,pytorch/pytorch,"The discussions highlight challenges with the LBFGS optimizer, particularly its instability and potential NaN outputs in flat or ill-conditioned domains, possibly exacerbated when using multi-batch data or specific activation functions like ReLU. There's concern about appropriate step size control, such as Wolfe line search parameters, and the stability of inverse Hessian updates under small curvature directions. Several issues remain unresolved or are marked as stale, indicating ongoing difficulties with optimizer robustness and implementation details. Additionally, there are hardware and deployment considerations, especially for ARM architectures and low-end chips, as well as general infrastructure and CI pipeline maintenance questions."
2021-12-27,pytorch/pytorch,"The discussions highlight issues with managing multiple tensorboard and TensorBoard plugin installations, suggesting diagnostic scripts and manual folder deletion as solutions. Several users experience environment discrepancies, particularly when installed packages (e.g., tensorboard, tensorflow, torchvision) are not recognized within virtual environments or Docker, often due to conflicting versions or multiple installations; reinstallation or environment cleanup is recommended. Issues with model quantization, especially handling of `BatchNorm` and `Dropout`, are discussed, with proposed workarounds like replacing Dropout with identity layers and setting qconfig to None. Multiple reports focus on runtime errors related to CUDA/cuSolver, such as slow SVD computations and divergent behaviors between CPU and GPU, suggesting specific cuSolver functions or batching strategies for performance gains. Ongoing compatibility and stability concerns (e.g., against CPU/GPU discrepancies, model serialization, and operator support) are addressed, but some unresolved questions, like reproducing certain errors or clarifying API behaviors, remain open."
2021-12-28,pytorch/pytorch,"The comments highlight various technical challenges and discussions surrounding PyTorch compatibility and development. Key concerns include difficulties in building and loading PyTorch on different hardware platforms like Raspberry Pi and AWS EC2, often due to dependencies like GLIBCXX or mismatched binary structures. There are ongoing debates about API design, such as adding reset functionality to optimizers or supporting more flexible container types (e.g., ModuleList addition), with considerations of backward compatibility and scripting constraints. Several issues also involve environment inconsistencies, build system modifications, and handling of experimental features like numpy support or distributed training. Overall, unresolved questions focus on improving robustness, flexibility, and compatibility of PyTorch across diverse systems and use cases."
2021-12-29,pytorch/pytorch,"The comments mainly revolve around troubleshooting and improving PyTorch's error handling and performance, such as providing clearer error messages for DataLoader worker failures and optimizing CPU/GPU operations with advanced instruction sets like AVX512 and AVX2. There are concerns about handling specific edge cases (e.g., NaNs caused by AMP or gradient clipping), managing tensor properties (like layout and subclass detection), and ensuring backward compatibility for API changes, such as operator modifications or new features (e.g., quantized modules or kernel support). Additionally, discussions include platform-specific issues (e.g., macOS, NVIDIA driver bugs, Apple Silicon performance) and CI/CD pipeline reliability. Overall, the main themes are precise diagnostics, performance optimization, API stability, and robust testing strategies."
2021-12-30,pytorch/pytorch,"The discussions highlight several technical concerns: the support and implementation of `torch.nn.ParameterList` in TorchScript, which currently lacks proper JIT support; the potential benefits and complexities of supporting `indexing='xy'` in `torch.meshgrid` to align with NumPy standards, including documentation updates and backward compatibility; and issues related to NANs in mixed-precision training that can be mitigated by techniques such as gradient clipping or adjusting batch norm epsilon, with some suggestions to disable certain tests or modify behavior for stability. Additional questions involve the correct way to handle DLPack capsule formats, potential improvements in error handling for CUDA kernel issues, and the overall process of supporting newer hardware architectures like Z arch support. Several proposals include deferring feature additions (e.g., quantized dropout), fixing specific build failures, and clarifying API behaviors or documentation. Unresolved questions remain around implementation details, backward compatibility, and ensuring stability across different experimental features."
2021-12-31,pytorch/pytorch,"The discussions notably address the potential use of Triton for tensor summation, weighing its benefits against dependency concerns and autograd implementation complexity. Several comments focus on practical troubleshooting for PyTorch installation, especially on Windows and macOS, highlighting environment management and CUDA compatibility issues. There are concerns about system-level integration and error handling in CUDA kernels, such as device asserts and debugging strategies. Additionally, there are technical discussions on building universal macOS binaries, including architecture-specific source management and linking challenges. Overall, unresolved questions include AI backend performance differences on Apple silicon, API design for advanced autograd graph manipulation, and detailed documentation of DLPack interop semantics."
2022-01-01,pytorch/pytorch,"The discussions primarily revolve around challenges in building and deploying PyTorch on ARM64 (specifically PX2 devices), including issues with missing build artifacts like `libc10_cuda.so` and excessive GPU memory consumption during source compilation. Users request for official pre-built binaries for ARM64, which would facilitate cloud inference platforms like AWS G5g instances. Several issues concern code stability and correctness, such as handling signals properly, attribute access behaviors in datasets, and potential improvements in dataset batching for large video datasets. Some discussions include ongoing CI/CD test failures or suggestions for temporarily disabling specific tests, with questions about their configurations and platform-specific considerations. Overall, the key concerns focus on build artifacts availability, hardware-specific performance issues, and ensuring robust testing across diverse platforms."
2022-01-02,pytorch/pytorch,"The comments highlight several technical concerns: the inapplicability of `__slots__` in `nn.Modules` due to inheritance initialization issues; the unsupported status of `grid_sample` in ONNX, with official support expected in ONNX 1.11; and the need for improving or fixing specific PyTorch functionalities such as `ModuleList.__add__` to ensure it returns a new list without in-place modification. Additional discussions include addressing a runtime error with `torch.utils.data.DataLoader` in Python 3.10, the stability and progress of CI/CD workflows, and requests for patch releases or documentation updates related to recent changes or dependencies. Overall, the dialogues focus on refining PyTorch features, supporting external integrations, and improving build and export reliability."
2022-01-03,pytorch/pytorch,"The discussions highlight several technical concerns, including challenges with exporting models to ONNX due to unsupported operators (e.g., `uniform`, `unfold`), and issues related to tensor type consistency and handling input tensors during graph trimming. There are questions about implementing support for specific operators (like `scatter` with reduction, `roll`, `qint16`) and improving the robustness of model export and conversion workflows (e.g., fixing model size and shape support, supporting complex half types). Issues also involve build system intricacies, such as adding custom linker arguments (e.g., `-lcurand`) in C++ extensions and managing tensor memory sharing (like concatenation without copying). Unresolved questions concern the best strategies for experimental features (like `clone()` vs `copy_()`), supporting new hardware architectures, and proper handling of input-output tensor invariants during graph transformations."
2022-01-04,pytorch/pytorch,"The discussions highlight several technical concerns about PyTorch's feature development: the incomplete implementation of advanced indexing utilities like `gather`, which could significantly ease tensor manipulation; the desire to implement custom external memory allocators, possibly through stream-ordered interfaces or build-on `cuda::memory_resource`; and issues related to ensuring GPU-CPU synchronization correctness in distributed or asynchronous contexts. Additionally, there are recurring questions about maintaining API consistency with libraries like NumPy and TensorFlow, such as support for `Tensor.shape`, `Tensor.size`, and their backward compatibility, as well as handling the deprecation or aliasing of functions like `max`, `repeat`, and `split`. Several discussions suggest the need for improved debugging, testing, and documentation around these features, while unresolved questions include whether certain experimental features (like uncoalesced sparse tensors or in-place tensor modifications) should be merged or deferred for future support."
2022-01-05,pytorch/pytorch,"The logged comments reveal multiple issues and discussions related to PyTorch development and CI infrastructure, including crashes (segfaults, memory sharing problems), the need for better warnings about nondeterministic or parallel operations (like scatter_add, sort, broadcast, reduce), and tracking support for sparse or large tensor operations in various devices and formats. There are questions about deprecation plans, consistency of attribute naming (e.g., size vs shape), and improvements in debugging or logging tools such as vitals and autograd graph offloading. Several discussions focus on build configurations, compatibility with mobile/OS platforms, and changes to core APIs or internal mechanisms like dispatch, kernels, and distributed communication. Unresolved concerns include ensuring accurate detection of nondeterministic operations, supporting more complex sparse or broadcasted tensors efficiently, and understanding timeline impacts for features like reference paths and the deprecation of certain model paths. Overall, the core topics are improving stability, usability, and clarity of PyTorch's functionality both at runtime and in CI validation, with attention to systematic changes and backward compatibility."
2022-01-06,pytorch/pytorch,"The discussions highlight several key areas of development and current limitations within PyTorch: 

1. Support for gradients in sparse matrix operations remains incomplete, with sparse autograd supported only for COO tensors, and legacy support or operator implementations (e.g., `sparse.mm`, `s_addmm`) often lacking autograd support or suffering from edge case issues. 
2. Enhancements for sparse tensor types, including support for complex dtypes and formats like CSR/CSC, are progressing but limited by dependencies, build issues, and the need for better operator support.
3. Improvements in testing infrastructure, such as better assertion utilities (e.g., `assert_allclose`) and enabling pytest-based infrastructure, are ongoing, with plans to better handle custom subclasses, tensor offloading, and batching scenarios.
4. System-level issues like CUDA build failures, driver compatibility, and memory management are addressed via workarounds like splitting builds or environment tweaks, but some bugs (e.g., in NVIDIA’s cuSPARSE, or low-precision computations) still require attention.
5. Several questions about architectural design choices (e.g., API consistency, backward compatibility for `ModuleList`, and interface support for masked or sliced sparse tensors) remain open, with ongoing discussions about error handling, API extensions, and internal infrastructure improvements."
2022-01-07,pytorch/pytorch,"The discussion highlights significant limitations in PyTorch's sparse tensor support, particularly regarding slicing, strides, reduction operations, and autograd compatibility, which hinder industry applications like recommender systems. Several comments suggest potential solutions: developing a mask-aware sparse tensor extension supporting slices and reductions, adding support for autograd-compatible sparse ops, or creating a dedicated autograd functional interface (e.g., `torch.autograd.functional.jacobian`) for batched Jacobian contraction computations. There are questions about integrating these features into existing APIs, potential performance impacts, and whether to introduce new classes or extend current ones with support for masking or autograd. Some proposals focus on enabling masked sparse tensor reductions akin to NumPy masked arrays, while others propose improving code support for non-contiguous tensors and tensor dimension flexibility. Unresolved concerns involve implementation complexity, backward compatibility, and the best design approach (e.g., additional arguments, new tensor types, or functional interfaces)."
2022-01-08,pytorch/pytorch,"The discussions highlight ongoing efforts to modernize PyTorch by removing deprecated and unsafe practices such as `.data` usage, with questions about alternative parameter update methods without `.data`. There are technical concerns regarding the implementation and testing of features like custom message types, quantization, and handling large matrices in linear algebra routines like `torch.linalg.eigh`, especially in distributed or GPU contexts. Several discussions address performance regressions, build and binary size considerations, and compatibility issues across different CUDA and platform versions. Additionally, there are procedural questions around release management, such as version string conventions and integration of new features like `torch.linalg.svd`. Unresolved issues include ensuring reproducibility of certain bugs, optimizing algorithms for batch operations, and clarifying the API design for extensibility and user-friendliness."
2022-01-09,pytorch/pytorch,"The discussions highlight challenges related to memory management and efficiency in PyTorch, particularly emphasizing that tensor views like slices do not allocate new memory, but certain operations such as `F.pad()` do, causing potential performance or reproducibility issues. Several issues involve CUDA and cuDNN bugs, memory leaks, or nondeterministic behaviors that can lead to out-of-memory errors or segmentation faults, especially under specific hardware/software combinations or with certain environment settings like `CUDA_BENCHMARK`. There is concern about how `einsum` performance optimization can be improved, with debates around algorithm selection, trade-offs between exact and approximate solutions, and vendor-specific support like cuTensorNet. Additionally, compatibility and support issues are raised, such as supporting `ForwardRef`, dynamic library loading, and ensuring deterministic or reproducible behavior across different system configurations. Overall, the community suggests a need for clearer documentation, better environment handling, and more robust, portable, and deterministic solutions for memory, performance, and compatibility challenges."
2022-01-10,pytorch/pytorch,"The discussions highlight limitations and ongoing development related to sparse matrix operations in PyTorch, specifically the inability to compute gradients through sparse matrix multiplication (`sparse.mm`) in versions before 1.7.0, and performance issues with autograd support in sparse operations like `torch.sparse.mm`. There are also concerns about ensuring autograd support for various sparse tensor operations, especially on CUDA, and about how to properly handle sparse tensors in multi-GPU and distributed settings, including support for in-place modifications and device transfers. Several discussions focus on improving API consistency, performance, and debugging tools—such as handling of `torch._values()`, extending support for batch Jacobian computations with `functorch`, and avoiding crashes—indicating active efforts to refine sparse tensor Autograd support and tensor operation correctness. Lastly, some comments address contributing enhancements, like better support for custom allocators, code restructuring for scripting models, and ensuring compatibility with evolving Python and CUDA versions."
2022-01-11,pytorch/pytorch,"The discussions primarily revolve around limitations and performance issues related to sparse matrix operations with autograd support in PyTorch, notably slow backward passes in sparse-sparse matrix multiplications, and the need for gradients support in sparse matrix functions like `torch.sparse.mm`. There are also topics on improving API consistency and documentation for functions like `ravel()` and `argmin/argmax`, handling of in-place operations with shape compatibility, and enabling determinism and reproducibility in various tensor operations. Additionally, discussions mention workarounds for quantized operations, threading and caching behavior with distributed training, and the importance of enhancing usability and correctness in both PyTorch core and ecosystem, with some proposals for API changes and performance improvements. Unresolved questions include how to make sparse operations faster, how to handle shape and memory format guarantees, and how to properly support complex numbers and mixed precision features."
2022-01-12,pytorch/pytorch,"The discussions primarily revolve around challenges in building or compiling PyTorch from source on ARM64 platforms (e.g., Nvidia Jetson, Qualcomm Snapdragon) and related performance issues, such as high GPU memory occupancy or slow convolution operations, especially with grouped convolutions at higher channel sizes. There are questions about the completeness and compatibility of custom or cross-compiled builds, as well as concerns about performance regressions that might be due to suboptimal algorithm choices or hardware-specific heuristics. Some comments suggest the need for better tooling and benchmarking to measure the impact of code changes, and there are unresolved issues regarding reproducibility of certain warnings, especially in checkpointing and autocast scenarios, along with how to properly handle data types and tensor views. Overall, unresolved questions include improving cross-platform build support, optimizing distributed training and specific operators, and clarifying expected behaviors around data type enforcement and tensor shape handling."
2022-01-13,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's linear algebra functionalities, including support for batched inverse operations (`pinverse`, `cholesky_inverse`) and more efficient SVD algorithms (discussing `gesdd` vs `gesvd`, and biasing memory-speed tradeoffs). There are concerns around API consistency and documentation clarity, such as LU pivot indexing conventions (1-based vs 0-based) and the transition from the deprecated `torch.lu` to `torch.linalg.lu_factor`. Several discussions address performance improvements, like supporting channels-last layouts in convolution implementations, optimizing determinant calculations, and making the autograd and autocast behaviors more consistent and user-friendly, especially with mixed precision. Additionally, there are notable concerns about CI infrastructure, build configurations, and security aspects related to torch hub trust and reproducibility, alongside general questions on the future integration of algorithms (e.g., quickselect for top-k operations) and API stability. Unresolved questions include balancing autospropagation of autograd state across threads, handling of different input shapes and batch sizes, and ensuring backward compatibility with different CUDA versions and build environments."
2022-01-14,pytorch/pytorch,"The discussions highlight several technical concerns in the PyTorch repository, such as build and compilation errors caused by CUDA driver bugs (notably with CUDA 9.2), and issues with DLL loading and compatibility, especially involving specific DLL files like `caffe2_detectron_ops.dll`. There are recurring questions about improving the distributed process group initialization, specifically whether to create new TCPStores or modify existing rendezvous mechanisms with fallback options. Several issues address the performance and correctness of tensor operations, including handling of Tensor subclasses, sparse tensor support, and ensuring deterministic behavior in functions like `repeat_interleave`. Additionally, there are concerns about code consistency and robustness, such as proper handling of auto-cast states, the need for enhanced debugging/diagnostics, and support for newer CUDA versions and hardware. Many unresolved questions involve build environment configurations, compatibility between package versions, and improving user-facing APIs with clearer documentation or extensibility features."
2022-01-15,pytorch/pytorch,"The discussions highlight issues related to unsupported features in ONNX export, such as torch.distributions, and compatibility problems with specific GPU architectures (e.g., sm_86 for RTX 3080, sm_80 for A100) due to PyTorch binary builds not supporting certain CUDA compute capabilities. Multiple users report errors when exporting models like ResNet or deploying on newer hardware, suggesting the need to update or rebuild PyTorch with appropriate CUDA architecture flags (e.g., TORCH_CUDA_ARCH_LIST). Some issues also involve difficulties timing GPU operations or integrating external codebases, indicating potential gaps in tooling or support for certain models and hardware configurations. There are suggestions to install official prebuilt binaries via conda or pip rather than custom compilation to resolve compatibility issues. Overall, unresolved questions remain about ensuring broad hardware support, proper export capabilities, and the right configuration for new GPU models."
2022-01-16,pytorch/pytorch,"The discussions highlight ongoing challenges with GPU compatibility, particularly with newer cards like RTX 3090 and CUDA versions, where users experience kernel image errors and runtime CUDA errors, often due to mismatched driver or architecture support. Solutions such as installing nightly PyTorch builds, building from source with specific flags, or disabling NEON are discussed, but issues persist, especially with hardware support and correct compilation flags. There's concern that prebuilt binaries may have limited logging capabilities because they disable GLOG, making debugging difficult; enabling more verbose logs like NCCL_DEBUG=INFO is suggested. Users also face runtime errors related to insufficient shared memory (SHM) in Docker environments, causing data loader segmentation faults or bus errors, prompting questions about configuring system limits. Unresolved questions include optimal practices for fixing GPU architecture support, logging, and system resource limitations to improve stability and debugging."
2022-01-17,pytorch/pytorch,"The discussions highlight numerous technical issues and questions related to PyTorch, including unclear documentation and searchability of certain features, challenges in code review for complex graph transformations like RNN blob renaming, and troubleshooting environment-specific problems such as CUDA driver mismatches and performance regressions with different CPU threading configurations. Several threads address package compatibility and installation, particularly around GPU CUDA versions, with recommended solutions like creating clean environments and matching CUDA versions to PyTorch builds. Performance and correctness concerns are evident in issues related to sparse tensor operations, the efficiency of memory operations, and dynamic shape handling during model export. Additionally, there are ongoing discussions about extending support for new features such as sharded tensors, hierarchical parallelism naming conventions, and experimental features like DTR and automatic memory management, alongside inquiries about future support plans for Python 3.10."
2022-01-18,pytorch/pytorch,"The discussions highlight several technical issues primarily related to PyTorch's internal operations and integrations. Notably, the incompatibility of newer GPU architectures (e.g., A100 with sm_80 and sm_86 capabilities) with existing PyTorch builds, which are limited to older CUDA compute capabilities, requiring environment upgrades. There are also concerns about the lack of GLog support in prebuilt binaries affecting logging visibility, suggesting possible build or configuration adjustments. Additionally, issues like deadlocks with multi-GPU setups, inconsistencies in autograd support for sparse tensor operations, and performance regressions due to recent PRs are raised. Several questions remain open, including best practices for extending PyTorch's linear algebra abstractions (like factored operators), managing build dependencies across environments, and optimizing distributed training diagnostics."
2022-01-19,pytorch/pytorch,"The discussions highlight issues related to DataLoader multiprocessing errors, especially ConnectionResetError, possibly caused by race conditions or race conditions in worker process communication; solutions such as increasing batch size, adding sleep, or code minimal reproductions have been explored. There are challenges with integrating older modules like `torch.utils.ffi` with newer PyTorch versions, suggesting migration to `torch.utils.cpp_extension` and creating custom wrappers, with concerns about compatibility and cleaner implementation. Questions about implementing version-dependent initialization schemes and how to document and support multiple versions effectively are raised, along with suggestions for structured changelog tracking. There's also discussion about GPU/ROCm compatibility, especially around missing cuDNN in pre-built binaries, and performance considerations for sparse and iterative linear solvers, with potential algorithmic and backend work needed. Lastly, there are inquiries regarding CUDA toolkit and driver version mismatches, and their impact on model serialization and inference on different hardware, emphasizing the need for proper environment setup and build processes."
2022-01-20,pytorch/pytorch,"The discussions predominantly highlight issues related to PyTorch's compatibility, performance, and debugging. Several concerns involve the reliable reproduction and mitigation of errors such as connection resets in DataLoader, illegal CUDA memory access, and synchronization issues, with suggestions like adding retries or adjusting code patterns. There is also a recurring theme about build and environment setup, especially regarding CUDA/cuDNN integration, and how to handle versioning and ABI compatibility. Some threads question whether current tooling, like logs and profiling, sufficiently supports debugging across platforms or in inference mode. Lastly, many raise questions about improving developer experience, such as better error messages, automated testing, and API clarity regarding device management and backend features."
2022-01-21,pytorch/pytorch,"The discussions highlight several ongoing challenges and initiatives in the PyTorch community. There is a prominent emphasis on enhancing GPU support and performance across diverse hardware such as AMD and Intel GPUs, with efforts like out-of-tree backends, OpenCL primitives, and vendor-specific libraries like GPUOpen and MKLDNN. Several comments address issues with benchmarking, profiling, and performance regressions, including slowdowns in convolution/backprop operations, shape/dtype handling, and inference issues, often tied to specific hardware or implementation details. There are also questions about the current support status for different accelerators, including HIP/ROCm, and mechanisms for improving interoperability (e.g., buffer protocol, DLPack). Unresolved matters include performance optimizations, compatibility for larger models, and the integration of new features such as lazy tensor support and dynamic shape fusing."
2022-01-22,pytorch/pytorch,"The discussions highlight ongoing challenges with CUDA and GPU support in PyTorch, particularly related to compatibility with new GPU architectures like RTX 2080 Ti and RTX 3090ti, where users experience hangs and runtime errors. There is a debate on the API design of the `cross` function, with suggestions to better align it with the array API standard and proper namespace placement, as well as considerations for separating linear algebra functions into more specialized namespaces like `linalg`. Multiple bug reports address internal build issues with LAPACK/OpenBLAS, shared memory limitations, and errors in quantized CPU operators dependent on specific third-party library versions. Concerns are raised about CI stability, test failures, and proper handling of deprecated or unsupported data types such as `uint8`. Overall, there is a mix of hardware compatibility, API standardization, internal implementation bugs, and testing infrastructure improvements that remain open."
2022-01-23,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch features and compatibility: there is a suggestion to add an `indexing` parameter to `meshgrid` for numpy compatibility; efforts to support custom Python ops in TorchScript for backwards compatibility are underway; and there are ongoing issues with internal build configurations for MKL and OpenBLAS, requiring conditional compilation fixes. Additionally, there is interest in integrating UCC as a fallback backend for distributed training, with questions about its build process, default linking strategies, and the best approach for support across CPU/GPU cases. Performance optimization behaviors, such as operator fusion during scripting and re-compilation for varying tensor sizes, are also discussed, along with the need for clearer documentation on these processes. Overall, unresolved questions involve build configuration management, backend integration strategies, and ensuring the stability and usability of new features."
2022-01-24,pytorch/pytorch,"The comments predominantly concern the robustness and correctness of various PyTorch features, including ONNX export, compatibility of model components like LSTMs with ONNX and other frameworks, and the support of specific operators (e.g., `unfolded2d_copy`, `Clip`, `nan`, `inf`). Several discussions recommend adding tests, handling optional/unsupported parameters, and clarifying behavior in cases like non-monotonically increasing offsets in embedding bag operations. There are recurring questions about the stability of dependency versions (like MKLDNN/oneDNN, third-party libraries), platform-specific limitations (e.g., Vulkan's dimensionality constraints, Android JNI libs), and build system considerations. Unresolved issues include build failures due to environment or dependency mismatches, internal design decisions (e.g., logging, autodiff), and supporting advanced features like straggler detection or sparse CUDA operations. Overall, the discussions reflect ongoing efforts to improve compatibility, performance, and usability across PyTorch's ecosystem, with some concerns about incomplete support or external dependencies' maintenance status."
2022-01-25,pytorch/pytorch,"The discussions primarily revolve around integrating and configuring logging systems in PyTorch, particularly the use of GLOG versus internal c10 logging, with concerns about environment variables, log level control, and unification strategies. There's interest in exposing a Python interface (`set_c10_log_level`) for user control over log verbosity, considering future unification of logging facilities to simplify usage and improve transparency. Additional topics include ensuring non-deterministic tests are robust, handling Vulkan backend limitations for tensors with more than 4 dimensions, and managing build/test configurations in CI/CD pipelines—sometimes suggesting splitting large PRs for easier review. Unresolved questions involve whether environment variables should be the standard interface, how to best set log levels across different build configurations, and how to harmonize logging and error handling in the broader codebase while considering user and system impact."
2022-01-26,pytorch/pytorch,"The discussions highlight concerns about ensuring compatibility and correctness for operators like `torch.nn.functional.interpolate` across different versions, especially regarding `recompute_scale_factor` warnings and output size calculations where discrepancies with libraries like OpenCV and scikit-image are noted. There is ongoing debate about the best way to handle `Clip` operator support for different ONNX opset versions, with suggestions to move implementation to a new file to support older versions via pattern replacements. Also, multiple comments mention the importance of exposing environment variables or APIs (e.g., log level controls), with a preference for environment variables for simplicity, while considering future unification with GLOG. Several issues relate to improving test robustness, handling of tensor subclasses, and ensuring stable, predictable behavior in different hardware setups (e.g., ROCm, CUDA) and environments. Unresolved questions involve finalizing support strategies for complex operators, operator version management, and logging infrastructure standardization."
2022-01-27,pytorch/pytorch,"The discussions primarily revolve around progress and implementation details for features such as ONNX support for specific ops, CPU optimizations, and support for new operator variations. There are concerns about compatibility and correctness, such as ensuring backward compatibility with internal APIs, proper handling of tensor layouts (e.g., strided vs. lazy tensors), and the impact of operator signatures (e.g., `upper` parameter in `linalg.cholesky`). Some questions focus on test coverage, flaky CI failures, and internal code consistency, especially around handling of subclassed or lazy tensors and ensuring that new features do not break existing behavior. There are suggestions to improve API design, reduce performance overhead, and clarify semantics (e.g., gradient support for sparse tensors or handling of specific operator attributes). Unresolved issues include CI failures related to linear algebra functions, version compatibility, and how to best abstract or extend internal APIs for custom backends or tensor types."
2022-01-28,pytorch/pytorch,"The discussions primarily revolve around improving CUDA tensor transfer efficiency, ensuring correct auto-configuration of device placement especially when models change device contexts or are transferred via `.to()`, and handling internal tensor creation or internal state updates that complicate CUDAification. Several suggest making distribution and tensor methods compatible with `.to()` by overriding internal attributes or recursively applying device conversions, with approaches involving subclassing `nn.Module` and deep tensor references. There are concerns about performance regressions, compatibility across different hardware and software configurations, and maintaining correctness in backward passes, especially for operations like SVD and `norm`. Open questions include optimal mechanisms to track device placement for internal tensors, how to handle repeated `.to()` calls efficiently, and how to ensure that internal tensor creation post-initialization does not break device synchronization. Unresolved details include the specifics of the best design to propagate device placement in all internal tensor and module states, balancing between ease of use, performance, and correctness."
2022-01-29,pytorch/pytorch,"The discussions highlight persistent issues related to GPU compatibility, particularly with newer hardware like the RTX 3090 and issues with CUDA kernel images not supporting certain compute capabilities, as well as problems with existing PyTorch versions on specific hardware and software configurations. There are ongoing challenges with ensuring compatibility between PyTorch, CUDA versions, and hardware architectures, especially with newer or older GPU models. Some users are requesting support for legacy compute capabilities (e.g., support for older CUDA compute architectures) to prevent obsolescence of older GPUs. Other points focus on build process complexities, maintenance of operator signatures, and the need for better support for custom CUDA kernels or advanced special functions in torch.special. Overall, unresolved questions include how to extend support for legacy GPU architectures while maintaining performance and compatibility across various hardware and software environments."
2022-01-30,pytorch/pytorch,"The discussions highlight ongoing efforts to generalize normalization functions in PyTorch, particularly considering whether InstanceNorm can be unified with LayerNorm and whether fused kernels for BatchNorm could be extended to support normalization over arbitrary dimensions, potentially treating all as first-class citizens. There is also an emphasis on optimizing performance for specific operations like PReLU, UpSample, and ConvTransposeND with support for different data types such as BFloat16 and INT8, often dependent on related PRs. Additionally, questions around the correctness of exporting models to ONNX, especially regarding dynamic axes, and about proper implementation of LDL and Cholesky decompositions for differentiability, are raised. Unresolved issues include ensuring backward compatibility with advanced decompositions and facilitating seamless integration of third-party dependencies."
2022-01-31,pytorch/pytorch,"The discussions mainly revolve around handling specific operational or API issues in PyTorch, such as the confusion around the new `output_size` parameter in `nn.Upsample`, which aims to allow constant `scale_factor` during arbitrary size inputs, and the approach to improve testing and robustness for CSR tensor operations to prevent explicit zeros from being modified. There are ongoing concerns about CI failures due to environmental issues or missing dependencies, and proposals for incremental feature development to avoid breaking existing functionality, especially for complex features like distributed training or support for new hardware backends like UCC or ROCm. Reverts and re-branches are often used to ensure stability, sometimes delaying feature releases, with discussions about whether to revert certain PRs entirely or to patch specific issues, like ROCm support or mobile build failures, to maintain momentum. Additionally, there's a focus on standardizing naming conventions across projects (DP/PP/TP) for clarity, as well as questions about specific API behaviors and backward compatibility in operations such as `svd`, `det`, or tensor shape manipulation in FX tracing."
2022-02-01,pytorch/pytorch,"The comments reflect ongoing challenges and discussions around several technical issues in PyTorch. Key concerns include performance limitations and potential GPU incompatibilities of certain ONNX ops like `grid_sample`, the complexity of implementing support for `torch.bmm` with sparse tensors, and the need for clearer, more consistent API semantics (e.g., for `interpolate` and `autograd` support with soft labels). Several discussions also address stability and correctness in gradient computations, backward compatibility, and serialization compatibility across environments. Furthermore, there are operational issues such as build failures, CI regressions, and support for specific hardware (e.g., M1, Metal). Overall, unresolved questions center on balancing API clarity, performance, and backward compatibility while handling evolving hardware and software infrastructure."
2022-02-02,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch support on newer GPUs (e.g., RTX 30 series with CUDA 11.x) often requiring nightly or custom builds to fix kernel image compatibility issues. Contributors express frustration over support dropping for older GPU architectures (compute capabilities like 3.5), seeking either official legacy builds or alternative solutions like building from source. Several discussions involve ensuring reproducibility and deterministic behavior, especially for functions like scatter, with suggestions to add flags or modify API behavior to handle duplicate indices without breaking `use_deterministic_algorithms`. There's also mention of internal build process issues, such as CI environment updates, and the importance of proper release and developer workflow practices. Overall, unresolved support for legacy GPUs and the need for clearer, more reliable build/support processes remain key concerns."
2022-02-03,pytorch/pytorch,"The discussions cover various technical challenges encountered in the PyTorch repository, including issues related to package download speeds and connectivity in restricted regions, compatibility and installation problems with CUDA and hardware architectures, and improvements in internal functions like `unravel_index`. Several discussions mention workarounds for exporting models to ONNX, supporting new hardware capabilities, and modifying dispatch mechanisms for better flexibility and extensibility, such as re-dispatching in `__torch_function__`. There are also reports of CI testing flakes and failures, often related to environment dependencies, build configurations, or infrastructure inconsistencies, which require nuanced handling like re-serializing models, updating dependencies, or patching build scripts. Overall, the exchange emphasizes refining support for diverse hardware, ensuring backward compatibility, and improving CI reliability while tackling platform-specific and runtime issues."
2022-02-04,pytorch/pytorch,"The comments largely reflect a variety of issues including feature requests (e.g., support for additional functions like gammaincinv, bessel functions, reparameterization support), bug reports (e.g., runtime errors, out-of-memory issues, and correctness concerns), and CI testing/infrastructure troubleshooting (e.g., flaky tests, environment setup, and build failures). Several discussions involve potential API enhancements (e.g., reparameterization, specialized tensor functions, deprecating unstable features), as well as questions about performance impacts and compatibility. The community also points out documentation gaps, testing challenges, and the need for improved stability and correctness, especially across different hardware and software configurations. Many issues involve a desire for backward compatibility, better debugging/profiling tools, and more transparent CI results. Overall, the focus is on advancing feature support, ensuring robustness, and maintaining compatibility with evolving Python versions and hardware environments."
2022-02-05,pytorch/pytorch,"The discussions mainly focus on handling edge cases such as batch normalization with batch size 1, CUDA memory fragmentation issues, and ONNX export limitations related to adaptive pooling. Suggestions include setting `drop_last=False` in DataLoader to avoid batch size 1 issues and configuring environment variables like `PYTORCH_CUDA_ALLOC_CONF` to manage memory fragmentation. There are questions about support for certain operators in ONNX, especially `adaptive_avg_pool2d`, and about maintaining tensor identity in profiling across inference and training modes. Some discussions recommend workarounds like downgrading PyTorch versions or reusing existing data structures, while unresolved issues include improving reproducibility, supporting scalar tensors with `len()`, and enhancing debuggability and deterministic behavior in CUDA operations."
2022-02-06,pytorch/pytorch,"The discussions highlight concerns about the security of PyTorch's serialization methods, specifically the insecurity of default pickle loading, with proposals to adopt ONNX protobufs or safer deserialization techniques. There is ongoing work to add features like the `logm` function for matrices, with challenges on GPU implementation and stability, and the TensorFlow implementation's limitations are noted. Some issues involve API consistency, such as supporting `out=` in `torch.nn.functional` operators, and backward compatibility considerations prevent certain fixes. Network download speeds and environment compatibility issues are also mentioned, indicating broader concerns about robustness and usability. Unresolved questions include how to effectively implement mathematical functions like `logm` on GPU, and how to balance BC with new API improvements."
2022-02-07,pytorch/pytorch,"The discussions reveal ongoing challenges with ensuring backward compatibility and correctness in PyTorch, particularly related to operator memory format handling, such as tensors non-contiguous in memory, the behavior of `at::assert_no_internal_overlap`, and the need for codegen support for safety checks. Several issues pertain to performance regressions like inefficient implementations of `stack`, `cat`, and functions like `addcmul`, which are being addressed with optimized operator replacements and operator overhead reductions. There are also hardware-specific limitations, notably with Vulkan backend's 5D tensor support, and environment setup issues affecting build and runtime stability, especially on Windows, CUDA, and macOS platforms. Some discussions involve adding user-facing features or deprecating unsupported operators, with attention to proper documentation, user warnings, and build configurations. Overall, unresolved questions remain around ensuring consistent, efficient, and safe operator behavior across CPU, CUDA, and specialized backends, while maintaining compatibility with evolving hardware and software environments."
2022-02-08,pytorch/pytorch,"The discussions highlight several technical topics, including the need for clearer documentation on current complex number support in PyTorch, support for autograd with complex tensors (notably in operations like `svd`, `logdet`, and `amax`), and the impact of data reshaping, padding, and sharding behaviors in models like batch normalization, `istft`, and distributed training. There are concerns about cross-engine compatibility and BC/FC issues, especially around the introduction of new features and backward compatibility for storage formats, as well as nuanced details like the behavior of eigen and SVD algorithms across hardware and software versions. Additionally, discussions cover build system intricacies such as CUDA version support, binary size reductions, and environment setup, particularly for complex ops and third-party dependencies. Unresolved questions involve how to best automate performance measurements for code changes, properly document and support operator behaviors, and ensure consistency across hardware (e.g., GPUs, CPU, Apple Silicon) and software components."
2022-02-09,pytorch/pytorch,"The discussion encompasses several technical issues and enhancements related to PyTorch's data loading performance, especially on Windows, where parallel dataloader throughput is hindered by CPU and multiprocessing limitations. Users have observed significant slowdowns when using multiple workers, but solutions like `persistent_workers=True` and custom loaders are suggested to mitigate these issues, often requiring datasets to fit entirely into memory. There are also concerns about the compatibility and correctness of recent changes, such as incorporating new functionalities (e.g., `scatter` with `unique_indices`) and ensuring backward compatibility in serialization and storage formats. Internal build failures and test flakiness, especially on Windows or with specific environments, highlight platform-specific challenges. Overall, the comments reflect ongoing efforts to improve performance, stability, and compatibility across platforms, with suggestions for better API design, documentation, and internal tooling adjustments."
2022-02-10,pytorch/pytorch,"The discussions reveal multiple technical concerns including the efficiency of CUDA JIT compilation and caching, serialization security risks with pickle vs ONNX protobufs, and correctness of tensor shapes and data types during model export, especially with autocast and mixed precision. There is interest in enhancing support for tensor layout detection (e.g., `is_strided`) and handling arbitrary strides in kernel design, as well as improving distributed training and evaluation tools to accommodate uneven batch sizes across processes. Several discussions also address internal build configurations and codebase hygiene (e.g., reformatting code with black, managing macro definitions for CUDA). Unresolved questions involve assessing the impact of new features on backward compatibility, build processes, and testing stability, with suggested solutions ranging from code annotations to experimental design changes."
2022-02-11,pytorch/pytorch,"The discussions encompass concerns about the implementation and API design of features like sharded tensor parallelism, ensuring consistency with industry standards, and providing high-level primitives that abstract low-level RPC or tensor slicing details. There are questions about the support for specific tensor attributes, such as strides for output tensors, and whether kernels handle arbitrary or fixed memory layouts, with some suggesting a push towards mandatory shape/stride invariants for kernels to simplify implementation. Additionally, there are considerations on how to support advanced use cases like mixed precision/autocast in ONNX/serialization workflows, and whether certain deprecated or internal functionalities (like `Parameter` as a wrapper or specific CUDA kernel optimizations) should be phased out or better integrated. Overall, the discussions reflect ongoing efforts to improve flexibility, standardization, and correctness in distributed/memory-aware tensor operations, while unresolved questions include API evolution, support for custom or experimental kernels, and consistency between implementation and user expectations."
2022-02-12,pytorch/pytorch,"The discussions highlight issues with slow server downloads via conda and the potential server misconfigurations or server-side bottlenecks affecting package retrieval speeds, with specific references to server throttling and infrastructure concerns. Additional comments focus on improving determinism in floating-point summations with parallel prefix sum techniques, and performance benchmarking of various optimizer implementations like AdamW variants, including considerations for hardware-specific tuning and code modernization. Some conversations address stability and robustness in model code, such as mapping tensors to identifiers, and the impact of build configurations like LAZYCUDA_LINALG. Questions also arise around self-contained dependency management, vendorizing external packages, and the behavior of CUDA operations under different build flags. Overall, unresolved issues include server performance, deterministic computation strategies, performance benchmarking, and build/configuration impacts."
2022-02-13,pytorch/pytorch,"The discussions highlight a recurring concern with the performance of grouped convolutions in PyTorch, primarily noting that setting `torch.backends.cudnn.benchmark=True` improves speed due to better heuristic selection, although there may still be underlying inefficiencies potentially requiring custom CUDA kernels. Several issues touch on the stability and correctness of specific functionalities, such as assertions in gradient computations, the behavior of `state_dict()` overloads, and handling non-used parameters in distributed or multi-GPU training, indicating possible design or implementation challenges. There are questions about compatibility and support for various Python versions, especially Python 3.9 and newer, as well as environment-dependent issues such as shared memory limitations on different platforms, affecting training and data loading. Proposed solutions include global or context-based flags for configuration, modified sampler implementations for distributed data loading, and better handling of conditional logic within autograd. Overall, unresolved questions concern optimizing kernel implementations, improving usability in diverse environments, and ensuring consistent, stable behavior across different hardware and Python versions."
2022-02-14,pytorch/pytorch,"The discussions primarily revolve around memory management issues in PyTorch, especially CUDA out-of-memory errors and fragmentation, with suggested solutions like reducing batch sizes, setting environment variables, or specific API changes. Several issues highlight the need for verifying forward/backward compatibility of storage formats after code refactoring (e.g., renaming `TypedStorage`), and ensuring code changes don’t break existing serialization BC. There are technical questions about supporting operations like `Im2Col` in ONNX, handling specific kernel API signatures across different PyTorch versions, and the support of sparse tensors with broadcasting. Some comments point out ongoing CI failures, platform-specific test skips, or build infrastructure quirks, with tentative fixes or workarounds suggested. Unresolved questions include ensuring proper API support for new features (like `vmap` in PyTorch) and verifying compatibility with older PyTorch versions or external tools like TensorRT."
2022-02-15,pytorch/pytorch,"The comments span a variety of technical issues and feature discussions, including feature requests, performance improvements, and bug fixes. Notable concerns involve enhancing or adding functionalities like `Im2Col`, `torch.vmap`, and `torch.meshgrid`, as well as code stability, benchmarking, and correctness of tests. Several discussions highlight the need for better error handling, API consistency, and backward compatibility, especially for serialization and dtype support in specific operations. There are also ongoing efforts to improve CI stability, incorporate new optimization techniques, and address platform-specific build issues, with some features pending release or review. Overall, the conversations reflect active development aimed at improving PyTorch’s functionality, robustness, and internal tooling."
2022-02-16,pytorch/pytorch,"The discussions highlight several technical issues and proposals related to PyTorch's error messaging, performance, and compatibility. Specific concerns include improving clarity of autograd warning messages, supporting type promotion for sparse tensors, and ensuring accurate error handling for functions like `mm` with mixed data types. There is interest in enhancing features such as `torch.vmap`, better handling of ONNX export limitations (especially with operators like `prim::Layout`), and extending support for quantization, quantized data types, and deployment scenarios (e.g., C++ API, mobile, macOS, CUDA). Additionally, there are ongoing efforts to address build and environment issues on macOS (universal binaries, architecture support), and to stabilize CI workflows to avoid flaky tests and merge conflicts. Some proposals involve substantial C++/Python interface modifications, performance optimizations, and compatibility fixes for different hardware and software configurations."
2022-02-17,pytorch/pytorch,"The comments highlight several recurring technical concerns: first, there's ongoing work to improve handling of tensor aliasing and zero-stride modifications, with efforts to implement warnings in pointwise kernels; second, issues around cross-compiling, especially on ARM64 and big-endian systems, including patching and compatibility challenges with serialization and system headers; third, frequent reports of CI build failures due to misconfigurations, missing dependencies, or flaky tests, with some being addressed by reverting or fixing PRs. Questions also arise regarding the stability and support of features like TorchScript scripting, serialization formats, and platform-specific binaries, as well as infrastructure decisions like moving workflows to periodic and handling capacity constraints. There are unresolved issues related to performance benchmarking correctness and proper CI validation, especially for new features or platform-specific builds. Overall, the discussions indicate active development, troubleshooting, and ongoing efforts to enhance robustness, compatibility, and reproducibility across diverse environments."
2022-02-18,pytorch/pytorch,"The comments across these GitHub threads primarily concern challenges related to training and exporting TorchScript modules, particularly regarding mode switching (`train()` vs. `eval()`), handling NaNs during mixed precision training with `autocast` and `GradScaler`, and the complexities of hardware acceleration, especially on Apple Silicon with Metal. Several discussions focus on the limitations of traced versus scripted models, the difficulties of decompiling or integrating Metal APIs into PyTorch, and the need for better tooling, API support, or documentation. Other topics involve infrastructure issues like CI capacity, reproducibility concerns, and feature developments (e.g., ONNX export support, RPC mechanisms, and code generation improvements). Unresolved questions include how to improve support for dynamic control flow in TorchScript, strategies for managing hardware-specific dependencies, and official updates on backend or cross-platform efforts. Overall, the discussions highlight ongoing work on robust mode switching, precision stability, hardware abstraction, and infrastructure scaling within the PyTorch ecosystem."
2022-02-19,pytorch/pytorch,"The discussions highlight recurring infrastructure instability and server outages affecting PyTorch's online presence, affecting user experience and package availability. Several comments focus on dependency management issues, especially around proper configuration of package repositories and dependency versions, with suggestions to improve official installation instructions for tools like Poetry and Pipenv. Concerns are raised regarding feature completeness, such as implementing autograd derivatives (e.g., `torch.linalg.lstsq`) in older PyTorch versions, and handling special cases like zero-dimension reductions with default values. Additionally, there are technical questions about code design, such as supporting dataclass-based `nn.Module` subclasses, and compatibility issues like conflicting OpenMP runtimes on macOS. Overall, unresolved questions about robustness of distributed training, handling of mixed backends, and improving CI reliability remain open."
2022-02-20,pytorch/pytorch,"The discussions highlight multiple issues and feature requests related to PyTorch, including the need for clearer and more informative error messages (e.g., memory exhaustion errors in DataLoader), support for additional data types like `uint16`, `uint32`, and `uint64`, and the ability to support distributions and custom autograd operations in TorchScript. There are also concerns about test stability across different environments, particularly on CI, and the necessity to improve support for complex number types and support for certain functions' derivatives in multiple PyTorch versions. Some unresolved questions include which PyTorch versions introduce specific features (like `lstsq` derivatives) and how certain operations impact backward compatibility, as well as infrastructure considerations such as process memory sharing and metadata synchronization. The maintainers are encouraged to improve error diagnostics, extend functionality to newer data types, and carefully evaluate platform-specific or version-specific regressions."
2022-02-21,pytorch/pytorch,"The comments primarily highlight persistent bugs and performance issues in PyTorch, particularly related to GPU memory handling, data parallelism, and JIT tracing accuracy, some of which have been addressed in recent or upcoming releases. Several users report crashes, stalls, and warning messages either caused by specific backend interactions (e.g., NCCL, gevent monkey patching, OpenBLAS linkage) or by complex operations like nested tensor calls and data loader misconfigurations, especially with multi-GPU setups. There is discussion on supporting features like spatial transformers in ONNX, proper logging behavior, and exporting certain modules like `nn.Fold()`. Additionally, users seek guidance on building static binaries, using DDP with multi-GPU, and handling legacy hardware or driver incompatibilities. Many unresolved questions remain about the stability of these bugs, the timeline for support of certain features, and best practices for mitigating these issues in production environments."
2022-02-22,pytorch/pytorch,"The discussions reveal a range of technical concerns including the trainability of window sizes and window functions in spectrogram-related operations, where window size remains discrete and non-differentiable. Multiple issues highlight memory leaks and performance regressions in DataLoader, especially with the FastDataLoader and persistent workers, often linked to system-specific factors like antivirus interference or improper shared strategy settings. Compatibility and support for complex data types such as `complex64`, `complex32`, and support for special functions like gammaincinv are also questioned, indicating incomplete support or the need for further extension. Additionally, there are broader concerns about CI infrastructure limitations, such as the number of available build agents, scheduling, and test flakiness, which impact development and testing workflows. Overall, unresolved questions focus on improving differentiability of window functions, ensuring stability and performance in data loading, expanding support for complex data types and special functions, and addressing CI scalability."
2022-02-23,pytorch/pytorch,"The comments highlight concerns around potential memory leaks, especially related to DataLoader's use of lists and numpy arrays of objects, which can cause copy-on-access issues not directly related to tensors. Several discussions focus on the need for more robust and flexible APIs to support complex return types (like dictionaries or tuples) in TorchScript, as well as improving automatic promotion and validation of dtypes during operations, particularly with mixed or complex data types. There are unresolved questions about the proper handling of dtype promotion in tensor operations, especially when involving structured kernels and implicit casting rules. Additionally, issues with specific operations (e.g., softmax with -inf inputs, GEMM shape regressions) and platform-specific bugs (e.g., Windows, ROCm, or CUDA driver problems) are recurrent, with some solutions pending further investigation or community validation."
2022-02-24,pytorch/pytorch,"The discussions mainly revolve around various issues and bugs in PyTorch related to GPU/CPU coherence, memory alignment, data type promotion, and compiler/hardware compatibility. Several entries mention potential fixes such as deep copying metadata to avoid shared state bugs, disabling certain fusion optimizations for specific data types, or handling data alignment issues during tensor creation. There are also ongoing questions about improving dispatch mechanisms (e.g., using templates vs. dispatch by namespace), support for specific features like inverse CDF for the Gamma distribution, and better handling of environment-specific constraints like cgroups or network rendezvous ports in distributed setups. Some discussions highlight the need for clearer documentation and more robust test coverage for edge cases, particularly on different platforms and hardware versions. Unresolved questions include the best approach for type promotion rules, cross-platform consistency, and safe integration of new features within existing PyTorch architecture."
2022-02-25,pytorch/pytorch,"The repository discussions cover a variety of technical concerns including bug fixes and feature enhancements, such as the need for more flexible attention modules, handling of sequence and map types in ONNX exports, and support for global tensors outside of Parameter objects. Several issues highlight inconsistencies and regressions introduced by recent PRs, requiring targeted fixes and potential workarounds, especially related to CUDA/cuBLAS, environment configurations, and test stability. Some discussions question the architecture choices around supporting additional data types, advanced shape semantics, and the implications of refactoring or deprecating existing functionalities. Overall, key unresolved questions involve how to extend PyTorch’s type system for better static analysis, how to ensure backward compatibility and stability across hardware and software updates, and how to improve CI robustness for complex features."
2022-02-26,pytorch/pytorch,"The discussions highlight challenges in extending PyTorch's tensor operations, such as enabling non-module tensors to be optimized without wrapping in auxiliary modules, and providing flexible activation functions like Entmax or Sparsemax. There is interest in enhancing the transformer modules, particularly in exposing attention weights and disabling head averaging, to support better interpretability and custom behaviors. Several issues involve backend development for hardware acceleration (e.g., Metal for Apple Silicon, GPU support for macOS), requiring deep API understanding and substantial implementation effort, with some efforts aiming to instantiate independent backends to avoid lengthy monolithic updates. Others concern CI stability, error handling improvements, and making debugging more efficient, such as by simplifying traceback extraction and error reporting. Unresolved questions include the feasibility of integrating advanced features into core functions, effective support for heterogeneous device inputs, and progress on hardware acceleration initiatives."
2022-02-27,pytorch/pytorch,"The discussions primarily focus on improving environment setup and kernel registration for PyTorch in Jupyter, especially within WSL, highlighting the importance of correctly registering the environment kernel with `ipykernel` to ensure proper import of `torch`. There are ongoing performance concerns comparing `LSTM` and `LSTMCell`, with hints that fused C implementations may influence their efficiency differences, and questions about numerical precision. Several issues involve CI/CD pipeline stability and merging delays, with references to various internal and external project statuses, including Apple's Metal backend and potential GPU acceleration on macOS. Some comments indicate troubleshooting steps, such as reinstalling packages or checking environment configurations, while others pertain to development progress, test failures, and dependency management. Unresolved questions involve transparency on feature timelines (e.g., PyTorch's CUDA/Metal plans), performance nuances, and community assistance in profiling and debugging."
2022-02-28,pytorch/pytorch,"The comments reflect ongoing technical concerns around PyTorch's support for various tensor operations and data structures, particularly in relation to exporting models to ONNX, handling `List[Tensor]` types, and compatibility with ONNX operators and runtime (e.g., TensorRT, ONNX Runtime). Several discussions emphasize the need for clearer, more stable support for features like `ceil_mode` in pooling, in-place tensor modifications, and level annotations in tensor subclasses, to facilitate correct autograd behavior and model export. Support for complex operations like sparse matrix solve, multi-label loss, and advanced control of dispatch keys (e.g., for functionalization and re-entrant dispatch) also feature prominently, with questions about how to avoid correctness and performance regressions. Unresolved questions include standardizing support for `Map` types in ONNX, improving debuggability and logs, and handling the interaction of multiple layers of tensor wrapping or subclassing related to autograd and functionalization. Overall, the discussions highlight a need for clearer API semantics, better support for various tensor types in export and runtime, and more predictable, robust handling of tensor subclassing and autograd mechanics."
2022-03-01,pytorch/pytorch,"The GitHub comments highlight multiple recurring themes in the 'pytorch/pytorch' repository discussions: (1) Clarifications and corrections to specific code implementations, such as the behavior of functions like `reverse_padded_sequence`, `where`, and custom modules (e.g., `LocalLinear`), often with concerns about version compatibility and correctness; (2) Maintenance and process issues, including rebase difficulties, merge conflicts, and CI build failures, sometimes due to environmental issues or code regressions; (3) Feature requests and improvements such as support for sparse shapes, better ONNX export capabilities, handling dynamic axes, and more flexible tensor type annotations; (4) Bugs and regressions, especially around JIT compilation, CUDA builds, and specific operator behaviors, with discussions about fixing these with code patches, PR reviews, or feature re-implementations; (5) General development concerns including the need for more comprehensive tests, stability of code in different configurations (CPU/GPU), and clarifications on policies for code contribution, sign CLA, or supporting specific environments."
2022-03-02,pytorch/pytorch,"The discussions highlight several recurring technical challenges, including CUDA initialization errors in multiprocessing contexts, the need for proper handling of unused parameters in Distributed DataParallel, and the importance of consistent serialization of sparse tensors with their coalesced state. There are questions about how to correctly implement re-entrancy and functionalization behaviors in custom Python tensor subclasses, with some proposals involving TLS state management and potential API changes like adding `debugName()` to `Node`. Additionally, concerns about shape inference in ONNX export, reducing merge conflicts with code refactoring, and handling non-deterministic behaviors affecting reproducibility are discussed. Overall, unresolved issues pertain to ensuring backward compatibility, robustness in distributed and serialization scenarios, and improving developer ergonomics around debugging and profiling tools."
2022-03-03,pytorch/pytorch,"The discussions highlight a common theme regarding the design and extensibility of PyTorch's data loading and serialization interfaces. There is an emphasis on the potential benefits of supporting memory-contiguous batches read directly from memory, and the need for clear examples to help users achieve higher throughput without multiprocessing. Several issues also touch on supporting dynamic shapes during ONNX export, including shape inference difficulties, especially with custom ops and complex data structures. Other topics include performance benchmarks for certain operations, compatibility concerns when expanding features (e.g., integer matrix multiplication), and ensuring that custom implementations or data structures (like batches or tensor collections) are flexible enough for diverse data types. Overall, many open discussions concern improving usability, performance, and flexibility through better documentation, example code, and API design refinements."
2022-03-04,pytorch/pytorch,"The discussions primarily revolve around the semantics and behavior of tensor operations within no_grad contexts, particularly the update and understanding of `requires_grad` and `detach()` behavior, including how views are tracked and whether views in no_grad mode should require gradients. Several issues pertain to user experience around modifying view tensors, error messages during in-place operations, and the shape inference in dynamic or custom ONNX-exported models. There's also ongoing work on enhancing C++ tensor representations, implementing masked tensor operations, and improving distributed training and debugging. Some concerns include hardware-specific performance differences, CUDA version support, and build environment consistency, especially related to long path issues on Windows. Unresolved questions highlight the need for clearer APIs, better error reporting, and more robust shape and tensor metadata handling to support advanced features like custom ops and dynamic shapes."
2022-03-05,pytorch/pytorch,"The discussions highlight several technical issues and enhancements within the PyTorch ecosystem, such as the support for zipping modules and tensors, handling recursive blob attributes in graph transformations, and improving ONNX operator support. There are ongoing efforts to enable multi-GPU training with DDP, support for new ops like GridSample in ONNX, and extending support for complex data types like complex32 and half precision for various hardware and production scenarios. Additionally, concerns are raised about supporting dataclass subclasses of nn.Module, compatibility with Python 3.10, and ensuring robust CI testing across different environments. Some discussions also touch on the internal architecture for XLA tensors, the need for precise shape inference, and resolving CI build failures, especially with external dependencies like NCCL and CUDA. Overall, these threads reflect active development, compatibility enhancements, and troubleshooting efforts to improve PyTorch's performance, usability, and hardware support."
2022-03-06,pytorch/pytorch,"The discussions involve several technical concerns: difficulties in designing a clean API for `nn.CRF` with suggestions to consider third-party libraries like pytorch-crf; build and compilation issues related to environment configurations, MKL static linking, and dependencies such as vmdLog2 and vmsLog2, especially during source build failures; potential interest in Mac ARM/M1 GPU acceleration with questions about Apple's plans or community-supported solutions; performance and optimization strategies, including the use of NVFuser and dynamic shape fusion to improve inference speed and manage TorchScript profiling; and the need for better documentation, build configuration clarity, and testing protocols to prevent regressions or incompatibilities, particularly with new features or operator library changes. Unresolved questions include the plans for Apple’s GPU support, how to disable or control TorchScript optimizations post-deployment, and ensuring CI tests align with code modifications."
2022-03-07,pytorch/pytorch,"The comments highlight several recurring concerns: the importance of ensuring contributors sign the Contributor License Agreement (CLA) before merging PRs; the need for better documentation and API support, particularly regarding shape inference with `dynamic_axes` and custom operators in ONNX export; challenges with multi-GPU NCCL errors often caused by system or environment misconfigurations such as port exhaustion or subnormal number issues; and the desire for improved build infrastructure and backward compatibility handling, especially for long path support on Windows and process group serialization. Additionally, some discussions focus on performance profiling, executor profiling, and shape fusion optimizations in JIT, and the potential necessity of providing user-friendly APIs for model wrapping and module materialization in distributed training. Unresolved questions include the precise handling of shape inference with dynamic axes, BC guarantees for in-tree vs. out-of-tree codegen utilities, and the support for certain operators like spatial transformers in ONNX."
2022-03-08,pytorch/pytorch,"The comments reveal issues related to serialization compatibility between Python and C++, particularly with the `torch.save` and `torch.load` functionalities. Several discussions highlight that loading Python-saved models in C++ fails when the models include unpickled `torch._UntypedStorage` objects or custom attributes, often crashing due to missing serialization support or incorrect assumptions about model formats. Troubles also involve complexities with serializing and deserializing workflow-specific objects like `ProcessGroup` or distribution states, where the current serialization approach is inadequate or too coupled. There are proposals and hints that support delaying registration or implementing custom serialization methods to avoid incompatibility issues, especially for complex objects beyond simple `state_dict`. Unresolved questions concern how to robustly support a wider range of saved models and internal objects natively without breaking existing workflows or requiring excessive custom code."
2022-03-09,pytorch/pytorch,"The discussion touches on several recurring topics within the PyTorch repository: the efficiency and backward-compatibility implications of weight transpositions in modules like `nn.Linear` and `RNN`, and whether internal matrices are stored transposed; issues related to support for complex tensors in collective operations like `broadcast_coalesced`, especially for complex tensor support, conjugate support, and view transmitting concerns; challenges with environment setup, such as build dependencies, linker paths, and GPU support on older hardware; difficulties with RPC tensor pickling due to storage serialization differences and version compatibility; and CI-related concerns, including test failures, build configurations, and proper labeling for PRs, alongside suggestions for improving testing strategies and dependency management. Many problems involve balancing performance with backward compatibility, environment configuration, and test coverage, with unresolved questions about implementation details and long-term architectural choices."
2022-03-10,pytorch/pytorch,"The collected comments highlight ongoing challenges with PyTorch support for complex data types, particularly around automatic differentiation and tensor type inference, especially for nested or Union types involving lists and tuples. Several discussions revolve around improving the symbolic representation for custom operations, compatibility and debugging issues with various CUDA versions, and the importance of maintaining support for specific environments like Windows, macOS, and older compilers. There are concerns about performance regressions with new features such as grouped convolutions and workload-specific optimizations, as well as issues related to build system compatibility and internal testing infrastructure. Additionally, several proposals suggest enhancing type safety, code maintainability, and supporting advanced features like mixed precision, all while ensuring backward compatibility and efficient execution across diverse hardware setups."
2022-03-11,pytorch/pytorch,"The comments highlight a range of technical issues and discussions, including the incremental addition of support for element-wise sparse * dense operations in PyTorch, limitations of torch.jit support for RNNs on GPU, and challenges with exporting models to ONNX due to operators like adaptive_avg_pool2d. Several entries discuss build and environment compatibility problems, especially on macOS and Windows, often related to linking, library versions, or deprecations (e.g., Eigen support, .pyi files). There are questions about device consistency in tracing and model serialization, particularly device mismatches during loading or JIT compilation. Additionally, discussions touch on upcoming features such as PyTorch acceleration on Apple M1, support for complex dtypes like complex32, and improving user experience with FSDP, auto_wrap policies, and testing processes, especially in large test suites like test_operators.py."
2022-03-12,pytorch/pytorch,"The discussions revolve around stability and memory issues in PyTorch, especially related to CUDA memory access errors after OOM events, with suggestions to update PyTorch versions or use specific inputs like `Tensor.contiguous().cuda()`. Concerns are raised about the support for complex number operations, particularly complex32, highlighting the need for broader operator support, documentation, and compatibility in both CPU and GPU contexts, alongside potential deprecation of two-channel representations. Several bug reports and patch requests target specific operator support in ONNX export, sparse tensor handling, and issues related to tensor serialization/deserialization, with suggestions to update dispatch macros, support boolean types, and improve testing. There's also mention of options to improve user guidance on gradient accumulation for FSDP and fixing NCCL communication hangs, especially in multi-GPU or multi-proc-per-node setups. Overall, unresolved questions include the best way to support complex types, stabilize memory access, and enhance operator/serialization support across the framework."
2022-03-13,pytorch/pytorch,"The discussions primarily revolve around the challenges of supporting multiple hardware accelerators and platforms in PyTorch, including issues with M1/M2 chips, Neural Engine, and GPU support on Apple silicon, as well as concerns about framework integration and compatibility. There is interest in leveraging Apple’s Neural Processor for training, though limitations exist due to microarchitecture constraints, and ongoing reverse-engineering efforts aim to improve support, such as CoreML inference acceleration. Some comments question the current lack of support for certain hardware and suggest exploring lower-level APIs or compatibility layers like Vulkan or HIP. Other discussions focus on improving documentation, tutorials, and features like gradient accumulation in Fully Sharded Data Parallel (FSDP). Overall, unresolved questions include how to effectively utilize Apple’s neural hardware for training and the best strategies for cross-platform support and deployment."
2022-03-14,pytorch/pytorch,"The discussions highlight several technical issues including environment and build compatibility problems, such as CMake version dependencies and system configuration impacts on compilation, especially on platforms like Jetson Nano and Mac M1. There are questions about improving NVIDIA GPU memory management and test performance, as well as concerns about the utility and maintenance of extensive diff-based testing and documentation practices. Several feature requests and code refactor suggestions are proposed, such as adding `torch.get_printoptions`, implementing deterministic `lstsq`, and enhancing API and documentation clarity. Persistent CI failures, some due to hardware limitations or known bugs, remain unresolved, with suggestions to update CI practices, improve error messaging, and support newer or older toolchains. Unresolved questions include appropriate ways to handle GPU memory, the necessity of synchronization calls, and how to better coordinate internal testing and external contributions."
2022-03-15,pytorch/pytorch,"The discussions mainly revolve around model conversion and export issues in PyTorch, including challenges with ONNX support for certain operators, handling of dynamic shapes, and potential API limitations. There are questions about how to properly implement or register new operations, ensuring backward compatibility, and managing the complexity of testing and benchmarking across different configurations and platforms. Some comments address build and environment reproducibility, especially related to external dependencies like numpy, and how internal tools and CI pipelines can be optimized or extended. Several discussions also touch on improving user-facing API features, such as customizing API behaviors without breaking existing models, and collaboration efforts for better support of new operators and frameworks. Unresolved questions include how to reliably test and verify partial operation support in diverse environments, and how to ensure smooth integration of new features or fixes into future PyTorch releases."
2022-03-16,pytorch/pytorch,"The comments primarily revolve around detecting and fixing gradient computation bugs and limitations in PyTorch autograd, especially related to NaNs, infs, and differentiating between ""zero gradient"" and ""no gradient"" at per-element levels, with discussions about the need for masking mechanisms like MaskedTensor. Several issues concern memory and performance bottlenecks in tensor operations (e.g., large allocations with contiguous tensors) and debugging complex behaviors such as NCCL communication errors and CUDA/driver incompatibilities. There are questions about API design, such as extending functions like `addmm`, and handling multiple inputs/outputs with TorchScript and JIT, as well as maintaining backward compatibility and simplifying user workflows. Some discussions highlight internal testing, CI failures, and the need for better reliability, documentation, and API usability, including suggestion to move expensive or infrequent checks to periodic testing. Overall, unresolved questions include how to improve autograd semantics for masking, API consistency for multi-element operations, and addressing memory/performance challenges in large tensor computations and distributed setups."
2022-03-17,pytorch/pytorch,"The comments across the GitHub issues in the 'pytorch/pytorch' repository reveal ongoing challenges with slow download speeds from the PyTorch servers, especially on certain hosting platforms like SLURM clusters, and considerations for improving data handling, distributed training stability, and checkpointing mechanisms. Several users report persistent network throttling issues, suggesting alternative solutions such as using AWS Deep Learning AMIs or custom streaming/loading approaches. There are also discussions about build issues, environment compatibility, and the security implications of unpickling, highlighting the need for safer serialization strategies. Additionally, discussions around distributed training configurations (e.g., NCCL, Gloo, process group initialization) and optimizations like overlapping communication and computation, as well as checkpoint loading methods, point to concerns about scalability and robustness in large-scale model training. Overall, unresolved questions include server reliability, secure serialization, and efficient multi-node coordination, indicating areas for potential future improvements."
2022-03-18,pytorch/pytorch,"The collected comments highlight ongoing technical issues and development discussions within the PyTorch repository. Concerns include data loader errors in multi-worker and Jupyter environments, especially around unexpected worker exits and memory allocation failures; questions about reducing binary size and custom kernel support for dynamic shapes; and challenges in exporting operators like `col2im` to ONNX at specific opset versions. Discussions also cover differences in package sizes across OSes, potential updates needed in onnx support for operators like `GridSample`, and discrepancies in behavior and implementation of functions such as `scatter_` and `unique`. Some comments relate to codebase refactoring, improving registration mechanisms for custom operations, and maintaining compatibility across various hardware and software configurations. Unresolved questions involve ensuring backward compatibility, supporting dynamic shapes, and fixing specific runtime errors or performance regressions."
2022-03-19,pytorch/pytorch,"The discussions highlight issues related to optimizing PyTorch's binary size for different platform architectures, with specific interest in building minimal custom binaries by tailoring architecture support. There are concerns about build environment constraints, such as incompatible compiler versions like gcc 9.3 with nvcc, which affects the ability to compile for certain hardware. Some developers seek guidance on configuring builds for CPU-only or specific device support, along with better documentation for advanced customization like building dynamic shape support or extending registration mechanisms for multiple devices. There are also technical questions about the correctness of gradient computations in certain algorithms, and performance considerations regarding reliance on proprietary libraries like cuBLAS versus alternative kernel libraries such as NVIDIA's CUTLASS. The overall theme emphasizes improving build flexibility, reducing binary size, and enhancing hardware and backend support within PyTorch's compilation and runtime systems."
2022-03-20,pytorch/pytorch,"The discussions cover several technical topics within the PyTorch repository. There is interest in refactoring specific quantized average pooling functions into a more generic kernel, with benchmarking needed to determine the most efficient implementation. One issue involves modifications to a training script for different datasets, which was fixed by enabling gradient requirements. Several comments address build and CI infrastructure concerns, including internal CI failure reports and progress updates. Additionally, there are discussions on advanced tensor concepts like ""meta tensors"" to optimize shape inference and potential issues with dynamic library loading, reflecting ongoing efforts to improve code modularity and dependency management."
2022-03-21,pytorch/pytorch,"The discussions mainly revolve around troubleshooting build and runtime errors in PyTorch, such as missing `lib`/`lib64` directories in GCC libraries, support and compatibility issues with CUDA and ROCm versions, and build configuration options that affect performance and compatibility. Several comments suggest potential fixes like modifying `LD_LIBRARY_PATH`, re-evaluating support for certain CPU features, or adjusting build options like enabling FBGEMM, MKLDNN, or QNNPACK for CPU performance. There are questions about specific functionality, such as support for complex tensor types, ONNX export limitations, and the impact of removing certain codegen or support for generator arguments. Many discussions also touch on CI configuration challenges, build rule validation, and the implications of changes on binary distribution and downstream projects. Unresolved questions include the precise support status of certain operators, the effects of unsupported configurations, and the potential for refactoring code to improve support or error handling."
2022-03-22,pytorch/pytorch,"The discussions reveal ongoing challenges in optimizing and debugging PyTorch's low-level operations, such as issues with C++ operator support, support for quantized and sparse tensors, and correct implementation of support functions like `make_scalar`. Concerns include handling of tensor strides, support for custom and internal operators, and the need for better API abstraction for tensor storage with metadata. Some debates focus on whether certain operations should be supported directly in `native_functions.yaml`, if support for private or internal ops should be exposed, and strategies for dealing with operator fallback mechanisms, especially when involving C++ bindings and support tags. Unresolved questions involve planning for eager mode support, proper handling of RNG ops in lazy graphs, and the overall design of operator tagging and support registration for extensibility and performance."
2022-03-23,pytorch/pytorch,"The comments retrieved stem from numerous GitHub issues and PRs related to PyTorch development. Key concerns include challenges with DataLoader's multiprocessing and memory sharing, the need for efficient sparse tensor operations and their proper handling of batch dimensions, and difficulties applying GPU and CPU support for quantization and tensor operations across different environments. Several discussions focus on improving error diagnostics, refactoring code for better modularity between CUDA and CPU implementations, and ensuring correctness of tensor shape/stride assumptions. Additionally, there are recurring questions about testing strategies in distributed and hardware-restricted setups, as well as updates and rebasings needed for PRs that have become stale or require conflict resolution. Overall, the issues highlight ongoing efforts to improve PyTorch's robustness, usability, and support for advanced features such as sparse, quantized, and distributed tensors."
2022-03-24,pytorch/pytorch,"The comments highlight issues related to debugging and stability in PyTorch, such as CUDA device-side assertions, segmentation faults, and runtime errors that occur under specific conditions or environments. Several discussions concern improving tooling and testing practices, including the need for better test automation, the complexity of correctly disabling flaky or environment-specific tests in CI, and suggestions for more robust mechanisms for serializing and handling complex data structures like sparse tensors or models with multiple attributes. There are also architectural debates, such as the design of the autograd/torch dispatch hierarchy and the handling of model scripting and TorchScript compatibility, as well as concerns about environment reproducibility and dependency management. Overall, key unresolved issues involve making debugging more reliable, enhancing test robustness, and improving module and model deployment workflows within the PyTorch ecosystem."
2022-03-25,pytorch/pytorch,"The comments reflect ongoing discussions about improving numerical stability and error messaging in linear algebra routines, such as replacing confusing rank deficiency errors with clearer messages aligned with scipy.linalg conventions, and considering LDL algorithms for support beyond positive definite matrices. There's also concern about performance regressions, especially inside cuDNN, where investigation is needed to determine if implementation mismatches are causing slowdowns. Additionally, there are discussions about tool and code architecture improvements, such as secure unpickling strategies, dynamic linkage support, and refactoring the hierarchy of node classes for the compiler IR to facilitate reuse and simplify backend implementations. Unresolved questions include how to balance safety and performance in unpickling, managing backward compatibility, and ensuring that new or refactored code integrates seamlessly without breaking existing tests or internal infrastructure constraints."
2022-03-26,pytorch/pytorch,"The discussions include requests and ongoing efforts to implement native ConvLSTM/GRU layers in PyTorch, inspired by Keras and Sonnet, with initial suggestions to create a Conv2dLSTMCell proto-layer. There are technical concerns around supporting flexible hyperparameters and API design, such as whether to assume shared convolutional hyperparameters or support separate configs. Several issues address synchronization and batching bugs, including handling zero-dimension batch sizes in SyncBatchNorm, with updates and fixes underway. Community members seek guidance on contributions, and there are discussions about code organization, build dependencies, and integration approaches, such as using macros vs. modular headers. Unresolved questions involve the best way to expose advanced convolution backward operations and how to efficiently support shape inference and meta-tensors in large-scale model development."
2022-03-27,pytorch/pytorch,"The discussions encompass various technical concerns, including performance issues when using `DataParallel` with large models, and errors related to the `logsumexp` function in PyTorch, which was addressed by schema modifications and bug fixes. Others involve CUDA context management in multiprocessing scenarios, with suggestions to initialize CUDA within worker processes, and issues related to undefined references in build linking, indicating potential build configuration or code placement problems. Several conversations also address CI failures, merge conflicts, and review processes, with emphasis on proper branch merging practices and testing when class/function names diverge in the codebase. Overall, the threads highlight ongoing debugging, performance optimization, code review clarity, and build reliability challenges."
2022-03-28,pytorch/pytorch,"The discussions primarily revolve around handling collective communication hangs in distributed training, with concerns about reliably detecting collective calls and managing dummy data exchanges, possibly via timeout mechanisms or early abort strategies. There are questions about supporting DDP in libtorch (C++ API), with user interest expressed but with incomplete progress updates. Several issues relate to ONNX export, especially handling mixed-precision models with opset differences, and whether to add automatic casts or enforce uniform input types. Other discussions touch on performance optimizations (e.g., reordering synchronization points), build configuration challenges (e.g., warnings, link errors, build branch strategies), and testing practices — including test coverage and validation methods for operator behaviors. Unresolved questions include the best way to improve usability, compatibility, and robustness of these systems in various environments."
2022-03-29,pytorch/pytorch,"The comments reveal multiple themes: a need for better test coverage and validation for new features and code changes, with suggestions to add specific tests before merging; concerns about the stability and correctness of certain implementations like SVD splits and operation emulation; ongoing issues with build system configurations, especially relating to CUDA, HIP, and internal dependencies, and suggestions for workarounds; and discussions about the design of autograd hooks, version tracking, and the importance of clear documentation for new APIs like SWA or custom operators. Many requests also emphasize rebasing, resolving merge conflicts, or adhering to strict code and testing standards—highlighting that certain PRs need more refinement, integration, or internal coordination before merging."
2022-03-30,pytorch/pytorch,"The discussions highlight several technical concerns regarding PyTorch's design and implementation. Key issues include the handling of `namedtuple` outputs in TorchScript tracing, with proposals to improve support for attribute access, and the challenge of supporting in-place modifications and inventory of tensor views with proper version tracking and autograd compatibility. There's also a recurring theme around improving or disabling specific features—such as `weight_norm`, `tensor factory functions`, and `view_copy` kernels—while ensuring backward compatibility, performance, and correctness. Questions are raised about the current limitations of CUDA kernel support, especially for larger matrix sizes and complex data types, as well as the need for better testing and validation strategies. Unresolved issues involve plans for fine-grained version counters, the extension of tensor layout capabilities, and more flexible mechanisms for user-defined tensor subclasses and custom autograd functions."
2022-03-31,pytorch/pytorch,"The comments reveal multiple technical concerns in the PyTorch repository, including compatibility issues across different versions and platforms (e.g., CUDA, ROCm, Windows). Several issues pertain to reproducibility and correctness, such as handling NaN losses with autocast/GradScaler, ensuring proper serialization/deserialization, and managing device-specific memory and threading behaviors. There are recurring problems with build configurations, compiler and linkage errors, and environment-specific bugs, often requiring patching or reverting patches. Discussions also highlight the need for better testing, clearer API semantics, and improved user experience—especially in distributed, quantization, and model exporting contexts—along with ongoing efforts to enhance robustness and platform support."
2022-04-01,pytorch/pytorch,"The comments reflect extensive discussions on PyTorch's ongoing support and development for PyPy, including updates to pybind11 and related build/test infrastructure, but lack definitive progress or confirmation of full support. Multiple issues are reported, such as package conflicts when installing PyTorch with PyPy, runtime errors, and compatibility obstacles with dependencies like torchvision, CUDA versions, and backend features. There are frequent mentions of debugging complex internal mechanics like overload resolution, tensor views, serialization, and execution barriers, often without clear resolutions. Concerns are raised about build failures, CI test stability, and the need for better handling of dynamic or incompatible configurations (e.g., complex tensors, specific hardware). Overall, the discussions highlight technical challenges in achieving stable, comprehensive PyPy support within PyTorch, with unresolved questions about compatibility, build procedures, and testing strategies."
2022-04-02,pytorch/pytorch,"The discussions raise several technical concerns including merge conflicts during pull requests, particularly in the codegen files, and the need for proper labeling of PRs with release notes and topics. There are questions about specific behaviors and bugs, such as NaN handling in integer tensors, GPU memory increases with TorchScript models, and the implementation of quantization observer sharing, suggesting potential extensions for better numerical support and framework flexibility. Some issues involve debugging and diagnostics, like verbose warnings or failure to reproduce errors, and there are suggestions to improve CI testing robustness with added benchmarks or reorganization of large PRs. Unresolved questions include how to extend support for numerical improvements in quantization and the best procedures to handle merge conflicts, split large PRs, or suppress warnings effectively."
2022-04-03,pytorch/pytorch,"The discussions highlight several technical issues: a recurring GPU hang issue in multi-GPU PyTorch training, likely caused by interference from the `gevent` monkey-patching of sockets, which disrupts NCCL communication; a problem with SSL certificate expiration affecting download URLs; and build/compatibility issues arising from model serialization across different PyTorch versions, including errors with custom operations and model exports. Additionally, there's interest in benchmarking performance improvements such as vectorized matrix multiplications in PowerSGD, and concerns about process hangs during data loading due to worker mutex locks. Some suggestions involve reinstalling the Linux system, rebuilding containers with matching NCCL versions, and adjusting DDP bucket sizes, while others focus on updating support for complex numbers, fixing pretrained model download URLs, and clarifying API expectations for gradient reduction callbacks. Unresolved questions mainly concern fixing the socket interference caused by `gevent`, ensuring backward compatibility in model serialization, and improving robustness in distributed and data loading workflows."
2022-04-04,pytorch/pytorch,"The discussion highlights several key concerns: first, recurring SSL certificate issues impacting downloads and dependencies, notably impacting user downloads from `download.pytorch.org`; second, the appropriate usage of `iter.dtype()` versus `iter.common_dtype()` in kernel implementations, emphasizing the importance of performing computations in `common_dtype()` rather than relying on `iter.dtype()`, which reflects output type; third, efforts to support deterministic versions of non-deterministic kernels like `scatter_add_cuda_kernel` for scientific reproducibility; fourth, internal testing failures and CI build instability, often due to global state issues or platform-specific bugs, with suggestions to improve test coverage and isolate issues; lastly, ongoing internal architecture discussions, such as refactoring of node classes for XLA backend support, handling of `view_copy` operators within functionalization, and deprecation plans for legacy profiling utilities, all indicating a focus on improving backend flexibility, reproducibility, and code maintainability."
2022-04-05,pytorch/pytorch,"The discussions primarily revolve around the complexities of sharing CUDA tensors across multiprocess or distributed contexts, highlighting issues with the current PyTorch design that prevent CUDA tensors from being sent or shared between processes without cloning. Several comments address the support for in-place operations like BatchNorm in differentiable graphs, performance bottlenecks, and potential improvements in operator support and implementation, such as transitioning to non-in-place BatchNorm or more efficient all_gather variants. Additionally, there are concerns about runtime and build failures, including CI-related issues, memory leaks, and compatibility breakages introduced by recent changes. Overall, key topics include CUDA tensor sharing, operation support and optimization, and ensuring stable, reproducible CI testing."
2022-04-06,pytorch/pytorch,"The discussions highlight persistent build and compatibility issues with PyTorch, often related to compiler support (e.g., GCC versions and CUDA), environment configuration, and outdated dependencies. Several users report failures due to compiler bugs (notably with GCC 7.5 and newer GCCs), or incompatibilities between compiler toolchains (e.g., Anaconda vs. system GCC), affecting the build process and code correctness, especially for GPU/CUDA functionalities. There are concerns about runtime errors like CUDA out-of-memory, and issues with onnx/export compatibility, shape inference, and debugging tools. Some discussions focus on improving CI workflows, environment management, and ensuring proper documentation and test coverage for different configurations. Overall, unresolved questions pertain to fixing build failures, ensuring deterministic and correct execution across platforms, and maintaining compatibility with evolving hardware and compiler toolsets."
2022-04-07,pytorch/pytorch,"The discussions across the GitHub comments highlight several recurring technical concerns: the complexity and ambiguity in exporting and converting models to ONNX, especially regarding support for dynamic control flow, shape inference, and correct node representation; issues related to model support for various hardware backends (e.g., CUDA 11.6, ROCm), including runtime errors and incomplete kernel support; challenges in ensuring correctness and compatibility of operators like flatten, flatten nodes in ONNX, and device-specific kernel issues; and the need for clearer documentation, test cases, and communication around backend support, memory formats, and BC-breaking changes. Several discussions also inquire about the status of support for newer hardware and tensor formats, as well as issues with build environments and CI infrastructures. The overarching concern is integrating support for new features, hardware, and model export capabilities while maintaining stability and clarity for users."
2022-04-08,pytorch/pytorch,"The discussions cover various technical issues encountered in the PyTorch codebase, including crashes related to memory corruption, interoperability with C++ and CUDA, and limitations in Python features such as infinite iterators and type annotations. Notable questions include the reasons behind specific failures (e.g., memory leaks when registering hooks, CUDA driver compatibility issues, and operator exporting limitations). Several suggestions aim to improve PyTorch's usability and robustness, such as adding a native ""repeat=True"" feature for DataLoader, supporting indexing in nested tensors, and handling special cases in model serialization and TorchScript conversion. Unresolved concerns involve enhancing compatibility across different hardware and software environments, refining API behaviors (like shape inference and versioning), and addressing flaky or failing CI tests that hinder development progress."
2022-04-09,pytorch/pytorch,"The discussions highlight ongoing challenges with sporadic CUDA and PyTorch errors, often related to memory management, nondeterminism, or specific operator behaviors (e.g., `pinv`, `cumsum`). Several issues concern the impact of nondeterministic functions on reproducibility and downstream effects, with suggestions to document these behaviors clearly. There are also technical questions about CUDA context handling in multiprocessing, the support of sparse tensor APIs, and the compatibility of certain ops with autodiff or fused implementations. Additionally, some discussions involve build stability, dependency conflicts, and platform-specific quirks, such as compiler or hardware-related issues. Overall, unresolved questions focus on ensuring deterministic results, improving error diagnostics, and maintaining compatibility across varied hardware and software environments."
2022-04-10,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch, such as build failures, performance regressions, and compatibility concerns with external libraries like DeepSpeed, as well as environment-specific warning messages (e.g., CUDA kernel cache directory). Several contributors seek guidance on reproducing and diagnosing errors, notably with debugging tools and code coverage, and propose potential fixes like adding missing checks or improving build scripts. There are questions about the impact of nondeterministic operations and how certain features (e.g., setting thread counts or using specific dispatch keys) influence performance and correctness. Overall, the focus is on debugging, optimizing build processes, ensuring compatibility, and clarifying code behavior across platforms and configurations."
2022-04-11,pytorch/pytorch,"The discussions primarily revolve around issues of download speeds and server reliability for installing PyTorch, with users reporting throttling and server/server-side bottlenecks, especially via conda and pip. There are concerns about improving the user experience when working with sparse tensor layouts, such as CSC and BSR, including better construction APIs and index access methods, suggesting a need for clearer, more consistent UI. Several bug reports address segmentation faults, build failures, and compatibility issues on different platforms (Windows, macOS, ARM, etc.), with suggestions for more robust error handling, environment setup, and potential API changes (e.g., disabling certain behaviors or adding new methods). Additionally, questions about exporting models to ONNX, handling special-float values, and ensuring correctness and performance in JVP computations are raised. Overall, the discussions highlight a mixture of infrastructure challenges, API design considerations, and correctness/compatibility concerns."
2022-04-12,pytorch/pytorch,"The comments reflect ongoing concerns about handling complex operators, particularly in the context of dispatch keys and operator registration. There is a proposal to introduce specialized set_output variants for sparse and quantized tensors, enabling precise control over tensor types in different contexts, and a related plan to generate new operator overloads that include dtype/layout parameters for better type and memory management. Challenges include ensuring backward compatibility, avoiding duplicate code paths, and addressing the current limitations of the operator meta code generation process, especially around supporting composite or custom kernels in a way that maintains correctness and performance. Additional considerations involve how to handle existing operators, API consistency between CPU, CUDA, and XLA backends, and integrating safety checks or GIL guarantees for thread-safe tensor metadata access. Overall, the key unresolved questions concern how best to extend operator registration and meta functions to support flexible, type-aware, and backward-compatible implementations without introducing performance regressions or complexity overhead."
2022-04-13,pytorch/pytorch,"The comments reflect ongoing discussions and concerns about several technical issues within the PyTorch codebase. Major topics include handling of out-of-place operations with potential memory leaks, especially in the context of device synchronization, and the support for `out=` parameters across various functions for better memory and performance management. Several discussions highlight the challenges of ensuring backward compatibility and correct semantic diffing of operator schema changes, especially for evolving APIs like `torch.nn.functional`. There are also repeated concerns about the stability and correctness of distributed operations (e.g., NCCL errors, process group initialization), the impact of tensor layout and data pointer manipulations, and the need for clearer documentation of behaviors like deformable convolutions or handling optional tensors. Some comments suggest moving certain functionality into more explicit or centralized APIs, or the need for additional checks and safeguards to prevent silent errors or inefficiencies."
2022-04-14,pytorch/pytorch,"The comments reflect ongoing discussions around specific implementation challenges and discrepancies in PyTorch features, such as the behavior of 'same' padding with stride > 1, issues with ONNX export for adaptive pooling with non-standard input sizes, and deployment on various platforms (e.g., MacOS, CUDA versions). Multiple users encountered build errors (e.g., illegal instructions, compiler or driver issues), often related to hardware specifics or compiler compatibility, with some seeking workarounds or fixes (e.g., manual building, configuration tweaks). Feature requests like support for new data types (e.g., int4 quantization) and API improvements (e.g., 'out' parameters, better error handling) are also discussed, alongside infrastructure-related concerns like shared memory management and CI pipeline failures. Unresolved questions include platform-specific failures, compatibility issues with backward compatibility, and the need for better error messages or test coverage. Overall, the discussions highlight efforts to improve functionality, compatibility, and stability amidst platform and build environment challenges."
2022-04-15,pytorch/pytorch,"The discussions reveal several consistent themes: there are ongoing issues with handling sparse tensors during model save/load and distributed training, highlighting the need for better support or workarounds; support for specific features like `dim=None` in reduction operations and support for half-precision sparse matrix multiplication remains incomplete or problematic, requiring further implementation or fallback strategies; compatibility across different CUDA versions, drivers, and hardware (e.g., A100, V100, ROCm) introduces instability, especially in multi-GPU or mixed-precision contexts; some internal infrastructure and CI/CD workflows (e.g., nightly builds, CI flow labels) encounter failures or require better documentation and automation; finally, there is interest in enhancing API consistency, performance, and maintainability—such as reducing internal changes via OpInfos, improving type safety, and refining backend integration—while balancing backward compatibility and usability."
2022-04-16,pytorch/pytorch,"The discussions highlight issues with DataLoader performance on Windows, particularly slow data loading with `num_workers > 0`, and solutions involving `persistent_workers=True` or custom loaders that load data entirely into memory. There are concerns about GPU utilization and circumventing CPU bottlenecks by preloading data on GPU, but this requires datasets to fit into memory. Several technical problems are also discussed, such as managing interleaved logging from multiple GPU ranks, handling memory leaks with weight normalization, and fixing backward hook timing for profiling. Additionally, compatibility issues with various PyTorch versions, build failures related to CMake and sanitizers, and ensuring proper API exports and public API compliance are mentioned. Overall, the main themes involve optimizing data loading performance, improving debugging tooling, fixing internal API and build issues, and maintaining code quality and compatibility across platforms."
2022-04-17,pytorch/pytorch,"The discussions encompass several key topics: the removal and potential reinstatement of private APIs like `torch.multinomial_alias_draw`, with suggestions to formalize such features as public APIs if deemed useful; challenges in implementing efficient tensor reductions and allocations, especially under TorchScript or TorchScript-support limitations; the handling of shape and padding information in nested tensors and the potential for new metadata structures (e.g., `NestedSize`) to improve padding masks and tensor composition; issues with LLVM code generation and JIT compilation, including crashes due to LLVM's internal assertions and workarounds like disabling LLVM fusions; and considerations around operator design, such as avoiding duplication in operator registration and exposing internal functions only via the operator set to prevent user-visible complexity. Unresolved questions include whether certain private APIs should be public, how to improve vectorized and fused operations for complex data types, and how to address internal compiler crashes."
2022-04-18,pytorch/pytorch,"The comments primarily concern issues related to PyTorch's build and installation processes, such as slow download speeds with conda or pip, server/server throttling problems, and environment-specific build failures (e.g., CMake, Ninja, LLVM, Nvidia/NVRTC-related complications). There are frequent requests for better documentation and tooling support for custom or complicated setups, particularly for integrating with ONNX, XLA, and custom operators, including shape inference and server updates. Several discussions highlight the need for more experimental or internal features (like nested tensor handling and functionalization modifications) to be better abstracted or documented for user and developer use, indicating a gap in the API stability and clarity. There's also ongoing work to improve reliability in CI workflows—handling flaky tests, large test exports, and ensuring proper labeling and review—reflecting the complex and evolving nature of PyTorch's development ecosystem. Unresolved questions mostly revolve around environment-specific build issues, compatibility with older ONNX or CUDA versions, and how to safely expose or hide certain internal APIs to avoid breaking backward compatibility or introducing bugs."
2022-04-19,pytorch/pytorch,"The comments highlight ongoing efforts and challenges related to PyTorch's development, including the implementation of operations like sparse matrix multiplication, support for half-precision (FP16/BF16) on CPUs and GPUs, and the correctness of quantization and export to ONNX. Several discussions focus on improving API usability (e.g., exposing parent modules of parameters, shared Pytree library), fixing bugs (e.g., related to `torch.nonzero`, `lazy tensors`, or environment issues), and addressing performance regressions or stability problems (e.g., TF32 accuracy, CUDA compile issues). There are also technical considerations around building and integrating C++ extensions, CUDA support, and ensuring compatibility across different configurations and hardware (e.g., A100 GPU, CUDA versions). Some questions concern proper testing, bug reproduction, and the impact of various changes on existing features and workflows, often requiring careful rebase, patching, or infrastructure fixes before proceeding."
2022-04-20,pytorch/pytorch,"The discussions mainly revolve around PyTorch's current limitations and ongoing improvements: CPU support for FP16 convolutions is absent, limiting model export/testing workflows; support for `torch.nn.ParameterList` in TorchScript is lacking but has been fixed; handling complex numbers, particularly support for functions like `log1p` and `logcumsumexp`, is incomplete; and certain features such as `torch.is_inference_mode_enabled` or handling variable modules in TorchScript require further development or careful handling. There are also concerns about performance impact and correctness, especially for operations like `LayerNorm`, and issues with CUDA/nccL errors on certain hardware/software configurations. Some discussions suggest potential workarounds, code improvements, or external tooling to better support tracing, serialization, or future extension points. Unresolved questions include how to properly extend or document certain APIs, manage compiler/linking errors, and ensure backward compatibility or deprecation strategies."
2022-04-21,pytorch/pytorch,"The comments highlight multiple technical issues and discussions regarding PyTorch's development, including memory management in CUDA contexts, support for complex-valued SVD operations in ONNX, and performance implications of various sampling methods like `multinomial`. There are concerns about compatibility and correctness with symbolic shapes in Torch FX, especially regarding how shape processing handles dynamic or non-contiguous tensors. Several PRs and feature requests involve extending API capabilities, such as returning parent modules of parameters or enabling more efficient, controlled execution of JIT/dispatcher features, sometimes requiring deep internal changes. Additionally, ongoing CI stability issues, build failures, and platform-specific bugs point to challenges in maintaining cross-hardware and version compatibility. The discussions also include debates about design choices, such as module attribute access, API deprecation, and safe integration of new features without breaking existing APIs or internal workflows."
2022-04-22,pytorch/pytorch,"The discussions primarily focus on performance analysis and optimization strategies for sampling methods like `torch.choice` and multinomial alias sampling, with suggestions to share setup or precompute shared components for efficiency. Several issues highlight the need for better dataset caching strategies, especially on slow I/O systems or remote storage, emphasizing on-disk caching and the limitations of in-memory caching with multi-worker DataLoader setups. There are concerns about the proper handling of API compatibility, including deconflicting attribute access in module containers like `ModuleDict`, and maintaining backward compatibility with subclassing and in-place operations. Multiple comments address potential bugs and inconsistencies in tensor indexing, assignment, and broadcast behaviors on CUDA, especially under deterministic settings or with non-contiguous tensors. Lastly, some discussions touch on the build and integration challenges across different platforms and environments (Windows, Mac, Linux, Apple Silicon), signaling ongoing efforts to ensure portability and stability."
2022-04-23,pytorch/pytorch,"The discussions center around support and stability issues in PyTorch, especially regarding mobile support for custom Python objects, and various bugs introduced by recent code changes. Notably, there are questions about whether certain operators like `CREATE_OBJECT` should be added to mobile supported ops, and concerns about whether recent PRs have fixed issues with the `PeriodicModelAverager` versus `PostLocalSGDOptimizer` in distributed training, including accuracy impact and hyperparameter tuning strategies. Several comments highlight failures in CI pipelines, such as problems with ONNX export, indexing, and CUDA compilation, and there are questions about platform-specific behaviors and environment inconsistencies. Some discussions also touch on performance optimizations, trade-offs with local SGD, and the need for more comprehensive or targeted tests to detect regressions, especially in complex distributed or hardware-accelerated scenarios. Overall, the key concerns include ensuring robustness, correctness, and efficiency of features in diverse environments, along with clear communication about testing and review processes."
2022-04-24,pytorch/pytorch,"The discussions primarily revolve around handling cases where certain losses or model parameters are unused during multi-GPU training in Distributed Data Parallel (DDP), with suggestions to return zero tensors with proper gradients to avoid backward hangs. There are concerns about the impact of such modifications on gradient reduction and autograd behavior, especially when conditionally skipping parts of the forward pass. Some suggest setting `find_unused_parameters=True` and embedding conditioning logic within the `forward` method to ensure gradient synchronization and autograd correctness. Other topics include recent changes and performance implications of profiling hooks, serialization compatibility issues between torch.save and torch::jit::load, and the impact of local/ hierarchal SGD on model accuracy, with proposed hyperparameter tuning and algorithm modifications. Overall, unresolved questions focus on ensuring correctness, performance, and compatibility when implementing these conditional or composite operations in distributed training and model serialization."
2022-04-25,pytorch/pytorch,"The discussions encompass a variety of issues within the PyTorch codebase and infrastructure, including the appropriateness of tackling unresolved or medium-complexity issues, implementation of structured vs. unstructured composites, thread pool scaling, and reproducibility concerns on different hardware. Several threads raise technical questions about specific features—such as shaping tensors, custom operator behavior, and the impact of certain PRs—often seeking clarification or proposing design improvements. Multiple issues involve build, test, and CI pipeline reliability, including merging failures, performance regressions, and environment inconsistencies like CUDA versions, setuptools, or header include problems. Certain discussions address API evolution – e.g., deprecating or moving functions, extending support for new devices such as XPU, or refining APIs around device modules and tensor creation behaviors. Overall, unresolved questions highlight ongoing debugging, refactoring, and feature planning efforts, with a focus on correctness, performance, and API consistency."
2022-04-26,pytorch/pytorch,"The discussions highlight several key issues: concerns about the correctness and comparison of custom PyTorch functions like `sqrtmh` versus scipy's `sqrtm`, including their gradient behavior and numerical accuracy; questions about the performance impact of profiling code, especially the overhead introduced by tracing functions like `torch.ops.profiler._record_function`; challenges with implementation specifics, such as handling complex data types, ensuring backend consistency for CUDA/ROCm, and proper support for quantization (e.g., preferred scales and zero points); problems related to the stability and safety of backend operations, like cuda context destruction and memory leaks; and the need for clearer documentation, reliable testing, and better handling of autograd, operator BC, and deprecations. Overall, there are unresolved questions around numerical stability, user-facing API design, and robust test coverage for new and existing features."
2022-04-27,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) challenges in DataLoader error handling and sharing strategies in multi-process training, with workarounds like re-creating dataloaders or setting 'file_system' sharing strategy; (2) issues with CUDA errors, illegal memory access, and driver faults in multi-GPU/cluster setups, sometimes fixed by updating CUDA/PyTorch versions; (3) the need for better support and testing of out-of-memory, distributed, and dynamic shape scenarios, including handling of the 'step' parameter in optimizers and shape inference in ONNX export; (4) API design questions around visibility and use of internal functions such as `prepare_sharded_tensor_read`, and the potential for public APIs or configuration for tensor sharding and checkpointing; (5) concerns about maintaining API consistency, backward compatibility, and the impact of changes like mode stacks, operator support, and inlining, as well as how to test these systematically."
2022-04-28,pytorch/pytorch,"The comments highlight several recurring challenges in the PyTorch ecosystem, including proper environment configuration for Jupyter notebooks, handling memory management with `from_dlpack`, and ensuring backward compatibility and correctness of operator behavior. Multiple discussions focus on improving or clarifying the implementation details for tensor size calculations in resizing operations (notably `align_corners=True`), reflecting uncertainties about documenting and standardizing behavior. Certain technical issues relate to build system configurations, specifically on Windows with CUDA, and the effect of newer CMake versions or environment variables. There are also concerns about correctness and performance trade-offs in operations like Adam optimizer's step counter placement, and whether certain features (like `torch.scan`) could be added for performance benefits. Overall, the discussions are centered on fixing bugs, improving usability, clarity in documentation, and making build/integration processes more robust, with some unresolved questions about best practices and design choices."
2022-04-29,pytorch/pytorch,"The discussions cover various technical topics in PyTorch, including designing a custom data sampler (`ClusterRandomSampler`) for multi-cluster datasets, handling variable tensor sizes in collate functions, and managing distributed training crashes and process termination. Several issues address testing and debugging challenges, such as adding tests for new operators, incorporating ONNX support, and resolving CUDA kernel or fuser-related errors. There is also a focus on improving documentation, API consistency, and feature support, such as eliminating the `increasing` argument in functions like `vander`, and managing model parameter untrainability. Unresolved questions include API design choices for referencing previous OpInfos, handling exceptions during distributed training, and environment-specific failures that require further investigation."
2022-04-30,pytorch/pytorch,"The discussions primarily revolve around enhancing robustness and usability of PyTorch's distributed and debugging features, such as introducing process termination controls for multi-GPU training and better error handling for process failures, especially on SLURM clusters. There are ongoing efforts to improve the internal and user-facing APIs, including adding features like `append` to `nn.Sequential`, refining dataclass source extraction for debugging and scripting, and managing documentation formatting for optimized builds. Several threads discuss compatibility issues with hardware (e.g., SM53 support, arm64 builds) and environment configurations, such as ensuring proper build support for different architectures and dependencies like libtorch and CUDA. Additionally, topics include handling internal operator support (e.g., GLOG, private operators), test stability across platforms, and version-specific behavior for functions like `torch.linalg.norm` to ensure clarity and correctness. Overall, the focus is on incremental improvements towards stability, performance, and developer experience."
2022-05-01,pytorch/pytorch,"The discussions highlight recurring issues with PyTorch's autograd engine, notably the lack of implementation for derivatives of aten functions like grid_sampler_2d_backward and hardswish_backward, which can cause runtime errors. Some users are experiencing build failures or crashes related to architecture mismatches (e.g., x86_64 clang vs. arm clang on M1 Macs) and configuration discrepancies, especially in reproducible environments like macOS with Rosetta. There are concerns about handling false-positive build failures due to API changes, such as argument renaming in `_validate_sparse_compressed_tensor_args`, and questions about standard procedures for addressing such issues. Multiple reports involve CUDA and GPU recognition errors, with TensorFlow and PyTorch sometimes conflicting or not working simultaneously, indicating environment setup challenges. Overall, the key concerns involve incomplete gradient implementations, build and environment configuration issues, and procedural clarity for maintaining stability amid API and infrastructure changes."
2022-05-02,pytorch/pytorch,"The comments highlight several technical concerns and discussions related to PyTorch development, such as addressing training deadlocks with DistributedDataParallel (notably requiring the use of `model.module.forward`), improving data loading efficiency via setting `torch.set_num_threads()` to reduce CPU contention, and modifying internal behaviors like padding limits, `__repr__` enhancements, and API stability (e.g., `torch.solve`/`linalg.solve`, `thrust::` namespace handling). There are ongoing discussions about expanding support for variable-size tensors in object detection, and APIs for collective communication (like `allgather_coalesced`) with performance comparisons, including proposals for new API designs such as `spectral_norm` support and optional parameter sharding. Additional concerns involve CI infrastructure, test stability across hardware (e.g., ROCm vs CUDA, architecture-specific issues), and ensuring proper API exposure and documentation, alongside handling version dependencies, reinstalling environment issues, and performance benchmarking. Overall, unresolved questions include API stabilization, performance optimizations (e.g., parallel sorting, multi-dimensional `kthvalue`), and correct handling of distributed and multi-device contexts."
2022-05-03,pytorch/pytorch,"The comments encompass a variety of technical issues related to PyTorch development, including environment and package configuration problems, support for additional functions like `log1p` for complex numbers, and build or CI failures on various platforms. Several discussions focus on improving the API and user interface for sparse or compressed tensors, handling special tensor types like `ZeroTensor`, and compatibility concerns involving different hardware architectures and PyTorch versions. There are also suggestions for refactoring or extending certain functionalities, such as enabling ops like `clone`, `detach`, and `apply_`, and documenting internal mechanisms like structured kernels. Unresolved questions include how to handle certain tensor behaviors at boundary conditions, the process for managing submodule changes (like in `ideep`), and strategies for managing large files or submodule updates. Overall, these discussions highlight ongoing maintenance, compatibility, and API usability challenges within the PyTorch codebase."
2022-05-04,pytorch/pytorch,"The discussions predominantly center around handling and improving specific PyTorch functionalities such as type promotion, operator registration, ONNX export, and distributed training. Several concerns involve backward compatibility, especially with deprecated or private APIs, and ensuring consistent behavior across different backends (e.g., CUDA, ROCm, XLA). There are technical nuances related to tensor serialization/deserialization, kernel compilation performance, and error handling in complex autograd and graph execution scenarios. Some conversations address CI and test infrastructure challenges, including build stability, queue times, and test coverage generation. Overall, the discussions reflect ongoing efforts to enhance robustness, performance, and usability while managing complexity and legacy compatibility in the PyTorch codebase."
2022-05-05,pytorch/pytorch,"The comments highlight ongoing challenges with CUDA-related CUDA errors, illegal memory access issues, and long compilation times, particularly with JIT and nvrtc in certain CUDA versions. Several discussions address the design and implementation of structured kernel APIs, aiming to improve flexibility, performance, and interface clarity, with proposals involving new kernel declaration keys, class-based wrappers, and layout abstractions like RLE. There are also concerns about correctness and reproducibility with mixed or finer control of math modes (e.g., tf32), as well as questions about proper handling of meta Tensors, process groups, and work abstractions in distributed and RPC contexts. Additionally, multiple PRs have encountered CI/test failures, merge conflicts, and build issues which require careful investigation, rebase, or parameter adjustments, often involving internal infrastructure or environment considerations. Overall, the discussions combine addressing technical correctness and performance, API design improvements, and practical build/test stability issues."
2022-05-06,pytorch/pytorch,"The comments reflect multiple ongoing discussions and concerns in the PyTorch development community, including issues related to CLA signing and PR merging processes, performance optimizations in data loading and kernel implementations, and technical challenges around kernel structuring, code generation, and support for various device types (e.g., CUDA, meta tensors). There are questions about handling specific edge cases, such as empty inputs and shape inference, as well as questions surrounding build dependencies like cuDNN, CUDA versions, and environment setup. Several PRs are being reverted or require rebase due to internal build failures, merge conflicts, or incompatible changes (e.g., with internal tools). There are also suggestions for improving code quality, testing strategies, and global design considerations such as namespace naming conventions and exposing certain modules. Unresolved questions include the appropriate use cases for meta tensors over RPC, the handling of multiple iterators in DataPipes, and the implications of internal changes for downstream dependencies like XLA."
2022-05-07,pytorch/pytorch,"The discussions primarily revolve around addressing limitations and bugs in PyTorch, such as the maximum category size for `WeightedRandomSampler` when exceeding 2^24, with proposed solutions involving partitioning the weights and sampling in chunks, or alternative approaches like custom samplers. Several issues involve performance optimizations, notably improving `index_select` for transformer model shapes on CPU, and reducing RAM usage and memory fragmentation during JIT tracing and model evaluation, with suggestions including process-based tracing and garbage collection. Compatibility and robustness concerns are also prominent, such as handling `ldl_factor` with complex tensors and hermitian flags, ensuring consistent shape inference with ONNX, and fixing potential segmentation faults or crashes in certain backends or input scenarios. Some discussions focus on internal implementation details, like the behavior of Kaiming initialization, shape setting in ONNX, and handling of `dim=None` in reductions for NumPy compatibility. Overall, the issues span functional correctness, performance improvements, stability fixes, and API behaviors, with ongoing work and proposed patches aimed at resolution."
2022-05-08,pytorch/pytorch,"The discussions reveal concerns about handling large sample sizes beyond the 2^24 limit in `WeightedRandomSampler`, with recommended code snippets for custom implementations and considerations for compatibility with Triton. Several issues involve CUDA kernel errors, memory leaks, and device compatibility, especially when working with newer GPUs like RTX 3090 and driver support. There are recurring questions about fixing or reproducing specific runtime errors (e.g., `CUDA launch failure`, `no kernel image available`, or `unspecified launch failure`) related to driver, CUDA, or environment mismatches. Also, some discussions focus on fixing bugs like `torch.svd` memory leaks, peculiar behavior of `kthvalue`, and the support for older GPU compute capabilities, with suggestions to update CUDA, drivers, or patch specific code components. Overall, the key themes involve ensuring compatibility, optimizing kernel implementations, and troubleshooting runtime CUDA errors."
2022-05-09,pytorch/pytorch,"The discussions highlight issues with large-scale data sampling and batching in PyTorch, with specific attention to implementing efficient custom Samplers that split large datasets into manageable chunks, avoiding memory bottlenecks. There are concerns about consistency and correctness in the implementation of features like `include_last_offset` in `EmbeddingBag`, with suggestions that the current CPU logic appears inverted relative to expectations. Several reports address CUDA kernel crashes, illegal memory accesses, and differences in behavior across hardware and software versions, raising questions about reproducibility and potential driver or framework bugs. Additionally, discussions include the maintenance and forward-compatibility of code modularity, the need for testing and validation of new features like SSD offloading in FSDP, and the importance of proper code signing and review procedures before merging contributions. Overall, unresolved questions focus on optimizing large dataset processing, ensuring correctness across diverse hardware, and streamlining CI/CD workflows for stable releases."
2022-05-10,pytorch/pytorch,"The comments highlight ongoing concerns with CUDA memory management and out-of-memory errors during training, with various troubleshooting approaches such as reducing batch size, emptying cache, and changing hardware configurations. There are issues related to environment and compatibility, specifically about supporting older GPU architectures (e.g., compute 3.5) and driver/CUDA versions, and potential discrepancies in support across different setups or PyTorch versions. Several discussions focus on feature implementation details, such as the correctness of the `include_last_offset` behavior between CPU and CUDA, reliability of `torch.to` and pytree utilities, and improvements to internal APIs for reshaping and expanding distributions. Dilemmas persist around ensuring backward compatibility, effective error handling, and precise test coverage, particularly for new backward implementations and distributed training. Finally, logistical challenges in CI pipeline speed, resource limitations, and CI hierarchy management are also recurring topics."
2022-05-11,pytorch/pytorch,"The discussions raise several key technical concerns, including issues with CUDA device-side asserts caused by dimension mismatches (e.g., softmax output vs number of classes), and the need to ensure softmax units match dataset classes in pretrained models. Questions are raised about the efficiency and design of device tensor transfer (particularly related to `torch.to`, `Tensor`, and `nn.Module` subclasses), emphasizing the importance of managing device contexts and avoiding multiple `to` calls. Some discussions also focus on improving ONNX export support for operators like `lift`, and handling shape inference with un-converted operators, especially for custom or proprietary operators. Additional concerns involve ensuring backward compatibility after operator changes, proper testing (including dynamic shape support and BC-breaking issues), and better user guidance for handling CUDA contexts in multiprocessing scenarios."
2022-05-12,pytorch/pytorch,"The comments encompass multiple issues and discussions from the PyTorch GitHub repository, including bug reports, feature proposals, and maintenance updates. Several comments reference specific code fixes or optimization strategies, such as enhancing the C++ API, improving performance for sampling algorithms, and handling custom operators in ONNX export. There are also many reports of CI failures, build conflicts, and environment setup issues, often accompanied by requests for guidance or clarifications on best practices. Some comments suggest potential feature enhancements or are follow-ups on ongoing development work, with multiple mentions of support for advanced features like mixed precision, custom serialization, and multi-node distributed training. Overall, the thread reflects active development, bug fixing, and infrastructure management efforts, with unresolved questions about testing, environment setup, and feature correctness."
2022-05-13,pytorch/pytorch,"The discussions highlight several recurring issues and questions: (1) The limitations of padding and stride calculations in max pooling with dilation, where the correct padding limit should be `1 + (kernel_size - 1) * dilation` rather than half of `kernel_size * dilation`. (2) Intermittent CUDA errors, illegal memory access, and driver faults linked to hardware, driver bugs, or synchronization issues, especially in multi-GPU or distributed settings, sometimes requiring server reboot or driver updates. (3) The handling of custom ONNX shape inference and how to properly register and propagate shape info for custom or unknown operators, with suggestions to utilize symbolic functions or shape inference tools, keeping in mind the complexity and maintenance concerns. (4) Compatibility and correctness issues arising from backward-incompatible changes, deprecated functions, and ensuring deterministic behavior in linear algebra routines like `svd_lowrank`. (5) The workflow and scripting support when dealing with dynamic control flow or custom tensor subclasses, and the difficulty of managing internal implementation details like strides or Python dispatch mechanisms. Overall, many issues revolve around ensuring correctness, reproducibility, hardware stability, and API extensibility within the PyTorch framework."
2022-05-14,pytorch/pytorch,"The discussions primarily focus on several technical concerns: ongoing issues with socket timeouts during distributed training, especially on SLURM clusters; serialization problems with CUDA tensors using the old zipfile format, leading to data corruption or segfaults; and compatibility challenges with previous versions of PyTorch (including legacy serialization and backward compatibility). Additional concerns include ensuring proper operator behavior with dynamic shapes, handling of BC-related setstate functions in nn.Module, and ensuring that new kernel implementations (e.g., dispatch-less kernels, composite kernels) are correctly integrated without breaking CI tests or creating regressions. Some discussions highlight modifications needed to improve build processes, such as correctly setting module attributes for public API compatibility, and updating test infrastructure to catch regressions earlier. Overall, unresolved questions about robustness, backward compatibility, and the stability of distributed and serialization features remain, with various proposals aimed at addressing failures, improving build consistency, and maintaining BC."
2022-05-15,pytorch/pytorch,"The discussions encompass a variety of technical concerns including proper citation practices for PyTorch research papers, integration of features like tensorboardX, and issues related to tracing warnings and device compatibility in model scripting. There are recurring questions about correctness and precision in functions like `cdist`, with suggestions to disable certain modes for accuracy, and concerns over silent propagation of gradient to non-leaf tensors. Multiple discussions address CI failures, test coverage, and the need for clearer error messaging around device placement and user guidance, especially when working with CUDA in multiprocessing contexts. Additionally, some comments focus on refining internal API behaviors, such as setting function/module attributes for public API exposure, and automating testing and build processes. Unresolved questions include how to best signal improper device usage, improve user feedback, and handle edge cases like empty tensors or device mismatches."
2022-05-16,pytorch/pytorch,"The discussions encompass several technical issues and questions related to PyTorch, including handling of infinities in complex tensors, and inconsistencies in tensor division involving complex numbers, which are expected to produce complex results. Users report bugs or unexpected behaviors in numerical operations such as 1/torch.tensor([inf]) and tensor division with infinities. There are issues with distributed training, including flaky tests in DDP with RPC and TensorPipe, and certain background process errors in specific platforms, indicating possible stability problems or configuration issues. Other discussions focus on potential performance regressions, improvements, and the need for better test coverage or documentation clarity, especially around sparse tensor representation and multi-process data loading. Several pull requests are pending merge, some with failed checks or needing rebase, reflecting ongoing development challenges."
2022-05-17,pytorch/pytorch,"The discussion highlights concerns about reproducibility and randomness seeding in PyTorch's DataLoader, especially across different libraries and platform variations, with plans to improve reseeding behavior and controllability. There are questions about the support and propagation of shape inference and custom ops in ONNX, emphasizing the limitations of existing symbolic shape inference and the need for registration of shape functions in Python. Several issues relate to CI test failures and flaky behaviors, often due to environment configuration, platform-specific bugs, or internal infrastructure faults, with ongoing efforts to stabilize tests and improve CI workflows. There are also technical debates about kernel implementation strategies, handling of tensor strides, and in-place operations, with suggestions to align with newer C++ standards and best practices. Overall, unresolved questions include ensuring consistent behavior across CPU and GPU, supporting complex inference and custom ops in ONNX, and refining CI testing and support infrastructure."
2022-05-18,pytorch/pytorch,"The main concerns involve CUDA and GPU-related errors such as illegal memory access, illegal address exceptions, and unsupported operators in specific backends like MPS and Apple silicon, with discussions on whether to add custom shaders, support for new operators, or opt-out mechanisms. Several threads address the compatibility and support of new ONNX opset versions, particularly 16, and issues with exporting or running models with newer ONNX features. There are also questions about enhancing documentation, testing strategies, and support for sparse, linear operators, especially for new hardware like MPS or Apple Silicon, as well as handling multiprocessing and distributed training failures related to NCCL and Flakiness in CI. Some discussions mention refactoring internal mechanisms, such as tensor strides, and handling of features like IterableDataset shuffling and different backends (fbgemm, qnnpack). Unresolved questions include when support for new hardware/backends will be enabled or fully supported, and how to handle unsupported operations or features gracefully across various runtime environments."
2022-05-19,pytorch/pytorch,"The discussions mainly revolve around debugging and resolving various build, serialization, and runtime issues in PyTorch, especially related to CUDA, MPS, and serialization formats. Concerns include handling of tensor serialization with old formats, cross-platform compatibility (including macOS and WSL), and device-specific features or backends like QNNPACK, QInt8, and MPS. There are questions about test stability, flaky test reruns, and ensuring consistent distributed behavior, as well as suggestions for improving documentation and build automation. Several discussions also address experimental or incomplete features (like neural engine usage and custom shaders), with some concerns about backward compatibility and workload correctness. Overall, unresolved issues include runtime errors on specific hardware backends, serialization bugs, and testing gaps for certain device configurations."
2022-05-20,pytorch/pytorch,"The discussions primarily focus on improving and troubleshooting various aspects of PyTorch, including backend selection (e.g., NCCL vs. Gloo), environment variable settings for distributed training, and compatibility with newer hardware such as NVIDIA RTX 30 series GPUs and AMD/Apple Silicon (M1/MPS). Several issues involve failure cases and errors like unsupported operations on specific backends (e.g., MPS, XLA) or in certain configurations, with potential workarounds such as patching or building from source. There are concerns about deterministic behavior, especially related to linear algebra functions like `lstsq`, and ensuring support for features like TorchScript compatibility and custom autograd/inheritance patterns. Additionally, build and test infrastructure-related questions are raised, notably around dependency management (e.g., gtest versions), caching, and CI stability, along with efforts to handle platform-specific issues on macOS and Windows."
2022-05-21,pytorch/pytorch,"The discussions raise several technical issues primarily centered around build and runtime errors on different hardware and software configurations, such as CUDA version incompatibilities and build failures related to specific GPU architectures (e.g., `libnnpack.a` issues, linker errors, CUDA 11.5 support). There are concerns about runtime stability and correctness in tensor operations on various backends (e.g., MPS, Metal, XLA), including issues with tensor shape/memory management, device movement, and support for specific operations or data types (like booleans and optional tensors). Some conversations focus on the need for better testing and validation strategies, especially to catch numerical discrepancies and backend-specific bugs, alongside discussing potential API or implementation refactors (e.g., dealing with the meta tensor interface, dispatch mechanisms, or operator registration). Unresolved questions involve ensuring compatibility and stability across diverse environments (macOS, iOS, GPU architectures, different compiler versions) and whether certain experimental features (e.g., implicit quantization, channels-last formats, fallback mechanisms) should be further supported or deprecated. Overall, the concerns highlight ongoing maintenance, platform-specific bugs, and the necessity of robust testing and clear API design to improve cross-platform reliability."
2022-05-22,pytorch/pytorch,"The discussions highlight issues related to PyTorch's device-specific implementations, particularly on MPS and iOS/M1 Mac hardware, emphasizing that many operations lack proper support and can cause errors or performance degradation. Several suggest employing or rethinking strategies like explicit shape/strides modeling, memory format handling, or reference consistency to improve correctness and performance, noting that strides and tensor views are complicated by existing auto-mutation and view semantics. There is concern over the stability of gradients, memory leaks, and device/backend abstraction designs, with some proposing more robust tracking (e.g., process groups, custom device abstractions) or static analysis tools rather than relying solely on runtime/inference semantics. Some discussions recommend deferring certain architectural changes (e.g., replacing process group abstractions, changing stride handling) until future refactoring plans are more mature. Unresolved questions include the best way to model stride and memory format semantics for cross-device compatibilities, and whether to introduce new APIs or rely on external, out-of-tree solutions to avoid compatibility and stability issues."
2022-05-23,pytorch/pytorch,"The discussions highlight several technical concerns including compatibility issues with PyTorch versions and external libraries (e.g., PyTorch Geometric requiring specific torch versions), complex bugs such as double free errors in CUDA code, and NaN loss occurrences related to autocast and GradScaler configurations. There are questions around proper testing procedures, including the need to add more robust tests for custom functionalities like MPS support, stream execution, and distributed synchronization. Some discussions involve potential API improvements, such as modifying sharding implementations, ensuring correctness in tensor shape checks, and clarifying certain function behaviors in documentation to prevent misuse. Unresolved issues include environment-specific bugs, the need for better validation, and the coordination to fix potential regressions or conflicts caused by recent code changes."
2022-05-24,pytorch/pytorch,"The discussions highlight ongoing challenges related to PyTorch's documentation system, with issues arising from documenting the same methods multiple times, which conflicts with Sphinx's restrictions; potential solutions involve modifying documentation templates or avoiding duplicate documentation. Several comments address support for various operators, including adding support for operators like `torch.inverse()` and `torch.block_diag`, with some suggestions to implement custom autograd rules or fallback mechanisms. There are concerns about pruning the codebase, such as removing deprecated or experimental features, and ensuring backward compatibility, especially regarding in-place and out-of-place operations affecting gradients. Some discussions focus on improving distributed training, such as better diagnostics, caching dependencies in CI, and fixing synchronization issues across multiple GPUs or distributed processes. Overall, unresolved questions include how to best handle operator support, documentation upkeep, and enhancing an efficient, user-friendly, and backward-compatible framework."
2022-05-25,pytorch/pytorch,"The discussions highlight issues related to data loading deadlocks in distributed training, which can be mitigated by setting `torch.set_num_threads(N)`. Several comments address ambiguity in the behavior of `nn.LayerNorm`, especially regarding its relation to GroupNorm, and the need for clearer documentation or implementation details. There are also repeated concerns about CI failures, build reproducibility, and proper handling of environment variables, such as for MKL threading or OpenMP settings, affecting runtime performance. Several issues involve understanding or fixing kernel support for device-specific (e.g., MPS, ROCm, XLA) operations, including operator support and numerical stability across data types. Unresolved questions include how to best model strides and memory formats in tracing and JIT, whether to support deprecated APIs or behaviors, and how to efficiently manage dependencies and build systems in CI pipelines."
2022-05-26,pytorch/pytorch,"The discussion primarily revolves around enabling CPU-only PyTorch installations with minimal dependencies, addressing issues related to installation flags and package indexing for CPU vs. GPU versions. There are ongoing concerns about CUDA/ROCm support, especially for quantization and backend compatibility, and the need to improve support for sparse tensors and multi-GPU testing regime. Several technical challenges are highlighted, including dealing with stride and memory format propagation during graph tracing, compiler errors on macOS, and performance differences between CPU and GPU for certain operations. Also discussed are build system and CI infrastructure issues, such as flaky tests, long build times, and test coverage coverage gaps, with some suggestions for better test assertions and code refactoring. Overall, unresolved questions include how to reliably support CUDA/ROCm, improving build/test robustness, and how to handle strides and tensor metadata during graph transformations and serialization."
2022-05-27,pytorch/pytorch,"The discussions primarily revolve around handling weight normalization removal for inference models in PyTorch, with custom functions provided for recursion and layer identification. There are concerns about the stability and correctness of behavior when returning `self` in certain functions, especially in JIT/traced contexts. Several issues involve CI/test failures, build system and compatibility problems (notably with CUDA, CMake, and Visual Studio), and implementation details for specific operator support, including support for meta and MPS backends. Contributors are also addressing testing challenges, code maintenance, and integration with ONNX, with some unresolved questions about proper testing strategies and platform-specific issues. Overall, unresolved questions include ensuring compatibility, fixing build/test failures, and improving operator and backend support."
2022-05-28,pytorch/pytorch,"The discussions highlight several recurring technical concerns in the 'pytorch/pytorch' repository, including the need for better GPU support and performance optimization (e.g., implementing asynchronous timing with CUDA Events, supporting half-precision sparse matrix multiplication, and optimizing tensor operations like triu). There's also concern over compatibility issues, such as handling complex numbers in CPU and GPU contexts, proper support for custom operators (e.g., aten::finfo, aten::_index_put_impl_), and ensuring correct behavior with multi-threading, distributed training, and initialization routines. Several comments note the importance of adding tests for new features, verifying performance improvements, and fixing bugs, especially in the context of CUDA and backend-specific code (e.g., HIP, MKL, cuDNN, onnx export). Unresolved questions include how to properly support optimized code paths (like delayed LockDevices or supporting `__tensor_ufunc__`), and handling platform-specific issues for Windows and macOS. Overall, the discussions reflect ongoing efforts to improve CUDA support, maintain compatibility, enhance performance, and ensure robust testing across diverse environments."
2022-05-29,pytorch/pytorch,"The discussions highlight several technical concerns including potential miscalculations in the `forward` function (Issue #1583.0), inconsistencies in librosa and torch.stft outputs (Issue #7038.0), and the need for additional tests to catch errors like incorrect input sizes (Issue #30456.0). There are questions about the correct way to initialize and reset parameters in inheritance models (Issue #71404.0), and issues with reproducibility and deterministic behavior, especially concerning CUDA/cuDNN settings and the `torch.use_deterministic_algorithms()` function (Issues #78475.0 and #78420.0). Some discussions pertain to build and environment setup problems, particularly on Linux with CMake and CUDA, and how to properly link libraries (Issues #33596.0, #78464.0, #78466.0). Several issues also focus on ensuring correctness and consistency of functions like `lu_factor`, and the proper addition of operators for different hardware backends, with an emphasis on testing, error handling, and proper documentation."
2022-05-30,pytorch/pytorch,"The discussions highlight several technical concerns, including issues with GPU and CUDA communication, especially related to P2P transfer and NCCL configurations, which impact multi-GPU training stability. There are questions about handling dynamic parameters like kernel size in ONNX export, requiring workarounds or static definitions, and concerns about model serialization and loading across devices. Some comments address errors and performance considerations in operations such as SVD, `avg_pool2d`, and tensor format conversions, often seeking clarification or improvements to compatibility and efficiency. Additionally, there are inquiries about supporting new data types like `ComplexFloat` and addressing backend-specific limitations, as well as suggestions for better testing practices to prevent regressions, alongside general issues related to build, environment collection, and feature support for hardware backends."
2022-05-31,pytorch/pytorch,"The comments reflect ongoing discussions about handling corrupted samples in DataLoader, including returning `None` in `__getitem__()` and filtering in `collate_fn()`, with concerns about edge cases where entire batches are corrupted, which may cause errors. There are issues related to memory overcommitment when importing PyTorch, especially on Windows with CUDA, linked to DLL preloading and DLL size. Several discussions address performance regressions with specific operations (e.g., `norm`, `unsqueeze`) and the need for better profiling, reproducibility, and possibly higher-level abstractions for device management. Other topics include stability and correctness of operations (such as `CTCLoss` deterministic modes), build failures related to compiler and library inconsistencies, and the handling of distributed training issues like process synchronization and rank assignment. Overall, the conversations indicate active maintenance efforts tackling robustness, performance, memory, and API consistency concerns."
2022-06-01,pytorch/pytorch,"The collected comments highlight several ongoing concerns in the PyTorch repository: (1) Memory management issues on Windows and CUDA devices, especially with initial VMS growth and fragmentation, as well as specific crashes related to CUDA operations like unpooling and matrix decompositions; (2) Challenges with supporting lazy tensors, especially in profiling, transfer learning, and autograd compatibility, along with the need for better tooling or documentation for profiling and metrics; (3) Experimental features and internal APIs (e.g., transfer learning, dataset splitting, dataset sharding, symbolic ops, custom dispatch keys) that require more careful handling for backward compatibility, testing, and documentation; (4) Upstreaming improvements such as fused kernels, operator fusion support, and support for new hardware backends (MPS, ROCm, AMD GPUs), with some issues due to hardware-specific or build configuration limitations; (5) Regressive performance and stability bugs, especially in distributed, ONNX export, or specific model workflows, indicating ongoing debugging and the need for additional testing and profiling."
2022-06-02,pytorch/pytorch,"The discussions primarily revolve around CUDA-related errors such as illegal memory access, illegal addresses, and kernel assertion failures, often linked to batch size misconfigurations or uninitialized memory. Several comments suggest warmup strategies, dummy inputs, or synchronization calls to mitigate GPU instability and performance inconsistencies. There are questions about documentation practices for environment variables affecting NCCL and CUDA, as well as issues related to integer list parsing, tensor contouring, and operator support on specific devices like MPS. Some concerns involve verification tests, profiling enhancements, and the correct designation of test disabling or skipping procedures. A recurring theme is ensuring correct environment setup, proper API use, and the need for clearer documentation and tooling to handle device-specific limitations and performance tuning."
2022-06-03,pytorch/pytorch,"The discussions highlight concerns about excessive memory consumption and fragmentation in PyTorch, especially on Windows and CUDA, linked to initial binary size, DLL sections, and caching behavior, with some suggesting updates to binaries and CUDNN improvements. Several issues involve reproducibility and correctness concerns around CUDA nondeterminism, especially with cuDNN algorithms and their deterministic modes, and the need for better documentation and explicit control over deterministic algorithms. Others focus on pybind error handling, data parallelism inconsistencies, and test flakiness, with proposals for better debugging, profiling, and error reduction mechanisms. There are also questions about supporting legacy or unsupported operations (like complex dtypes on MPS, or custom ops in TorchScript) and improving the build and continuous integration infrastructure. Overall, unresolved questions regarding binary dependencies, deterministic behavior, and integration of profiling and error handling mechanisms remain central to ongoing improvement efforts."
2022-06-04,pytorch/pytorch,"The discussions highlight several technical concerns, notably with data loading performance and CPU-core contention when using multiple processes, which can be mitigated by setting `torch.set_num_threads(N)` or `OMP_NUM_THREADS=N`. Memory management issues are also prominent, including GPU memory fragmentation, unfreed reserved memory, and differences in behavior across PyTorch versions and environments like Windows, Colab, or MacOS. Several tests exhibit flaky or failing behavior, often related to distributed training, RPC, or specific operator support (e.g., `aten::_index_put_impl_`, `aten::cumsum.out`), sometimes due to system or environment incompatibilities such as LLVM symbol conflicts, or missing operator implementations in libraries like cuSOLVER. There are also ongoing discussions around disabling flaky tests and handling signals with newer operator support, as well as interface suggestions such as exposing diff results with assertion functions. Unresolved questions include the root cause of memory reservation vs. fragmentation issues, and system-specific symbol/export conflicts affecting multilib environments."
2022-06-05,pytorch/pytorch,"The discussions primarily revolve around performance issues, notably slow device initialization and memory allocation on various GPU platforms and configurations, suggesting possible underlying problems with library dependencies, environment settings, or hardware compatibility. Several issues highlight discrepancies in behavior or performance between different CUDA versions and hardware setups, prompting questions about compatibility and best practices for installation and environment configuration. There are technical concerns around supporting complex module behaviors in PyTorch, such as handling optional tensors, first-input handling in modules, and compatibility with TorchScript, which pose challenges for debugging and feature implementation. Some discussions also address internal testing and CI failures, with suggestions for improving test coverage, handling multithreaded GPU memory sharing, and stabilizing builds across environments. Unresolved questions include how to support specific operator behaviors, manage memory more efficiently in multi-process situations, and improve code coverage and robustness for various hardware backends."
2022-06-06,pytorch/pytorch,"The comments reflect widespread memory management issues, including out-of-memory errors when loading models or performing operations like `torch.cdist`, often mitigated by garbage collection or system restarts. Many discussions address limitations or bugs in specific operators (e.g., `load_lua`, `group_norm_backward`, `aten::sort.values_stable`, `aten::native_group_norm_backward`) across different backends such as CPU, CUDA, MPS, or XLA. Several comments highlight that certain features, like deterministic algorithms, are either not fully supported or their guarantees are misunderstood, especially concerning cuDNN's behavior and reproducibility. API design concerns include the difficulty of handling Python's `StaticMethod` support under TorchScript, and the desire for better error messages and compatibility checks for old Python versions, as well as more flexible, user-intuitive APIs for modes like quantization or sharding. The unresolved questions mostly revolve around operator support, backend consistency, and how to better handle user-controlled toggling of determinism and other runtime configurations in a way that is safe, predictable, and compatible."
2022-06-07,pytorch/pytorch,"The collected discussions cover various topics related to PyTorch development, including unimplemented or unsupported operations on specific hardware backends like MPS, issues with ONNX export compatibility, and the need for better tooling such as scan operations or support for dynamic shapes. There are concerns about the correct handling of broadcasting in loss functions, the complexity of internal kernel support (e.g., cuSOLVER, cuDNN, NCCL), and the architecture of APIs exposing process groups and work objects. An ongoing theme is the necessity to improve interoperability and robustness across hardware (e.g., Mac M1, CUDA versions, specific devices), along with code quality issues like proper resource cleanup (e.g., LMDB files) and testing coverage, especially on less common platforms. Some discussions suggest that fundamental fixes are preferred over workarounds or disabling features, with emphasis on fixing vectorization issues, ensuring backward compatibility, and providing clear error reporting for unsupported configurations."
2022-06-08,pytorch/pytorch,"The discussions highlight various technical issues, including the need for sequence reversal utilities compatible with newer PyTorch versions, handling of `PackedSequence`, and the behavior of certain operators like `dropout2d` across different PyTorch versions. Several concerns involve deep copying modules with weight norm, proper handling of type annotations and compatibility, and ensuring correctness and performance, especially regarding CUDA and MPS backends. Questions also address CI management, merge conflicts, build environment dependencies (like LAPACK, GCC), and handling of deprecations and compatibility with Python versions. Unresolved issues include specific operator implementations, resource leaks in data loading, and potential API design improvements, such as device modules and in-place operation constraints."
2022-06-09,pytorch/pytorch,"The discussions largely revolve around technical challenges and improvements in PyTorch's internal systems, such as handling GPU-specific errors like cuDNN mapping errors, and support for features like TorchScript compatibility with Apex Mixed-Precision training. Several comments suggest the need for better error reporting, testing, and debugging tools, including more informative assert macros and improved CI testing procedures. There are ongoing efforts to enhance dispatch mechanisms, organize PRs into logical, reviewable stacks, and improve build processes, especially concerning dependency management, code generation, and platform-specific issues (e.g., macOS MPS and CUDA compatibility). Some discussions highlight the importance of code organization, such as separating refactors from feature additions, and adjusting implementation details (e.g., dtype checks, namespace support) to ensure robustness. Unresolved questions include handling multi-namespace kernel registration, supporting nested tensors, and managing multi-interpreter C++ ops, indicating areas for future development and review."
2022-06-10,pytorch/pytorch,"The discussions highlight issues related to PyTorch's `PackedSequence` interface and its non-intuitive behavior or incomplete documentation, especially regarding sequence packing support for unsorted or unordered data. There is a recurring theme of enhancing user-facing features, such as support for non-sorted sequences in packing/unpacking, and providing utility functions like mean over varying sequence lengths. Several technical concerns involve the implementation of specific operators and their device support, e.g., `aten::linalg_solve` and CUDA fallback mechanisms, as well as issues with ONNX export compatibility, especially for transpose operations and quantized models. Additionally, the community discusses build, CI, and cross-platform compatibility challenges, including memory exhaustion and dependency management, along with suggestions for better testing and plugin mechanisms. Overall, unresolved questions pertain to API consistency, operator support across devices, enhancing user experience with robust error handling, and optimizing build and testing workflows."
2022-06-11,pytorch/pytorch,"The comments reveal multiple recurring issues and questions related to PyTorch development. A significant concern is the compatibility and support of specific operators (e.g., `aten::index.Tensor`, `aten::logical_and.out`) on various devices like MPS, and the handling of unsupported operators, requiring environment variables or potential fallback strategies. There's ongoing discussion about enhancing the internal testing infrastructure, such as verifying attributes like `DEFAULT_QCONFIG_PROPAGATE_ALLOW_LIST`, and ensuring internal CI passes, especially for internal vs OSS differences. Several suggestions involve refactoring or optimizing code, such as fusing optimizer operations for performance gains, or handling dynamic/static quantization with explicit flags and comprehensive documentation. Unresolved questions include managing merge conflicts, supporting older hardware (e.g., CUDA compute capabilities below 3.0), and dealing with build failures caused by upstream bugs or environment-specific issues."
2022-06-12,pytorch/pytorch,"The discussions highlight ongoing issues related to PyTorch's compatibility and operation across different hardware backends, especially CUDA, MPS, and ROCm, with specific concerns about missing operators and backend support. There are concerns about API stability and compatibility, such as glog API issues and the impact on downstream dependencies, with suggestions for backward compatibility fixes. Several discussions focus on performance optimizations, like fusing operations in optimizers (e.g., Adam) and improving in-place or composite function implementations for reduced memory usage and speed. Additionally, some threads raise build and CI pipeline failures, signaling ongoing integration challenges, and questions about enabling features like TF32 and handling large memory buffers on different hardware platforms. Unresolved questions include missing operators on specific backends, the best approach for handling API changes, and strategies for testing in isolated container environments."
2022-06-13,pytorch/pytorch,"The comments reveal ongoing discussions and concerns about various PyTorch development issues, including the need to update documentation and YAML files for specific functions, support for distributed data parallelism in libtorch, and bugs related to autograd, device handling, and operator implementation. Multiple comments question the status and progress of certain features or bug fixes, often noting delays or the need for further review and testing. There are technical debates on modeling tensor strides and memory formats in tracing, ensuring backward compatibility, and handling edge cases in functions like `Dropout2d` and `histogram`. Additionally, several discussions highlight internal CI failures, conflicts in PR merges, and the importance of properly documenting proposals, fixes, and design decisions. Overall, the discussions focus on improving robustness, documentation, feature support, and maintaining backward compatibility within PyTorch development."
2022-06-14,pytorch/pytorch,"The comments reveal ongoing concerns about performance bottlenecks and data copying in PyTorch operations, such as `torch.flip()` and tensor flipping, where copying data is costly and views are preferred but not yet supported with negative strides. There are issues related to data loading inefficiencies due to CPU core contention when using multiple processes, which can be mitigated by adjusting `torch.set_num_threads()`. Problems with debugging and correctness, especially on MPS devices and on Mac, as well as specific errors like `aten::index.Tensor` not being supported or operations not being correctly handled in certain backends, are mentioned. Moreover, there are CI stability concerns, including flaky tests, timeouts, and build failures, often addressed by rerunning jobs and adjusting configurations. Overall, unresolved questions include improving operator support for advanced indexing, backward compatibility fixes, and performance optimization in distributed and GPU-specific contexts."
2022-06-15,pytorch/pytorch,"The comments highlight specific technical issues and discussions around PyTorch features, bug fixes, and performance optimizations. Key concerns include the support for in-place operations with MPS (metal performance shaders), CUDA-related bugs and device compatibility, and the support for exporting to ONNX, especially with operators like `grid_sampler` and `BlackmanWindow`. Several discussions focus on improving numerical consistency, shape inference, and backward autograd behavior involving in-place ops, nested tensors, and dynamic shapes. There are also mentions of infrastructure changes, such as reformulating CI processes, handling special types like sharded tensors, and managing environment or locale-related bugs impacting reproducibility. Unresolved questions remain about performance regressions, cross-platform support, and ensuring backward compatibility in evolving APIs."
2022-06-16,pytorch/pytorch,"The comments highlight ongoing challenges in enhancing PyTorch's serialization of tensors, especially for faster pickling of structures containing tensors and numpy arrays, with considerations around custom picklers and data formats like HDF5. Several discussions revolve around bugs and fixes related to CUDA, MPS, and distributed training, including issues with GPU memory management, in-place operations, and device-specific operator support. There are questions about expanding support for container classes like `Sequential`, addressing test failures and CI flakiness, and the need for better documentation and testing, including reference implementation snippets. Some comments suggest architectural changes, such as returning strings instead of objects for metrics, handling large model checkpoints, and supporting meta Tensors with Python and C++ APIs. Overall, the discussions center on improving serialization efficiency, device compatibility, test stability, and extending support for complex model workflows in PyTorch."
2022-06-17,pytorch/pytorch,"The discussions highlight challenges related to data loading and multiprocessing efficiency in PyTorch, especially in multi-process training and DDP setups, with issues like freezing Dataloader validation processes and CPU core contention due to `num_workers`. Several responses suggest setting `torch.set_num_threads(N)` or controlling OpenMP thread count to improve runtime and CPU resource competition. There are ongoing efforts to handle legacy modules (like `torch.utils.ffi`) when upgrading PyTorch versions, including code modifications and potential refactoring to support reference and primitive operations, as well as handling custom kernel namespace support and internal build dependencies. Additional concerns involve stability and reproducibility of CI/CD pipelines, including flaky tests, CI failures due to internal rules or environment issues, and the use of external tools (like poetry) for dependency management. The overall focus remains on optimizing data loading, maintaining compatibility across versions, and improving automation and testing reliability."
2022-06-18,pytorch/pytorch,"The discussions reveal ongoing challenges with PyTorch's GPU memory management and CUDA compatibility, particularly issues with high memory usage, `torch.cuda.is_available()` returning false, and inconsistencies across CUDA versions and installation methods. There are technical concerns regarding failures in specific GPU operations, such as `make_sprite`, and correctness issues in MPS (Metal Performance Shaders) backend, especially with half-precision computations and tensor conversions between CPU, GPU, and MPS. Some comments highlight difficulties in merging code changes due to CI failures, stale PRs, and testing limitations. Additionally, questions are raised about the proper placement and testing of new code, as well as configuration options for ONNX export and quantization workflows."
2022-06-19,pytorch/pytorch,"The discussions highlight persistent issues with excessive GPU memory consumption in PyTorch, especially when running simple tensor operations on CUDA devices, indicating potential problems with memory management or binary optimization. Multiple reports concern failures or errors related to GPU driver compatibility, such as insufficient driver versions or missing NVIDIA drivers, which impact CUDA-related features and tests. There are suggestions for API improvements, particularly around the serialization and state management of sharded tensors and custom tensors, emphasizing the need for more flexible, user-friendly interfaces that handle submodule state recursively without extensive hook registration. Some discussions also involve structural code reorganizations, such as moving linear algebra functions into specific modules for better maintainability and historical traceability. Lastly, several issues relate to build, test failures, or race conditions, often tied to environment configurations, driver versions, or concurrency handling, with calls for better debugging tools like stack dumps for stuck jobs and refined infrastructure support."
2022-06-20,pytorch/pytorch,"The discussions highlight several technical challenges including issues with quantization workflows, particularly fusing models before qconfig selection, and operator support for quantized backends like 'aten::dequantize'. Support for complex tensors, especially handling conjugates in DataParallel, and support for sharded tensors in distributed modules are ongoing concerns. There are also discussions about improving the user experience around tensor conversions and handling edge cases such as unsupported ONNX operations or memory allocations. Additionally, enhancements like supporting a 'force' argument for NumPy conversions and stabilizing new distributed testing strategies are proposed. Overall, unresolved questions revolve around expanding operator support, optimizing distributed and mixed-precision workflows, and improving API robustness and documentation."
2022-06-21,pytorch/pytorch,"The discussions predominantly revolve around memory management and data handling issues in PyTorch, especially related to data loading, tensor copying, and process communication overhead. Several comments address specific bugs, like memory leaks when using lists vs numpy arrays, and inefficiencies in passing large model state dictionaries across processes. There are concerns about the compatibility and support of certain features on backends like MPS and XPU, as well as the need for better tooling and abstractions for distributed systems (e.g., handling CUDA streams, ONNX integration). Many unresolved questions focus on improving performance (e.g., using GpuDirect, reducing copy overhead), ensuring proper correctness (e.g., in gradcheck with sparse tensors), and handling platform-specific issues (e.g., locale settings, build configurations). Overall, the discussions identify critical areas for stability, efficiency, and extendability in PyTorch's data and distributed computation ecosystem."
2022-06-22,pytorch/pytorch,"The comments revolve around troubleshooting and adapting code to different PyTorch versions and system configurations, especially when dealing with deprecated modules like `torch.utils.ffi` and issues with `create_extension`. Several users face performance discrepancies and memory limitations when running on Windows versus Linux or using different drivers/environments, prompting suggestions to configure drivers or system settings (e.g., TCC mode, CUDA paths). Compatibility issues also arise when loading models or using certain operators (e.g., `aten::convolution`, `aten::bitwise_not`) in various backends (MPS, CUDA, Metal). Some discussions focus on ensuring proper testing, coverage, and CI stability, noting flaky tests and needing correct macro handling or support for new operators, as well as addressing system-specific build failures. Overall, unresolved questions include system-dependent performance variations, operator support in different hardware backends, and proper integration of features like ONNX conversion and distributed training."
2022-06-23,pytorch/pytorch,"The discussions predominantly revolve around Python and PyTorch performance issues in multiprocessing and data loading, particularly memory usage and potential leaks caused by Python object sharing (lists, dicts) in DataLoader workers, and the impact of using numpy arrays or tensors instead. Several comments mention bugs and workarounds related to NCCL errors in distributed training, especially in multi-node or multi-GPU setups, with suggestions to verify network connectivity, system configuration, and NCCL environment variables. Compatibility and build issues are frequently discussed, including patches for CUDA, MPS, and library dependencies, alongside code refactoring proposals for operator and backend implementations. Additionally, CI flakiness, merging rules, and contributor sign-off processes are recurrent topics, with specific suggestions for improving test sharding, handling deprecated APIs, and fixing build/test failures. Unresolved questions include system-specific performance regressions, multi-process reliability, and debugging strategies for distributed training and hardware-specific bugs."
2022-06-24,pytorch/pytorch,"The discussions highlight various technical issues, including challenges with C++ API integration on macOS, notably the need to set `Torch_DIR` for proper package configuration. There are ongoing serialization challenges, particularly loading PyTorch `.pth` files directly in C++, complicated by differences in `pickle` handling, with workaround suggestions involving container modules and `script`. Serialization aspects also involve handling quantized tensors, requiring explicit casting to match ONNX expectations. Additionally, concerns about CUDA and NVLink topologies on A6000 GPUs, especially regarding PCIe and ACS settings affecting P2P communication, are discussed for performance optimization. Lastly, multiple issues touch on test failures, flaky tests, and build consistency, emphasizing the need for careful rebase, environment setup, and debugging to ensure CI stability."
2022-06-25,pytorch/pytorch,"The discussions mainly focus on troubleshooting performance issues and hangs when using PyTorch on specific hardware (e.g., Jetson Xavier, macOS with MPS backend, multi-GPU/distributed setups, and recent driver updates). Several comments highlight the importance of proper device management, such as ensuring all processes participate in distributed operations to avoid deadlocks (e.g., NCCL deadlocks when only rank-0 calls reduce). Other concerns involve compatibility and build configuration challenges, like enabling Objective-C++ in CMake for MPS, or addressing slow CUDA memory copies when mixing different tensor types and memory formats. There are also issues related to CUDA kernel performance with AVX512 or complex number support, as well as build failures due to environment and compiler settings. Lastly, some suggestions involve optimizing code reformatting, ensuring correct handling of scalar and nested tensors, and improving documentation or adding tests for new features."
2022-06-26,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch's CUDA and MPS support, especially regarding performance and compatibility, such as slow bfloat16 operations, slow stack operations on GPU, and errors during RPC communication. Several discussions involve the need for better build configurations, managing dependencies like the contributor license agreement, and improving code portability across different compiler versions. There are also technical concerns about the efficiency of tensor operations (e.g., `torch.stack`) and the impact of data loader design on performance and memory usage, with suggestions to optimize or centralize such implementations. Some comments relate to merging conflicts, documentation, and adding new scheduling or optimizer features, indicating active development and maintenance challenges. Unresolved questions include system-specific issues affecting RPC, the timing for supporting certain operations on MPS, and how to streamline support for various hardware backends."
2022-06-27,pytorch/pytorch,"The comments reveal ongoing discussions about code review bottlenecks, merge permission policies, and build/test stability, especially related to CUDA, ONNX, and internal build systems. Several comments address the need for improved numerical accuracy in functions like `log_ndtr`, advocating for re-implementations based on SciPy/TFP standards, with some proposing dedicated ATen specializations. Issues also highlight performance regressions and the necessity of profiling and benchmarking, particularly for AVX512 and triangular solvers, as well as system-specific problems (e.g., macOS, Windows, and internal CI failures). Discussions include plans for API improvements, clarifications on behavior, and handling of backward compatibility in various modules. Unresolved questions mainly focus on optimizing GPU kernels, fixing flaky tests, and internal policies for permissions and build configurations."
2022-06-28,pytorch/pytorch,"The comments cover a wide range of issues, including file corruption problems when loading models, performance and datatype promotion issues (notably with int8 and long types), and difficulties with specific operations in scripted or JIT-compiled models such as gradient overwriting, complex functions, and custom operators. Several bugs relate to CUDA and MPS backend performance, especially on certain GPU architectures or with specific versions, and with mixed device computations, such as CPU and GPU/Metal. There are also challenges associated with exporting models to ONNX, including handling dynamic axes, shape mismatches, and supporting custom operators, as well as with extension compilation failures and build environment inconsistencies. Many discussions highlight the need for better tooling, test coverage, more accurate benchmarking, and infrastructure improvements for CI, deployment, and hardware-specific optimizations. Unresolved questions often relate to handling asynchronous CUDA operations, cross-device consistency, and specific bug workarounds, emphasizing ongoing efforts to stabilize and optimize the framework."
2022-06-29,pytorch/pytorch,"The discussions reveal recurring concerns about PyTorch's type promotion and scalar type mismatches in functions like `torch.where`, leading to runtime errors when different tensor types are involved. Several issues highlight difficulties with model loading, file corruption, and serialization errors, exacerbated by potential data corruption or malformed checkpoints. There are also notable performance and portability challenges on hardware accelerators like MPS and CUDA, including significant slowdowns and incomplete support for certain operators or tensor types (e.g., bitwise ops, int8 quantization). Additional discussions focus on build and dependency management, including compatibility issues with package versions, compiler toolchains, and third-party libraries like ONNX and cuDNN. Overall, unresolved questions remain around type promotion robustness, cross-platform support, and efficient build/test workflows, indicating areas for potential API fixes, better error handling, and infrastructure improvements."
2022-06-30,pytorch/pytorch,"The discussions primarily revolve around addressing various technical issues and feature requests in PyTorch, such as fixing hang bugs caused by multiprocessing, enabling support for more operators like `col2im` in ONNX export, and handling device-specific limitations like MPS support for bitwise operations. Several comments mention the need for better support matrices for different device and dtype combinations, especially for fp16 on CPU and CUDA, and highlight ongoing efforts to improve support for features like reversible networks, symbolic function handling, and symbolic registry updates. There are also concerns about build failures caused by dependency configurations, symbol limits on Windows, and merge conflicts, with some proposing more robust testing strategies and better support for backward compatibility. Overall, unresolved questions include ensuring support for new operators, supporting mixed device/dtype combinations, and refining error handling for platform-specific issues."
2022-07-01,pytorch/pytorch,"The comments highlight persistent technical challenges within PyTorch's development, including difficulties in deploying Spatial Transformer Networks due to unimplemented affine_grid functions, CUDA resource limitations causing runtime errors in large matrix inversions, and device support inconsistencies (e.g., MPS fallback performance issues and unsupported operators). Several discussions focus on optimizing internal functions for performance (such as string handling in error checks), improving support for different tensor formats and sparse matrix operations, and managing CI pipeline failures due to flaky tests or build configuration issues. There are also recurring questions about GitHub PR management (merging, reverting, labeling), as well as specific implementation concerns like the semantics of mode calculation in TransformedDistribution and handling of symbolic shapes. Overall, unresolved questions involve supporting certain device-specific operations, reducing symbol export bloat, and aligning internal optimizations with user-facing stability and performance needs."
2022-07-02,pytorch/pytorch,"The discussions primarily revolve around addressing NaN and overflow issues during training, often caused by residual layers, improper initialization, or insufficient normalization, with solutions such as increasing epsilon in optimizers, gradient clipping, and selective precision (fp32 in attention layers). There are concerns about compatibility and stability of fp16/automatic mixed precision (AMP) in transformer/attention models, especially on MPS and MacBook M1 hardware, with suggestions to run certain parts in fp32 explicitly. Several questions relate to handling unsupported or unsupported view operations on particular backends like MPS, which can silently produce incorrect results or errors, highlighting the need for explicit erroring or support. Docker and environment setup issues, such as Python version inconsistencies and hostname-related distributed training failures, are also discussed. Overall, the key themes involve stability, precision management, hardware-specific issues, and infrastructure challenges."
2022-07-03,pytorch/pytorch,"The discussions predominantly revolve around GPU memory management and efficiency, with concerns about PyTorch's memory consumption, interactions between threading and multiprocessing, and potential framework limitations such as CUDA kernel compilation or backend support. Several users encounter specific errors, such as OOM crashes, CUDA compile issues, or autograd handling anomalies, often seeking clarification, workarounds, or bug fixes. There are questions about the correctness of certain autograd behaviors (e.g., in-place zeroing of tensors) and about the functionality and API completeness (e.g., numpy compatibility, on-the-fly kernel compilation). Issues related to build, dependency management, or multi-node distributed training also appear, indicating ongoing challenges with scalability and integration. Many discussions involve troubleshooting, verifying runtime configurations, or working around bugs to improve PyTorch's stability, performance, and user experience."
2022-07-04,pytorch/pytorch,"The discussion covers several technical issues related to PyTorch, including compatibility problems with package versions (e.g., scipy, libshm.dylib, NCCL), and platform-specific challenges such as crashes on MacOS and errors with MPS backend. There are concerns about proper error checking and handling of unsupported operations, especially for the MPS backend and view operations that may silently fail or produce incorrect results. Several discussions address build system limitations, like the DLL object limit in Windows and splitting CUDA build artifacts. Lastly, there's ongoing work on improving test coverage, precision, and performance, along with questions about certain algorithmic choices (e.g., Welford's algorithm for variance calculation) and the need for better documentation and debugging tools."
2022-07-05,pytorch/pytorch,"The discussions reveal several key technical concerns: limitations in extending support for complex numbers within PyTorch, especially for functions like log1p and logcumsumexp, due to library and implementation complexities; challenges in managing floating point overflow, overflow guard strategies, and mixed-precision issues, notably with attention mechanisms and transformer models; difficulties in ensuring reproducibility and stability across various hardware configurations, particularly on AMD CPUs and different CUDA versions; issues related to build system failures, such as linker errors, dependency mismatches, and environment inconsistencies (e.g., compiler version support, CMake and library loading issues); and the need for better API design, unification of functional/concrete interfaces like functorch, and clearer documentation for expected behaviors, especially regarding tensor contiguity and model or operator representations."
2022-07-06,pytorch/pytorch,"The discussions primarily revolve around issues related to PyTorch's distributed data loading, GPU and hardware acceleration (such as MPS and XLA), and model parallelism strategies, including pipeline parallelism and device management. Several comments address the need to improve or clarify existing functionality, such as support for custom pytrees in functorch, better device-agnostic APIs, and support for nested tensors, as well as performance concerns in specific operations like matrix multiplication and linear algebra routines. There is also recurring mention of CI/test stability, potential regressions, and the need for proper testing and benchmarking of decompositions and backend capabilities. Unresolved questions include how certain operations should handle overlapping indices, memory format preservation, and handling of repeated tensor indices, alongside requests for clearer documentation and better failure diagnostics."
2022-07-07,pytorch/pytorch,"The comments from the GitHub issues predominantly address technical challenges with PyTorch's integration with various hardware and software components. The key concerns include: managing memory leaks caused by shared Python objects in DataLoaders, especially when using multiprocessing; ensuring compatibility and troubleshooting issues with different GPU vendors such as AMD ROCm and NVIDIA CUDA, including kernel support and binary deployment; optimizing operator performance (e.g., `torch.index_select`, `layer_norm`) with memory format support and parallelization; handling edge cases and correctness in functions like `max_unpool2d` with overlapping indices, and documenting these limitations; and supporting backend-specific decomposition and lowering strategies for efficient code generation in nvFuser, including consideration of op lists and fallback behaviors. Unresolved questions involve when to apply API-level decompositions, how to handle repeated indices in differentiation and backward passes, and ensuring cross-platform reproducibility and correctness during various stages of model execution and compilation."
2022-07-08,pytorch/pytorch,"The comments reflect a mixture of technical concerns including the need for detailed API design proposals for a modularization utility in PyTorch, addressing runtime behavior differences when enabling certain features (e.g., lazy tensor backend registration), and clarifying support and stability issues across various backends like MPS, ROCm, CUDA, and XLA. Several discussions highlight specific test failures, potential regressions, and the stability of features like scripted models, ONNX export, and backend compatibility. There are recurring questions about how to improve support for backward compatibility, serialization, and cross-platform performance, especially on M1 Macs and other specific hardware. Additionally, some comments suggest infrastructure and CI workflow improvements, such as better test support, rebase processes, and handling of internal build artifacts and dependencies. Overall, the core concerns center around robustness, API clarity, backend support, and test stability in the development cycle."
2022-07-09,pytorch/pytorch,"The collected comments highlight ongoing development issues, feature requests, and bug discussions across the PyTorch repository. Key concerns include the support for packing boolean tensors into bitstreams for memory efficiency, handling uneven batch sizes in Distributed Data Parallel, and improving the support and performance of sparse CSR tensor validation, especially on CUDA. Several discussions focus on ensuring proper device handling, like MPS fallback support and correct behavior of RNNs with batch-first configurations. Additionally, there are PR review comments requesting clearer descriptions, rebase and merge conflicts resolution, and questions about external integrations such as TorchX and the build process for nightly releases. Unresolved questions mainly involve device-specific operator support, serialization behaviors, and build infrastructure improvements."
2022-07-10,pytorch/pytorch,"The discussions highlight performance issues with downloading PyTorch and related packages via conda, particularly on slow networks, and suggest using tools like `mamba` for better reporting. There is interest in adding support for reparameterization techniques in PyTorch, similar to TensorFlow's `MixtureSameFamily`, to facilitate modeling multimodal distributions. Concerns are raised about proper handling of multiple inheritance in `nn.Module`, with recommendations to order inheritance to ensure correct initialization, and clarification on how `super().__init__()` behaves in such contexts. Several issues involve build and runtime problems, such as support for specific GPU architectures, CUDA versions, and distributed training setups in Docker, with suggestions for conditional guards based on environment detection. Lastly, there's a focus on licensing and documentation compliance, including ensuring license files are included in distributions and verifying support for different hardware configurations."
2022-07-11,pytorch/pytorch,"The comments reflect recurrent issues with Python shared memory interactions in DataLoader, especially when using Python lists versus numpy arrays, and advised workarounds like replacing lists with numpy arrays for memory efficiency. Several discussions note potential bugs or regressions in specific PyTorch ops (e.g., `resize_`, `matmul_out`, `max_unpool`), often linked to the handling of edge cases such as zero-length sequences or overlapping indices, sometimes resolved by code fixes or version upgrades. There are multiple reports of flaky or CI failures, sometimes due to merge conflicts, incompatible dependencies, or inconsistent build environments, with suggestions to improve CI stability, such as building docker images or bypassing unreliable tests. Some discussions focus on internal infrastructure improvements, like adding optional dependencies (ZenDNN, FFmpeg) or enabling new features (e.g., channels_last support), as well as specific bug reports (e.g., NaN generation in convolutions, incorrect function naming in FX tracing). Unresolved issues include detailed debugging of internal kernel errors (nvfuser, nvfuser specific behavior) and ensuring compatibility of certain ops across versions or architectures."
2022-07-12,pytorch/pytorch,"The discussions primarily concern the performance and reliability of PyTorch's package distribution, especially regarding slow download speeds from the conda/pytorch channel, which may be due to caching, regional issues, or infrastructure misconfigurations. Several issues highlight challenges with backward compatibility, API deprecations, and API enhancements—such as supporting optional arguments in functions like `rfft`, or updating API behavior to match new standards. There are also technical debates on the best approaches for testing, code organization, and integrating new features or hardware support, including handling large package files, improving export diagnostics, and optimizing CUDA and MPS operations. Additionally, some discussions involve infrastructure management, such as build cache consistency, repository permission controls, and internal CI failures, which impact development workflows. Overall, unresolved questions include how to improve package download speed, API backward compatibility, and the handling of large package files in a secure and efficient manner."
2022-07-13,pytorch/pytorch,"The discussions primarily revolve around best practices for moving data to GPU devices in PyTorch, highlighting concerns with multi-processing data loading and CUDA tensor sharing, and suggesting use of `pin_memory=True` and custom `collate_fn` approaches while noting performance trade-offs. There are also questions about compatibility of certain APIs (like `irfft`/`ifftn`), and issues related to build and deployment environments, especially for AMD GPUs with ROCm, manylinux support, and dependencies like GLog. Some discussions mention problems with specific operations (`resize_`, `resize_` vs `reshape`) and their support in TorchScript/ONNX, as well as handling of specific tests, errors, or flaky failures in the CI/CD pipeline. Unresolved questions include whether recent workarounds for device transfer in DataLoader are officially supported, how to best support mixed precision training with FSDP, and the impact of internal changes like itest/islint configurations or code formatting tools (isort vs usort)."
2022-07-14,pytorch/pytorch,"The discussions reveal concerns about CUDA and GPU memory management, particularly issues with system initialization errors, memory layout caching for cuDNN, and performance regressions on specific hardware like AMD CPUs and different GPU architectures. Several threads focus on build failures related to compiler support, such as compatibility issues with GCC 8.x and CUDA versions, and the need for proper environment configuration for NCCL/NVCC. There are questions about the correctness and reliability of shape inference, operator support (e.g., for torch.histogram, aten::argmin.out, aten::index.Tensor_out), and the impact of system-specific settings like `OMP_NUM_THREADS` or environment variables. Discussions also highlight ongoing efforts to improve CI workflows, merge strategies, and the integration of features like FSDP, quantization, and new operator support, with some proposals about handling backend-specific tensor dispatch and functionalization of tensor wrappers. Unresolved questions include how to best handle compatibility across different hardware, software versions, and deep integration with internal testing tools and build systems."
2022-07-15,pytorch/pytorch,"The discussions primarily concern technical issues related to PyTorch and its ecosystem, including GPU runtime errors (e.g., cuDNN ""CUDNN_STATUS_NOT_INITIALIZED""), memory consumption leading to OOM errors, and compatibility issues with specific hardware like MPS and NVIDIA's A10/A100 GPUs. Several reports highlight unreproducible or flaky test failures on different backends (CPU, CUDA, XLA, MPS), often linked to bugs in tensor operations, shape inference, or operator support (e.g., missing header files, unsupported data types like float16). Questions also arise about potential memory-hungry algorithms in dispatch implementations and the impact of certain algorithmic choices on resource usage. Some discussions address design considerations for distributed training paradigms (SPMD vs. MPMD, process group behaviors), along with future improvements for tooling, build infrastructure, and operator support. Overall, unresolved issues revolve around compatibility, memory efficiency, and correctness of various tensor operations across diverse hardware and execution environments."
2022-07-16,pytorch/pytorch,"The comments highlight ongoing discussions around implementing and testing features such as HierarchicalSoftmax, Negative Sampling, and efficient mixed-precision training with FP16, including handling of overflow issues in transformer attention modules. Several comments concern bug fixes related to learning rate schedulers, model export, and optimizer states, often requesting clarifications, minimal reproducible examples, or reviews before merging. Additionally, issues related to hardware-specific performance (e.g., M1 Ultra, MPS, CUDA) and compatibility with newer PyTorch versions or external tools like functorch are raised. Some discussions involve refactoring or documentation improvements, including better debugging outputs and consistent API support across CPU, GPU, and hardware backends. Overall, the focus is on resolving bugs, enhancing feature support, and clarifying implementation details amidst hardware and version compatibility challenges."
2022-07-17,pytorch/pytorch,"The discussions mainly revolve around issues with loading models from `torch.hub` due to branch name discrepancies between 'master' and 'main', with a fix proposed to modify `torch/hub.py`. Several comments address approximation and precision concerns in multi-head attention implementations, particularly involving mixed precision (fp16 and fp32) and associated runtime errors and numerical stability. Memory management and out-of-memory (OOM) error handling, especially in distributed training with NCCL, are also discussed, with suggestions for using `NCCL_ASYNC_ERROR_HANDLING`. Some comments mention build failures on specific architectures (e.g., PPC64) due to relocation issues, and the need for version compatibility notes. Overall, key unresolved questions include ensuring model load robustness with branch changes, stability and correctness of mixed precision attention, and platform-specific build issues."
2022-07-18,pytorch/pytorch,"The comments highlight several recurring technical issues: (1) Branch name conflicts in torch.hub.py due to default 'master' vs 'main' in repositories, which can be fixed by code patching; (2) Download failures (HTTP 404 errors) when loading models from GitHub, suggesting pointing to local files or updated URLs; (3) CI test failures due to flaky dependencies, unsupported operators in specific backends (like MPS), or internal errors like `cuDNN_STATUS_INTERNAL_ERROR` and device context issues; (4) Compatibility concerns with different PyTorch versions, CUDA, and backend support, such as for complex tensors, sparse layouts, and deprecated API behavior; (5) General maintenance and review tasks, including rebase, merge conflicts, license file concatenation, and proper labeling for release notes and topics. Key unresolved questions involve fixing branch name conflicts dynamically, improving error handling and retry logic for dependency downloads, and ensuring operator support across different hardware acceleration backends."
2022-07-19,pytorch/pytorch,"The discussions primarily revolve around GPU backend support in PyTorch, highlighting the challenges and ongoing efforts to implement an in-tree OpenCL backend for AMD and Intel GPUs, contrasted with vendor-locked solutions like CUDA/NVidia. There are concerns about driver latencies, vendor politics, and the completeness of existing open-source backends versus potential in-tree implementations. Additionally, several issues pertain to improving PyTorch's device compatibility, automatic mixed precision stability, reproducibility, and testing infrastructure, with notable questions about support status, performance implications, and testing strategies for new features like nested tensors, quantization, and custom kernels. Overall, the community advocates for more vendor-agnostic, open support, and emphasizes the need for detailed benchmarking, robust testing, and clear plans for extending or replacing current limited backends."
2022-07-20,pytorch/pytorch,"The comments reveal a range of technical issues, primarily involving installation difficulties with PyTorch and its dependencies due to network errors or misconfigurations, such as HTTP 403 errors from package servers and inconsistencies in package sizes across platforms. There are concerns about reproducibility and performance discrepancies, especially in GPU inference multi-instance deployment, latency scaling on different GPU architectures, and floating-point accuracy issues that vary with hardware (e.g., A100 vs T4). Questions also arise around deep tracing fidelity in FX, handling of __torch_function__ overloading, and trade-offs in schema validation and overload resolution, as well as broader architectural considerations like SPMD vs MPMD, and how to support subset-based process groups. Additionally, several issues relate to build system stability, CI flakiness, and the need for better tooling or API design to improve developer experience and correctness guarantees."
2022-07-21,pytorch/pytorch,"The comments largely revolve around debugging and resolving specific technical issues in PyTorch, including handling of warnings (e.g., MAGMA inverse warnings), compatibility with different multiprocessing backends (`fork`, `spawn`, `loky`), and ONNX export limitations (notably unsupported inverse operations). Several discussions suggest fixes or workarounds, such as modifying internal code or changing configurations like `multiprocessing_context`. There are also concerns about test stability, flakiness, and the correctness of certain operations (e.g., `betas` vs `momentum` in optimizers, device and dtype handling for fake tensors). Additionally, some questions address support status for different platforms and details about custom operator implementation. Overall, unresolved questions include proper handling of complex operations during export, ensuring compatibility across multiple backends, and managing platform-specific limitations."
2022-07-22,pytorch/pytorch,"The comments reflect a variety of technical challenges across the PyTorch repository, including environment setup and dependency management (e.g., handling libtorch and CUDA versions), API limitations (e.g., lack of uint16 support, dtype issues with torch.linspace), and compatibility bugs (e.g., handling of FuncTensor, kernel performance concerns, and serialization issues). There are repeated discussions about build and testing infrastructure, such as CI failures, flaky tests, build system integrations, and licensing concerns, especially with external libraries like ffmpeg. Specific use cases like functionalization, tensor modes, and tensor serialization highlight difficulties in maintaining backward compatibility and performance optimizations. Several unresolved questions involve extension support (e.g., onnx operators, autocast in JIT), as well as internal internalization processes like code review, PR orderings, and build system documentation. Overall, the thread indicates ongoing efforts to stabilize, optimize, and extend PyTorch’s functionality amid complex environment and dependency challenges."
2022-07-23,pytorch/pytorch,"The discussions primarily revolve around intricate technical issues encountered in PyTorch, such as managing multiprocessing sharing strategies in DataLoader workers, addressing precision loss in grid_sample operations due to coordinate normalization, and handling unsupported operators in various devices like MPS and ROCm. Several comments concern debugging runtime errors, such as CUDA errors, and inconsistencies in distributed training performance, especially on Windows. Others involve modifications to the PyTorch codebase, like implementing hooks, shape inference, or operator support, as well as infrastructure-related actions like merging PRs, updating labels, and resolving build or test failures. Overall, the discussions highlight ongoing efforts to improve device support, numerical accuracy, debugging tools, and code robustness within the PyTorch ecosystem."
2022-07-24,pytorch/pytorch,"The discussions mainly revolve around issues with static linking of PyTorch, particularly when using CUDA and cuDNN, including the need for specific linkage flags like `-Wl,--whole-archive` and fixes for CUDA/CUDNN version detection. Users report compatibility problems with newer GPUs like the RTX 3060 (sm_86), which are not supported by current PyTorch builds, and some issues with mismatched CUDA capabilities and driver versions. There are also concerns about build system intricacies, such as handling large tensor support, optimizations like LTO, and plugin dependencies like MKL, NCCL, and NCCL error debugging. Additionally, various build failures, merge conflicts, and CI pipeline issues are discussed, alongside questions about proper versioning and configuration for different hardware. Some improvements are suggested, such as adding fallback mechanisms to avoid blanket `whole-archive` usage and refining build diagnostics."
2022-07-25,pytorch/pytorch,"The comments cover a range of technical concerns about PyTorch, including issues with autograd behavior in presence of NaNs, NaN gradient propagation policies, and the support and design of nested or masked tensors. Several discussions focus on improving the API clarity, such as the correct use of `Optional` types in docs, better handling of `scale_factor` in `interpolate()`, and managing dependencies for features like `MaskedTensor`. There are ongoing questions about the support and integration of features like sparse matrix operations, distributed training with FSDP, and support for specialized hardware backends (e.g., ROCm, MPS). Some comments discuss the state of CI failures, merge workflows, and the development process, including potential refactoring of internal data structures like `TensorImpl`. Overall, unresolved issues include handling of NaNs in autograd, support for complex tensor types, and streamlining development and testing workflows."
2022-07-26,pytorch/pytorch,"The comments cover a wide range of technical issues, including implementation plans for features like HierarchicalSoftmax, negative sampling, and support for static or shared libraries in PyTorch extensions. Many discussions address performance bottlenecks, such as control flow with tensors, backend fallback mechanisms, and build system inefficiencies related to static initializers and build times. There are ongoing concerns about correctness and debugging, such as handling of sparse tensor transposes, cuda memory leaks, and complex control flow in TorchScript and FX-based transformations. Several discussions suggest potential API improvements, including more flexible pattern matching, support for complex-valued neural networks, and better support for nested tensors and dynamic shapes. Lastly, some issues relate to CI stability, build failures, and proper test coverage, especially in the context of unreproducible errors, regressions, or internal infrastructure changes."
2022-07-27,pytorch/pytorch,"The comments reveal ongoing discussions and concerns around various PyTorch features and development processes, such as the implementation status of sparse tensor features, the integration and support for different backends like ROCm, CUDA, MPS, Vulkan, and their respective compatibility issues. Several issues involve code stability, testing failures, and the need for rebase or fix of seemingly regressions or flaky tests, sometimes due to environment incompatibilities or internal bugs, such as in the tensor factory functions, distributed training, and CUDA memory/driver issues. There are also discussions on code maintenance and architectural decisions, like the handling of meta tensor fallbacks, global decomposition registration, legacy support for certain features, and the management of multiple build configurations or internal vs. OSS workflows. Additionally, various deployment, build, and documentation pipelines are being refined, with attention to supporting newer platforms, Python versions, and internal integration challenges. Unresolved questions include how to improve test stability, manage backward compatibility, and streamline internal workflows, especially concerning environment setup, build caching, and support for new hardware or software standards."
2022-07-28,pytorch/pytorch,"The comments highlight various technical concerns related to PyTorch, including issues with CUDA environment setup and device visibility, model export errors in onnx, and performance regressions on newer hardware and software versions. Several discussions focus on improving or fixing PyTorch’s support and stability across different hardware devices such as MPS, ROCm, and XLA, often pointing out existing limitations or regressions. There is also debate over the support scope, especially around CUDA-only API labeling, and how to better manage third-party dependencies, caching, and build processes for stability and reproducibility. Additionally, multiple comments address internal infrastructure, test flakiness, and CI automation, emphasizing the need for more robust testing, faster IR and graph transformations, and better handling of edge cases like sparse tensors, fake tensors, and in-place operations. Overall, unresolved questions involve improving cross-device compatibility, enhancing debugging and reproducibility, and aligning internal policies with user-facing support."
2022-07-29,pytorch/pytorch,"The discussion spans multiple topics, primarily centered around PyTorch's internal development and user-facing issues. Key concerns include the complexities of CUDA IPC sharing (notably tensor scope and lifetime issues), compatibility and installation challenges across different OS and CUDA versions, and performance discrepancies observed with newer hardware like the RTX 3090. There are technical questions about handling PyTorch's internal representations—such as support for meta tensors, tensor subclassing, and gradient handling during data loading—and debates on API designs like supporting `dim=None` in reduction operations. Additionally, several issues involve CI/CD pipeline failures, code reverts, and internal build consistency, with some discussions on experimental features like device abstraction and tracing, but many are unresolved or pending further review."
2022-07-30,pytorch/pytorch,"The discussions primarily revolve around technical challenges in PyTorch, such as issues with model weight conversion (e.g., MobileNetV2 unsupported nodes during conversion), debugging and profiling complex data pipelines and graphs (e.g., NVFuser, nvFuser-specific graphs, and tracing), and memory management concerns (memory fragmentation, GPU memory leaks due to retained references in hooks). Several threads mention the need for better visualization tools for memory events, improved handling of complex data pipelines with custom batching, and the stability of model conversion across different PyTorch versions and backends. Additional questions include addressing numerical precision and floating-point errors in gradient calculations, and the need for clearer documentation or support improvements. Overall, unresolved questions focus on debugging, compatibility issues, and enhancing existing tooling for better model and memory analysis."
2022-07-31,pytorch/pytorch,"The discussions highlight several technical issues in PyTorch, including persistent hangs during multi-GPU training potentially related to P2P communication bugs, and problems exporting models to ONNX, possibly connected to resizing operations in models. There is concern over support for operators on specific devices like M1's MPS, which currently lack implementation for certain aten operators, and about the stability of advanced features like ProxyTensor modes and their handling in isolated graph mode. Some discussions also address numerical stability and inaccuracy in floating-point operations, especially in gradient computations on different hardware, suggesting potential synchronization or implementation issues, such as with matrix exponential or in-place division. Additionally, there are ongoing effort notifications, rebase conflicts, and questions about best practices for distributed training setup and code maintenance. Unresolved questions include improving operator support on hardware, addressing in-place operation bugs, and clarifying development workflows."
2022-08-01,pytorch/pytorch,"The discussions highlight recurring issues related to the configuration, installation, and compatibility of PyTorch, especially concerning CUDA, ROCm, and system-specific dependencies like MKL and QNNPACK. Several comments request clarifications or fixes for build failures, version mismatches, and platform-specific bugs, often proposing workarounds such as environment variables or manual patches. There are also discussions on improving CI workflows, including robustness of tests, handling resource leaks, and better error reporting. Several entries mention ongoing efforts in feature implementation, bug fixes, and infrastructure improvements, but unresolved questions remain about system-specific issues, build reproducibility, and correctness of recent changes. Overall, the conversations indicate active troubleshooting and development efforts with some persistent unresolved environment and build-related challenges."
2022-08-02,pytorch/pytorch,"The discussion covers a variety of issues in PyTorch development, including limitations in TorchScript support for dictionary types due to static typing constraints, challenges with quantization and runtime support (e.g., missing engine 'NoQEngine' errors), and debugging difficulties related to CUDA, MPS, and memory management. Several suggestions propose improving user experience through codegen simplifications, better error diagnostics (such as enhanced stack traces and error handling), and more consistent API behaviors (e.g., using IndexError for MapDataPipe). There are ongoing efforts to address build environment inconsistencies across hardware (like AVX/AVX512 support), and discussions about test handling, CI failures, and merge workflows highlight the complexity of maintaining large-scale repo health. Unresolved questions include how to handle advanced features like AOTAutograd with certain modules, and how to improve debugging outputs and error handling to aid development and deployment."
2022-08-03,pytorch/pytorch,"The comments cover a broad range of technical topics and ongoing issues within the PyTorch repository, including the handling of CUDA and MPS device support, memory management, and test flakiness. Several discussions suggest improvements in test infrastructure, such as better handling of expected failures, subtest reporting, and blanket disabling of flaky tests, often linked to merge or CI failures. There are also debates surrounding API stability, such as management of tensor strides in DLPack exports, support for reentrant checkpointing, and the implications of in-place operations or certain operator behaviors. Many comments indicate efforts to fix specific bugs (e.g., memory leaks, regression regressions, and unsupported ops) and to streamline operational workflows (e.g., PR merges, code signing, and maintaining contributor documentation). Unresolved questions include the root causes for certain device inconsistencies, the best way to manage test failures and coverage, and the policy for handling updates involving complex side effects like hooks or tensor properties."
2022-08-04,pytorch/pytorch,"The discussions mainly revolve around improving PyTorch's infrastructure and usability, including integrating xdoctest into CI for better test review, and refining the pipeline API for distributed model parallelism, possibly separating device placement from the `DistributedPipeline` class. There are concerns about the stability and compatibility of various features across different systems and configurations, such as M1/M2 Macs, Windows CUDA, or ROCm environments, with specific issues like slow compilation, build failures, or missing operators for certain hardware. A recurring theme is clarifying or fixing the behavior of specific functions (e.g., `torch.onnx.is_in_onnx_export()`, `torch._C._get_tracing_state()`, and `make_fx()`), as well as handling the complexity of auto-differentiation graphs, hooks, and subtest reporting. Several discussions also involve ensuring the robustness of features and backports across different PyTorch versions and builds, addressing build failures, and maintaining compatibility with external systems like MPS, ONNX, or third-party libraries. Unresolved questions include the best approaches for test skipping, handling system-specific operator support, and maintaining cross-platform and hardware compatibility."
2022-08-05,pytorch/pytorch,"The comments reveal several recurring themes in the PyTorch repository discussions: (1) the need for specialized or more elegant primitives for certain distributions, such as a Logistic distribution, and the prototyping of a `Logistic` class as a `TransformedDistribution` subclass; (2) ongoing challenges with hardware compatibility and build issues, particularly regarding CPU instruction sets (AVX512), GPU support (CUDA, ROCm, MPS), and build configurations, with suggestions to update build systems, CMake versions, and support multi-arch images for better cross-platform compatibility; (3) problems related to code correctness and robustness, including the proper handling of gradients on constrained manifolds, cycle detection in reference graphs, and proper testing strategies using subTests and expected failures; (4) the importance of aligning documentation, API naming, and internal implementations (e.g., `should_compute_grad`, `type_promotion_wrapper`) to ensure consistency and avoid regressions; and (5) practical issues around code reviews, merging workflows, and infrastructure setup, such as ensuring proper labeling, rebase requests, and build cache management. Overall, the discussions emphasize improving distribution support, build stability, hardware compatibility, testing frameworks, and collaboration workflows."
2022-08-06,pytorch/pytorch,"The discussions highlight concerns about managing CUDA and PyTorch version compatibility, with suggestions to ensure proper package reinstallation and environment matching the installed CUDA version. There are questions on how certain implementation choices, such as the use of `@staticmethod` in JIT compilation, impact functionality and support, especially regarding unsupported features in the JIT or the implications of virtual methods. Several issues explore the need for better API support or refactoring, such as removing `dynamic_cast` to simplify type handling, and enhancing output registration or profiling for custom kernels. Concerns also include the management of external dependencies like protobuf and ONNX, and how environment variables or build options influence support and compilation, particularly related to ONNX support and licensing issues. Lastly, some discussions involve performance investigations on hardware accelerators like MPS and ANE, and the challenges or strategies for optimizing transformer workloads on Apple silicon."
2022-08-07,pytorch/pytorch,"The discussions predominantly focus on handling edge cases, such as zero-length sequences in RNNs and dataloader deadlocks caused by multiprocessing strategies, with proposed solutions including special handling for empty sequences and setting the multiprocessing start method to 'spawn'. Several comments highlight ongoing issues with build and integration failures, like missing dependencies, segmentation faults, or compilation errors, often related to CUDA or internal build configurations, with suggested remedies such as synchronizing CUDA handles or adjusting environment variables. There are notices about CI failures due to flaky tests, external dependencies, or changes like removing deprecated functions without proper deprecation warnings, leading to internal revert strategies. Other concerns involve merge conflicts, unreviewed pull requests, and the need for better testing, doc generation, and explicit labeling of PRs for clearer contribution tracking. Unresolved questions include specifics about supporting certain features (e.g., FX quantized model export), internal build peculiarities, and how to properly interpret complex test outputs and dependency issues across diverse platforms."
2022-08-08,pytorch/pytorch,"The discussions highlight ongoing challenges and debates around developing and supporting an OpenCL backend for PyTorch, emphasizing that an in-tree implementation is feasible and should promote vendor/vendor vendor/vendor vendor/vendor vendor/vendor vendors’ support to achieve better vendor equality, despite some limitations in performance compared to cuDNN or MIOpen. There are concerns about driver latencies, vendor politics, and maintainership sustainability, with some advocating for community-driven support and others citing driver latency issues and vendor resistance. Several discussions also concern improving backend support for multiple hardware vendors, handling specific operator limitations, and ensuring support for features like custom device registration, tensor operations, and proper benchmarking. Additionally, there are operational considerations around CI stability, test flakiness, repository merges, and documentation accuracy. Overall, the main theme is enhancing vendor neutrality and community-supported GPU backend development in PyTorch, alongside tackling technical and operational hurdles."
2022-08-09,pytorch/pytorch,"The comments primarily revolve around the development and integration of new backend devices and APIs in PyTorch, such as private use devices and dynamic device registration, with discussions on how to simplify their usage and naming conventions. There are several issues related to sparse tensor operations, autograd support, and sparse tensor slicing and indexing, highlighting the need for better support and autograd compatibility in sparse operations. Additional concerns include optimizing performance for specific hardware (e.g., ROCm, MPS) and addressing build/compiler warnings or errors across different platforms (macOS, Linux, Windows). Some discussions involve debugging crashes and synchronization issues on specific hardware configurations (e.g., CUDA, ROCm, MPS), along with PR review and testing complexities, code refactoring, and correctness of certain features. Unresolved questions include how to best introduce new API mechanisms for device management, managing build failures, and clarifying behavior for certain operations like flop counting or decompositions."
2022-08-10,pytorch/pytorch,"The comments encompass a wide range of technical concerns and discussions, such as handling OpenMP dependencies on macOS, resolving build and installation issues (e.g., missing package files during PyPI distributions, conflicting environment variables like MKL and OpenMP), and addressing specific runtime errors (e.g., tensor shape/tensorization issues, CUDA/HIP compatibility, and TorchScript scriptability). Several discussions suggest potential solutions or improvements, like reworking certain APIs for better safety, clarity, or performance (e.g., `torch.split`, `batch_norm`, `var` decomposition, and handling of in-place operations or memory sharing). A recurring theme involves the support, stability, and correctness of features across various hardware backends (CUDA, ROCm, MPS, HIP), with some issues being resolved in newer versions or requiring rebase and dependency adjustments. There are also procedural notes about merging, rebase failures, CLA signing, and CI failures, indicating operational challenges alongside the core code concerns. Overall, the discussions focus on fixing bugs, improving API robustness and usability, and integrating backend support, while some questions remain about specific implementation choices and future plans."
2022-08-11,pytorch/pytorch,"The comments cover a range of technical concerns including the need for clearer API documentation (e.g., `affine` support in normalization layers), behavior inconsistencies across tensor types and backends, and specific implementation issues such as handling variably shaped tensors, shape functions in torchgen, and refactoring for device-agnostic DDP. Several discussions highlight the importance of improving debugging, testing, and backward compatibility (e.g., the `should_compute_grad` API, support for higher-order gradients, and handling of `FlatParameter` gradients in FSDP). Unresolved questions include the best ways to support dynamic or data-dependent shapes in ref ops, the impact of shape-changing operations on tracing, and how to streamline cross-backend or device-agnostic behavior (like xpu support). Overall, the discussions suggest ongoing efforts to improve robustness, clarity, and flexibility in core PyTorch features, with many issues pending resolution or requiring further API refinement."
2022-08-12,pytorch/pytorch,"The discussions center around several key issues:

1. Handling of tensors within autograd functions, including translation of empty tensors and support for functions not implemented fully in autograd, specifically in C++ and autograd graph translation.
2. Memory management challenges, especially CUDA out-of-memory errors during training with large datasets or batch sizes, and potential solutions like clearing cache, reducing batch size, or job sharding.
3. PyTorch/ONNX interoperability concerns, including support for operators like ISTFT, Fold, and LayerNorm across different opset versions and concerns over backward compatibility and feature support.
4. Build and infrastructure issues, such as slow download speeds from channels, merge failures due to stale or conflicting commits, and CI errors related to build configuration and code checks.
5. Proposal for enhancing the subgraph rewriting API to accept graphs from alternative tracers, and for better tracking of breaking changes caused by opset updates, including explicit labeling for BC-breaking changes and version management."
2022-08-13,pytorch/pytorch,"The discussions primarily concern compatibility and integration issues within PyTorch, such as mismatched versions of CUDA and cuDNN, and the need for supporting multiple architectures including ARM and Apple’s MPS backend, especially in Docker environments. Several issues highlight problems with build dependencies, like missing or improperly included header files (e.g., PadNd.h), and build configuration errors, such as incorrect maximum LOC checks and compiler warnings about narrowing conversions. There are also recurring questions about testing procedures, particularly whether xdoctest is properly enabled in CI, and about signing CLA agreements for contributions. Additionally, some discussions address versioning and support for older PyTorch releases and building from sources, as well as the challenges of maintaining compatibility across different hardware and software ecosystems, including GPU-specific configurations and cloud-based CI pipelines."
2022-08-14,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's integration with visualization tools like TensorBoard, with issues such as passing complex inputs to `add_graph` and supporting custom tracing. There is a focus on tracking and controlling CuDNN algorithm choices for performance debugging, including standardized hints and framework-wide tracing. Multiple issues concern enhancing cross-platform and hardware support, especially for Mac ARM architectures and Docker images, such as building multi-arch images, enabling MPS (Metal Performance Shaders), and addressing performance bottlenecks on Apple Silicon. Other topics include fixing CI failures, handling model export/import issues like ONNX conversion, and simplifying tensor subclass handling for tracing (e.g., ProxyTensor). Unresolved questions remain about maintaining compatibility, handling in-place/masked/nested tensors, and improving tooling/testing to ensure correctness across diverse environments."
2022-08-15,pytorch/pytorch,"The comments primarily discuss technical issues related to PyTorch development and testing, including the handling of multi-GPU distributed training, code performance regressions after updates, and edge cases in tensor operations (e.g., support for odd number of heads in nested tensors), as well as concerns about CI stability and flaky tests causing false positives. Several threads suggest potential fixes such as reworking the way support for non-literal indices or complex distributions is implemented, adjusting tolerances for floating-point operations, and improving test infrastructure for flaky test suppression and build consistency. Questions also arise around the maintenance burden of supporting various advanced features like custom dispatch keys, complex number distributions, and in-place reinitializations, with some topics pointing to specific API proposals or future planning (e.g., handling symbolic tensors with proxies or combining quantization support for non-CPU backends). Many discussions involve re-basing, merging, or prioritizing patches, as well as verifying environmental issues (CUDA context, memory limits, or system updates) impacting test reliability. Overall, unresolved issues include flaky tests, performance regressions, and the long-term support and design choices for complex tensor features across diverse hardware targets."
2022-08-16,pytorch/pytorch,"The comments highlight key technical concerns regarding PyTorch development, including the complexity of implementing mode restoration semantics, especially for nested or non-lexical contexts, and whether to support rerooting the mode stack or keep it as-is. There are discussions about the intricacies of handling fushing, unsharding, and resharding in Fully Sharded Data Parallel (FSDP) to optimize memory management, with considerations for simplifying the core logic and future improvements. Questions are raised about ensuring consistent behavior in distributed training, handling edge cases like already unsharded parameters, and the impact of user-facing API changes versus internal refactoring. Additionally, some comments address build and integration issues, testing inconsistencies, and the correct documentation and expected behaviors for functions like `torch.equal`, as well as potential performance regressions introduced by recent modifications. Overall, unresolved questions concern the clarity of mode semantics, memory management strategies, and ensuring code correctness and usability."
2022-08-17,pytorch/pytorch,"The discussions span a variety of technical concerns related to PyTorch, including compatibility of PyTorch versions with older CUDA and GPU hardware, issues with ONNX model export involving tensor types and symbolic shape representations, and performance regressions or bugs introduced by recent commits (e.g., in MPS, CUDA graphs, or certain operators like `kl_div`). Several threads address environment and build reproducibility, including GPU support under specific hardware and platform configurations, as well as CI stability problems due to flaky tests or infrastructure failures. There are also suggestions for API improvements, such as introducing direct `SymInt` annotations or better handling of tensor memory synchronization on Apple M1/MPS devices. Overall, the key unresolved questions involve fixing specific bugs (e.g., numerical precision issues, average pooling inconsistencies), managing environment and dependency issues, and enhancing support for older hardware and cross-platform build stability."
2022-08-18,pytorch/pytorch,"The comments reveal ongoing discussions about various technical issues and enhancements across PyTorch's codebase. Key concerns include the proper handling of optimizer states and their device placement (e.g., 'step' tensors in Adam), improvements in APIs like `SummaryWriter` for TensorBoard, and the need for better error messages and support for newer hardware (like CUDA 11.4+ and Apple M1). Certain issues involve the stability and correctness of features such as distributed training, tensor conversion/exporting, and functional decomposition, with some discussions around refactoring or rethinking core design patterns, like mode stack semantics and unsharding logic in FSDP. There are also specific debugging efforts related to crashes, floating-point support, and compatibility with different backends or hardware architectures. Overall, the discussions aim to ensure robustness, usability, and consistency of PyTorch's evolving features."
2022-08-19,pytorch/pytorch,"The comments reflect diverse technical concerns, including challenges with implementing and integrating algorithms like LARS/LARC with PyTorch's distributed and parallel processing frameworks, and issues around the stability and convergence of optimizers such as RMSProp. There are recurring questions about proper usage and support for specific features such as quantization (including BF16 support), tensor comparisons, and model serialization with FX and TorchScript. Several discussions involve debugging errors stemming from compatibility issues (e.g., CUDA versions, operator support, and device-specific bugs), as well as broader design questions around mode stack semantics and the handling of nested contexts in PyTorch's API. Overall, unresolved questions include how to properly extend and maintain support for advanced features and custom backends, and how to improve error messaging and API consistency for users."
2022-08-20,pytorch/pytorch,"The discussions highlight ongoing efforts to improve consistency between PyTorch's interpolation methods and TensorFlow, with specific solutions like using `half_pixel_centers=True` in TensorFlow to match PyTorch's `align_corners=False`. Several issues relate to permission errors in build directories and the need for environment variable adjustments (e.g., `TORCH_EXTENSIONS_DIR`) to resolve build failures, especially in shared or containerized environments. There are questions about updating or replacing deprecated functions like `torch.eig` and handling internal API considerations such as `_to_copy`, especially in the context of tracing and low-level code generation. Miscellaneous concerns include intermittent CI failures, resource resource management in doctests, and the integration of newer features or internal refactorings, often accompanied by suggestions for code decomposition or proper placement of decompositions. Overall, unresolved questions include maintaining consistency across frameworks, managing build environments, and updating deprecated or internal APIs to better support tracing and optimization workflows."
2022-08-21,pytorch/pytorch,"The discussions largely revolve around the conceptual understanding and implementation of parallel modules in PyTorch, clarifying that `nn.Parallel` is intended more for ease of defining models with multiple outputs or inputs rather than just computational efficiency, and questioning whether a simple custom implementation suffices. Several issues focus on performance concerns, such as the impact of avoiding data copying when converting between different tensor storage types and the effects of model size reduction techniques like `torch.jit.optimize_for_inference`, which can cause significant model size reduction but may affect performance. Some discussions highlight potential compatibility problems, such as warnings related to `autocast` in CPU-only environments, and the correctness of certain behaviors like the influence of `deepcopy` on storage types. Additionally, there's concern over ongoing test failures and CI build issues, some attributed to unrelated factors or external dependencies, with several requests for clarifications, performance implications, or readiness to proceed with merging after addressing these issues."
2022-08-22,pytorch/pytorch,"The discussions primarily revolve around PyTorch's ongoing support for complex features on MPS, CUDA, and other backends, including operator support, performance concerns, and compatibility issues, especially on Macs with Apple Silicon. Several issues highlight unsupported or partially supported operators (e.g., `aten::index.Tensor_out`, `aten::view_as_real`, `aten::_to_copy`) and how these impact functionality and performance, with suggestions for implementation improvements or workarounds. There are concerns about build configurations, such as CUDA version mismatches, protobuf support, and build time optimizations, and questions about how to better manage test sharding and CI workflows to reduce flaky failures and build times. The maintainers discuss potential improvements like better backend operator support, code refactoring, and CI automation strategies, but many issues remain unresolved or dependent on ongoing patches and support in the PyTorch core and related projects."
2022-08-23,pytorch/pytorch,"The discussions reveal persistent issues with gradient checkpointing, particularly involving in-place operations like `ReLU(inplace=True)` and layers like `InstanceNorm2d`, which affect the correctness of gradient computation and gradient hooks. There are also significant concerns about CUDA/MPS device compatibility and performance, with reports of gradients being `None` or errors due to unsupported operators like `aten::_linalg_inv_out_helper_`, especially on newer hardware or with specific PyTorch configurations. Furthermore, some ongoing CI and build failures—due to environment mismatches, failed tests, or specific ops (e.g., `index.Tensor_out`)—raise questions about compatibility, correctness, and the proper handling of dynamic inputs, auto-differentiation, and model serialization. Discussions also touch upon infrastructural issues like test sharding, build efficiency, and the management of maintainers, code policies, and build environments for different platforms such as M1/M2 Macs and ROCm. Overall, unresolved questions include how to reliably handle in-place operations during checkpointing, ensure device/operation compatibility across environments, and improve CI stability and test coverage."
2022-08-24,pytorch/pytorch,"The comments primarily revolve around handling device-specific features and limitations, such as the incomplete implementation of certain operators on MPS devices, and the need to properly profile GPU memory usage and optimize memory savings during model export/export. Several discussions mention potential bugs or inconsistencies in the implementation of linear algebra operations, automatic differentiation, and backends (e.g., new APIs replacing old ones, and backend-specific behaviors). There are technical concerns about preserving numerical accuracy, handling type promotion, and ensuring compatibility across different hardware, especially for newer architectures or external libraries like Intel’s SYCL or CUDA. Many comments also highlight the importance of proper testing, profiling, and avoiding regressions with changes—especially on crucial components like autograd, quantization, or operator kernels—along with clarifications on CI processes, error handling, and build infrastructure. Overall, unresolved questions include how to effectively handle incomplete operators, manage device-specific limitations, and ensure robust, portable, and numerically stable changes."
2022-08-25,pytorch/pytorch,"The comments highlight several technical issues and questions related to the PyTorch ecosystem, notably concerning GPU compatibility and performance, including driver and driver compatibility issues, CUDA and MPS support inconsistencies, and behaviors of environmental variables affecting GPU usage. There are discussions on ensuring reproducibility and determinism in training, with concerns about floating-point operations, non-deterministic operations, and the impact of different hardware backends (like XNNPACK, QNNPACK, ROCm). Additionally, there are questions about build and testing infrastructure, including failure handling, test sharding, CI flakiness, and the impact of code changes on performance and correctness. Some comments propose potential solutions and workarounds, such as environmental variable toggles and code refactoring, while unresolved issues remain around specific hardware support, build regressions, and runtime behaviors."
2022-08-26,pytorch/pytorch,"The discussions encompass a wide range of technical concerns related to PyTorch's internal and external workflows, including memory management, API deprecations and replacements, profiling GPU memory, and build/test infrastructure issues. Several threads focus on improving or understanding performance, particularly regarding static linking, CUDA/CUDNN kernel availability, and tensor operations' correctness, especially on diverse hardware like ARM, M1, and ROCm. There are ongoing discussions about API usability and correctness, such as handling ignored indices, tensor subclass support, and the impact of scripting vs. tracing for ONNX export. Issues related to CI stability, merge rules, and automation (rebase/merge failures, stale PRs, land checks) are also prevalent. Unresolved questions include how to properly profile GPU memory, how to support mixed-precision and sparse tensors, and how to improve CI reliability and developer experience."
2022-08-27,pytorch/pytorch,"The discussions primarily focus on the need for fixing or improving the behavior of the torch.std() function, especially regarding its gradients and numerical differences, with suggestions for implementing subgradients and referencing related PRs. Several comments raise questions about GPU memory management, profiling, and the performance of MPS/GPU on MacBooks, highlighting driver overhead and benchmarking results that suggest current GPU performance is sluggish compared to CPU. There are technical concerns about correctly supporting certain operators like 'aten::remainder.Tensor_out' and missing support for ONNX operators such as quantized 'onnx::Slice'. Contributors also discuss deprecating or removing features like named tensors, ensuring proper synchronization on CUDA, and supporting ONNX export issues, alongside administrative notes about PR labels and build failures. Overall, unresolved questions revolve around improving GPU utilization, API support, feature deprecation plans, and systematic testing and profiling strategies."
2022-08-28,pytorch/pytorch,"The discussions primarily focus on the challenges of ensuring compatibility and performance in PyTorch, particularly on Mac systems with MPS or M1 chips, where certain operators like 'aten::remainder.Tensor_out' are not yet implemented or are slower than expected. There are ongoing efforts to improve support for features like `jit.amp.auto_casting()` with ScriptModules and issues related to NaN generation during matrix operations on CPU and MPS, suggesting underlying bugs in the hardware-specific code paths. Additionally, development workflows involve complex CI/CD processes with frequent reverts and re-lands due to build failures, conflicts, or outdated code, highlighting the need for better stability and testing. Some discussions also touch on code quality concerns such as global constructors in headers, and the importance of platform-specific test cases, especially for Mac M1/MPS support, as a way to identify and address performance and correctness issues. Unresolved questions include how to effectively support MPS/GPU acceleration on Mac, improve operator support, and stabilize cross-platform functionality."
2022-08-29,pytorch/pytorch,"The comments reflect ongoing challenges with build failures and flaky tests, often related to CI infrastructure issues such as ccache caching inconsistencies, driver overhead, or environment mismatches, especially on MPS, CUDA, and ROCm platforms. There are frequent concerns about ensuring deterministic behavior, correct handling of tensor contiguity, and proper error handling to prevent silent failures or infinite loops, notably in custom kernel and extension implementations. Several discussions highlight the need for better testing, profiling, and debugging tools, as well as improved CI automation, like handling of sign-offs, merge rules, and test disablement. Unresolved questions include how to fix specific crashes, performance regressions, and intricacies of multi-tenant build environments, alongside guidance on best practices for environment setup and code contributions."
2022-08-30,pytorch/pytorch,"The discussions highlight several technical challenges, including difficulties in building 32-bit LibTorch on Windows, support for CUDA 11.x, and MPS backend issues with certain operators in PyTorch. There are concerns about the stability and correctness of custom kernel implementations, such as ConvXd and the handling of `node.meta[""tensor_meta""]` in FX graphs, alongside questions about API consistency, deprecation, and usage restrictions. Compatibility issues, especially with legacy systems and internal build/test infrastructure, are also prevalent, compounded by merge conflicts, flaky tests, and CI failures, some related to internal environment or configuration. Additionally, persistent questions about feature deprecation (e.g., named tensors, Softmax `dim=None`) and external dependencies (LAPACK, XNNPACK) suggest ongoing evolution and incomplete feature support. Overall, unresolved questions involve build environment setup, operator support on alternative backends, and safe, compatible API practices."
2022-08-31,pytorch/pytorch,"The discussions highlight several technical concerns related to PyTorch's multi-processing tensor sharing behavior, GPU memory management, and driver compatibility issues, often involving environment variable configurations (e.g., CUDA_VISIBLE_DEVICES) and debugging strategies. There are questions about the correctness and safety of certain code implementations, such as tensor sharing in multi-GPU setups, ensuring reproducible results with generators on different devices, and the correctness of operator attributes. Several proposals suggest improving testing robustness, handling environment variable changes, and refactoring build/dependency management for optional components like ZenDNN. Additionally, issues concerning CI flakiness, build failures, and the need for better documentation or verification procedures are discussed. Overall, unresolved questions focus on ensuring correct behavior across hardware and software configurations, and improving testing and build reliability."
2022-09-01,pytorch/pytorch,"The comments reflect diverse technical concerns including the need for improved support of gradient reversal techniques, inheritance in custom `nn.Module` classes, and distributed training issues—particularly regarding NCCL errors and proper environment setup. There are ongoing discussions about the implementation and support of shared-memory models like TorchStore, support for symbolic functions, and handling of various ONNX and quantization-related challenges. Several comments address testing stability, bug fixes, and the impact of compiler/platform differences, especially on ARM64 and CUDA environments. Overall, unresolved questions indicate a focus on enhancing compatibility, robustness of autograd and distributed training, and better tooling for model serialization and runtime support."
2022-09-02,pytorch/pytorch,"The discussions highlight several technical concerns: difficulties in building PyTorch from source due to build failures (e.g., CMake errors, memory limits, DLL issues), lack of support for certain features such as `aten::sgn.out` in TorchScript, and architectural limitations like the inability to load models directly from shared memory without deep copies, which affects shared-inference scenarios. There are proposals for enhanced APIs to load models directly from shared memory, support for additional ops (e.g., LayerNorm, FFT, LayerNorm), and improvements to the symbolic function calling mechanism to support nested symbolic function calls. Additional concerns involve ensuring compatibility with hardware backends (like MPS and CUDA), addressing test flakiness, and handling discrepancies caused by floating point precision errors near boundary conditions. Overall, the discussions suggest ongoing efforts to improve build robustness, feature support, API design, and hardware optimization, with some unresolved issues related to build stability, feature completeness, and correctness of model serialization."
2022-09-03,pytorch/pytorch,"The discussions highlight ongoing efforts to improve performance profiling in PyTorch, such as integrating FVCore's FLOP counting to provide more accurate and granular operator-level metrics. Several issues address support and stability concerns across hardware backends, notably MPS (Metal Performance Shaders), with problems like non-determinism, NaNs during matrix operations, and device-specific limitations on generators and data types, especially float64. There are questions about JAX features versus PyTorch, with suggestions for potential collaboration or integration, emphasizing JAX's JIT compilation and parallel processing capabilities. Many issues pertain to stability and correctness on newer hardware, including Mac M1/M2 and GPU configurations, with fixes and discussions around support, deterministic behavior, and compatibility. Overall, key concerns focus on enhancing profiler accuracy, hardware support stability, and cross-framework collaboration."
2022-09-04,pytorch/pytorch,"The comments highlight ongoing challenges in implementing certain advanced linear algebra functions, such as `logm` and `sqrtm`, directly within PyTorch, citing reliance on CPU-based scipy implementations and their limitations for GPU and autograd support. Several discussions address CUDA and MPS device-specific issues, including out-of-memory errors, lack of operator support, and fallbacks to CPU execution, which impact both training stability and reproducibility, especially involving random number generation and deterministic seeds on non-CUDA devices like MPS. Users also raise concerns about quantized operations, nested tensor support, and the need for improved handling of padding in models (e.g., LayerNorm in NLP tasks), as well as infrastructure problems like shared memory limits and the complexity of updating PyTorch's internal operator categorizations (`aten` vs `prim`). Some threads aim to extend or refine PyTorch's functionality (e.g., pytrees for JAX compatibility, custom operators, lookahead optimizers), while others grapple with device compatibility and backend limitations, including the removal of deprecated features and implementation gaps in newer hardware/software environments. Overall, the discussions reflect active efforts to enhance PyTorch's numerical methods, device support, and user experience amidst architectural and hardware-specific constraints."
2022-09-05,pytorch/pytorch,"The discussions mainly revolve around advanced implementation and stability issues in PyTorch, such as the challenges of implementing stable matrix logarithm functions, eigenvalue computations, and handling diagonalizable matrices, especially on GPUs. Several questions address the limitations and potential improvements for operations like matrix logarithm, eigenvector inversion, and nested tensor support, with suggestions for more stable, device-agnostic algorithms. There are concerns about performance regressions observed when using JIT compilation, as well as issues related to CUDA, MPS, and environment setup affecting memory management and backend compatibility. Additionally, the maintenance and merging workflows are discussed, including CI/CD pipeline issues, rebase strategies, and contributor onboarding. Overall, unresolved questions focus on achieving numerically stable, efficient linear algebra operations across hardware and improving API support for complex tensor structures."
2022-09-06,pytorch/pytorch,"The discussions predominantly revolve around training stability and concurrency issues in PyTorch, such as dataloader hangs with multiple workers, process spawning errors on Windows and macOS, and errors related to the MPS backend on Apple Silicon. A recurring suggestion is to ensure proper code structure, such as placing main protection (`if __name__ == '__main__'`) and setting multiprocessing start methods (`spawn`) to avoid deadlocks. Several issues highlight compatibility limitations of current backends (e.g., MPS, CUDA, or LIBNVML) and lack of support for certain operators or features on specific hardware or software configurations. Additionally, questions about the integration of third-party tools (like open3d, Julia, or PyJulia), correctness of custom implementations, and the need for better documentation or testing are prevalent. Overall, unresolved key questions include how to improve error handling, support diverse hardware/backends, and streamline debugging of runtime issues."
2022-09-07,pytorch/pytorch,"The comments cover a wide range of issues encountered in the 'pytorch/pytorch' repository, including problems with outdated mirrors affecting package installation, negative loss outputs possibly caused by implementation bugs, support limitations for specific hardware (e.g., ARM, M1 Macs, TPU), model serialization security concerns, and compilation errors due to incompatible compiler features or missing dependencies (e.g., Vulkan, ROCm). Several discussions also relate to performance improvements (e.g., TorchScript recompile behavior, FSDP prefetching, ONNX opset support), API usability (e.g., creating modules on specific devices, Tensor serialization formats), and support for specific PyTorch features or backends. Numerous issues are unresolved or require further investigation, often involving compatibility with hardware, compiler, or backend specifics, and some involve coordination across different code components or external dependencies. Overall, the maintainers are addressing both stability/security concerns and performance/feature enhancements, but many challenges are still in progress or need more systematic solutions."
2022-09-08,pytorch/pytorch,"The discussions primarily focus on shared memory management in DataLoader, suggesting replacing lists/dicts with numpy arrays or tensors to prevent memory bloat due to reference count issues. There's also a recurring concern about handling Python objects (like dictionaries containing classes or mixed types) in multiprocessing contexts, with suggestions to use deepcopy or custom implementations. Several issues relate to compatibility and support for operators on specific hardware/backends (e.g., CUDA, MPS, sparse tensors), demanding API adjustments, bug fixes, or fallbacks. Additionally, some discussions involve code robustness, such as ensuring correct seeding in CUDA graphs, fixing test failures, and improving API design to avoid user pitfalls. Lastly, multiple mentions address merge conflicts, internal CI failures, and the importance of maintaining documentation and test stability amid evolving features."
2022-09-09,pytorch/pytorch,"The comments highlight ongoing issues and discussions related to PyTorch's infrastructure and features, including server outages, API and implementation enhancements, and platform-specific support. Several comments question the progress and integration of features such as multinomial sampling improvements, C++/Python API consistency, MPS device support on Apple hardware, and correctness of backend operations like einsum and matmul on MPS. Others address internal build and testing infrastructure concerns, such as merging conflicts, test failures, and environment reproducibility, especially on Apple Silicon and older CUDA versions. Multiple comments still indicate stalled PRs due to conflicts, stale statuses, or internal build issues, with some suggesting improvements in testing robustness, code decomposition internalization, and better documentation. The unmet challenges involve handling platform-specific limitations, ensuring proper API exposure, and maintaining reliable CI/CD pipelines amidst evolving hardware and software environments."
2022-09-10,pytorch/pytorch,"The discussions primarily concern ongoing technical issues with PyTorch, including the proper handling and conversion of models to MKL-DNN, issues with support for operators like `aten::index.Tensor` on MPS devices, and the complexity of managing CUDA memory fragmentation, especially when using CUDA Graphs. Several questions also address the correctness and API design of functions like `make_graphed_callables`, suggesting enhancements such as cloning outputs to avoid unexpected in-place modifications, and ensuring input tensors are contiguous for stable execution. Additionally, there's interest in improving shared memory model loading via TorchStore, with suggestions for read-only, zero-copy, and in-process sharing, as well as API adjustments to better support loading models directly from shared memory without deep copying. Unresolved questions include compatibility issues across devices, proper seed management in CUDA graphs, and suggestions to refine user-facing APIs for clarity and robustness in models' serialization/deserialization and memory management."
2022-09-11,pytorch/pytorch,"The discussions highlight challenges with PyTorch's internal design and usage, such as the proper management of `named_parameters` for optimizer parameter filtering, and the ambiguous naming conventions like `c10`, which are clarified to relate to core tensor abstractions. Several issues involve GPU and device management, particularly with handling multi-GPU setups, `fork()` behavior in production environments like Gunicorn, and cross-device data transfers, which may cause bugs or performance bottlenecks. There are concerns regarding correctness and stability in specific modules like `LSTM` on MPS backend, especially regarding input tensor shapes, batch vs sequence dimensions, and backward operation support, with potential fixes involving tensor permutations or `contiguous()` calls. Additionally, some discussions focus on CI infrastructure, disk space management on Mac, and process management strategies to avoid bad-fork states. Overall, unresolved questions center on improving API clarity, device handling, and runtime stability for different hardware backends."
2022-09-12,pytorch/pytorch,"The comments encompass a range of issues including support for specific operations like torchvision::nms in TorchScript, challenges with model serialization and loading in C++, and performance bottlenecks during large model export and quantization. Several comments urge for clarifications on implementation details, bug reports, or feature requests such as handling non-contiguous tensors, simplifying complex models, or extending support for certain layers like Conv3D on MPS. Discussions also include CI stability, code rebase issues, and internal process improvements. Many concerns highlight the need for better handling of device-specific operations, compatibility, and performance optimization, with some suggestions for temporary workarounds or future feature enhancements. Unresolved questions often pertain to support for newer APIs, integration of features like functionalization, and ensuring robust, maintainable code evolution."
2022-09-13,pytorch/pytorch,"The comments reflect ongoing development and exploration of features such as TimeDistributed modules, handling of nested and packed sequences, and support for multiple tensor types and layouts, with some proposing utility functions and custom modules for sequence application and batching. Several issues involve performance optimizations, particularly reducing memory copies, improving operator support for multiple batch dimensions, and enhancing kernel and operator behaviors, often with patches or potential backward-compatible changes. There are also discussions on API design, such as whether to make certain APIs more user-friendly or consistent (e.g., for warnings or dtype handling), and technical concerns about correctness and stability, especially around CUDA memory management, memalloc fragmentation, and distributed communication. Some comments address build and environment issues, including container, CUDA, and NCCL upgrades, as well as flaky or failing tests, with suggestions to rebase, fix, or update dependencies. Overall, the discussions show active ongoing contributions, performance tuning, bug fixing, and API refinement, with some unresolved technical and infrastructural questions noted."
2022-09-14,pytorch/pytorch,"The discussions primarily revolve around PyTorch's internal implementation and testing infrastructure, including issues with merging conflicts, build configurations, and environment setup, especially on M1 Macs and WSL2 systems. Several comments highlight specific technical problems such as device topology for multi-GPU setups, performance discrepancies in GPU kernels (e.g., matmul), and incomplete or inconsistent feature support across hardware and software variants (e.g., MPS, CUDA, NCCL, ONNX operations). There are considerations about code refactoring, such as code coverage, test flakiness, API design (e.g., naming conventions, device handling), and merging policies. Some discussions suggest disabling flaky or incompatible tests temporarily pending fixes, and many involve dependency management, build caching, and environment reproducibility. Overall, unresolved questions involve fixing system-specific bugs, ensuring build consistency, and managing test robustness while progressing on various feature and performance improvements."
2022-09-15,pytorch/pytorch,"The community discussions encompass various technical concerns, notably the compatibility of GPUs (e.g., TITAN RTX, Tesla T4) with specific CUDA versions, and issues related to mixed-precision training and AMP autocast. Several reports of runtime errors such as cublas runtime errors, NaNs, and GPU memory leaks are linked to mismatched tensor types, outdated CUDA versions, or bugs in specific CUDA kernels. There are ongoing efforts to improve reliability in sparse tensor operations (e.g., `zeros_`, `to_sparse`), and concerns about large model export times and system-specific behavior (e.g., M1 Macs). Several discussions involve build and compile issues, including dependencies, environment configurations, and stability of CUDA/cuBLAS, alongside feature enhancements like adding `concatenate` alias and improving function functionalization. Unresolved questions include how to properly handle tensor aliasing for views, testing in various environments, and system-specific bugs, with suggestions to improve testing, documentation, and code consistency."
2022-09-16,pytorch/pytorch,"The discussions encompass a range of technical issues and proposals related to PyTorch, including hardware compatibility and stability (e.g., power limits and overclocking fixes for GPUs), debugging and memory management (e.g., adding gc.collect() for memory stability), and operator/function support (e.g., support for `aten::unfold` on MPS, support for non-static dict inputs in ONNX export). Several concerns are raised about build configurations, such as the impact of CUDA_VISIBLE_DEVICES on CUDA availability checks, and reproducibility issues across different devices and backends (e.g., CUDA, MPS, XLA). There are also multiple discussions on code maintenance topics, such as preserving git blame during file renaming, handling thread safety in mode stacks, and ensuring backward compatibility. Overall, unresolved questions include ensuring proper support for new features in various backends, optimizing build and runtime behavior, and addressing stability or performance regressions."
2022-09-17,pytorch/pytorch,"The comments highlight several persistent issues with PyTorch's distributed training and multi-node setup, notably socket timeouts and complex environment variable configurations when running on SLURM, which hinder multi-node training on platforms like SLURM and cause socket timeout errors during rendezvous. Additionally, there are discussions about support for quantized operators such as softmax in quantization workflows, with active development and requests for features like aten::_softmax.out support. Certain issues are related to backend-specific bugs, such as MPS-related BatchNorm inaccuracies on macOS Ventura and potential backend support gaps for custom operators like DCNv2 in TorchScript. There are also ongoing internal efforts to improve thread safety of modes, dispatch key management for fake tensors, and code refactoring for better maintainability, along with typical CI and merge failure discussions. Overall, unresolved technical challenges focus on distributed environment stability, backend/operator support, and internal code robustness."
2022-09-18,pytorch/pytorch,"The discussions highlight several technical concerns, including persistent issues with multi-node training in PyTorch Lightning and the need for solutions such as Hugging Face Accelerate or Nvidia Apex. There are ongoing challenges related to specific operator support and implementation details, such as the `aten::cumsum.out` operator on MPS devices, and the handling of sparse CSR tensors across devices. Some PRs are stalled due to missing CLA signatures or failing mandatory checks, while others involve complex debugging of failures in autograd, JIT, or CUDA memory management, with suggestions to improve error reporting and default behaviors (e.g., showing C++ stack traces). Overall, the discussions emphasize debugging, compatibility, and infrastructure improvements, often requiring targeted patches or additional validation."
2022-09-19,pytorch/pytorch,"The discussions mainly revolve around ensuring compatibility and correctness of PyTorch's build and runtime behavior, including issues related to environment configuration (GCC and CUDA versions), missing operator support (e.g., softmax, chain_matmul, and complex type handling), and build artifacts (such as updating Android operator files). Several comments point out specific bugs or discrepancies, such as the inability of certain operators to run with quantized backends, failure of tests under certain conditions, or backends producing inconsistent numerical results. There are suggestions to improve error checking (adding TORCH_CHECK for parameter validation), extend support for certain features (e.g., uneven collectives, complex dtype support, and softmax in quantization), and ensure proper build configuration (matching compiler flags, build types). Some unresolved questions concern the planning of long-term strategies (e.g., for quantized operators, onnx support, and leveraging TorchDynamo), and mechanisms for fixing environment-related issues (like missing standard libraries in Python 3.10 or backend support)."
2022-09-20,pytorch/pytorch,"The discussions highlight issues with multiprocessing start methods on Windows and Mac, with solutions involving the spawn start method and proper use of `if __name__ == '__main__'`. There are concerns about GPU memory management, particularly with PyTorch's memory leaking and how to accurately profile GPU memory usage, suggesting the use of `torch.cuda.memory_allocated()` over `nvidia-smi`. Several threads address problems with CUDA and MPS operations, including crashes and performance degradation, often attributed to driver issues or API limitations, with workarounds involving input padding and data handling. Questions also arise about proper documentation of API features, support for new operators in ONNX export, and the need to explicitly show supported opset versions and features. Overall, unresolved issues include platform-specific multiprocessing behaviors, GPU memory profiling accuracy, hardware/software compatibility, and better API documentation."
2022-09-21,pytorch/pytorch,"The discussions encompass various technical concerns related to PyTorch's development, including issues with model loading and module import errors, especially when loading third-party checkpoints without maintaining the original file structure. There are ongoing discussions about CUDA and NCCL communication optimizations, especially on multi-GPU setups, including timeout issues, PCIe/IB network configurations, and potential fixes like static linking of NCCL and handling of peer-to-peer access. Several PR reviews highlight the importance of precise API semantics for memory formats, stride handling, and serialization consistency, especially for debugging and reproducibility. Recurrent topics include testing strategies (unit, integration, internal CI) for correctness, performance regressions, and handling flaky CI failures, as well as procedural questions about merging, signing CLAs, and automation commands. Overall, unresolved questions concern addressing system-dependent bugs, improving internal API clarity, and maintaining backward compatibility while evolving the framework."
2022-09-22,pytorch/pytorch,"The discussed comments cover a wide range of technical concerns including memory management, API behavior, and API design strategies in PyTorch. Key issues include how to better monitor GPU memory release, especially regarding the nightly updates and API fixes, as well as handling nuances with tensor storage, especially for custom or backporting purposes. Another common theme is improving the usability and clarity of APIs such as `autocast`, `Parallel` modules, and tensor operations like `permute` and `view`, often with proposals for API refactoring or better documentation. Several questions also focus on specific bugs (e.g., with `deform_conv`, `lstsq`) and the need for more robust error handling, type annotations, and version compatibility. Unresolved issues include how to handle dynamic input shapes, copy and sync tensor states, support for various hardware backends, and managing complex internal state like CUDA graphs or custom operator registration for ONNX."
2022-09-23,pytorch/pytorch,"The discussions cover various technical concerns around PyTorch development, such as issues with distributed training setups (e.g., using mpi4py and Slurm), specific operator implementations (e.g., `as_strided`, `reduce_grad`), and hardware backend support (e.g., MPS, CUDA, NCCL). Multiple comments point to benchmarking, performance optimizations (like kernel golfing and memory management), and potential code refactors for clarity and efficiency. Several discussions suggest improvements in testing, CI reliability, and internal code structure modifications (such as namespace usage and operator support coverage). Unresolved questions include fixing specific buggy behaviors, addressing internal infrastructure problems, and improving usability and documentation for features like ONNX export and custom kernels."
2022-09-24,pytorch/pytorch,"The discussions highlight various technical challenges including training hangs related to process group destruction, data storage inconsistencies on CUDA tensors, and the need for better test coverage, especially for synchronization-related bugs. Several comments address build and merge failures due to outdated PRs, branch conflicts, or missing CLA signatures, emphasizing ongoing CI/CD infrastructure management. Issues with specific features such as advanced indexing, quantization, and operator overloading (e.g., the use of '@' and `torch.mm`) are highlighted, with some fixes incorporated via PR merges or updates to nightly builds. There are also questions about debugging options, environment configurations (e.g., `capturable` in CUDA graphs), and platform-specific bugs, notably on MPS and Vulkan, underlining the need for improved testing and documentation support. Overall, these discussions reflect active troubleshooting, incremental fixes, and infrastructure adjustments to improve stability, compatibility, and developer experience."
2022-09-25,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's compatibility and environment setup, especially on Windows and for device-specific issues like MPS on Apple Silicon and Vulkan on mobile. Several comments focus on installation difficulties due to missing runtime libraries, the need to rebuild or adjust library placement, and strategies for debugging operator mismatches or errors during GPU operations. There are also concerns about codebase complexity, such as the convoluted implementation of loss functions and the need for better test coverage, as well as discussions on merging PRs and contributor CLA signings. Overall, the key issues revolve around improving build reproducibility, debugging GPU/ device-specific errors, and managing contributions effectively."
2022-09-26,pytorch/pytorch,"The comments highlight efforts to enhance PyTorch's internal tooling and debugging capabilities, such as making memory allocators more observable, tracking tensor allocations, and reworking the operator registration and dispatch mechanisms. Several discussions focus on issues related to specific features and backends, including ONNX export limitations with sparse tensors, device-agnostic distributed training, and backend-specific operator and quantization support (e.g., QNNPACK vs. XNNPACK). There are recurring reports of bugs, crashes, and performance regressions, often with suggestions to upgrade to nightly versions or improve error handling and testing. Additionally, questions about build configurations, reproducibility, and compatibility across platforms like macOS, Windows, and diverse GPU hardware are prevalent. Unresolved concerns include handling of specific operator implementations, memory management intricacies, and ensuring cross-platform stability and correctness."
2022-09-27,pytorch/pytorch,"The discussions cover a range of technical concerns including handling out-of-memory issues in distributed training, strategies for patch registration in PyTorch's build system, compatibility of custom operators and operator schemas, and challenges with certain GPU and MPS-related failures, especially in the context of CI stability and correctness. Specific proposals involve leveraging beartype for function annotations, introducing foreign function interface support, and enhancing build tooling to support more portable and maintainable code generation. There are ongoing efforts to improve reliability of distributed RPC, address theoretical and practical issues with GPU/Metal implementations, and ensure correctness in many edge cases like non-determinism and memory leaks. Questions remain about the best practices for operator registration, cross-platform consistency, and rigorous testing of new features or internal modifications. Overall, the primary focus is on improving the robustness, portability, and correctness of PyTorch's system infrastructure and runtime."
2022-09-28,pytorch/pytorch,"The discussions largely revolve around build and CI issues in the PyTorch repository, including automated test failures, merge conflicts, and environment-specific problems such as those on Mac, Windows, or Linux with various hardware (e.g., ROCm, MPS, CUDA). Several comments concern the need for better documentation, refactoring of code for consistency (e.g., header include paths), and the handling of specific features like symbolic shape support, sparse data compatibility, and operator behavior across different backends. A recurring theme is the desire to improve robustness and clarity in the build system, including fixing flaky tests, ensuring environment setup correctness, and addressing hardware-specific limitations (e.g., 32-bit indexing). Some concerns also focus on feature proposals or bug fixes related to specific operators, autograd, and tensor behaviors, sometimes tied to internal or external dependencies. Overall, the conversation reflects ongoing efforts to stabilize CI, improve code hygiene, and enhance feature support, with clear calls for detailed testing, documentation updates, and code review for planned changes."
2022-09-29,pytorch/pytorch,"The collected comments highlight several recurring issues: (1) Many discussions revolve around CUDA out-of-memory (OOM) errors during training or inference, with suggested fixes including reducing batch sizes, clearing cache (torch.cuda.empty_cache()), or resizing input data to lower dimensions; (2) Some comments focus on code consistency and best practices, such as making TensorImpl methods virtual for better extensibility or refactoring header include paths for internal OSS merge compliance; (3) Multiple issues pertain to testing stability and flaky tests, especially on CI, with recommendations to rerun or rebase; (4) There are discussions about proper handling of data types, mixed precision (AMP), and numerical stability, including managing NaNs, automatic casting, and addressing type mismatches in modules like attention; (5) Several PRs relate to fixing specific bugs or supporting new features, some with associated documentation updates or performance benchmarks, and multiple merge failures due to lack of approvals, stale PRs, or unrelated internal checks."
2022-09-30,pytorch/pytorch,"The discussions primarily revolve around troubleshooting issues related to PyTorch's multiprocessing and data loading (e.g., setting start method to 'spawn' and configuring sharing strategy) and CUDA-related bugs such as memory leaks, out-of-memory errors, and support for various hardware backends (including MPS, ROCm, and different CUDA versions). Several threads mention problems with specific operators on certain devices, kernel support issues, and the need for better testing, documentation, or feature support for features like warnings and in-place operations. Some discussions involve code reverts, rebase requests, or merge failures due to conflicts, flaky tests, or CI rules not being met. Overall, key concerns include stabilizing CUDA/memory behavior, improving backend support and device compatibility, and ensuring proper documentation and testing practices."
2022-10-01,pytorch/pytorch,"The comments highlight recurring issues with PyTorch JIT tracing, particularly warnings about mismatched outputs and the importance of model.eval() to avoid stochastic behavior from layers like dropout. Several users report the necessity of explicitly setting the model to evaluation mode before tracing to prevent discrepancies, and warnings related to constants registered during tracing. There are ongoing discussions about support for certain operations on specific backends like MPS, as well as concerns about compatibility with ONNX exports and device-specific operator implementations. Some issues involve performance regressions, stability problems, and correctness in backpropagation, with fixes often requiring code simplification, better environment management, or feature additions. Overall, maintaining consistency across devices, improving tracing robustness, and resolving backend support limitations remain key challenges."
2022-10-02,pytorch/pytorch,"The discussions highlight several technical concerns including the relationship between Functorch's memory-efficient fusion and reversible transformer formulations, and whether current PyTorch features like in-place gradient computation or data streaming support align with larger-scale or streaming data use cases. Multiple issues refer to specific bugs or features such as the need for more flexible dataset streaming from disk, improved support for in-place or overlapping tensor updates, and compatibility of operations like indexing or upsampling in scripted and ONNX-exported models. There are recurring questions about hardware-specific problems, such as MPS backend crashes, and the need for better error messaging, documentation, and robustness against system or version mismatches. Overall, the discussions suggest ongoing challenges in extending PyTorch's flexibility for new hardware, advanced model architectures, and seamless deployment, with some unresolved or experimental features requiring further validation or refactoring."
2022-10-03,pytorch/pytorch,"The comments highlight ongoing challenges with memory debugging in CUDA and in PyTorch, including the lack of API access to allocator details, memory fragmentation, double-free bugs, and specific device-related issues such as MPS and Metal performance shaders errors on Apple Silicon. There are also discussions about the need for enhanced profiling tools, the complexity of correctly analyzing or tracing CUDA allocations, and efforts to improve the robustness and accuracy of quantization APIs. Additionally, developers are addressing build and environment inconsistencies, infrastructure failures, and the need for better testing strategies, especially for dynamic shape support and multi-interpreter management, to ensure stability across different hardware and frameworks. Unresolved questions include stream capture bugs, handling of scalar promotion, and environments for testing complex models confidently."
2022-10-04,pytorch/pytorch,"The discussions highlight concerns about handling non-deterministic operations and guaranteeing reproducible results, especially when dealing with complex data types or multi-GPU/distributed setups. There are questions about how to properly implement warnings and errors, such as whether to embed warning types as enums or hierarchies, and how to expose internal state and configuration options (e.g., prefetch limits, memory attributes) for testing and debugging. Several issues involve platform-specific behaviors (e.g., Windows vs. Linux, macOS issues, CUDA version mismatches) that complicate testing and deployment, with suggestions to unify configurations and improve documentation. There are also ongoing efforts to enhance out-of-tree plugin support (e.g., for DML, MPI backends) and to make the testing framework more robust and comprehensive, including better coverage and reporting of non-deterministic or GPU-specific errors. Overall, the discussions address both ensuring correctness and reproducibility in complex execution environments and improving the API, documentation, and tooling for broader extensibility."
2022-10-05,pytorch/pytorch,"The discussions primarily revolve around improving and fixing various PyTorch functionalities on the MPS backend, such as support for data types (e.g., float64, int32), operator coverage (e.g., aten::normal, aten::sum, aten::index_select), and consistency issues between CPU, CUDA, and MPS executions. Several comments suggest modifications or add new tests for better coverage, including handling of non-deterministic operators, support for `__torch_function__` dispatch rather than `__torch_dispatch__`, and ensuring performance improvements or correct behavior in specific cases like `einsum`, `sum`, `where`, and tensor attribute functions. There are also discussions about restructuring certain internal code pathways, handling consistent behaviors across multiple backends, and fixing CI flakiness or build issues. Overall, the main concerns are ensuring correctness, support for data types and operations on MPS, and improving test coverage for cross-backend consistency and reliability."
2022-10-06,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of dynamic and static tensor shapes in JIT-exported models, specifically with auxiliary functions like interpolate, which may not preserve intended output shapes when invoked on different input sizes. There are questions about the support for various tensor data types across backends (e.g., bfloat16 in HDF5, int64 effects on MPS), and issues with consistency and correctness in functions like `linalg.inv` due to backends' numerical differences. Some comments address stability and correctness of distributed training, such as proper handling of `requires_grad` tensors with FSDP, as well as potential race conditions and leakages in autograd's thread management. A recurring theme is the need for more comprehensive, formal testing (including cross-device and forward/backward correctness) and better understanding of internal backend support and limitations (e.g., for src_key_padding_mask or specific operators). Overall, unresolved questions involve improving model export robustness, supporting various data types and tensor behaviors, and ensuring correct distributed and autograd runtime behavior."
2022-10-07,pytorch/pytorch,"The comments highlight challenges with hardware-specific and platform-dependent bugs, such as CPU BF16 kernel slowdowns potentially related to Intel, and flaky CI tests across Linux, macOS, and Windows, often due to resource contention or environmental instability. Several discussions focus on improving error handling and safety in autograd, warning hierarchies, and device dispatch mechanisms, including virtual functions, warnings hierarchy, and thread/device initialization concerns. There are issues with implementation details of certain operators (e.g., `upsample_bicubic2d_backward_vec`) affecting compatibility, especially with XLA, and the need for better testing, documentation, and safe defaults—particularly for distributed support on varied platforms like M1 or Apple Silicon. Also, multiple revert and rebase actions imply difficulties stabilizing the main branch against complex environment and code interactions. Overall, the main themes involve addressing platform-specific bugs, improving robustness and safety of the core runtime, and managing CI/test infrastructure stability."
2022-10-08,pytorch/pytorch,"The discussions primarily revolve around stability and correctness issues in PyTorch, including CUDA-related errors such as illegal memory accesses, launch failures, and execution failures, often linked to multi-GPU and distributed training setups. Several reports highlight the need to upgrade CUDA and PyTorch versions to resolve these errors, with some issues seemingly mitigated by environment changes or updates. There are also concerns about specific operator support, such as channels-last memory format operations, and the correctness of algorithms like matrix inversion across devices. Additionally, some discussions address the maintainability of tests, the impact of code changes on internal workflows like FSDP checkpoint loading, and the accuracy of certain functionalities under different modes or configurations. Overall, unresolved questions include troubleshooting flaky tests, ensuring broad operator support, and improving error handling and reporting mechanisms."
2022-10-09,pytorch/pytorch,"The discussions cover several technical challenges and proposals: (1) Managing shared memory and worker issues in multi-process environments, with suggestions to improve model loading directly into shared memory or via memory-mapped files for robustness. (2) Accurate detection and handling of package version conflicts, particularly for tensorboard, and methods to resolve duplicate installations. (3) Implementing safe, zero-copy model loading APIs to avoid unwanted tensor copying and deep copies during deserialization. (4) Extending support for quantized ONNX operators and addressing unsupported ops in various backends like MPS and CUDA, including improving operator implementation and visibility. (5) Addressing build and compatibility issues arising from compiler versions, device support, and code cleanup, with some ongoing work on reproducibility and infrastructure stability. Overall, unresolved questions focus on enhancing memory management, package handling, and cross-device support for more efficient and reliable PyTorch functionalities."
2022-10-10,pytorch/pytorch,"The discussion encompasses various technical concerns related to PyTorch development: issues with platform-specific builds (e.g., cross-architecture builds on ppc64le, macOS/MPS support, and CNMeM-related bugs), CUDA and cuBLAS/ MAGMA backend inconsistencies, and performance regressions or correctness issues in linear algebra routines across different libraries and hardware backends. There are questions about improving the robustness and correctness of autograd, distributed training, and support for non-standard environments (e.g., on-device training, mobile, alternative compute backends like XPU). Several proposals include refining build configurations, fixing existing bugs (with proper test coverage), enhancing debugging and documentation, and ensuring consistency of numerical results across different devices and precisions. Unresolved questions highlight the need for better platform support, clearer documentation for internal behaviors, and conditional execution logic in tests and codepaths to better handle various hardware/software configurations."
2022-10-11,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's memory management, especially related to GPU memory tracking, allocator behavior, and allocator snapshots, which are complex due to shared memory blocks, views, and allocator snapshots at the block level. There are several reports of training stalls or hangs in multi-GPU or distributed setups, often linked to device configuration, CUDA/CUDA versions, or environment issues, and some instances of flaky CI tests/or builds on specific platforms like ROCm or macOS. Issues also include difficulties in handling model serialization across different PyTorch versions, incomplete support for M1/MPS backend and other hardware, and bugs with specific operators like `aten::index.Tensor` or `upsample_bilinear2d`. Debates also involve lifecycle concerns such as merge conflicts, CLA signing, test disabling, and CI sharding strategies, with suggestions including more granular flags, virtual warning types, and better instrumentation for pass tracking and diagnostics. Overall, key unresolved questions relate to improving allocator traceability, memory efficiency, CI reliability across platforms, and robustness in model export and serialization workflows."
2022-10-12,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of CUDA and MPS device-specific issues, such as memory limits, scalar semantics, and reproducibility of results. There are ongoing questions about proper API support for sparse tensors, custom memory allocators, and device compatibility, especially surrounding new features like `cudaMallocAsync` and stream-ordered deallocations. Multiple comments address debugging, test stability (notably flaky CI tests and test disabling), and the need for clearer error messaging or API updates related to specific operations, device behaviors, and backward compatibility. Some suggestions involve improving documentation, test infrastructure, and code robustness to ensure consistency across platforms and runtime environments. Unresolved questions remain about the proper design of APIs for custom allocators, scalar handling on different devices, and ensuring testing & CI processes effectively detect regressions."
2022-10-13,pytorch/pytorch,"The discussions highlight various ongoing and resolved issues in the PyTorch repository, including asking questions about version compatibility, build and optimization issues on specific hardware (e.g., MPS, CUDA, ROCm), and feature enhancements like nested tensors, autograd support, and API consistency. Several comments involve troubleshooting errors with specific operators on certain devices, or performance regressions in newer nightly builds, with proposed solutions such as updating dependencies, changing API implementation, or experimental patch notes. There are also multiple internal and CI-related concerns including flaky test results, test disabling procedures, and merge/rebase workflow notifications. Additional discussions focus on API design choices (e.g., aliasing, type safety, and functionalization) and development guidance for contributors. Many unresolved questions relate to testing, stability, and compatibility across different hardware environments, as well as version support strategies."
2022-10-14,pytorch/pytorch,"The comments encompass a range of technical concerns, primarily centered on optimization and scalability issues. Several discussions highlight limitations of certain functions, such as `multinomial()` being restricted to 2^24 categories and the slow performance of custom weighted samplers in large datasets, suggesting sampling-twice methods as workarounds. There are questions on extending ONNX support for operators like `addcmul` and `triu/tril` across multiple opsets, with proposed custom symbolic functions as interim solutions. Additionally, certain internal infrastructure challenges are discussed, such as build failures related to CUDA, QNNPACK, and internal CI workflows, often requiring rebase, patch fixes, or external system updates. Many comments also touch upon the need for better documentation, proper sign-off for contributions, and improving upstream test coverage or feature support, especially for sparse tensors and MPS-specific operations."
2022-10-15,pytorch/pytorch,"The discussions highlight several areas of concern, including the lack of native `load_state_dict` / `state_dict` functions in the C++ API and ongoing work to implement them, as well as issues related to MPS device support (notably unimplemented operators like `bucketize`, `nan_to_num`, and LU decomposition) and performance regressions specific to MPS, especially in einsum operations. There are also discussions about the stability and compatibility of sparse tensor operations with autograd, requiring updates to support recent changes in nightly builds. Additionally, questions about CI failures, build dependencies, and the process for adding new C++ APIs indicate ongoing infrastructure and API development challenges. Overall, key unresolved issues include improving MPS operator support, debugging performance regressions, ensuring stable sparse autograd, and managing build/test infrastructure."
2022-10-16,pytorch/pytorch,"The discussions predominantly revolve around improving the efficiency and usability of large parameter tensors in PyTorch's Fully Sharded Data Parallel (FSDP) implementation, particularly concerning memory management, bucketing communication overlaps, and checkpoint APIs. Several comments suggest refactoring the flat parameter layout, attribute management, and integrating new features like backward prefetch, non-recursive wrapping, and out-parameter loading, often with plans for re-architecting core components such as `FSDPParameter` and `FlatParamHandle`. Issues with GPU memory allocation, fragmentation, and support for various operators (e.g., on MPS devices or for specific functions like `nan_to_num` or `range`) are also highlighted, alongside challenges in build configuration and environment consistency across platforms. Some discussions include efforts to migrate or test features, with questions about conditional logic to maintain backward compatibility or optimize performance, and unresolved environment-related errors indicate underlying setup or system-specific constraints."
2022-10-17,pytorch/pytorch,"The comments highlight several ongoing issues and efforts within the PyTorch community. Key concerns include the support and correctness of ONNX operators such as `adaptive_max_pool2d`, `size()` return types for traced modules, and certain sparse matrix operations and their gradients, especially on specific hardware backends like MPS and ROCm. Many discussions revolve around improving and stabilizing performance, including runtime improvements for channels-last tensors, FSDP and DDP configurations, and binary build support for different architectures. There are also topics about codebase maintenance, such as the aliasing of Python functions, handling C++/Python interoperability, and build/test infrastructure stability, especially related to CI flakiness and external dependencies. Unresolved questions include ensuring feature support across platforms, fixing specific bugs (e.g., in autograd and memory management), and orchestrating the integration of internal and external contributions effectively."
2022-10-18,pytorch/pytorch,"The discussions highlight issues related to build and integration challenges, such as persistent warning messages during Windows CUDA builds, broken or flaky CI tests across platforms, and discrepancies caused by outdated or inconsistent codebases (e.g., header file locations, deprecations). There are concerns about performance regressions introduced by recent changes, particularly regressions in einsum and shape inference, which may be mitigated by further profiling and optimization. Several PR reviews involve addressing CI failures, flaky tests, and ensuring proper support across different hardware backends (e.g., MPS on macOS, ROCm, and CUDA devices), with attention to maintaining correctness and compatibility. Unresolved questions include handling of compatibility with external tools like multipy, understanding the root causes of specific memory errors or platform-specific bugs, and improving testing and validation strategies to prevent regressions, especially in platform-dependent contexts."
2022-10-19,pytorch/pytorch,"The discussions primarily revolve around PyTorch's ongoing development issues, including build failures, integration of new features, and compatibility concerns. Several comments note problems with CUDA versions, compiler errors, and environment setup, often resolved by updating dependencies, switching to nightly builds, or adjusting build configurations. There are recurring concerns about testing coverage, especially related to operator support (e.g., ONNX export, MPS backend issues, operator registration), and the need for better documentation or test automation. Some suggestions involve refactoring API behaviors for better backward compatibility, and requests for more comprehensive guidance on new features or internal code changes. Several discussions also highlight administrative issues like CLA signing, merge permissions, and build infrastructure stability."
2022-10-20,pytorch/pytorch,"The discussions reveal ongoing challenges with building and compiling PyTorch across various environments, including Windows Visual Studio errors, CUDA and cuDNN version mismatches, and specific hardware issues like MPS on Apple Silicon. Several comments highlight the importance of correct build configurations, environment setup, and handling of sparse or quantized tensors, with some suggesting improvements in API semantics, documentation, and testing strategies. There are concerns about performance differences on hardware, correctness of gradient computations in sparse contexts, and reducing memory overhead for certain operations. Additionally, some discussions focus on refining build infrastructure, dependency management, and ensuring proper validation of new features before release, with questions around compatibility, stability, and reproducibility on different platforms."
2022-10-21,pytorch/pytorch,"The discussions highlight several key issues: the rationale for explicit `_RequiredParameter` types versus relying on Python's default keyword requirements; the implementation complexity of uniform stream sampling in stream datasets; the handling of tensor memory formats, specifically `contiguous_format`, and its API behavior expectations; compatibility and correctness of exporting PyTorch models to ONNX, especially with operators like `prim::Layout` not supported in certain opsets; and concerns over build environment challenges, including CUDA and library compatibility, and CI stability. There are questions about potential API improvements, best practices for model quantization, and ensuring backward compatibility and reproducibility across different PyTorch releases. Some unresolved issues concern internal API behaviors, testing strategies for custom extensions, and platform-specific runtime considerations."
2022-10-22,pytorch/pytorch,"The discussions highlight issues with PyTorch-related installations and environments, such as DNS/DNS proxy problems affecting package downloads, deprecated commands (`conda clean --source-cache`), and system-specific hardware or configuration limitations (e.g., IOMMU being enabled causing NCCL timeouts). Some comments address bugs or performance regressions, such as possible slowdowns or incorrect behaviors in tensor operations (e.g., `torch.linalg.cross`, `torch.istft`, and `torch.unique`) due to uninitialized memory, device limitations, or operator support. Several issues involve handling of sparse, masked, or memory format tensors, with suggestions for API clarity, preserving sparsity semantics, and improving API design. Additionally, there's talk about CI system bottlenecks, merge restrictions, and project management concerns, including CLA signing and proper labeling of PRs. Unresolved questions involve operator support on MPS devices, performance regressions, and fixing specific bugs like `RuntimeError` during backward passes."
2022-10-23,pytorch/pytorch,"The discussions highlight persistent issues related to memory leaks and inefficient shared memory handling in PyTorch's data loading and multiprocessing workflows, with specific suggestions like using `deepcopy` in `__getitem__`, adjusting start methods (`spawn` vs `fork`), and manual cleanup strategies. Some contributors emphasize that copying shared memory data significantly impacts performance, prompting ideas such as conditional function calls to optimize speed, and concerns about the support and stability of features like `torch.nn.ModuleList` in TorchScript. Numerous technical questions also focus on the performance bottleneck of `at::col2im` in `torch.istft`, alternative implementations like `unfold`, and potential impact of FSDP and autograd modifications on tracing and gradients. Additionally, there are ongoing discussions on improving operator support for MPS devices, handling of edge cases, and ensuring compatibility across different backends and configurations. Overall, unresolved issues around memory management, operator support, and performance optimization remain central themes."
2022-10-24,pytorch/pytorch,"The comments reflect a variety of issues related to PyTorch's ONNX export functionality, including support for dynamic shapes, operator implementations (such as `MaxUnpool` and `FMod`), and shape inference; workarounds and challenges when exporting models with certain layers or configurations (e.g., `pack_padded_sequence`, `batchnorm` in training mode, or in-place ops). Several discussions suggest evolving the API to better support dynamic or in-place operations, improving error messages, and extending support for new hardware features (e.g., Hopper sm_90). Many issues involve internal complexity, such as maintaining backward compatibility, ensuring correct shape inference, and improving test coverage, especially for complex or less common scenarios. Some comments point toward ongoing work, pending PRs, or internal tracking, indicating a mix of known limitations and active efforts to enhance ONNX support and runtime compatibility. Unresolved questions include how best to support certain operator semantics, support for flexible shape modeling, and ensuring consistent, user-friendly error reporting."
2022-10-25,pytorch/pytorch,"The comments encompass a wide array of technical concerns in the 'pytorch/pytorch' repository, including build and environment issues (e.g., CUDA driver discrepancies, Windows build instructions, and support for multiple Python versions), feature implementation and API questions (notably `register_forward_hook` in C++, spectral normalization in C++ APIs, and support for `register_forward_pre_hook` in C++), and debugging or reproducibility challenges (such as out-of-memory errors, inconsistent speed of `stft`, and testing failures due to environment discrepancies). Several discussions focus on potential improvements or fixes, for example, the handling of `AccumulateGrad` after recomputed forward passes, supporting fast computation for Jacobian diagonals and inter-device RNG determinism, and the integration of special tensor types like NestedTensor or FakeTensor. Additionally, questions about internal workflows (e.g., merging PRs, internal build environments, and testing setup) are prevalent, with some suggestions for modularization or better documentation. Overall, unresolved questions concern compatibility of features with different hardware/OS combinations, API consistency, and maintaining reliable, reproducible workflows amidst environment variability."
2022-10-26,pytorch/pytorch,"The discussions reveal several recurring technical concerns: users report nondeterministic behavior and performance issues with PyTorch, especially related to data loading (`num_workers`), deterministic algorithms (e.g., CTC loss backward non-determinism), and GPU backend (CUDA, MPS, ROCm, NVIDIA, AMD). There are concerns about the stability of some operations on specific devices (e.g., MPS, ROCm, NVIDIA pre-volta), in-place and in-graph behaviors (e.g., in ONNX, in-place scatter_reduce), and API design decisions (e.g., memory_format, functional API, nested tensors). Some suggest fixes involve environment variable tweaks, refining error handling, or architectural changes (e.g., deterministic RNG, API for memory layouts, functional APIs). Unresolved questions include how to unify device-specific nondeterminism, improve reproducibility, and extend support for new hardware and future features like nested tensors, symmetric quantization, and new memory formats."
2022-10-27,pytorch/pytorch,"The comments primarily revolve around handling tensor outputs from models, especially when dealing with multiple outputs or tuple-based outputs, and how to correctly convert or access specific elements in C++. Several users discuss issues with exporting models to ONNX, particularly with operators like `col2im`, and the need to customize or add symbolic registrations for new operators. Other concerns include supporting autograd in nested tensors, supporting complex-valued parameters, ensuring compatibility of PyTorch with different CUDA and CUDA library versions, and resolving model-specific or platform-specific errors (e.g., on iOS, M1, ROCm). There are also discussions about enabling or disabling specific fuse patterns, handling environment-specific build failures, and ensuring reproducibility and deterministic behavior across different hardware and software configurations. Overall, the discussions highlight ongoing efforts to improve model export, runtime support, compatibility, and correctness in diverse environments."
2022-10-28,pytorch/pytorch,"The discussed comments reveal several technical issues and suggestions: dataset download failures (e.g., MNIST 503 errors) and workarounds involving manual downloads; CUDA and PyTorch version compatibility issues, particularly with torchvision and the proper channels for installation; advanced topics such as adding bitwise primitives and binary ops on tensors for efficient kernels; bugs related to `torch.jit.trace` and constant tensor handling in graph fusion, as well as issues with `NestedTensor` autograd support, which may require API changes or additional normalization steps. There are also questions about improving CI robustness, deterministic reductions in CUDA, and support for different hardware and software configurations. Several patches and bugs are being patched upstream, with ongoing discussions on implementation details, API stability, and test coverage. Unresolved questions include handling of getitem support in quantization, and integration of new features like deterministic parallel reduction or argument normalization."
2022-10-29,pytorch/pytorch,"The discussions primarily revolve around troubleshooting and improving PyTorch features, especially in distributed training setups, such as socket timeout errors during multi-node initialization and the deprecation of the `torch.distributed.launch` utility in favor of `torchrun`. There are ongoing performance regressions and regressions in specific tensor operations (e.g., `einsum`) after recent code changes, with detailed benchmarking comparisons indicating slowdown factors in different nightly builds. Users seek guidance on correct tensor quantization handling in ONNX exports and casting issues with MPS backend on macOS, as well as concerns about reproducibility across devices and the impact of certain code modifications on training speed. Additionally, there are ongoing efforts to refine tensor type handling, testing workflows, and CI/CD processes, with some suggestions to streamline label checks and merge procedures. Unresolved questions include fixing performance regressions, clarifying code patches, and ensuring build correctness in diverse hardware environments."
2022-10-30,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's multiprocessing start method conflicts, especially in environments where external modules like Hugging Face datasets trigger RuntimeError exceptions due to existing context settings. Workarounds such as using `multiprocessing.get_context(""spawn"")` and adjusting start methods are suggested, along with notes about limitations when start methods are explicitly set. Several issues relate to build and compatibility problems on different systems, including macOS, RISC-V, and integration with alternative frameworks like JAX, as well as CI test flakiness and error handling. Some comments focus on build configurations, dependency specifications for platform-specific packages, and the need for clearer guidance on compiling or supporting architectures. Overall, these discussions emphasize the importance of robust environment handling, cross-framework interoperability, and clearer documentation to resolve persistent and environment-specific integration issues."
2022-10-31,pytorch/pytorch,"The comments reflect several core concerns, including ongoing difficulties with exporting models with TorchScript to ONNX, especially when using `torch.jit.script` or trace, due to unsupported operators like `col2im`, and the need to support custom operators in export workflows (e.g., `col2im`, `remainder.Tensor_out`). Researchers and developers are struggling with model determinism and reproducibility issues across devices, as well as bugs introduced by recent code changes affecting things like shape inference, precision, and CUDA/RoCM related discrepancies. There are frequent discussions around improving and consolidating API capabilities for sparse, masked, and native tensor types, especially regarding gradients, memory formats, and operator support, alongside ensuring stability of backend code (e.g., linking issues, compiler errors). Additionally, the team is balancing maintenance of legacy code, evolving features like FX normalization, distributed support, and test infrastructure, while managing internal process constraints like labels, CI failures, and review workflows."
2022-11-01,pytorch/pytorch,"The discussions highlight several recurring issues in the PyTorch repository, including the impact of environment-specific or hardware-related bugs on reproducibility and performance (e.g., CPU core misconfigurations, hardware support for certain features like `cudaMallocAsync`, or performance regressions due to API changes). There are concerns about BC-breaking API modifications, such as the changes in the `torch.topk` API and distributed training behaviors, which require careful management of deprecations and user expectations. Several threads mention the need for clearer documentation, better testing, and more robust handling of edge cases, such as floating-point determinism, serialization, and special tensor memory formats (channels_last/3d). Multiple discussions revolve around build system, CI stability, and code correctness issues, often suggesting the addition of new tests, refactoring, or infrastructure adjustments to improve stability and clarity. Finally, some internal or external tooling limitations, deprecation handling, and licensing/CLA compliance are also addressed as challenges to ongoing development."
2022-11-02,pytorch/pytorch,"The discussions highlight several recurring technical issues and areas for improvement:

1. Distributed training hangs or socket timeouts—often related to networking, environment variables (like NCCL and socket interfaces), or cluster configuration—requiring more robust NCCL/env setup guidance and troubleshooting steps.
2. PyTorch interop and compilation issues—such as missing symbols, ABI mismatches, and extension linkage—are tied to environment specifics (CUDA versions, compiler configs) and may necessitate more explicit build instructions or API stability clarifications.
3. Model analysis and tracing challenges, especially with TorchScript/FX, shape inference, and scripting issues, often involve specific limitations (e.g., unsupported operators, dynamic shapes, complex custom models) and require targeted API workarounds or enhanced dev tooling.
4. Internal infra flakiness (e.g., CI failures, instance termination) and external dependencies (like NVIDIA driver versions) create unstable testing environments, highlighting a need for clearer test isolation, environment validation, or more resilient CI configs.
5. Feature deprecations, API changes, and interface improvements (e.g., for lazy modules, tensor subclasses, BC-breaking changes) raise questions about best migration practices, backward compatibility, and the design of public vs internal APIs, with a repeated call for clearer deprecation paths and more comprehensive testing or documentation."
2022-11-03,pytorch/pytorch,"The discussions predominantly revolve around optimizing PyTorch's memory management, especially related to multiprocessing and shared memory handling, with strategies like copying shared dictionaries and manually managing garbage collection. Several questions target specific implementation concerns, such as the implications of sorting eigenvalues in eigen decomposition, and the need for public APIs to manage custom observer insertion for quantization. Other concerns address compatibility (e.g., on MPS backend, CUDA versions), CI failures, and build issues stemming from environment or driver support, especially on Windows and Mac systems. Multiple discussions involve code refactoring for codegen, deprecation policies, and the behavior of operator export and serialization in ONNX. Overall, key unresolved issues include handling of numerical stability and correctness in eigen/svd computations, improving API clarity and stability, and addressing platform-specific performance and compatibility problems."
2022-11-04,pytorch/pytorch,"The discussions highlight issues related to PyTorch's handling of in-place mutations, especially with modules like BatchNorm, where mutation may not be properly captured in computation graphs, affecting tracing and autograd. There is a suggestion to unify mutation semantics for modules like layer_norm and batch_norm and to improve the handling of sparse or structured tensors' gradients, possibly through custom subclasses like MaskedTensor. Several technical concerns involve ensuring consistency between AOTAutograd and Dynamo's path for handling mutations, as well as fixing in-place operations during graph capture, such as resize_as_. Some developers emphasize the importance of maintaining structure-preserving gradients and suggest clearer, more explicit documentation and examples for users, particularly regarding observer insertion, graph capture on specific devices, and supported features. Overall, unresolved questions revolve around correct mutation modeling, the interface for users with custom modules, and ensuring robustness in complex graph transformations."
2022-11-05,pytorch/pytorch,"The discussions highlight issues related to CUDA compatibility, with users experiencing errors due to mismatched CUDA driver and toolkit versions, and a fix was suggested by upgrading PyTorch to a version compatible with CUDA 11.5+ (e.g., 1.11.0+cu113). There are queries about efficiently moving datasets to GPU memory during data loading, with attempts to implement custom `collate_fn` functions, but challenges with pickling and multi-worker setup remain. Several bugs involve missing operator implementations on specific devices like MPS, and efforts are ongoing to address these by setting fallbacks or raising internal assertions, with some fixes pending release. Additional technical concerns include internal assertion violations, precision issues in tensor constraints, and internal API stability, which require code changes and improved error handling. Overall, the discussions focus on ensuring CUDA compatibility, improving data pipeline efficiency, and stabilizing device-specific operator implementations."
2022-11-06,pytorch/pytorch,"The discussions reveal ongoing challenges with TensorBoard's behavior, particularly related to the display and filtering of hyperparameter sets, and inconsistencies in visualization results, suggesting potential front-end issues. Several issues highlight functional gaps and bugs in PyTorch's support for the MPS backend, including unimplemented operators (e.g., `aten::_symeig_helper`, `aten::index.Tensor`, `aten::_ctc_loss`), performance slowdowns, correctness discrepancies, and unsupported operations, often addressed temporarily via fallback mechanisms or environment variables. Some discussions also involve implementation details like data view handling, and updates in PyTorch's codebase (e.g., C++ standards requirements, internal asserts) to improve stability and compatibility. Overall, unresolved questions concern improving support for older macOS versions, enhancing MPS performance and correctness, and fixing visualization inconsistencies. Contributions and bug reports are often hindered by CLA signing issues and lack of comprehensive documentation or guides about current MPS capabilities."
2022-11-07,pytorch/pytorch,"The discussions largely revolve around troubleshooting and improving PyTorch functionalities. Common themes include resolving module loading issues when loading external models, fixing bugs related to ONNX export for specific opsets, and ensuring proper support for advanced features like mixed precision, distributed training, and multi-GPU configurations. Several comments highlight performance benchmarking discrepancies, potential regressions, or API behaviors (e.g., `torch.topk` stability, `bitwise_left_shift` edge cases). Some suggestions pertain to code maintenance, such as clarifying documented behaviors, updating test cases, and managing dependency or environment configuration issues. Unresolved questions include compatibility of certain internal APIs (e.g., `torch._C._will_engine_execute_node()`), proper handling of special cases (like duplicate values in `topk`), and early loading/saving strategies for benchmarking caches."
2022-11-08,pytorch/pytorch,"The discussions highlight several technical concerns including the correctness of gradient computation in multi-stream networks, especially when slicing features before convolution, which can cause crashes or accuracy issues. Several issues involve bugs or limitations in PyTorch's ONNX export, such as support for specific opset versions or behaviors with non-contiguous tensors, and some are addressed via specific protobuf or opset updates. Others focus on framework internals, like in-place mutation handling, serialization of weak references, and ensuring deterministic gradients, which require careful handling of mutability and aliasing in the computational graph. Additionally, there are operational concerns related to CI stability, hardware-specific bugs (e.g., CUDA/cuFFT errors, ROCm support), and documentation or infrastructure updates, some of which involve kernel re-implementations, refactoring, or environment setup. Overall, unresolved questions include ensuring correctness across different hardware/software configurations, the proper handling of in-place operations, and framework features like multi-GPU training strategies and support for dynamic features in export and serialization workflows."
2022-11-09,pytorch/pytorch,"The discussions highlight several issues, including compatibility and build problems across different platforms and compiler versions, especially on Windows with MSVC, and CUDA version mismatches affecting compilation and runtime. There are concerns about the stability and correctness of autograd and autograd-related features such as autograd.grad, autograd.gradcheck, and autograd.grad with complex or backward workflows, alongside the usage of scripting vs tracing in ONNX exports. Several technical suggestions involve improving serialization of hooks, handling of dynamic shapes or lists within the IR and ONNX, and ensuring backward compatibility for modules with hooks and parameter groups. The conversations also touch upon infrastructure issues such as CI flakiness, dependency management, and build failures, with some proposing better testing, error messaging, or configuration fixes. Overall, the key unresolved questions involve ensuring cross-platform robustness, correctness of autograd and export features, and stable build and CI processes."
2022-11-10,pytorch/pytorch,"The comments indicate ongoing discussions and issues related to PyTorch's JIT tracing and export support, particularly around the unsupported `resize_` operation and its trace warnings, as well as tracing errors impacting ONNX model conversion. Several issues involve improving the handling of tensors and configurations in quantization and backend compatibility, including clarifying behavior in `QConfigMapping` and `BackendConfig`, and addressing quantization support for various operators. There are concerns about build process stability, CUDA driver and CUDA version mismatches, and the impact of environment setup, with some discussions about better handling of CUDA initialization time and compatibility across different hardware (e.g., Apple M1/MPS). Reversions and flaky test failures related to CI stability are noted, alongside feature requests for in-memory sharded datasets, nested tensor support, and improvements to reproducibility and determinism, with some contributions involving refactoring and new utility functions. Overall, key unresolved questions include handling unsupported operations during tracing and export, clarifying configuration semantics, dealing with build and environment inconsistencies, and incorporating feature requests for advanced tensor management."
2022-11-11,pytorch/pytorch,"The discussions involve several technical issues and questions related to PyTorch development. Notably, handling of cuDNN errors such as CUDNN_STATUS_MAPPING_ERROR and how to properly disable or configure cuDNN usage. Concerns about improving performance and robustness of certain operations (e.g., `batch_norm`, `embedding_bag`, and quantization workflows), including issues with backends, device compatibility, and correctness verification. Questions around API design choices, such as wrapping modules versus functions for better usability, and adding support for new data types or features like nested tensors, in-memory sharded datasets, and support for new dtype formats in HDF5. Additionally, there are discussions about CI stability, build configuration, and version compatibility, especially for GPU/ROCm or cross-platform issues. Proposed solutions include toggling backend flags, refactoring APIs for clarity, and improving testing/validation approaches."
2022-11-12,pytorch/pytorch,"The discussions primarily revolve around the incremental implementation of `einsum_path` optimization in PyTorch, with an emphasis on adding an `optimize` argument to `torch.einsum` for now, deferring full `einsum_path` support for future requests. Some comments address API confusion and documentation inconsistencies, especially surrounding the negative binomial distribution, highlighting the importance of clear documentation of parameter semantics (failures vs successes). Several issues relate to backend compatibility and hardware-specific problems, particularly on Apple MPS and AMD GPUs, often due to driver overhead or API limitations, with suggestions for alternative approaches like SYCL or ROCm. There are also discussions about test failures, CI infra updates, and potential BC-breaking changes, emphasizing the need for careful testing and backward compatibility, especially in quantization and dynamic shape handling. Unresolved questions largely concern integrating new features into PyTorch 2.0 for future fusion support, hardware-specific optimization, and verification of fixes through added tests."
2022-11-13,pytorch/pytorch,"The discussions primarily revolve around ensuring consistency and clarity in the definition of the Negative Binomial distribution in PyTorch versus SciPy, with emphasis on the interpretation of the parameters (`n` and `p`) and the distribution's support. There are concerns about accurately documenting the differences between success/failure-based parameterizations to avoid user confusion. Additionally, some technical issues are highlighted, such as handling of `logits` versus probabilities in PMF calculations, the impact of fallback paths on specific operations like `native_group_norm_backward` on MPS, and mismatches in testing or code behaviors related to versions or environment-specific quirks. Several discussions point toward refining implementation details (e.g., division behavior, stable top-k on CUDA), improving documentation, and addressing performance or compatibility regressions. Unresolved questions include clarifying the precise definitions used for distributions, ensuring comprehensive test coverage, and planning for native implementations of unsupported operators."
2022-11-14,pytorch/pytorch,"The comments encompass a variety of issues related to PyTorch's development, including compatibility with ONNX exports (specifically when exporting scripted models), dependency management (like NCCL linking and system libraries), and limitations with complex number support. Several discussions focus on debugging and fixing CI/CD pipeline failures, flakiness, and hardware-specific issues such as GPU/ROCm tests and driver bugs. There are multiple feature requests and internal design questions around model exporting, quantization, and operator support (e.g., stable top-k, and support for certain ops in ONNX). Also mentioned are concerns about security (unverified code injection risks) and improvements in testing infrastructure, error diagnostics, and future changes like PyTorch 2.0 integration. Overall, the discussions reflect ongoing efforts to improve compatibility, stability, performance, and feature support within the PyTorch ecosystem."
2022-11-15,pytorch/pytorch,"The discussions cover various issues related to PyTorch's interoperability with ONNX and backend execution, including model conversion fidelity, support for dynamic shapes and specific operators, and compatibility with hardware accelerators like XLA and ROCm. Several reports highlight bugs or limitations in exporting models, operator support (e.g., `atan2`, `scatter_add`, `linalg_matrix_exp`), and runtime errors that often stem from outdated or incomplete support in ONNX or backend implementations. There are ongoing efforts to enhance operator support, shape inference, and runtime robustness, with some fixes merged or under review. Key unresolved questions involve ensuring accurate model export, especially with dynamic or complex operators, and aligning CPU/GPU floating-point behavior in quantization and inference. Overall, these discussions reflect active troubleshooting and development to improve PyTorch's export, execution, and hardware compatibility layers."
2022-11-16,pytorch/pytorch,"The comments cover a wide range of issues, including troubleshooting environment-specific bugs (e.g., data loader deadlocks, onnx export errors due to unsupported operations or dynamic axes), deep dives into specific codebase behaviors (such as custom backward functions, memory management, and codegen practices), and planning for future features (like support for DTensor, PyTorch 2.0 quantization fusion, and documentation updates). Several discussions involve fixing or improving existing functionalities, addressing CI pipeline failures, and managing internal workflows. There are also questions about API design choices, compatibility concerns, and how to modify or extend codegen and build processes—often with an emphasis on better modularity, maintainability, and future-proofing. Overall, the discussions highlight ongoing debugging efforts, feature planning, and infrastructural refinements across the PyTorch ecosystem."
2022-11-17,pytorch/pytorch,"The snippet contains a series of GitHub discussion comments and PR updates from the pytorch/pytorch repository, covering topics such as CUDA and CUDA version compatibility, wheel building for manylinux, interop with C++ extensions, bugs in JIT compilation and tracing, and various CI and build issues. Several discussions revolve around improving build and distribution processes, such as adding new wheel platforms, ABI compatibility, or handling multi-GPU/heterogeneous device support. There are reports of specific runtime errors and bugs like out-of-bounds storage, incorrect numerical results in vectorized operations, and CI failures on certain architectures (macOS, AMD, ROCm). Some comments suggest workarounds, partial fixes, or ongoing efforts to enhance robustness, correctness, and performance, including re-landing PRs after resolving conflicts, and clarifying the impact of upcoming PyTorch 2.0 features. The overall concern is maintaining reliable, compatible, and performant PyTorch builds across diverse platforms, with recurring questions about test results, merge conflicts, and future plans for features like quantization fusion or distributed training."
2022-11-18,pytorch/pytorch,"The comments comprise a variety of technical issues and discussions from the PyTorch GitHub repository, including troubleshooting tips for installation and environment issues, support for features such as dicts in ONNX, and device-specific operator support (e.g., MPS, CUDA, ROCm). Several threads focus on debugging and resolving runtime errors, memory leaks, and performance bottlenecks, often with suggested workarounds like environment variable settings or code modifications. There are recurring concerns about unsupported operators on specific hardware (e.g., MPS, GPU, or custom backends), and ongoing efforts to address these via patches, new features, and test coverage. Additionally, some discussions involve code maintenance, rebase workflows, and CI pipeline management, including handling flaky tests, merge conflicts, and approvals under CLA requirements. The overall focus is on improving robustness, compatibility, and performance of PyTorch across diverse hardware and software environments."
2022-11-19,pytorch/pytorch,"The discussions reveal ongoing efforts to enhance PyTorch's functionality, with specific focus on user-facing features like pre-padding in sequence batching, and internal improvements such as support for diverse tensor memory formats, DTensor abstractions, and fused operations. Several questions are raised about the necessity and implementation details of optional parameters like padding direction, indicating some uncertainty about feature design and user needs. Issues related to build and environment configurations, especially regarding hardware acceleration (e.g., QNNPACK, AVX512, CUDA versions) and package management (e.g., package availability, caching), are also prominent. Additionally, there are concerns about test coverage, stability of ongoing features, and the integration of internal tooling (e.g., Kineto, inductor). Overall, the discussions emphasize balancing feature development, stability, and clarity in documentation while navigating infrastructure and compatibility challenges."
2022-11-20,pytorch/pytorch,"The discussions primarily revolve around improving build and compatibility issues within PyTorch, such as resolving mismatched CUDA versions, ABI compatibility, and linking against MKL libraries correctly using modern CMake practices. Several issues highlight performance regressions in functions like torch.median and top-k, suggesting updates and optimizations in later versions. Challenges with unsupported operators on specific devices (e.g., MPS, CUDA, or hardware-specific backends) are frequent, alongside ongoing questions about correct experimental features, operator implementations, and behavior of unrecognized kwargs. There are also concerns about documentation workflow approvals and the reproducibility of problems in different environments, especially regarding package downloads, runtime errors, and internal build failures. Overall, the discussions focus on stability, performance, and compatibility improvements, with some unresolved questions about proper integration and backend support."
2022-11-21,pytorch/pytorch,"The comments reveal several technical concerns: (1) confusion around the correct usage and expected behavior of `DistributedDataParallel.no_sync()`, with suggestions to improve tutorials; (2) reproducibility issues with complex functionalities such as complex number support, model exporting, and certain features on different PyTorch versions, hardware, or environments, along with requests for minimal reproductions; (3) implementation and correctness debates regarding parallel reduction determinism, backend support, and floating-point precision issues; (4) the need for clearer error messages, better documentation, and potential deprecation or clarification around certain functions or behaviors; (5) questions about internal changes, codegen adjustments, submodule updates, and version-specific behaviors, with ongoing discussions about backing out or refining certain PRs and addressing platform-specific issues."
2022-11-22,pytorch/pytorch,"The discussions primarily revolve around aligning the behavior of PyTorch's resizing and interpolation operations with TensorFlow, highlighting differences in src/dst index mappings and pixel center considerations, with suggestions involving parameter adjustments like `half_pixel_centers`. Several issues address framework and API design concerns, such as the handling of complex number support, the necessity of explicit linking against `libtorch_python`, and API improvements like adding a `named_optimizer` mechanism or supporting tensor memory formats in functions like `torch.cat`. Other concerns involve improving error messaging, supporting platform-specific details (e.g., libcaffe2 vs. libc10 on different architectures), ensuring consistency across device types and operator implementations, and addressing failures in CI, bugs in distributed training, and deployment configurations. There are also ongoing discussions about function behavior (e.g., `torch.no_grad` as a decorator), adding test coverage, and managing version compatibilities or platform-specific features (e.g., CUDA/cuDNN interplay, M1 GPU support). Overall, the focus is on resolving framework inconsistencies, platform support, API ergonomics, and CI stability to enhance PyTorch's usability and robustness across environments."
2022-11-23,pytorch/pytorch,"The discussions encompass a range of technical concerns related to PyTorch, including the handling of tensor memory formats (e.g., NHWC vs NCHW, channels_last flags), operator compatibility and optional parameters (e.g., optional window arguments in spectrogram functions), and specific bug fixes or performance optimizations (e.g., small vector kernel efficiency, CUDA memory leak detection). Several issues address framework design choices, such as the usefulness of adding explicit `channels_last` flags versus automatic inference, and the implications of shape and dtype mismatches in operations like `linear`. There are also concerns about flaky CI tests, unsupported dist operators in ONNX export, and stability of parallel or distributed workflows. Additionally, some discussions touch on the future support of new data types (e.g., quantized and bit-packed types), and the importance of clear, actionable error messages to improve user experience."
2022-11-24,pytorch/pytorch,"The discussions span a variety of technical concerns including environment/environment setup issues affecting TPU and CUDA operations, discrepancies in batch normalization behavior between PyTorch and TensorFlow, and handling of models in eval mode with BN parameters. Several comments address the support and implementation of sub-byte data types (e.g., int4, 5-bit) for tensor communication or storage, emphasizing the need for explicit dtype support versus encoding semantics in operators. There are ongoing efforts to improve kernel support, tensor format handling (e.g., NHWC vs NCHW), and operator integration, with questions about environment configurations, bug fixes, and performance optimization (e.g., memcpy vs vectorized loops). Additionally, some discussions involve debugging and extending the framework's features such as sparse matrix operations, onnx export quirks, and support for new hardware backends. Overall, unresolved issues include ensuring environment stability across platforms, refining model deployment behaviors, and expanding dtype and operator support."
2022-11-25,pytorch/pytorch,"The discussions cover several technical concerns, including the integration of a fundamental feature (Issue #16897), issues with CUDA and profiler libraries (Issue #65393), support for NCCL on Windows, and handling of unsupported operators on the MPS device (Issue #77764). There are questions about improving error messaging related to tensor proxies (Issue #89513) and managing build failures or flaky tests, sometimes involving manual intervention or rebase actions. Some discussions emphasize the importance of comprehensive testing, including OpInfo tests, and clear documentation practices. Overall, unresolved issues involve platform support limitations, compatibility of features across architectures, and ensuring stability during code merges and profiling."
2022-11-26,pytorch/pytorch,"The discussions highlight several technical concerns within the PyTorch ecosystem, including environment configuration issues affecting Jupyter kernel registration, and system-specific runtime errors such as cuDNN mapping errors related to CUDA versions and hardware compatibility. There are ongoing efforts to implement features like returning attention matrices in transformer modules, with various suggestions on API design and whether to hook into underlying kernels like `gather`. Multiple issues center around platform-specific support, especially for Apple Silicon's MPS backend, where operator support for operations like `aten::linalg_inv_ex.inverse` is lacking, prompting contributions to extend these capabilities. Some discussions also address stability problems due to multithreading or background process management, notably in profiling tools like Kineto, and handling flaky test failures. Unresolved questions include ensuring consistency of error messages across hardware, managing multithreading impacts, and identifying appropriate fixes to system-specific bugs."
2022-11-27,pytorch/pytorch,"The discussions primarily revolve around updating and maintaining proper citation mechanisms for PyTorch and its components, such as torchvision, including integrating proper BibTeX entries and automating citation updates. Several issues address ongoing development and bug fixes, notably adding support for specific operators (e.g., aten::linalg_inv_ex.inverse), resolving schema inconsistencies, and streamlining backend alias annotations, often with PRs and code reviews. There is interest in improving model export workflows, particularly converting models to ONNX using tracing and scripting, with some work on supporting newer ONNX opset versions and export validation through tests. Additionally, various issues highlight CLA signing concerns, CI build failures, and efforts to rebase or cherry-pick changes to master for cleaner CI results, along with requests for test additions and better contributor onboarding. Overall, progress hinges on merging updates, fixing operator support and schemas, ensuring proper licensing compliance, and enhancing export and citation tooling."
2022-11-28,pytorch/pytorch,"The comments highlight ongoing issues with PyTorch's handling of certain operations, such as numerical instabilities in matrix functions, support for complex data types, and specific operator behaviors (e.g., `scatter_add`, `pow`, `where`) in both CPU and CUDA contexts. Several discussions involve potential regressions versus expected behavior, and the need for thorough testing, documentation updates, and improvements in error reporting. There are technical considerations around compiler and backend support (e.g., `c10::complex` in CUDA, guard generation for dynamic shapes, and operator support for different backends), as well as adoption issues related to CLA signing and CI stability. Some comments suggest adding missing features (e.g., batching rules, support for specific ops in ONNX), while others discuss potential architectural or maintenance changes, such as renaming internal modules or adjusting guard mechanisms. Unresolved questions include handling support for specific operators in certain environments, improving error messages, and prioritizing stability and correctness over optimization complexities."
2022-11-29,pytorch/pytorch,"The comments span various topics, including modifications or enhancements to convolution matrix operations, support for specific CUDA versions, handling of warnings, and issues with PyTorch's ONNX export compatibility, especially with operator support like `aten::_scaled_dot_product_attention`. Several discussions involve bug regressions, particularly related to ONNX support and operator compatibility, with some fixes being patched and awaiting release, while others remain open questions (e.g., fallbacks, guard mechanisms, runtime type support). The conversations also include infrastructure-related concerns such as flaky CI tests, driver support updates, and build configurations, as well as efforts to improve code structure, test reliability, and security issues (e.g., unsafe functions, memory leaks, and user warnings). Overall, the dialogues reveal ongoing maintenance challenges, feature requests (like dynamic convolution kernels, tensor packing, and optimizer fixes), and the need for better tooling, testing, and documentation to handle both current bugs and future enhancements. Unresolved questions include precise ETA for certain bug fixes, detailed reasoning behind some architectural decisions, and potential impacts of deprecated features or operator support levels."
2022-11-30,pytorch/pytorch,"The comments highlight issues related to the reproducibility of certain errors, particularly around DataLoader settings like `pin_memory`, `persistent_workers`, and multi-threaded data loading, with some suggesting the problem occurs when worker processes are terminated prematurely. Several discussions address unsupported operations or features, such as MPS backend limitations, lack of Python 3.11 support, and missing CUDA/cuDNN support, with workarounds or future support plans noted. Other recurring concerns involve correctness and stability of new features like quantization, complex gradients (notably for `torch.pow` at zero), and custom type casters, alongside the need for better testing, documentation, and code maintainability (e.g., unification of SyncBatchNorm and error-prone guard checks). Some threads involve merge conflicts, rebase challenges, or infrastructure issues, with external dependencies like CUB bugs and compiler behavior affecting stability. Overall, the developers suggest ongoing improvements, careful handling of unimplemented operators, and clearer support policies, especially for experimental or hardware-specific features."
2022-12-01,pytorch/pytorch,"The comments reveal multiple recurring technical concerns and questions. Key issues include ensuring correct CUDA and PyTorch version compatibility, particularly regarding CUDA-aware features and ONNX export performance with different opset versions and PyTorch releases. There are a number of build and test failures, some related to CI infrastructure, flaky tests, or environment mismatches, raising questions about reproducibility and robustness. Discussions also touch on adding new features such as zero-copy DTensor APIs, and the correctness of gradient behaviors at boundary cases like `xlogy` at zero, as well as ensuring that functions like `torch.einsum` and `embedding_bag` handle edge cases safely. Unresolved questions include handling of unimplemented features, support for variable input argument types, and the safety and cleanup of native memory allocations, especially related to in-place modifications and custom ops."
2022-12-02,pytorch/pytorch,"The discussions primarily focus on various technical challenges and proposals related to PyTorch's hardware backend support and internal development processes. Key concerns include simplifying device specification for private use and OpenCL devices, handling memory leaks during JIT tracing, improving reproducibility in GPU RNGs for certain sampling operations, and addressing failures in automatic code generation or model serialization (e.g., ONNX export, quantization-related patterns, and shape inference). There are suggestions for refactoring code (like removing unused first arguments), introducing new optimized operators, enhancing testing coverage, and clarifying implementation details (e.g., pointwise operator definitions). Unresolved questions involve ensuring backward compatibility, debugging complex kernel and stream synchronization issues, and better managing test regressions or failed CI checks. Overall, the dialogue reflects active efforts to stabilize, optimize, and extend PyTorch’s multi-backend support, while also addressing internal tooling and build system hurdles."
2022-12-03,pytorch/pytorch,"The comments reflect ongoing efforts to enhance PyTorch's OpenCL and device dispatch support, with suggestions for implementing a `torch_function` mode to translate opencl strings into privateuse space for better backend support and human-readable naming. Several issues and questions are raised regarding the support and performance of Apple Silicon and Intel GPUs, including support for Apple Silicon processors and support for 16-bit floating point, as well as support for 32-bit Windows programs with CUDA. There are technical discussions about the design of operators, naming conventions for pointwise operators, handling of symbolic expressions in the new inductor code, and ensuring the correctness and completeness of operator lists for inductor and autograd optimizations. Additionally, some discussions address ongoing test failures related to memory consumption, codegen bugs, and build environment issues, with some patches and workarounds being considered or proposed. Unresolved issues include supporting OpenCL support with human-readable names, maintaining an up-to-date list of pointwise operators, fixing symbolic shape errors in codegen, and resolving memory leak and build failures in specific environments."
2022-12-04,pytorch/pytorch,"The discussions highlight concerns about PyTorch's distribution and installation processes, particularly around CPU vs. GPU binaries on PyPI, and the lack of support for GPU binaries in Windows and Mac environments. There are ongoing issues with memory leaks during JIT tracing, especially when repeatedly compiling models within the same process, which are suggested to be addressed by garbage collection or process isolation. Several issues mention compatibility and performance regressions related to CUDA versions, cuDNN settings, and hardware-specific behaviors, including system configuration problems like IOMMU and I/O rendezvous on clusters. Questions also arise about improving ONNX export consistency for certain operations, handling batch norm conversion warnings during ONNX export, and establishing better tooling or APIs for users to select hardware-optimized installs or clear memory caches. Overall, unresolved technical challenges revolve around distribution practices, memory management during model tracing, hardware compatibility, and export reliability."
2022-12-05,pytorch/pytorch,"The discussions reveal a variety of concerns about platform-specific support and performance optimization, including the need for cross-platform APIs like SYCL, and vendor-specific kernels for platforms like Apple Silicon and DirectML. There are technical questions about the behavior of certain operators, such as `matrix_power`, `pow`, and `autograd.Function`, with suggestions for clearer documentation and default behaviors. Several issues involve performance regressions, benchmarking, and reproducibility concerns, indicating a focus on maintaining efficiency across hardware. Some discussions touch on code maintenance, such as test deduplication, handling of default compiler flags, and build infrastructure stability. Unresolved questions include the timing of feature releases, support for newer Python versions, and the correctness of specific operator behaviors regarding autodiff and memory management."
2022-12-06,pytorch/pytorch,"The comments reflect ongoing discussions about improving cross-platform GPU backend support in PyTorch, highlighting the advantages of vendor-neutral APIs such as OpenCL and SYCL versus vendor-specific solutions like DirectML, Metal, or vendor intrinsics. There is a push toward leveraging generic, high-level APIs for broader hardware compatibility and future-proofing, despite current vendor-specific kernels being heavily optimized. Debates also include technical considerations such as the static versus dynamic kernel compilation, the complexity of supporting distinct intrinsics (e.g., tensor cores, M1 instructions), and the potential for unified kernel implementations that abstract over vendor-specific instructions. Additionally, there are concerns about build infrastructure, such as ensuring proper environment variable management and support for reproducible, cross-platform binary releases. Unresolved questions center on whether to invest in vendor-agnostic approaches like SYCL, or to prioritize vendor-specific optimization, and how to navigate the trade-offs between flexibility, performance, and development effort."
2022-12-07,pytorch/pytorch,"The discussions encompass various concerns including the need for better documentation and environment management for reproducibility (e.g., CUDA and package installation issues), the importance of clear API stability versus BC-breaking impacts, and the necessity for precise, well-tested modifications especially related to quantization, sparse tensor conversion, and runtime behavior. Several comments highlight the importance of avoiding code duplication in tests, maintaining consistent and stable numerical results across CPU and GPU, and ensuring compatibility with evolving backend features such as PyTorch 2.0 quantization and TorchScript. Issues around environment-specific bugs, potential memory leaks, and ensuring proper support for new features like autograd, onnx, and dynamo are also persistent themes. Many discussions involve review and rebase workflows, emphasizing the need for clear change management, proper review approvals, and attention to flaky CI failures."
2022-12-08,pytorch/pytorch,"The discussions highlight several key issues in the PyTorch codebase and development process. Notable concerns include handling of data types in MKL support for ILP64, potential BC-breaks from API changes, and the implications of in-place tensor mutations with fake tensor mode. There are also recurring questions about GPU driver and environment setup, especially compatibility and reproducibility issues across various hardware and software configurations. Some discussions focus on specific bug fixes, performance regressions, and CI stability, with proposals for better dependency management, version control, and test automation to prevent regressions. Unresolved questions include the future of API stability regarding tensor types, handling of complex autograd, and infrastructure improvements for build and test reliability."
2022-12-09,pytorch/pytorch,"The discussions highlight a variety of issues encountered across the PyTorch codebase, including memory leaks, numerical stability concerns in backend operations (e.g., linear algebra routines), inconsistencies in distributed testing on different hardware and configurations, and challenges with support for extensions and third-party dependencies (like custom C++/CUDA ops or specific device backends like MPS and XLA). Several questions revolve around the correctness and stability of numerical algorithms under different backend implementations, as well as how to properly support and test extensions and special tensor behaviors (including fake tensors and in-place modifications). Other concerns include ensuring API stability and documentation consistency, managing environment-specific issues (e.g., platform-dependent compiler support, environment settings), and handling CI failures related to platform and dependency mismatches. Overall, unresolved issues include differences in numerical results across configurations, supporting support of extensions (like custom C++ kernels), and improving testing and documentation practices to catch regressions earlier."
2022-12-10,pytorch/pytorch,"The collected comments primarily revolve around troubleshooting and improving features related to PyTorch's build modes, RNG precision, and operator support. Discussions highlight issues with debugging builds (Debug vs. Release effects), numerical precision discrepancies, and exposing distribution functions. Several threads concern performance optimizations, such as JIT kernel auto-generation, memory visualization tools, and efficient use of quantization and fusion techniques. Others address build and compatibility problems across various backends (XLA, MPS, ROCm) and dependencies (ONNX, ideep), including handling backward-incompatibilities, BC-breaking changes, and internal infrastructure adjustments. There's also ongoing effort to enhance support for serialization, distributed training, and low-level kernel implementations, along with managing internal policies like code quality checks."
2022-12-11,pytorch/pytorch,"The discussions highlight concerns about the appropriateness of leaving certain bugs open, the need for clearer guidance on handling symbolic or uncertain shapes in torch's JIT and inductor, and the importance of guard checks to ensure correctness during compilation. Several comments address specific technical issues such as the handling of low entropy in uniform sampling, model loading errors in C++, and the proper use of guards and decompilation in the presence of symbolic shapes or autodifferentiation. There are questions about the impact of certain warnings, how to implement more robust guard mechanisms, and whether certain optimizations or fallbacks are correct. Overall, unresolved issues involve improving accuracy, robustness, and clarity in handling symbolic shapes, guards, and code generation."
2022-12-12,pytorch/pytorch,"The discussions highlight the challenge of optimizing matrix multiplication across multiple hardware vendors and backends, with emphasis on leveraging vendor-specific intrinsics like tensor cores. There is interest in exploring cross-vendor solutions like SYCL, but skepticism about its flexibility and performance compared to OpenCL, as well as concerns about current vendor-specific support and dynamic kernel compilation. Contributors also discuss the potential of using unified approaches for vendor instructions and the necessity of dynamic kernel building in PyTorch's backend library. Several comments touch on issues related to performance regressions, hardware-specific compilation and support, and the need for better tooling, testing, and balancing flexibility with efficiency. Unresolved questions include how to create universal kernels compatible with evolving hardware, how to improve support for various accelerators, and how to streamline development workflows across diverse platforms."
2022-12-13,pytorch/pytorch,"The comments reflect various ongoing discussions and issues related to PyTorch, including debugging and resolving errors with gradients (`param.grad` being `None`), export to ONNX and differences between PyTorch and ONNX operations, performance regressions and profiling in TorchScript, and specific stability or accuracy issues in certain models like stable diffusion. Key technical concerns involve ensuring correct API design (e.g., for public API declarations), compatibility and correctness of numerical operations (like `log1p`, `expm1`, and handling of `bfloat16`), addressing flaky tests in CI, and complex issues like in-place operations with tensors requiring grad, meta vs device tensors, and backward/in-place mutation incompatibilities. Several questions are raised around improving testing, documentation, and performance benchmarks, as well as refactoring challenges in core components (like mode management, inlining, and operator support). A common theme is balancing API stability and flexibility with backward compatibility, especially around custom operations, tensor subclasses, and tensor metadata, often coupled with debugging of failures in distributed or hardware-specific contexts."
2022-12-14,pytorch/pytorch,"The comments reflect ongoing discussions about various technical challenges in PyTorch development, including issues with build configurations (e.g., enabling/disabling OpenMP, compiler flags like -fPIC, and handling large symbol tables), CI flakiness and flaky tests, and specific failures in features like quantization, autograd, and inductor compilation. Several discussions focus on improving code robustness and performance, such as merging refactors separately, refining compiler and runtime behavior, and handling sparse tensors or custom kernel registration more effectively. There are recurring themes around test stability, including disabling flaky tests temporarily and considering better mechanisms like tagging or warnings instead of errors, as well as managing codebase consistency with auto-formatting and build system configurations. Unresolved questions include how to best streamline validation checks for user vs. internal code, how to prevent build and testing failures from environment or configuration issues, and how to evolve the codebase architecture to support more reliable, scalable auto-tuning and compilation workflows."
2022-12-15,pytorch/pytorch,"The comments encompass a range of issues including segmentation faults linked to environment conflicts (notably with TensorBoard and protobuf versions), so-called trunk flakiness and intermittent CI failures, and specific bug reports on functionalities such as sparse tensor conversions, CUDA behavior with complex types, or particular operator support. Several discussions suggest workarounds, like rearranging import orders, patching low-level data structures, or patching dataloaders, while others document ongoing fixes, improvements in pattern matching or operator registration, or enhancements in testing strategies. Support requests include verifying if certain features like automatic differentiation for complex types are supported, or if certain behaviors (like the placement of dropout in attention) are correct per frameworks like Huggingface or Google. There’s also a recurring theme of dealing with flaky internal tests and CI instability, with some proposed solutions involving rerunning, disabling flaky tests, or modifying CI configurations. Overall, the key concerns involve environment stability, operator and operator pattern support, and ensuring correctness and robustness in complex, dynamic, and distributed scenarios."
2022-12-16,pytorch/pytorch,"The discussions reveal pervasive out-of-memory (OOM) issues during training, often sensitive to small hyperparameter adjustments like batch size, which complicates debugging. Multiple users experiment with cache clearing, batch size reduction, and data preprocessing to mitigate OOMs, indicating typical strategies are often insufficient on constrained hardware. Others note that memory fragmentation and GPU fragmentation are contributing factors, suggesting that more advanced memory management or fragmentation-aware approaches might be needed. Several comments discuss the challenge of maintaining code and tests with evolving hardware and software dependencies, alongside the importance of robust CI testing to catch regressions early. Unresolved questions include the best practices to handle fragmentation, the feasibility of automatic memory optimization, and improving reproducibility of performance benchmarks across different hardware setups."
2022-12-17,pytorch/pytorch,"The discussions raise concerns about the accuracy and correctness of gradients, especially in relation to log_softmax and its backward implementation, as well as the impact of quantization and GPU hardware specifics (e.g., Tensor Cores on Nvidia GPUs, AMD and Intel GPU driver overheads, and MPS fallback behavior on Apple Silicon). Several comments focus on the need for improved testing, including the difficulty of reproducing bugs across different hardware and software configurations, and the challenge of ensuring consistency between CI and local results. Updates on framework features like support for mixed precision, handling of deprecated operations, and the organization of XLA integration suggest ongoing efforts to improve performance and reliability across platforms. Reversions and build failures indicate active maintenance and troubleshooting, often strongly tied to hardware-specific or configuration issues. Unresolved questions include support for newer Python versions, CUDA 12, and platform-specific optimizations, as well as procedural aspects like code organization and test coverage."
2022-12-18,pytorch/pytorch,"The discussions cover multiple technical concerns related to PyTorch development. Key issues include the support and documentation for enabling FlashAttention in training and inference, especially on different platforms like Windows and macOS, with some reports of missing wheels or platform support for Python 3.11. There are ongoing efforts to optimize backward passes, notably through xFormer's implementation, and questions about best practices for DDP usage during evaluation. Additionally, some PRs and changes have encountered build failures, flaky tests, or require rebasing; there are also discussions about ensuring compatibility and correct error handling in operators like `uniform_`, as well as API design considerations around internal debugging tools and exposing PyTorch internals safely. Unresolved questions involve platform-specific support status, build environment issues, and ensuring stable integration of new features."
2022-12-19,pytorch/pytorch,"The discussions mainly revolve around improving documentation consistency (e.g., missing entries for MultiheadAttention), feature requests (adding dilation in max pooling, moving dataloaders to GPU memory), and performance optimizations (fusion of tanh and linear layers, kernel tuning). Several questions address the support and stability of recent features (e.g., complex-valued losses, support for complex inputs, multi-GPU/multiprocessing issues, CUDA version compatibility). There are recurring concerns about test stability, flaky CI, and proper handling of specific hardware backends like ROCm, NVIDIA GPUs, and ARM CPUs, including memory management and patching related to CUDA and cuDNN. Some suggestions target API usability, such as exposing debugging tools effectively or automating backend configuration selection to simplify user workflows. Unresolved issues include detailed performance impacts for new features, compatibility across hardware/software environments, and ensuring test coverage aligns with production stability."
2022-12-20,pytorch/pytorch,"The discussions highlight several technical issues and questions: challenges exporting PyTorch models to ONNX, especially involving non-contiguous inputs and symbolic shape inference; differences in numerical stability and accuracy when solving linear systems with ill-conditioned matrices across backend implementations like MKL, cuBLAS, MAGMA, with suggested workarounds and backend reporting; complexities around CUDA and MPS device behaviors, including memory management, tensor contiguity, and support for specific operations; questions about improving testing infrastructure, including increasing timeouts, handling flaky tests, and better capturing error contexts; and considerations for API and naming conventions, such as renaming functions for clarity, managing forward and backward pass behaviors, and integrating with documentation workflows. The discussions reflect ongoing efforts to understand low-level implementation details, improve robustness, and streamline development tooling."
2022-12-21,pytorch/pytorch,"The discussions highlight several technical issues and questions encountered across different aspects of the PyTorch codebase and build environment. Key concerns include ensuring proper sequencing of environment variable settings for CUDA and GPU device initialization, managing subprocess and extension build errors (e.g., related to multithreading, multiprocessing, or extension loading on various hardware and OS), and handling backend-specific behaviors (such as numerical stability for LAPACK/BLAS routines with ill-conditioned matrices). There are also questions about build reproducibility and correctness, especially with mixed CPU-GPU setups, and about maintaining compatibility across different hardware architectures (x86, ARM, ROCm, macOS, etc.) and package distributions (conda-forge vs PyTorch channels). Additionally, concerns about test flakiness, infrastructure maintenance, and ensuring code and documentation consistency are discussed. Overall, unresolved questions include backend stability for numerical routines, environment setup order, and the need for clearer documentation on supported hardware and recommended build practices."
2022-12-22,pytorch/pytorch,"The discussions highlight several areas of concern: issues arising from multiprocessing contexts in PyTorch, especially with 'fork' vs. 'spawn'; the complexity of integrating custom DLLs in C++ projects; updates to distributed tensor sharding APIs; performance and correctness considerations for certain kernels and operations; and various CI, build, and testing challenges including flaky tests, flaky infra failures, and regressions. There are questions about benchmarking procedures, potential BC-breaking changes, clarifications needed for feature usage, and requests for additional tests or documentation, particularly around new features and experimental code. Some issues relate to compatibility across Python versions and hardware backends, as well as the ongoing maintenance of submodules, CI workflows, and code refactoring efforts. Overall, the comments reflect active troubleshooting, feature development, and infrastructure improvements, with a need for further validation, review, and documentation to resolve specific technical uncertainties."
2022-12-23,pytorch/pytorch,"The discussions highlight several issues including inconsistent API design in learning rate schedulers, especially for multi-parameter groups, and the desire for a unified `get_lr` method. Users express challenges with negative stride indexing in tensors, numpy, and PyTorch, suggesting `torch.flip()` as a workaround, but note limitations with complex slicing. There are ongoing efforts to improve deterministic behavior for cuDNN in CTC loss, clarify environment configurations, and address flaky tests that cause CI failures, often involving platform-specific disabling or retries. Some technical proposals involve patching internal classes, leveraging `pytree` for better function transformations, or exposing utility functions for memory reporting. Unresolved questions include proper handling of environment flags, defining safe and consistent APIs, and managing long-term test stability."
2022-12-24,pytorch/pytorch,"The discussions primarily address issues related to memory management in PyTorch's DataLoader, with solutions such as deep copying shared memory objects, and clarify that using numpy arrays for internal objects only resolves the problem when using the ""fork"" start method. There are questions about compatibility when building or using libtorch with different CUDA versions, and specific errors encountered on macOS or MPS backend due to unsupported data types like complex numbers and int64, especially in FFT and reduction operations. Other concerns include ensuring backward compatibility when adding API hooks, improving support for certain operations (like upsampling and reduction), and handling unsupported features in specific backends (e.g., MPS, CUDA). Additionally, several issues relate to ONNX export compatibility, JIT tracing, and the influence of internal implementation details on user-facing features, with some discussions about fixing or improving these pipelines. Unresolved questions involve enhancing support for non-primitive data types, ensuring consistent behavior across different hardware and runtime environments, and addressing build/test failures on specific platforms."
2022-12-25,pytorch/pytorch,"The discussions highlight several key issues: (1) Dependency and build environment challenges, such as missing `typing_extensions` and networking issues on Windows with IPv6, suggesting environment-specific solutions and the use of Docker/WSL2. (2) Discrepancies and limitations in PyTorch's operator support, notably errors with MPS on Mac and unimplemented operators (`aten::remainder.Tensor_out`), raising questions about fallback mechanisms and the need for proper operator implementation for symbolic inputs. (3) Quality and correctness of features like `FloorDiv`, which currently do not support certain inputs correctly, with suggestions to implement robust symbolic value checks or allow lossy operations. (4) CI stability issues, build failures, and test skips for specific platforms, emphasizing the need for better testing infrastructure and conditional test execution. (5) Specific feature requests and proposals, such as aligning `istft` normalization with librosa or adjusting SpectralOps to handle zero window sums, indicating ongoing refinement of PyTorch's API behaviors."
2022-12-26,pytorch/pytorch,"The discussions highlight several core issues including the need for safer and more efficient shared memory handling in DataLoaders, suggesting the use of deepcopy instead of shared memory to prevent memory leaks. There are ongoing challenges with GPU support and device-specific operator implementations, such as improving MPS fallback behavior and supporting operators like `svd` for ONNX export. Compatibility and automatic differentiation for complex data types, as well as ensuring correct behavior of functions like `torch.stft`/`torch.istft` with padding and windowing, are also key concerns. Additionally, some discussions involve extending PyTorch's API, like support for named tensors or nested tensor structures, and improving testing and CI workflows to prevent regressions."
2022-12-27,pytorch/pytorch,"The discussions raise issues related to improving memory management during data loading, with suggestions like using `deepcopy` or transitioning to `torch.Tensor` to avoid leaks, emphasizing the need for consistent utility support for RAM usage profiling. Several threads focus on advanced PyTorch features such as Hessian computation with `functorch`, handling of custom hooks, and support for new backends or hardware (e.g., Vulkan, CUDA enhancements). There are recurring concerns about CI failures and flaky tests, often tied to infrastructure or environment inconsistencies, with some PRs being reverted or stalled due to unrelated errors. Specific technical questions include whether to remove device IDs from kernel APIs, better support for nested or first-order tensor structures, and clarifications on distribution parametrizations and their implications. Overall, the discussions reflect ongoing efforts to optimize performance, expand hardware support, and stabilize the development workflow, with some unresolved issues around testing stability and backward compatibility."
2022-12-28,pytorch/pytorch,"The comments largely reflect ongoing issues related to PyTorch's serialization, multi-GPU and distributed training stability, and internal test flakiness. Several discussions highlight specific bugs or failures, such as pickle compatibility across Python versions, CUDA kernel issues, and OOM errors, often with suggested workarounds or investigation steps. There are concerns about flaky tests, CI stability, and how certain features (like in-place operations, autograd behavior, or inductor decompositions) are handled or deployed, sometimes with proposals for code changes or additional diagnostics. Some comments address build and environment compatibility, especially for CUDA and Python versions, and the need for better tooling or documentation. Overall, the discussions mix bug fixes, testing challenges, and planning for future improvements in PyTorch's robustness and performance."
2022-12-29,pytorch/pytorch,"The discussions highlight several technical areas: first, the need to consolidate batch normalization operations across different backends to prevent redundant work and ensure consistency; second, persistent issues with multiprocessing in data loading, particularly with shared memory limits, CUDA interactions, and setting appropriate start methods; third, the potential for Vulkan support on desktop Linux to expand PyTorch's portability, with considerations about API differences and driver support; fourth, the importance of accurate testing and coverage, including re-enabling disabled tests and managing flaky CI runs; and fifth, the challenges in extending autograd and tensor subclass behaviors—specifically, ensuring that mode stacks and dispatch mechanisms handle subclasses, fake tensors, and functionalization correctly without mode or dispatch skipping unintended ways. Overall, the discussions revolve around reliability, portability, and correctness improvements in PyTorch's core and tooling infrastructure."
2022-12-30,pytorch/pytorch,"The discussions highlight several recurring issues: freezing or deadlocks during data loading or augmentation, especially with multi-worker setups; compatibility and testing challenges across different OSes (notably macOS), backends (MKL, OpenBLAS, Caffe2), and hardware (M1, ROCm); incomplete support for certain operations (e.g., `aten::chain_matmul`, complex FFTs on MPS, certain operators on MPS/backend fallbacks); and code maintenance concerns regarding API consistency (e.g., `ignored_modules` vs `ignored_parameter_names`, constructor argument unification). Several proposals suggest refactoring for robustness, such as enhancing CI coverage, exposing more flexible APIs, or fixing specific operator implementations. There are also ongoing discussions about the expected semantics of dispatch behavior, security considerations with script execution, and evolving internal API stability. Unresolved questions include whether to merge or deprecate certain parameters, how to handle platform-specific code guards, and ensuring that test coverage remains comprehensive."
2022-12-31,pytorch/pytorch,"The discussions highlight ongoing challenges with handling Python `dataclasses` in PyTorch's distributed and data-parallel APIs, especially regarding serialization and compatibility with backends like NCCL and GLOO. There are specific issues related to errors like `RuntimeError: Only Tensors created explicitly by the user support the deepcopy protocol` when converting dataclass outputs, suggesting the need for improved support or alternative data structures. Several comments express concerns about platform-specific limitations, such as missing operator implementations for MPS on Apple Silicon, and relevant workaround strategies. Additionally, some discussions address code maintenance tasks like rebasing PRs, managing CI failures, and handling license or CLA requirements. Overall, the community seeks more robust support for Python data structures in distributed contexts, platform-specific operator implementations, and streamlined contribution workflows."
2023-01-01,pytorch/pytorch,"The discussions highlight several core challenges and proposals:

1. Packaging and distribution of GPU dependencies: There is concern that large CUDA assets (~2.4GB) hinder distribution via PyPI, with suggestions to dynamically load large shared objects (.so/.dll files) outside of the main package, and to improve CUDA dependency preloading to mitigate installation issues.
2. Implementation of support for missing CUDA operators on certain devices (e.g., MPS on macOS): Contributors request the addition of operators like `aten::remainder.Tensor_out` for MPS, with some pointing to the need for backend support.
3. Framework stability and testing practices: Multiple comments address test organization, particularly splitting large PRs, handling header changes, and managing BC-compatibility.
4. API and internal design discussions: Debates on unifying ignored parameter/module arguments, simplifying constructor arguments, and managing ignored states for modules, emphasizing a balance between usability, backward compatibility, and long-term API clarity.
5. Handling of device-specific memory errors and exception cleanup: Suggestions include better error handling to avoid memory leaks during out-of-memory situations, and potential default buffer cleanup strategies.
Unresolved questions include implementation priorities for unsupported operators, handling complex argument unification (like ignoring specific parameters), and improving build and distribution workflows for GPU-enabled PyTorch features."
2023-01-02,pytorch/pytorch,"The discussions encompass a wide range of technical concerns including issues with build stability (disk space and timeouts), package management and distribution (large wheel sizes, runtime dependencies, and onnx export limitations), compatibility and support for newer Python versions (notably Python 3.11 across platforms), and model exportability (particularly ONNX serialization issues involving infinite loops and argument ordering). Several questions focus on improving code stability, including deprecating unsafe functions, optimizing tensor operations, and balancing speed-accuracy tradeoffs for specialized probability distributions like Stable. Working group considerations also include infrastructure updates, such as CUDA support, and processes like rebase conflicts, PR management, and test failures. The overall pattern indicates ongoing efforts to enhance robustness, portability, and performance, with unresolved issues around compatibility, documentation, and code quality improvements."
2023-01-03,pytorch/pytorch,"The discussions cover several technical concerns in the PyTorch repository:

1. Clarification on the `.size()` method: It is determined that `.size()` returns the shape (e.g., `(10, 3)`), similar to `.shape`, while total elements = `.numel()`, and `.shape` is an alias for `.size()`.
2. Feedback on user interface and documentation for features like `torch.nn.utils.prune`, optimizer reset methods, and experimental context managers for device handling, with suggestions to improve clarity and backward compatibility.
3. Issues around complex tensor handling in `DataParallel` and `DistributedDataParallel`, with proposals to better support complex tensors on multiple GPUs.
4. Questions about internal implementation details such as profiling precision issues, how to improve reduction kernels on CPU, and managing internal build failures.
5. Feature requests and discussions on adding new functionalities (e.g., support for `ignore_parameter_names`, different distribution classes, better ONNX backward compatibility, and clear specification of APIs), along with some ongoing bug fixes, deprecations, and internal project management actions like reverts and rebase requests."
2023-01-04,pytorch/pytorch,"The comments reflect various technical issues and discussions within the PyTorch repository, including bugs and reliability concerns with specific operators, device compatibility, and floating-point precision, especially on MPS and CUDA backends. Multiple comments mention the need for clearer error handling, improved testing (e.g., for TorchScript, ONNX export, and optimized kernel implementations), and correct use of APIs like `functorch` and `make_functional`. There are also queries about internal processes like rebase failures, CI failures, and build configurations, alongside some questions about the API design choices like device support and activation of features. Several issues are resolved with code fixes or merges, but some underline ongoing challenges, such as ensuring stability across different hardware and software environments, and clarifications on proper API behavior. Overall, the discussions highlight ongoing maintenance, testing, and feature improvement efforts in PyTorch’s development."
2023-01-05,pytorch/pytorch,"The comments highlight ongoing efforts to improve PyTorch's robustness, portability, and build system. Notable issues include the lack of official instructions and support for building PyTorch with MinGW/Cygwin on Windows, as well as driver and environment-related failures in GPU and ROCm contexts. Several discussions address test failures, especially on specific hardware or configurations, emphasizing the need for more precise handling of device compatibilities and model tracing issues. There's interest in adding features such as automatic differentiation support for complex types, label smoothing for loss functions, and better conformance for features like `logaddexp`. Unresolved questions involve strengthening the testing framework, improving build consistency across platforms, and ensuring future compatibility and performance stability, especially around fusion, quantization, and backend integrations."
2023-01-06,pytorch/pytorch,"The distributed training configuration of a PyTorch model on Windows has encountered limitations, particularly with setting intra-op and inter-op threads—setting intra-op threads to more than one causes hangs, and only setting to one avoids crashes, potentially impacting performance. In the context of compiling and linking PyTorch from source, build errors and warnings (related to CUDA, MKL, NBC, etc.) suggest potential misconfigurations or incompatible environments, especially when attempting to rebuild or verify custom builds. Several issues relate to unsupported operations in ONNX export (e.g., `aten::_scaled_dot_product_attention` at certain opset versions) and runtime errors (such as NCCL failures, illegal memory accesses, or runtime CUDA errors), indicating incomplete support or environment-specific bugs. Additionally, some internal PyTorch developments involve low-level optimizations, kernel implementations, and internal APIs, which require careful inspection, proper documentation, and validation through testing, including code reproduction and benchmarking. Unresolved questions include how to safely enable more intra-op threading without hangs, how to address unsupported ONNX operators at certain opset levels, and ensuring stable, reproducible builds and runtime behavior across different hardware and software configurations."
2023-01-07,pytorch/pytorch,"The discussions highlight the need for efficient bitwise operations in tensor computations, especially for binary data processing on CPU and GPU, with suggestions for GPU-based unpacking and caching strategies. There is interest in optimizing attention mechanisms, particularly with flash attention and mem-efficient kernels, including understanding their kernel selection and differences. Several issues involve compatibility and device support, such as the lack of implementation for MPS devices on Apple M1 and kernel errors with newer architectures like sm_89. Additionally, questions about dependencies, code organization, and licensing (e.g., for torch import) are raised, alongside efforts to improve build processes, rebase workflows, and handling of test expectations and flaky tests. The overarching concerns revolve around performance optimization, hardware compatibility, and codebase robustness."
2023-01-08,pytorch/pytorch,"The discussions primarily revolve around addressing limitations and unsupported features in PyTorch, such as workarounds for the unsupported `GatherElements` in TensorRT, handling non-leaf tensors for serialization, and issues with dynamic shapes and shape inference in JIT. Some discussions focus on improving interpolation implementations, including batching support with `interp`, and refining the internal serialization schema, especially around FX graph representation and tensor argument types. There are also concerns about performance optimizations, such as kernel selection in attention mechanisms, and ensuring correctness in model tracing and compilation, especially with reentrant calls and rebase operations. Overall, the main themes involve enhancing compatibility, correctness, and efficiency of PyTorch's compilation and serialization workflows, with unresolved questions about internal API motivations and handling of non-fake tensors."
2023-01-09,pytorch/pytorch,"The comments encompass a variety of technical concerns related to PyTorch development, including error messaging improvements for platform incompatibilities, flaky test resolutions, and dynamic shape support in TorchScript, with a focus on tracing and scripting complexities. Several discussions address issues in CUDA and ROCm compatibility, performance regressions, and environment setup challenges, such as handling CUDA versions and dependencies in container images. There are recurring questions about the proper usage of PyTorch’s compilation and autograd tooling, the semantics of mode interactions, and the importance of robust testing and benchmarking, especially for distributed training and performance-critical modifications. A number of comments also involve PR reviews, merge procedures, and infrastructure setups, including CI failures, code reverts, and environment configurations. Overall, the key unresolved themes involve improving error handling, ensuring environment consistency, enhancing dynamic shape and tracing support, and stabilizing CI pipelines."
2023-01-10,pytorch/pytorch,"The comments highlight various technical issues and discussions related to the PyTorch codebase, including potential API design considerations (e.g., handling symbolic integers and device contexts), build system and dependency management (such as the support for `c10d` headers in different PyTorch versions and the support for newer CUDA architectures like sm_89), and runtime behavior (such as the handling of `len()` with symbolic shapes and the integration of quantization backends like onednn on ARM). Several conversations address build failures, CI/testing flakiness, and merge conflicts, often suggesting improvements like adding tests, refining API behavior, or cleaning caches/systems. There are also discussions on documentation, contribution processes, and ongoing internal work (e.g., MLIR migration, memory tracking). Overall, the key concerns revolve around ensuring compatibility, correctness, and robustness of PyTorch’s features across different hardware, software, and contributed code."
2023-01-11,pytorch/pytorch,"The discussions highlight several technical concerns including the need for proper support and testing for autograd and layout conversions, especially when layouts are unchanged (to avoid ""leaf variable has been moved into the graph""), and the importance of ensuring consistent handling of containers like `dict` versus `OrderedDict` for modules like `Sequential` and `ParameterDict`—potentially moving towards a global support for plain dicts. Several issues involve build and environment problems, such as handling different compiler and CUDA versions, Intel compiler support, and complex integration challenges with third-party dependencies like oneDNN and Apex. There are questions about the future direction for PyTorch's FX graph tracing, especially regarding `make_fx` and the support for dynamic shapes and control flow, with some recognizing that certain recent changes might be by accident and could be reverted. Additionally, some discussions focus on build infrastructure, CI flaky failures, and the need to add specific tests for new features or bug fixes, often preferring to delay or force-merge changes when faced with flaky or environment-specific failures."
2023-01-12,pytorch/pytorch,"The comments reflect ongoing discussions and issues related to PyTorch build processes, especially for Windows, CUDA, and ROCm, with some concerns about build stability, including flaky tests, timeouts, and environment inconsistencies. Several comments address specific technical problems like handling sparse tensor conversions, autograd behavior with layout preservation, and complex operator registration, often requesting tests or clarifications on implementation details. Others discuss the need to improve documentation, build system configurations, and internal infrastructure, including dependency management, compiler flags, and CI/CD workflow issues. There are also questions about API behavior modifications, such as default parameter initializations, handling default args, and support for features across different hardware and operator schemas. Overall, the discussions focus on fixing bugs, improving performance and compatibility, and ensuring robustness and maintainability across different platforms and configurations."
2023-01-13,pytorch/pytorch,"The accumulated comments highlight several key technical concerns: (1) issues with complex CUDA kernel support, especially around half-precision types (e.g., `__half`, `__half2`) and related recompilation errors on specific architectures like Volta and due to macro flags; (2) failures and inconsistencies in CI testing related to memory violations, long-duration training crashes, and environment/setup nuances, notably on macOS, ROCm, and Windows platforms; (3) challenges with reusing FX and `make_fx` for tracing and scripting certain functions like Jacobian or custom operations, including proper handling of views, contiguous tensors, and symbolic representations; (4) problems and proposals around autograd compatibility with sparse tensors, `to_sparse()`, and in-place mutations, especially regarding whether to return views or copies conditioned on `requires_grad()`; and (5) broader infrastructure issues such as build system inconsistencies, dependency management (e.g., oneDNN, NCCL), test flakiness, and recommendations for upstreaming modifications (like in ZenDNN) or improving type/dtype handling for numerical accuracy and robustness across hardware backends."
2023-01-14,pytorch/pytorch,"The discussions highlight several technical concerns, notably the need for improved error handling and validation (e.g., raising errors for negative tensor dimensions), and efforts to enhance code compatibility with SciPy by implementing Python-based versions of certain operators like `torch.signal.windows`. There's ongoing work to support large matrix eigenvalue computations with better input validation, and considerations for integrating Python-native implementations to reduce complexity and review burden during development. Additionally, questions about environment consistency and build processes, such as ensuring correct installation of torch to match different Python versions and handling rebase failures, are prevalent. Some discussions involve merging conflicts and deprecations, indicating active development and maintenance challenges."
2023-01-15,pytorch/pytorch,"The comments highlight recurring technical challenges such as incomplete or missing documentation for features like `FeatureAlphaDropout`, building and testing PyTorch on Windows with various compilers (MSVC, MinGW), and issues with CUDA availability and device synchronization across environments. Several discussions focus on the need for more flexible control over `dtype` and device placement for complex weights and modules, with suggestions for better APIs or conversion methods like `complex_double`. Others address compatibility concerns, such as ensuring C++ headers maintain C++14 compatibility, and handling shape mismatches or runtime errors in ONNX exports and backend functions. Additionally, ongoing performance and correctness issues are debated, including the impact of mixed precision training, fallback behaviors in low-precision norm calculations, and the need for better test reliability and documentation improvements. Unresolved questions mainly revolve around how to improve build robustness, API consistency, and testing strategies across diverse environments and use cases."
2023-01-16,pytorch/pytorch,"The discussions highlight persistent challenges in achieving reproducibility in PyTorch models, especially under GPU/Deep Learning workloads, due to nondeterministic behaviors influenced by CUDA/cuDNN settings (e.g., setting `deterministic` and `benchmark` flags). Some contributors question whether certain API behaviors (e.g., distribution over complex Gaussian distributions) are intrinsically complex or could be simplified via transformations (e.g., real-to-complex). There are discussions around the impact of low-level library issues, such as MKL's thread management or OpenMP's thread pooling, on performance and memory leaks, including workarounds like adjusting threading or disabling dynamic scaling. Also, many issues involve toolchain and build system concerns, such as handling dependencies, compiler bugs, or platform-specific features (e.g., ROCm, Vulkan). Unresolved questions persist about the ideal strategies for ensuring cross-platform reproducibility, the structure of certain distribution implementations, and how to maintain backward compatibility while evolving APIs."
2023-01-17,pytorch/pytorch,"The collected GitHub comments reveal ongoing discussions around several key issues, including challenges with building or installing PyTorch with CUDA or specific compilers, problems with reproducibility and flaky tests within CI pipelines, and integration of features like mixed precision, autocast, and compilation modes (native, FX, or external backends). There are notable concerns about ensuring compatibility across different environments, managing dynamic shape and shape inference bugs, and handling edge cases such as small floating-point divisions or complex number operations, especially when exporting models to ONNX. Some comments focus on improving tooling support, error messaging, and developmental workflows, including rebase procedures, dependency management, and build configurations, particularly in relation to specific hardware (e.g., MPS, ROCm, or older compiler versions). Overall, unresolved questions pertain to fixing stability and correctness in features like custom hooks, op support, distributed training, and dynamic shape handling, as well as making informed decisions on deprecation, BC-breaking changes, and external backend integrations."
2023-01-18,pytorch/pytorch,"The discussions highlight multiple areas: the complexity of implementing reentrant API support for checkpointing and gradient recomputation in PyTorch, suggesting adding benchmarks and unit tests to ensure correctness; the challenge of handling different device UUIDs, especially with MIG devices, and whether to fall back to runtime queries or extend NVML, along with considerations for parsing CUDA_VISIBLE_DEVICES; the need to consider backward compatibility, performance impacts, and BC-breaking concerns of new approaches, especially around module parameter and buffer registration; ongoing flakiness in CI tests across platforms, and the need for better test management and stable checkpoints; and the importance of clear documentation, tests, and proper code review for large refactors or feature additions, like the new functional API or backend support. Unresolved questions include suitable fallback strategies for device UUID matching, performance benchmarks for new implementations, and coordination for deprecating or removing stale infrastructure files."
2023-01-19,pytorch/pytorch,"The comments encompass a variety of discussions related to custom loss implementations (like Dice Loss), conversion of ground truth labels to one-hot format, and concerns about the precision and hardware support (especially for newer CUDA architectures like sm_89). Several threads address errors in code, such as tensor reshaping issues, meta-function definitions, and the impact of auto-cast and mixed-precision on runtime correctness. There are recurring questions about testing strategies, bug fixes (e.g., for structuring the code in PyTorch's internals), and hardware compatibility (e.g., Ada vs. Ampere, or multi-GPU configurations). Notably, some discussions involve the intricacies of debugging CUDA errors, build failures, and the effects of new features or deprecations on performance and correctness. Unresolved questions often center on ensuring proper backward compatibility, hardware support, and stability of reference APIs across different software versions and environments."
2023-01-20,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of gradient all-reduce operations in distributed training, particularly the need to divide gradients by world size before all-reduce, and the removal of delay_allreduce in DDP v1.13, for which recommended solutions are sought. There are issues related to CUDA and NCCL test configurations, especially on different hardware (A100s, 3060s, 4090s), with workarounds such as updating cuDNN versions and arch flags, but uncertainty remains about compatibility and optimal architecture support on various platforms. Additionally, there’s a recurring concern about the stability and correctness of PyTorch’s JIT, Autograd, and inductor code involving complex tensor operations, fake tensor propagation, and support for new data types like complex dtypes. Some comments discuss how to properly test, document, and improve support for these features, as well as handling flaky CI tests and automatic test disabling. Overall, unresolved questions include best practices for distributed gradient synchronization, CUDA arch support, and robust testing for new or edge-case tensor behaviors."
2023-01-21,pytorch/pytorch,"The discussions highlight challenges with TorchScript tracing, specifically around in-place tensor modifications (e.g., `mu[1] = 55`) affecting trace correctness and model behavior. There are concerns about the stability and generalization of traced models, especially when model mode (train/eval) or random state (e.g., CUDA RNG, generators) influence outputs, leading to warnings and inconsistencies. Compatibility issues are noted with CUDA architectures (e.g., sm_86), where certain operations like `aten::grid_sampler_2d` are unsupported, and the need to adjust `TORCH_CUDA_ARCH_LIST` handling for older architectures is discussed. Several questions revolve around debugging and improving the tracing process, ensuring compatibility across environments, and handling deprecated or unsupported features like byte masks in multi-head attention. Unresolved issues include dependencies on specific hardware, environment configurations, and ensuring that retracing strategies or in-place modifications don't undermine model correctness or performance."
2023-01-22,pytorch/pytorch,"The discussions predominantly center on handling serialization formats compatible with TensorBoard, with suggestions to support `ndarray.tofile()`'s binary files and npy files. There are recurring concerns regarding support for 3D convolutional layers like Conv3D, particularly for platforms like MPS, with ongoing efforts and dependency considerations. Several issues highlight bugs or limitations in specific operations (e.g., `topk` on MPS, symbolic shape handling, and ONNX model conversion errors) with proposals for restructuring or safeguards, such as adding warnings or refactoring APIs. Discussions also involve improving the usability and robustness of functions like `stft`/`istft` by shifting towards default warnings and handling of window functions, with an emphasis on backward compatibility and user experience. Stakeholders are also addressing ongoing CI failures, merge conflicts, and dependency management, seeking clearer timelines and fixes for feature support."
2023-01-23,pytorch/pytorch,"The comments mainly revolve around issues with PyTorch's build environment, compatibility, and internal testing failures, often related to missing dependencies, improper handling of symbolic or sparse tensors, or CI configuration problems. Several discussions suggest that certain failures (e.g., related to CUDA, ONNX, or specific operators) are either environment-specific or due to bugs in the code that have been fixed in newer master or nightly builds. There are also recurring concerns about merging policies, reverts, and version compatibility, especially related to internal versus external infrastructure. Additionally, some comments propose changes to testing practices (e.g., improving error handling, version checks, or API signings) and infrastructure management (e.g., dependency handling, build timeout adjustments). Overall, unresolved questions include how to handle build dependencies like libcuda, ensure proper test coverage for new features, manage internal and external CI consistency, and improve the transparency and reproducibility of internal failures."
2023-01-24,pytorch/pytorch,"The discussions predominantly revolve around issues related to PyTorch's build and compatibility, including efforts to shrink binary sizes, enable link-time optimization, support unsigned data types, and improve cross-platform portability. Several comments highlight the importance of correctly handling endianness, memory layout, and serialization details, especially for model loading and inference optimization. There are concerns about stability and correctness in areas such as distributed training, operator support (e.g., aten::grid_sampler_2d), and handling of special data types (e.g., uint16 support). Some threads mention dealing with driver and system-specific issues, particularly on Windows, macOS, and with specific hardware backends (e.g., AMD, MPS). Overall, key unresolved questions focus on improving build efficiency, serialization safety, operator support, and cross-platform compatibility."
2023-01-25,pytorch/pytorch,"The collected comments highlight various technical issues and discussions primarily around build, installation, and compatibility of PyTorch across different platforms, especially ARM-based systems, macOS, and Windows. Many reports involve runtime errors, missing modules, or incompatibilities due to system libraries (e.g., GLIBCXX versions, missing `_C` symbols), as well as build failures related to compiler flags, file permissions, and environment mismatches. There are recurring concerns about the sufficiency of CI testing for different configurations, the need for better documentation links, and how to handle complex distributions or tensor representations more effectively (e.g., tensor shape embedding in repr). Several comments touch on ongoing efforts to refactor or improve internal components such as pytrees, TorchDynamo, and code generators like Inductor, often in conjunction with platform-specific fixes. Overall, unresolved questions include how to better automate cross-platform build support, improve error handling and debugging, and refine internal APIs for safer, more efficient usage, especially on newer hardware and system configurations."
2023-01-26,pytorch/pytorch,"The discussions highlight several technical concerns including the reproducibility and correctness of serialization with respect to system endianness, especially for untyped storage and view relationships. There are issues with GPU freezes and crashes linked to PCIe peer-to-peer communication, IOMMU configurations, driver or driver-related bugs, as well as specific kernel and driver version incompatibilities, particularly when using multiple GPUs or mixed hardware platforms. Several contributors question the stability and performance implications of runtime configurations, environment variables, and in-place versus out-of-place tensor operations, especially with TorchScript, FX tracing, and new compilation features like TorchInductor and TorchCompile. There are also ongoing discussions about improving the CI testing infrastructure, sharding, flakiness, and test disablement strategies across platforms, along with questions about recent code refactors, ABI compatibility, and support for features like FakeTensor, DeviceMesh, and native kernel writing in Metal or other backends. Unresolved issues include crashes, flaky tests, performance regressions, serialization correctness, and platform-specific bugs, particularly on newer GPU models and Apple Silicon."
2023-01-27,pytorch/pytorch,"The discussions encompass several technical areas, including the need for improved implementation and testing of certain PyTorch features such as in-place tensor operations, serialization, and CUDA behavior, often hinting at underlying bugs or incomplete functionality. Key concerns involve ensuring correctness and stability across different hardware platforms and compiler environments, with specific attention to issues like CPU instruction set support, version tracking in inference tensors, and compatibility of serialization formats like pickle between big-endian and little-endian systems. There are suggestions for architectural improvements, such as making certain features (like FLOP counting) more extensible, or redesigning how in-place operations or metadata copying are handled to improve performance. Several unresolved questions relate to the proper way to support custom tensor-like constructs in tracing, how to handle deprecated or BC-breaking changes, and the deployment of new features across varied systems and configurations."
2023-01-28,pytorch/pytorch,"The discussions highlight multiple issues in the PyTorch repository, including intermittent test flakiness resolved through reruns, and the need for platform-specific test disabling. Several conversations focus on performance bottlenecks, especially on CPU reduction kernels like `var` and `mean`, and implementation considerations for features such as recursive export, shape environment handling, and argument modeling. Some tasks involve build system and packaging challenges, such as proper inclusion of prebuilt object files from third-party libraries, and compatibility issues with compiler flags and environment configurations on different OSes (e.g., macOS, OSX). Additional threads address integrations with third-party libraries (e.g., ideep, learned_optimization) and tooling improvements like clearer documentation for lintrunner and better CLA enforcement. Overall, the issues encompass test reliability, performance optimizations, build robustness, code design questions, and contributor onboarding workflows."
2023-01-29,pytorch/pytorch,"The discussions mainly revolve around extending PyTorch's capabilities, including support for transposed locally connected networks and addressing issues with model serialization and tracing, especially for models with dictionary outputs or complex return types. There are concerns about CUDA memory management and kernel performance when integrating features like FlashAttention and EfficientAttention, as well as compatibility issues with version dependencies, such as CUDA, torchvision, and third-party libraries like optree. Several questions target improving development workflows, including better documentation of lintrunner commands, handling of source guard logic in JIT, and ensuring build reproducibility across diverse environments. Unresolved technical questions include optimizing guard conditions, fixing bugs in custom kernel handling, and clarifying design choices like memory format propagation during backward passes."
2023-01-30,pytorch/pytorch,"The summaries of the comments reflect various technical concerns including configuration issues (e.g., NUMA and affinity settings affecting performance), reproducibility of flaky tests, and specific bug reports in features like model exporting, ONNX support, and operator behavior (e.g., `aten::index.Tensor`, `tensor_split`, `torch.linalg.inv`). Some discussions involve code maintenance and refactoring strategies, such as handling of `NamedTuple` operations in JIT, proper handling of inference mode tensors, and decisions around supporting specific functionalities or dependencies (like `pytrees`, cub headers, or Enum support). Questions about build failures, CI flaky tests, and implementation details (e.g., kernel specialization, tensor version tracking, and graph partitioning) also appear frequently. The overall concerns are about resolving bugs, improving compatibility, and maintaining stability across different system configurations and models, with some ongoing work being closely tied to internal infrastructure and external library support."
2023-01-31,pytorch/pytorch,"The discussions encompass a variety of technical concerns, including proper weight initialization for layers like Conv, Linear, Embedding, RNN, and Transformers, with debates on whether Kaiming initialization with fan_in or fan_out should be used, highlighting potential confusions in implementation comments. There are questions about API support and feature availability, such as the reverse grid sampler, reduced build sizes, and platform support for frameworks like ROCm and oneAPI, along with details on binary size optimizations and dependency management. Several issues relate to testing stability and reproducibility, especially regarding flaky tests, CI failures, and hardware-specific bugs, such as CUDA version regressions and out-of-memory errors. Guidance is also given on code signing, licensing (CLA signing), and code layering practices, including the handling of autodiff/autograd features, inlining, and dependencies to prevent layering violations. Overall, the discussions reflect ongoing efforts to improve initialization, compatibility, performance tuning, testing robustness, and codebase robustness across multiple platforms and configurations."
2023-02-01,pytorch/pytorch,"The comments predominantly discuss various technical issues, bugs, and feature requests related to PyTorch, including memory leak solutions in data loading, model exporting differences (`model` vs `model.module`), improvements in scatter/reduction APIs, and integration challenges with newer CUDA versions, PyTorch build issues, and dynamic shape handling. Many entries are about diagnosing specific runtime errors, performance regressions, and compatibility problems across platforms, especially on Windows, macOS, and with different CUDA versions. There are multiple discussions about the need for better tooling, API consistency, and infrastructure support for features like in-place tensor operations, collective communication, and codegen in the presence of dynamic shapes. Several comments reference unresolved bugs, feature planning (e.g., for scatter operations, inductor support, FSDP/dynamic shapes), and ongoing work to improve stability. Overall, the issues reflect active development, debugging, and planning efforts, with some problems awaiting fixes or requiring further design clarification."
2023-02-02,pytorch/pytorch,"The discussions primarily revolve around addressing various implementation issues and feature gaps in PyTorch, such as support for certain operators (e.g., aten::roll, aten::im2col, aten::unique2), and ensuring compatibility with newer versions and features like in-place operations, quantization, and mixed precision (amp/fp16). Several comments indicate that certain bugs (e.g., related to CUDA descriptors, CUDA version mismatches, and incorrect modeling of tensor views) are either fixed or being actively worked on, sometimes requiring patches or temporary workarounds. There are suggestions to improve testing infrastructure—such as enabling CI jobs that catch regressions early (e.g., with LTO, build capacity, or dedicated logs)—and to avoid pitfalls like static initialization order issues. Some concerns involve ensuring feature support across different backends (e.g., XLA, ROCm), handling model serialization issues, and dealing with performance regressions. Overall, unresolved questions include how to best support dynamic shapes, in-place updates with FSDP, and maintaining backward compatibility amid rapid feature additions and fixes."
2023-02-03,pytorch/pytorch,"The comments reflect several technical concerns and ongoing work related to PyTorch's features and internal infrastructure. Key issues include support for custom extensions in the autocast context, potential backward incompatibilities or regressions (e.g., with `sympy.Floor`, `call_method` on dynamic shapes), and performance regressions on specific models or configurations (notably involving inductor, CUDA, and AMP). There are discussions about optimizing binary size, build systems (LTO, static linking), and support for more operators (e.g., `linalg_matrix_exp`, `operator support for quantization). Some comments also highlight dependency management, environment setup issues, and correctness in TorchScript scripting and tracing, especially with regard to environment mismatches or unsupported features. The unresolved questions mostly relate to stability of features across different hardware/OS configurations, backward compatibility, and precise support for new operators or features like quantization and autonomous-graph execution."
2023-02-04,pytorch/pytorch,"The comments highlight several technical concerns: (1) Support for `SymFloat` in `aten::layer_norm` schema is incomplete, suggesting a need to either extend schema support or handle `SymFloat` via guards. (2) There is a bug in `aot_eager` where nested graph compilation occurs due to many graph breaks, leading to excessive recursion and potential correctness issues, especially around dynamic shapes and recompilation. (3) Other points include the difficulty of supporting `conj()` views on `SymFloat` due to internal assertions, as well as considerations for tensor tagging mechanisms—whether they should be propagated or stored eagerly—along with complexities in maintaining correctness and usability in the presence of Tensor copies and aliases. Overall, key unresolved questions involve how to properly support dynamic shapes, the correctness of recompilation behavior, and improving debugging/meta-data mechanisms in `torch._dynamo`."
2023-02-05,pytorch/pytorch,"The discussions highlight several technical concerns: the need to improve CUDA stream management and propagation—particularly in dynamic graph and FX/AOTAutograd contexts—to fully utilize overlapping communication/compute and avoid breaking optimizations; the importance of enhancing common subexpression elimination (CSE) to detect and optimize duplicated lowering patterns, especially in lower-level AOT, to prevent redundant computations; ensuring correctness and stability of runtime transformations, especially when altering model dependencies or data types (e.g., in PyTorch's type promotion or size checks); and the necessity for thorough testing and validation of these changes, as some regressions and flaky behaviors are observed despite potential improvements. Overall, the focus is on refining graph tracing and optimization passes to improve performance without sacrificing correctness and to address pipeline and runtime stability issues."
2023-02-06,pytorch/pytorch,"The comments highlight several technical concerns centered around CUDA/cuDNN support, including issues with library versions, kernel implementation inconsistencies, and the compatibility of inductor with quantized models and tensor subclassing. Discussions address the challenges of correctly handling symbolic and runtime tensor types, especially for autodiff and export workflows, suggesting the need for better documentation and possible API extensions. There is also debate over the correct strategy for error handling in custom memory allocators—whether to rely on exceptions or error codes—and the importance of fixing regressions without breaking existing features. Some comments warn about build environment discrepancies, platform-specific API adjustments, and compatibility constraints with internal infrastructure, emphasizing that certain fixes might require internal coordination and more thorough testing or documentation updates."
2023-02-07,pytorch/pytorch,"The discussions cover several issues related to PyTorch development, mainly focusing on software correctness, performance, and reliability improvements. Key concerns include ensuring proper error handling and boundary checks in internal functions like `torch.where`, improving the serialization and management of tensor metadata (e.g., tagging and hypothetical fields), and supporting hardware-specific features or optimizations (e.g., C++ ABI compatibility, CUDA driver versions, and alternative libraries like RMM and ONNX). Additionally, there are ongoing efforts to optimize compilation times and memory footprint, as well as ensuring stability across different platforms and hardware backends. Many questions remain about the correct way to propagate errors from C APIs, the serialization of tensor metadata, and the support of specific hardware/software configurations."
2023-02-08,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for more explicit and user-friendly in-place buffer handling and the potential for performance improvements via in-place buffers at the cost of reliability; the importance of comprehensive testing for inplace view mutations and in-place buffer reuse, especially in complex models like Lennard-Jones; and ensuring consistency between local and CI results, particularly when balancing performance and correctness. There are also concerns about supporting virtual tensor mechanisms like FakeTensor, accurate guard management, and proper virtual memory handling, which affect both debugging and correctness. Additionally, some issues relate to build systems, ancient compiler support (e.g., CUDA and GCC version compatibility), and the need for clearer test and error reporting, especially for new features like async dispatchers or guard invariants. The unresolved questions involve how to best structure code for maintainability (e.g., integrating support into existing CI workflows or abstracting guard logic) and clarifying the internal mechanisms (e.g., serialization, guard support, bug fixes)."
2023-02-09,pytorch/pytorch,"The comments reveal ongoing work and discussions regarding the implementation and API design of `register_forward_hook` in the C++ API, spectral normalization support, and the `torch.trace()` function. There is an active effort to align `torch.trace()` with the new array API’s signature, with some debate over whether to add `dim1`, `dim2`, or alias parameters like `axis`. Several issues relate to kernel and library (e.g., NCCL, cuBLAS, CUDA versions) incompatibilities, runtime errors, and performance regressions, often influenced by hardware (GPU types) or environment misconfigurations. There are also discussions about CI flaky tests, build system dependencies (like CMake version), and the impact of recent code refactors on existing features, especially around in-place buffers, symbolic shapes, and API stability. Unresolved questions include proper handling of in-place tensor operations, endianness in serialization, and how to best interface complex subsystems with stable APIs for broad support."
2023-02-10,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's support for complex operations, including extending or fixing support for custom ops, kernels, and specific features like random seeding, cross-module references, and complex number handling. Several topics concern ensuring reproducibility, correctness, and compatibility with different hardware architectures and software environments, such as CUDA versions, ROCm, and CPU vendors. There are recurring mentions of the need for additional tests (unit, integration, regression) to validate changes, especially after API modifications or bug fixes. Unresolved issues involve stability of distributed operations, support for various backends (e.g., RoCm, Habana), and compatibility of advanced features like gradient calculations and tensor views. Overall, the discussions seek robust, portable, and correct enhancements while addressing implementation details and testing adequacy."
2023-02-11,pytorch/pytorch,"The comments across these threads primarily raise issues related to CUDA and GPU memory management, including CUDA out-of-memory errors, fragmentation, and fragmentation avoidance strategies such as setting `max_split_size_mb`. Several suggest reducing batch sizes or images during preprocessing to manage memory constraints, especially for large datasets or models. There are also discussions about improving in-place operator handling, dependency management (e.g., avoiding direct dependency of c10 on torch), and ensuring correctness in autograd/gradcheck scenarios, particularly with custom or experimental operators, multi-GPU and distributed training, and debugging such failures. Additionally, there's concern about CI failures unrelated to code changes, balancing build/test concurrency, and enabling more comprehensive testing (like CPU inference checks) in CI environments. Overall, the key unresolved questions involve effective memory management strategies, operator correctness under various conditions, dependency design, and improving CI reliability."
2023-02-12,pytorch/pytorch,"The discussions raise concerns about the complexity and usefulness of detailed recompilation logging in Dynamo, suggesting that a simple cumulative counter might better serve tracking regressions. There is debate over whether recompilations indicate regressions or simply normal graph breaks, especially given the different impacts on large versus small kernels. Some suggestions involve introducing environment variables to control or disable detailed logging, and improving the API to better distinguish between soft and hard recompiles. Additionally, there are ongoing questions about the proper structure and management of guard mechanisms, and the need to balance detailed diagnostics with simplicity for users. Unresolved issues include how to best implement and interpret recompilation metrics to aid UI and debugging without overwhelming the user with excessive data."
2023-02-13,pytorch/pytorch,"The discussions raise concerns about the implementation and usefulness of detailed recompilation tracking in PyTorch's dynamo, with questions about how to best leverage the information for debugging and optimization. There is debate on whether a simple recompile count or more granular function-level data would be most informative, especially considering the impact of small vs. large block recompiles. Additionally, some suggest integrating or improving guard analysis and guard caching strategies to reduce unnecessary recompiles, while others note the importance of documenting how to use such diagnostic outputs effectively. Overall, there is interest in enhancing visibility into recompilation behavior without overwhelming users with excessive logs, and a need for clear guidance on how to interpret and utilize the data."
2023-02-14,pytorch/pytorch,"The discussions highlight concerns over the support and implementation of certain features in PyTorch, such as the handling of masked and sparse semantics, especially when transitioning away from legacy operators and `MaskedTensor`. Multiple comments suggest the need for clearer API design, error messaging, and better support for backward functions, forward functions, and optimizer behaviors under different semantics, often advocating for contextual switches or more robust checks. There are also issues related to build failures, platform-specific problems, and the need for testing improvements, refactoring, or moving utility functions to avoid circular dependencies. Overall, key unresolved questions involve the systematic handling of semantics (masked vs. non-masked), backward support, and ensuring usability and correctness during implementation transitions."
2023-02-15,pytorch/pytorch,"The threads highlight various technical concerns, including challenges with PyTorch's serialization and C++ API efforts, particularly on how to handle custom operators and serialization compatibility between Python and C++. Several discussions focus on compiler support issues, such as softmax softmax incompatibilities on certain hardware, the need for improved softmax implementations, and serialization of models with custom ops. Other points involve performance regressions with softmax and eigen decomposition algorithms, especially on GPU, and considerations around maintaining correct semantics for sparse versus masked operations, with ideas for context management to handle different behaviors. Overall, unresolved questions center on how to provide flexible, robust serialization, debugging, and kernel support while balancing performance and semantic correctness across diverse hardware and use cases."
2023-02-16,pytorch/pytorch,"The discussions primarily focus on performance and correctness issues in PyTorch, such as the unexpected growth in execution time for sequential samplers, potential memory leaks and fragmentation with `torch.combinations`, and deep integration challenges between PyTorch's operators and ONNX's shape inference for custom ops. Several contributors raise concerns about the efficiency and stability of internal components like the `torch.combinations` implementation, suggesting the need for algorithmic improvements to reduce space complexity and avoid excessive memory allocations. There are also discussions around debugging and profiling challenges, especially for GPU performance issues and errors caused by hardware specifics (e.g., AMD driver problems or CPU instruction set differences), as well as how to better control logging and debugging output for large-scale distributed setups. Additionally, questions about API design choices, such as exposing internal configuration via environment variables versus explicit function arguments, and the interaction of features like `functionalize`, `autograd`, and `torch.compile`, are recurring themes, often highlighting the need for clearer documentation or more robust interfaces."
2023-02-17,pytorch/pytorch,"The discussions reveal concerns about handling symbolic shapes and data types inTorchDynamo, especially regarding how operations like `size()` cause errors when working with symbolic or fake tensors, and how to implement reliable, efficient guards (e.g., `platform.processor() == ""arm""` vs `IS_JETSON`) for platform-specific behavior. Several comments suggest that current debugging and error propagation mechanisms (e.g., error handling in ctypes or runtime exceptions during library loads) are insufficient, and improvements are needed for better diagnostics and robustness, especially in multi-process or distributed environments. There are ongoing questions about integrating custom code generation (e.g., `SetVariable`) into the compilation flow to support features like `aot_autograd`, ensuring performance and correctness without significant overhead, especially for dynamic shapes. Some discussions emphasize the importance of establishing clear policies for logging, error reporting, and handling flaky tests to improve visibility and maintainability across the developer workflows. Unresolved questions mainly concern how to implement these features in a way that balances efficiency, usability, and platform compatibility."
2023-02-18,pytorch/pytorch,"The discussions highlight several technical challenges: handling unused parameters and zero-loss batches in Distributed Data Parallel training, especially with find_unused_parameters, and ensuring proper gradient synchronization; addressing M1/M2 support issues and operator support for MPS, including operators like cumsum and pow, with specific attention to support for newer architectures like sm_89; managing large tensor operations that exceed 2^31 elements (notably in Conv3D with shape constraints); and improving logging control via a fine-grained API to avoid excessive verbosity across components. Additionally, there are ongoing efforts to resolve build dependencies and internal infrastructure issues, such as dependency mismatches and CI stability, alongside plans to formalize APIs and performance testing strategies."
2023-02-19,pytorch/pytorch,"The discussions highlight various technical challenges, including GPU freeze issues on certain hardware configurations and possible misbehavior of non-AVX-compatible CPUs, suggesting hardware or driver-related factors. There are ongoing efforts to implement fixes for numpy-related errors and to improve the integration of numpy functions, like meshgrid, into TorchDynamo, with some proposed code patches and considerations for correct handling of tensor conversion and function tracing. Several PRs involve complex refactoring, including reworking functions like `wrap_fx_proxy_cls` and ensuring correct tracking of numpy operations, while also maintaining backward compatibility and avoiding downstream bugs. Questions remain about the proper design for handling custom ops and escape hatches within the Torch compile framework, as well as the need for better documentation and testing practices for features like MPS support and precision modes. Unresolved issues also include the signing of contributor agreements and managing PR merge and rebase workflows under CI infrastructure constraints."
2023-02-20,pytorch/pytorch,"The discussions highlight several technical concerns, including challenges with BatchNorm behavior across training and evaluation modes, and strategies for proper batch processing (e.g., using `unfold` versus aggregating during inference). Issues around shared memory model loading emphasize the need for APIs that support zero-copy loading and read-only memory-mapped models for inference, with considerations for model serialization/deserialization to avoid unnecessary data copying. Device-specific problems, such as operator support on MPS and GPU architecture compatibility (e.g., sm_89), are also discussed, alongside efforts to handle proper environment detection and device availability. Several PR updates and merge conflicts suggest ongoing work on stability, compatibility, and feature enhancements, with questions about backporting fixes, handling failures, and API design for tool integrations like fx/dynamo. Overall, unresolved questions remain on improving cross-platform consistency, efficient memory management, and expanding device support while addressing ongoing merge conflicts and test stability."
2023-02-21,pytorch/pytorch,"The discussions primarily revolve around improving default initialization schemes in PyTorch, with considerations for breaking backwards compatibility to align defaults with TensorFlow/Keras, and how default initializations impact model performance and reproducibility. There are technical challenges in implementing certain features: for example, supporting large tensor shapes via 64-bit indexing in libraries like cuDNN, and handling the in-place autograd graph cleanup to prevent memory leaks with no-grad contexts. Concerning code modifications, suggestions include enhancing the `torch.compile()` framework, improving error/warning messaging regarding `train()`/`eval()` states, and addressing ONNX export issues for large models and operators, such as `multilayer LSTM`. Lastly, ongoing efforts include tracking internal regressions, benchmarking, and fixing specific bugs in operators or integration points, with some discussions about API design and testing strategies."
2023-02-22,pytorch/pytorch,"The comments reflect ongoing discussions about integrating a user-facing `torch.export()` API distinct from low-level `dynamo.export()`, to enable better user control, compatibility, and extensibility while maintaining internal flexibility. There are concerns about ensuring the export produces fully functional, correct, and optimizable graphs, especially regarding functionalization, guard resolution, and handling mutable inputs or side effects. Multiple PRs and fixes target specific issues such as handling certain operators, managing configuration states during export, and improving robustness against known errors or flaky tests across platforms like macOS, Linux, and Windows. In addition, there's ongoing work to support features like multi-layer LSTMs, backends, and variants of the graph IR, with emphasis on clarity, minimal API surface, and future compatibility. Unresolved points include defining the precise API semantics, testing strategies, and integration approach to balance experimentation vs. stability."
2023-02-23,pytorch/pytorch,"The discussions highlight several technical concerns: the need for better API design in PyTorch's export and tracing mechanisms, emphasizing a clear distinction between low-level building blocks and user-facing APIs; challenges with ensuring correct semantics and compatibility during model export, especially regarding tensor layouts, dynamic shapes, and data types (e.g., BFloat16 support and shape inference); issues with performance regressions, long compile times, and unpredictable kernel behaviors, notably in Triton and inductor kernels, requiring detailed profiling and potential configuration fixes; and the importance of managing environment-specific bugs, such as CUDA kernel issues, CI build failures, and hardware-specific bugs (e.g., MPS inconsistencies). Overall, there's a call for more modular, robust APIs, detailed diagnostics, and careful handling of forward/backward compatibility and hardware variations."
2023-02-24,pytorch/pytorch,"The discussions highlight recurring issues with CUDA kernel performance regressions, kernel launches, and profiling interactions, especially on newer GPU architectures like A100 or M1 Macs, often affecting inductor and Triton kernels. There are concerns about correctness and reliability, such as inconsistent var implementations and operand handling, especially for complex number computations (e.g., `torch.var` on complex tensors). Several comments point to the need for better diagnostics, profiling, and debugging workflows, including the use of `explain()` and profiling tools to understand kernel executions. Additionally, multiple pull requests are focusing on API stability, interface design, and experimental features (e.g., tensor sharing, custom backends), emphasizing a cautious, iterative approach. Unresolved questions relate to kernel behavior under various execution modes, proper handling of symbolic shapes, and ensuring correctness without introducing performance regressions or graph-breaking issues."
2023-02-25,pytorch/pytorch,"The comments reveal ongoing challenges with ensuring correct virtual address tagging in PyTorch, especially for CUDA and MPS backends, which affect virtual memory management and debugging tools like `compute-sanitizer`. Several discussions focus on virtual address aliasing, the importance of consistent handling of tensor views and mutations, and the need for improved virtual address validation and tagging mechanisms, including potential refactoring of the `MaybeOwned` class and virtual address source plumbing. There are also concerns about the impact of these virtual address issues on correctness, debugging, and performance, with proposals to enhance the system by embedding source information into tensor metadata, standardize virtual address sources, and improve runtime validation. Unresolved questions include how to reliably distinguish mutated inputs that need re-registration and how to integrate address source tracking without disrupting existing core functionalities. Overall, the key concern is to establish a robust, consistent virtual address tracking and validation system across CPU, CUDA, and other backends to prevent dangling pointers and improve debugging fidelity."
2023-02-26,pytorch/pytorch,"The discussions primarily concern stability, correctness, and regression issues related to recent PyTorch updates, including CUDA, JIT, and backend modifications. Several reports highlight inconsistent behavior or crashes (e.g., illegal memory access, graph breaks) that appear to be caused by regressions in recent PRs, with some fixes suggested or applied, such as coating old versions with workarounds or reverting problematic changes. There are multiple mentions of the need for better diagnostics (e.g., explain(), synchronization) to troubleshoot performance and correctness regressions. Additionally, distribution-related issues are discussed, including confusing documentation on `torch.version` and environment setup problems. Overall, unresolved questions center on stability of newer features, regression verification, and ensuring environment compatibility before merging fixes."
2023-02-27,pytorch/pytorch,"The discussions cover several technical points, notably the support and behavior of sparse tensor semantics in autograd, where there's debate about enforcing masked semantics versus relying on sparse parametrization with `sparse_mask`. Some suggest that current tensor operations like `to_dense()` and `gradcheck` could be fixed or improved by avoiding masked semantics altogether, promoting a more straightforward approach based on densification. Others highlight issues with hardware-specific bugs, such as the MPS backend errors, which are being addressed via patches and specific test arrangements, and the importance of clearer error handling or API design for complex and sparse tensor operations. Additionally, some questions arise about backward compatibility, performance regressions, and platform-specific behaviors, including architecture support and driver/compiler interactions. Overall, the main concerns involve ensuring API correctness, performance, hardware compatibility, and clear documentation for sparse and mixed-precision behaviors during autograd and training workflows."
2023-02-28,pytorch/pytorch,"The comments broadly address several complex issues: 

1. **Memory and Resource Management in Multiprocessing/Sharing**: Several discussions highlight system-level limits and shared memory handling (file descriptors, `ulimit`, system-wide settings) that affect DataLoader and multiprocessing performance, with solutions involving environment/system configuration changes.

2. **Correctness and Compatibility of Sparse and Masked Tensors**: Multiple comments debate the semantics—particularly, the behavior of `to_dense`, `to_sparse`, and masked semantics under backward passes, gradcheck, and API consistency, emphasizing the need for clear, flexible policies and careful handling of autograd and view/mutation rules.

3. **Internal Implementation Details and Proposals**: There’s ongoing work on supporting custom backends, handling of tensor cloning/deepcopy for modules, specific support for complex/bfloat tensors, and infrastructure like tracing, codegen, and proper autograd bookkeeping, with discussions on minimal invasive changes (e.g., context managers, guard sources) versus structural redesigns.

4. **Test Infrastructure and CI Stability**: Multiple reports about flaky tests, build failures, and the need to improve test infrastructure, filtering, and CI reporting, with suggestions to improve test isolation, error handling, and dev infra robustness.

5. **Design, Usability, and API Management**: Concerns about user-facing API utility, default behaviors, a unified approach to semantics across tensor types, and the impact of deprecations or future support plans, especially around dynamic/static semantics, complex data types, and extension points like custom backends or runtime checks."
2023-03-01,pytorch/pytorch,"The discussion spans multiple issues related to PyTorch's development, including challenges with loading state dictionaries in C++, handling of global variables and type annotations, CUDA environment inconsistencies across versions, and bugs or regressions in specific operators under different configurations. Notable topics involve improving the `load_state_dict`-like functionality in C++, fixing internal errors such as `isGenericDict()` assertions, and ensuring consistent behavior of operations like `to_dense`, `to_sparse`, and `masked` semantics. Several suggested solutions focus on code restructuring—such as removing redundant `THPVariable_Unpack` calls, refining pattern matching in graph transformations, or better managing fuse and kernel dispatch—in addition to clarifying documentation and API behaviors. Unresolved questions include correctly handling device-related ambiguities in `PrivateUse1` dispatch keys, addressing CUDA kernel errors with Triton, and implementing robust testing for new features across different hardware and software configurations."
2023-03-02,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of multiple model inputs during ONNX export, especially for Models with preprocessing steps detached from `forward` (Issue #22488), and passing multiple inputs correctly to ONNX Runtime (Issue #27971). There are ongoing efforts to improve pattern matching in Torch FX, such as handling nested operations and guard emission, to better support graph transformations and debugging (Issues #95431, #95492, #95849). Compatibility and correctness issues also arise with library features like sparse tensor conversions, shape guards, and kernel compilations, prompting fixes and refactoring (Issues #95517, #95811, #95817). Additionally, the discussion about re-implementing or augmenting autograd and operator support, especially for in-place and in-place-like operations, indicates work towards more robust, declarative, and flexible graph rewriting and debugging capabilities (Issues #95431, #95437). Overall, the main concerns revolve around improving pattern matching, tracing, and export workflows, ensuring backward compatibility, and optimizing internal representations for better debugging and performance."
2023-03-03,pytorch/pytorch,"The comments primarily revolve around refining PyTorch's automatic differentiation and backend compilation behaviors, identifying specific bugs, and proposing enhancements. Key concerns include ensuring correct shape and guard inference in symbolic shape handling, properly handling optional or null shapes during compilation, and improving diagnostics via logging and error messaging. There are suggestions to adjust the implementation of `average`/`ema_update` to leverage existing `foreach` APIs for efficiency, while maintaining backward compatibility, including device-awareness and optional function interfaces. Additionally, some discussions address fixing CI test flakiness, merging strategies, and contextual issues like SIGSEGVs, build system adjustments, and signature clarifications. Overall, the focus is on stabilizing core autograd/compilation code, enhancing extensibility, and improving diagnostics for a more robust and flexible PyTorch infrastructure."
2023-03-04,pytorch/pytorch,"The comments span a variety of issues from the 'pytorch/pytorch' repository, mainly concerning bug fixes, feature requests, performance regressions, and build/test failures across different versions, hardware configurations, and platforms (including CUDA, ROCm, MPS, and CPU). A recurring theme involves fixing or deprecating internal or unsupported functions (e.g., functions related to storage sharing or internal APIs), and improving robustness of features like MPS support or autograd behavior. Several discussions reference the need for proper testing (unit tests, performance regression checks) and addressing environment-related failures (e.g., CI pipeline issues, shared memory constraints, or missing dependencies). There are multiple merge requests and backport strategies, along with necessary rebase and fix steps, indicating ongoing and incremental code maintenance. Key unresolved questions include handling specific platform or hardware quirks, how to improve error reporting, and how to support newer features like static or no-hooks modules effectively."
2023-03-05,pytorch/pytorch,"The discussions highlight various complex issues in PyTorch involving CUDA and MPS errors, performance regressions, and accuracy failures, often linked to specific operations (e.g., MaxPool1, baddbmm, in-place tensor modifications) or device-specific behaviors. Several comments suggest debugging steps, workarounds, or point to dependencies (like `torch._dynamo`) that may need updates or modifications. There are recurring questions about managing internal state (push/pop), ensuring correctness with different dtypes (fp64 vs fp32), and handling specific bugs or regressions that sometimes require code patching or extra testing. Some conversations involve integrating new features, fixing build or runtime issues, or clarifying the scope and testing strategies for internal or external contributors. Overall, unresolved questions remain about stability, correctness, and performance impacts of ongoing changes or fixes."
2023-03-06,pytorch/pytorch,"The discussion covers multiple issues related to PyTorch's development, including compatibility problems with prebuilt binaries or custom builds (such as missing libraries or version mismatches), challenges with multi-process training, and the need for better testing, validation, and profiling tools. Specific concerns are raised about auxiliary features like model quantization, inductor kernel optimization, and the behavior of certain functions (e.g., torch.unique with NaNs). There are also discussions on improving the CI infrastructure, such as more efficient benchmarking workflows, better error handling, and reducing flaky tests—some of which are temporarily disabled for stability. Overall, the core challenges involve ensuring compatibility, improving correctness validation, optimizing performance profiling, and streamlining CI processes for rapid iteration."
2023-03-07,pytorch/pytorch,"The discussions highlight several technical issues, including the need for proper support and testing of new features such as the inductor compiler, support for dynamic shapes and in-place tensor type conversions, and handling of operators like `torch.where` and `autocast`. Multiple comments address the importance of correct registration strategies for custom backends (e.g., alternatives to direct registration or fallthrough mechanisms) and ensuring compatibility with features like mixed precision and sharded tensors. There are ongoing efforts to improve performance, error handling, and robustness, with particular focus on fixing regressions, flaky tests, and functional correctness, especially across different hardware (e.g., CPU, CUDA, MPS). Some unresolved questions pertain to the adoption of user-facing APIs (e.g., in-place `to()`), the support of certain operators in ONNX, and the proper way to extend or customize IR and operator support in TorchDynamo and Inductor. Overall, the discussions reflect active work on enhancing PyTorch's compiler infrastructure, correctness guarantees, and user experience for advanced use cases."
2023-03-08,pytorch/pytorch,"The discussions primarily focus on improving CUDA and profiling support in PyTorch, including adding runtime controls for specific kernel optimizations like onednn, and enhancing profiling to better diagnose performance variability and overheads. There are concerns about ensuring compatibility and correctness in the JIT compilation process, especially when handling complex or unsupported operations, and the need for better test coverage for such cases. Several comments suggest refactoring or API changes, such as making certain features optional, handling dynamic shapes, or introducing more explicit user controls for profiling and optimization toggles. Additionally, some discussions highlight issues with slow or flaky CI tests due to hardware or software environment inconsistencies, and there are proposals to improve error handling, documentation, and test robustness in these areas. Overall, the main concerns involve improving observability, configurability, robustness, and correctness of JITs, profiling, and related build/validation processes."
2023-03-09,pytorch/pytorch,"The comments reveal several recurring technical concerns in the 'pytorch/pytorch' repository. Key issues include compatibility and support for custom ONNX operators (e.g., for normalization layers like MVN, instance normalization), and handling of normalized inputs and weight precision (e.g., FP16 support and stability in training or inference). There are also ongoing efforts for improving automation, such as better handling of fake tensors and avoiding unnecessary cloning, and issues related to CI flaky tests, build regressions, and error messaging clarity. Several discussions focus on how to extend or improve support for sparse and custom modules, especially for export and backend integration (e.g., Torch.compile, fx custom leaves). Lastly, synchronization and profiling are needed to understand and fix performance anomalies, particularly in the context of specific hardware (like MPS or AMD CPUs) and operations."
2023-03-10,pytorch/pytorch,"The discussion highlights several technical concerns related to PyTorch's development. These include the need to better support device-agnostic or shape-generic operations, such as modifying `torch.Tensor.is_set_to()` semantics and ensuring compatibility with different device types (CPU, CUDA, MPS, ROCm). There are also issues with internal code, such as the proper re-exposure of `torch._inductor` APIs like `torch._inductor.is_available()`, and the handling of tensor conversions and graph breakpoints, especially across dynamic shapes and device transitions. Additionally, some regressions and flakiness in CI tests (e.g., flaky tests on ROCm/MPS, performance regressions) require investigation. Proposed solutions include improving or exposing device-check APIs, reconsidering internal assumptions about tensor device states, and refining handling of shape/aliasing/casting to ensure correctness and stability across hardware configurations."
2023-03-11,pytorch/pytorch,"The discussions primarily revolve around managing dependencies and build configurations in PyTorch, including header bundling, CMake checks for CuDNN headers, and potential inclusion of more CUDA headers for self-contained headers. Several issues focus on runtime errors and performance concerns on various hardware platforms, such as M1/M2 Macs with MPS support, GPU-specific speedups/slowdowns, and CUDA-related kernel errors indicating invalid memory access. There are also concerns about API usability, such as handling kwargs in JIT tracing, and the stability of certain features like lazy modules and in-place view operations. Some discussions address release management, version signing with CLA compliance, and merging workflows. Unresolved questions include how to fix build errors related to functorch/BatchRules and the impact of system differences (e.g., CUDA versions, hardware capabilities) on performance and correctness."
2023-03-12,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's MPS backend, particularly its limited support for complex numbers, inconsistent behavior across different OS versions, and performance issues such as high overhead with 'channel_last' memory formats. There are concerns about compatibility and stability on newer hardware (e.g., macOS 13.3 beta) and different GPU architectures (e.g., RTX 3090), as well as specific bug regressions from previous versions (e.g., issues in 1.13.0 and 1.13.1). Several issues relate to the need for enhanced tensor property assertions, more robust backend support (e.g., for complex dtype, atomic add for BF16), and improvements in performance benchmarks. Additionally, there’s interest in refining pattern matching mechanisms to improve graph transformations efficiently, with suggestions to leverage torch.fx pattern_matcher.py for better scalability, while unresolved questions remain about ensuring cross-platform consistency, support for advanced tensor constraints, and fixing specific bugs affecting accuracy and speed."
2023-03-13,pytorch/pytorch,"The discussions cover various technical issues related to reproducibility, hardware support, and code stability in PyTorch. Key concerns include the need to serialize RNG states for reproducibility, hardware-specific bugs (e.g., IOMMU, ROCm, and CUDA crashes), and compatibility of features like quantization, complex number support, and custom backends across platforms. Some responses indicate fixes or workarounds, such as kernel improvements, environment setup, or code refactoring, while others focus on API alignments and internal infrastructure challenges. Several issues highlight flaky tests, CI failures, and potential regressions that require further investigation or stabilization. Overall, unresolved questions remain about hardware support, consistent error reporting, and maintaining backward compatibility while evolving features."
2023-03-14,pytorch/pytorch,"The discussions highlight several notable issues in PyTorch development: the need to properly support and expose `torch._dynamo.runtime.cuda.is_available()` in C++ for better hardware compatibility; the importance of handling dynamic shapes and interval analysis more robustly within the compiler, including potential enhancements like widening or adding constraints; the complication of in-place mutation handling in the context of autograd and torch.compile, especially with smuggled tensors and the need for guard checks or modified version counters; concerns about improving test coverage for specialized operations, such as `torch.sum` behavior with large tensors or edge cases like empty tensors, to ensure correctness across compilation modes; and the general desire for clearer API semantics and consistent behavior across different device types and data types, especially regarding support for mixed and half-precision operations, as well as improving the integration of external components like Triton kernels and runtime environment management."
2023-03-15,pytorch/pytorch,"The discussions highlight challenges in managing GPU/CPU memory during evaluation phases, implementing sparse tensor operations (notably with internal functions like `_sparse_addmm`), and ensuring correctness and efficiency in custom layers, quantization, and automatic differentiation. Several issues revolve around ensuring proper hooks support, bug fixes for numerical stability, and handling edge cases such as empty tensors, mixed sparse/dense tensors, or model export inconsistencies. There are recurring concerns about test flakiness, CI stability, and proper merge/revert procedures, especially when internal tests or infrastructure updates cause transient failures. Additionally, questions about environment configurations, library dependencies, and versioning—particularly for CUDA, cuDNN, and PyTorch builds—are prominent, emphasizing the need for consistent, supported deployment setups."
2023-03-16,pytorch/pytorch,"The discussions highlight several technical concerns including the need for efficient tensor indexing with indices requiring int64, challenges with stable sorting and deduplication in sparse/voxel operations, and the difficulty of supporting low-bit data types (e.g., 4-bit, 8-bit) in PyTorch's dtype system, especially regarding hardware-specific implementations. There are issues with managing dependencies and library versions for CUDA and cuDNN, particularly on macOS, where discrepancies between system-installed and bundled libraries cause performance and compatibility concerns. Several code-related topics involve enhancing JIT support and autograd behavior, fixing memory leaks, and improving the robustness of distributed, function and graph transformations. The overall questions revolve around balancing hardware support, API generality versus special cases, and maintaining compatibility across diverse platforms, with several ongoing PRs and plans for more principled fixes."
2023-03-17,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's support for large models and specialized backends, such as handling large tensors in inductor and addressing issues with integration of third-party libraries (e.g., KeOps, MKL, and ONNX Runtime). A recurring theme is the need for better handling of dynamic shapes, especially in the context of accurate gradient computation, fixed point fixing in interval analysis, and maintaining backward compatibility after API changes (e.g., sharding APIs and in-place tensor operations). Several questions involve ensuring correctness of autograd graph tracking, especially around custom functions, view behaviors, and in-place updates, as well as managing error scenarios and performance regressions. Overall, unresolved issues include crash fixes, compatibility with system libraries, and more robust testing for correctness, performance, and support on diverse hardware (e.g., Apple Silicon, ROCm, and CUDA)."
2023-03-18,pytorch/pytorch,"The discussions highlight various technical concerns including compatibility issues with different CUDA and cudnn versions, particularly the static vs. shared library linking, and how this affects binary size and environment setup. Several comments address accuracy and performance regressions across different precision modes (float16, amp, bfloat16, etc.), emphasizing the need for proper validation and potential reversion of code changes causing discrepancies. There are also frequent mentions of debugging specific errors such as gradient unscaling with FP16, incorrect tensor dispatch, and runtime errors in torch compile, especially in relation to dynamic shapes, graph capturing, and distributed training. Unresolved questions include how to ensure correctness when mixing precision modes, handling environment-specific memory behaviors, and fixing or avoiding regressions in training and inference workflows. Overall, the main concerns are about stability, correctness, and environmental compatibility in evolving features like torch compile and custom optimizations."
2023-03-19,pytorch/pytorch,"The discussions primarily revolve around handling namespace conflicts in LibTorch, particularly the scope of using `namespace` directives and their impact on type declarations like `Scalar` and `DeviceType`, with solutions including removing or adjusting `using namespace` statements. Several issues involve compilation errors, runtime support discrepancies across hardware (e.g., M1, Apple Silicon, or MPS devices), and the need for better control over custom or unwrapped operations in graph export and decomposition workflows. There is also concern about test failures and regressions caused by changes in shape handling, such as the mesh-like shape mismatch in index operations, which may require skipping or adjusting certain models and operations. Additionally, some discussions suggest improvements in API flexibility for custom operation support and the importance of maintaining test coverage for error handling and namespace correctness."
2023-03-20,pytorch/pytorch,"The collected discussion highlights various technical concerns in the PyTorch repository, including challenges with environment-specific CUDA and library dependencies, especially related to proper hardware compatibility and installation methods (pip, conda, brew, system libraries). Key issues involve supporting complex datatypes in autograd functions, particularly complex numbers and their differentiation, with debates on whether to support holomorphic functions and how to handle these cases safely. Several performance and correctness regressions from recent PRs are noted, with fixes involving caching, batching, and in some cases disabling flaky tests. There are also ongoing efforts to improve inductor support, control flow integration, and runtime memory management, along with architectural questions about tensor grouping, dynamic shape handling, and cross-component compatibility. Unresolved questions include balancing performance with robustness, managing environment-specific quirks, and correctly extending support for new datatypes and control flow features."
2023-03-21,pytorch/pytorch,"The comments highlight several technical issues and areas for enhancement within PyTorch's development: notably, the need to improve support for control flow operators (like `cond`) to enable more comprehensive graph tracing and better hardware optimization, including introducing runtime configuration fields to the FX graph's metadata. There are concerns about the stability and correctness of backend operations, such as the proper handling of `scatter_reduce` and low-level CUDA/cuDNN behaviors, which impact correctness, performance, and debugging. Also, discussions touch on the challenges of supporting various device-specific features (e.g., MPS, NVIDIA driver compatibility) and the importance of maintaining consistent, backward-compatible APIs, especially for third-party integrations and custom operators. Several progress and follow-up tasks involve implementing better support for higher-order functions, fixing bugs related to CUDA kernels, and improving ease of use through documentation and API design, all thereby ensuring more robust, performant, and user-friendly PyTorch workflows."
2023-03-22,pytorch/pytorch,"The discussions highlight several technical issues: a need for lightweight, environment-agnostic backends for GCS logging and storage in 'torch.utils.tensorboard', and a desire for broader support for control flow operators and higher-order functions (like while_loop, scan, map) to enable more expressive and optimizable neural network constructs within the PyTorch IR and its transformations. There's a recurring concern about performance regressions and bugs in specific modules (like tensor ops, autograd, autograd/gradcheck behaviors, and inductor's handling of dynamic shapes), particularly on different hardware platforms (CUDA, ROCm, Apple silicon, ARM). Some threads address compatibility and stability issues, such as the difficulty in supporting autograd with dynamic shapes, handling complex tensor types, or proper serialization of custom modules and operators. Proposed solutions include adding dedicated APIs, extending existing infrastructure for better shape and type handling, supporting custom ops with registration, or improving default behaviors and backward compatibility. Unresolved questions involve how to unify behavior across devices, how to safely and efficiently handle complex control flows and dynamic shapes for autograd/autograd functions, and how to optimally support hardware-specific features (e.g., CUDA's native support for bfloat16 atomics)."
2023-03-23,pytorch/pytorch,"The discussions primarily revolve around optimizing PyTorch's kernels and APIs for various hardware and software configurations, including issues with redundant code, kernel performance bottlenecks on both CPU and GPU (notably with `LazyConvTranspose2d` and `ConvTranspose2d`), and handling complex or large data types (like `double` and `int64_t`) in low-level operations. There are concerns about ensuring consistency between CPU and GPU computations, particularly when running with different CUDA versions or on different architectures (e.g., Apple Silicon, ARM-based systems). Some suggest exporting models with different signatures or providing APIs to allow the compiler to skip certain custom kernels to prevent runtime errors and performance regressions. Unresolved questions include how best to implement fallbacks or API support to handle custom ops, data types, and device-specific quirks without compromising performance or correctness."
2023-03-24,pytorch/pytorch,"The discussions highlight issues related to performance overheads, especially on low-version cuDNN (e.g., 8.5), and suggest that certain performance regressions may stem from unoptimized or unsupported operations, such as deprecations or missing operator registrations in custom or backend implementations. There are ongoing efforts to improve the compilation process, such as using `cudaSetDevice` judiciously or supporting custom operators via `onnxscript`. Some discussions revolve around improving transparency and documentation for support surfaces, including clarifying which ops backends should handle. Additionally, there's concern about the stability and correctness of reproducibility, especially with CUDA seeds and multi-GPU/async executions, and about aligning internal and external versioning of dependencies like cuDNN. Overall, the focus is on fixing regressions, supporting newer features/operations, and ensuring performance stability across environments."
2023-03-25,pytorch/pytorch,"The discussions raise concerns about inconsistent unification between `at::Tensor` and `torch::Tensor`, and confusion caused by the mixing usage despite unification efforts. There are issues with build failures and runtime errors related to CUDA version support and missing functions, often requiring workarounds, upgrades, or conditional compilation. Several PRs are stalled or rejected due to CI failures, need for rebase, or missing labels, highlighting ongoing CI infra challenges. Some users seek clarifications on test failures, reproducibility, and future plans for default behaviors or support for specific operators and backends. Overall, unresolved questions include improving build stability, correctly implementing and testing operators, and ensuring compatibility with various CUDA, compiler, and platform versions."
2023-03-26,pytorch/pytorch,"The discussions highlight issues with PyTorch's support for dynamic shapes and certain operator behaviors in JIT and ONNX exports, with suggestions to implement additional guard-based checks and correctness testing to ensure soundness. There are specific concerns about the behavior of multi-head attention in evaluation mode, recommending potential code modifications for consistency. Some internal build and import errors, such as CUDA device ordinal issues and runtime hangs, require fixes or workarounds, including version adjustments and code revisions. Several PRs involve experimental features or safety checks (e.g., range refinement, detailed test failures) that need validation for production readiness. Overall, the key concerns revolve around improving support for dynamic models, operator consistency, and robustness of model export/testing workflows."
2023-03-27,pytorch/pytorch,"The discussions highlight several key issues: the need to support different backends (like Triton and OpenMP) for tensor operations, with questions on how to specify device flow control in user code, and the potential to implement a uniform backend registration mechanism to facilitate extensibility. There are concerns about handling in-place tensor modifications—particularly how to ensure correctness and consistency with autograd, especially when views and storage are mutated or shared across threads. Additionally, recent regressions involve compatibility and correctness issues with CUDA versions, protobuf 4, and bug fixes affecting onnx export and tensor initialization, requiring either code adjustments, environment management, or further testing strategies. Finally, multiple discussions address enhancing the robustness and testing of compiler guard simplification and the handling of special tensor types (such as sparse tensors or complex/large integer tensors), aiming to prevent silent correctness errors and improve build reliability."
2023-03-28,pytorch/pytorch,"The primary concerns involve ensuring consistency and correctness of computational behavior across different frameworks and configurations. Notably, questions about the impact of in-place tensor modifications on autograd correctness and the proper handling of view operations in gradient computations are raised. Several discussions highlight the need for better API support, such as customizable assertion checks for numerical closeness, and handling of user-defined and sparse tensor behaviors in testing. Additionally, noise from flaky CI tests, environment-specific issues, and the overall stability of features like torch.compile, onnx support, and various backend integrations are recurring themes. Overall, the discussions emphasize improving robustness, API clarity, and correct semantics in both runtime and testing behaviors."
2023-03-29,pytorch/pytorch,"The comments reflect multiple ongoing technical discussions, including efforts to improve deterministic behavior in PyTorch operations such as in MaxUnpool2d, UpSample, and index_select, with proposed solutions involving custom implementations, codegen changes, or API modifications. Several issues highlight the challenges of maintaining backward compatibility while introducing new behaviors, especially around view operations, quantization, and the interaction between scripting, tracing, and different backends. There are also ongoing efforts to support multiple hardware backends (e.g., XPU, ROCm) and to refine experimental features like float8, while managing CI stability, flaky tests, and build system complexities. The general theme emphasizes cautious incremental changes, thorough testing (including benchmarks and backtrace analyses), and thoughtful API design to balance performance, correctness, and compatibility. Unresolved questions include how to best integrate custom device support, handle view-related in-place modifications, and ensure consistent behavior across dynamic and static graphs."
2023-03-30,pytorch/pytorch,"The discussions highlight challenges in extending support and ensuring correctness for specialized tensor operations, such as sparse tensor support in MPS, the implementation of custom operators in ONNX, and proper handling of tensor mutability and const correctness. There are concerns about the efficiency and safety of introducing new API wrappers versus modifying existing functions, especially regarding device and memory management, and the potential for regressions or performance issues in complex, multi-framework environments. Additionally, several issues involve debugging infrastructure, flaky tests, and ensuring compatibility across different hardware (e.g., Apple silicon, CUDA). The community emphasizes the need for careful consideration of API design, stability, and detailed diagnostics when proposing changes, particularly around the integration of external libraries or low-level optimizations. Unresolved questions include how to reliably detect and fix memory leaks, manage dependencies (like `pyg-lib`), and coordinate framework-level support for features like quantization, sparse support, and composite operators."
2023-03-31,pytorch/pytorch,"The discussions highlight several technical issues, including the need for proper handling of multiple `FakeTensorMode` instances, especially in relation to `torch.compile` and `torch.autograd` functionalities, with suggestions to prevent nested or conflicting modes. There are concerns about supporting different tensor layouts and data types (like float/double or complex) in the context of quantization and conversion functions, emphasizing the importance of consistent dtype handling and extension of functions like `corresponding_real_dtype`. Other key points address compatibility issues with external libraries such as protobuf and third-party dependencies (e.g., libtorch, miniz), as well as system-specific build failures, especially on ROCm, and proper management of device guards and distributed training workflows. Some discussions also involve making CI tests more robust, including handling flaky tests and enabling better support for open registration backends. Overall, unresolved questions focus on improving API consistency, broadening support for heterogeneous environments, and refining mode management to ensure system stability and correctness."
2023-04-01,pytorch/pytorch,"The discussions highlight several technical areas: ongoing issues with compiler performance, especially slow graph compilation and build failures on certain hardware configurations; the need for improved testing and validation of new features such as open device registration and fake tensor mechanisms; and challenges in handling environment-specific problems like mismatched compiler ABI, GPU environment issues, and runtime errors related to tensor properties and autograd behavior. There is also discussion about extending support for the `corresponding_real_dtype` functionality, improving auto-retry and test stability, and ensuring proper integration with external tools like ONNX Runtime. Overall, these points indicate the priority of stabilizing compilation, runtime stability, and environment consistency in the PyTorch codebase."
2023-04-02,pytorch/pytorch,"The discussions highlight concerns about implementing support for complex number tensors in PyTorch, noting that current limitations in Triton (which does not support complex types) hinder adding operations like addition directly, and that full support would require significant work on Triton kernels and operator override implementations. Several users point out that current binary distributions (e.g., the ""large"" wheel versus the standard pip package) have differing dependencies, especially regarding cuDNN versions, which affects performance and compatibility across CUDA versions. There are also ongoing issues with CI failures, build inconsistencies, and the need to ensure that testing, especially for functionality like ONNX export and CUDA dependency management, is comprehensive and aligned with production distributions. Overall, there's a call for better handling of dependencies, explicit support for complex tensors, and more consistent testing to prevent silent errors or performance regressions."
2023-04-03,pytorch/pytorch,"The discussions revolve around experimental enhancements and debugging in PyTorch, such as improving support for sparse and complex tensors, fixing inconsistent in-place operations in the graph, and optimizing the compilation and runtime behavior under different configurations (e.g., inductor, torch.compile, and environment variables). Several issues highlight the need to better handle runtime errors (like segmentation faults, CUDA compilation failures, and unsupported ops) potentially caused by the architecture of Triton/Inductor or mismatched assumptions about tensor types. There is also concern about test coverage and correctness verification, including managing flakiness, ensuring proper error handling in operators, and enhancing debug tools (e.g., compilation metrics, backtraces). Additionally, some discussions address build system and CI infrastructure challenges, such as build failures, dependency management, and release automation. Overall, the key unresolved questions involve ensuring type safety, improving support for advanced tensor types, and stabilizing the compilation machinery for diverse hardware and software environments."
2023-04-04,pytorch/pytorch,"The discussions cover several technical issues including support for variable-sized tensors in collate functions, potentially defaulting to a `list` of tensors, and concerns about performance overhead when adding new features or API functions—particularly the need for more explicit naming like `nonzero_static` versus overloading `nonzero`. There are bugs and limitations identified in the behavior of operators such as `resize_`, `torch.argmax`, and operations on MPS devices, as well as quality concerns about safety patches, build configurations, and compatibility with different device backends like ROCm and CUDA. Some suggest maintaining more explicit, locally configurable mechanisms or improving the infrastructure (like the `Registry` pattern or constant pooling) for better extendibility, especially for custom backends and multi-device support. Additionally, there's mention of several ongoing review and merge issues, including build failures, CI test disabling, or rebase conflicts, indicating active maintenance challenges."
2023-04-05,pytorch/pytorch,"The comments reveal ongoing concerns about handling edge cases in various components: the need for a more extensible design for `ExtraMeta` to avoid bloating with unused fields, and the potential for better abstraction in distributed backends to support custom devices. Key issues include fixing specific bugs like non-autograd `Tensor.where_()` support, handling tensor shape mismatches, and ensuring correctness in operations like `torch.angle()` for edge cases. Several discussions focus on improving testing robustness, including deterministic input generation, avoiding flakiness, and reducing memory overhead or latency. There are also maintenance considerations such as proper baseline merging, dealing with build failures due to dependencies or conflicts, and ensuring compatibility with changes like private `device` APIs or new operator overloads."
2023-04-06,pytorch/pytorch,"The collected comments highlight ongoing development challenges including the need to implement custom handling for sparse tensor operations, such as deprecation or adjustment of samplers for broken tests, and proper propagation of shadow storages across interpreters in MultiPy. Several issues involve ensuring correct behavior and compatibility of PyTorch with extensions like Autograd, ONNX export, and distributed training (including FSDP and NCCL), especially regarding in-place operations, datatype consistency, and device-specific logic. There are recurring concerns about flaky tests, CI failures, and the need for better test coverage, reproducibility, and patch management. Some discussions focus on fixing specific operator bugs (e.g., `repeat_interleave`, `smooth_l1_loss_backward`, `attention_mask` handling), with proposals for refactoring core components or extending existing mechanisms like operator registration or batching rules. Overall, unresolved questions include the proper abstraction for device-specific behaviors, efficient sharing of storage in multi-interpreter contexts, and ensuring deterministic, performant behavior across diverse hardware and software setups."
2023-04-07,pytorch/pytorch,"The discussions highlight issues with CUDA/cuDNN library loading on certain Linux distributions and hardware, causing runtime errors and build failures, which can sometimes be mitigated by manual library linking or environment configurations. Several PRs aim to improve robustness, such as fixing runtime fusion limitations, enhancing memory sharing, and fixing shape-related bugs in both CPU and GPU backends. There are ongoing concerns about test flakiness and CI stability, especially when disabling or rerunning tests, and the need for better coverage via platform-specific or more comprehensive tests. Some contributors suggest waiting for upstream fixes (e.g., cuDNN 3.1, CUDA 12.1) before finalizing certain changes, while others work around issues with environment modifications or patching. Overall, the key unresolved questions involve handling compatibility, build consistency, and ensuring reliable CI testing across diverse environments and hardware."
2023-04-08,pytorch/pytorch,"The discussions highlight concerns about efficiency in certain PyTorch operations, especially `cumsum` and its bandwidth limitations on CUDA, with efforts to improve CUDA kernel bandwidth. Several issues focus on understanding and fixing microarchitectural or microarchitectural-specific bugs, such as `GCC` path issues on macOS, and handling of `rand` states in `fx.graph`, which is crucial for correctness and reproducibility. There are also multiple internal code management and review process questions, including merge failures due to missing labels or outdated branches, and the need for better testing and handling of edge cases in model optimization, especially for feature-guarded code and dynamic shape handling. Additionally, issues with build configurations and missing internal files (e.g., `GeneratorForPrivateuseone.h`) point to infrastructure management needs. Finally, several discussions are about testing, CI failures, and the need for clearer procedures for contributions, code review, and internal compliance."
2023-04-09,pytorch/pytorch,"The discussions primarily revolve around optimizing PyTorch performance, memory, and compatibility: building from source for binary size reduction and lower RAM usage, addressing operator implementation gaps on MPS and CUDA (notably 'aten::unfold_backward' and other operators on MPS), and managing CUDA driver/library linking issues. Several comments suggest code changes or workarounds for compiler and runtime problems, including patching the codebase to fix specific bugs and handling runtime errors related to CUDA libraries. There are also recurring points about simplifying guard conditions for better CSE, reducing compile time, and benchmarking the effects of different approaches on training/inference performance and stability. Many unresolved questions include handling unimplemented operators on MPS, how to improve compile-time for large graphs, and ensuring proper driver-library linkage without conflicts."
2023-04-10,pytorch/pytorch,"The discussions highlight issues with PyTorch's export and runtime behaviors, especially relating to dynamic shapes, guard reordering, and interactions with features like FSDP, inductor, and cuDNN. There are concerns about accurately modeling guard failures, enabling guard CSE for efficiency, and handling attribute differentiation, especially for buffers and parameters. Several reports mention regressions or performance discrepancies when toggling flags like `cuda.cudagraphs`, or upgrading dependencies like Triton and CUDA, indicating possible bugs in guard handling or compatibility issues. Some developers are working on fixing these by refining guard logic, enhancing auto-generated method support, and addressing known issues like the `sym_size` operator for MPS. Overall, the key unresolved questions revolve around improving guard failure detection, cache consistency, and compatibility across environments."
2023-04-11,pytorch/pytorch,"The discussions cover several technical issues: implementing efficient bit-level tensor operations like packbits/unpackbits and their GPU unpacking on the back burner despite merge readiness; handling self-overlapping tensor operations—particularly ensuring `inplace` operations like `abs_()` or `unfold.backwards()` raise errors to prevent incorrect autograd behavior; enhancing autograd and graph export functionalities—such as supporting `ModelOutput` classes, adding dynamic shape handling, and improving manual seed setting across devices; refining PyTorch's logging API to separate components and artifacts for better configurability, and unifying C++/Python logging states; addressing compatibility and build issues, especially regarding ONNX export errors for models with shape-dependent ops, and real device support on newer hardware. Many of these are ongoing work, with proposals for dedicated PRs and refactoring needed for better robustness, usability, and inter-framework consistency."
2023-04-12,pytorch/pytorch,"The comments predominantly raise concerns about softmax masking behavior in PyTorch's implementations—highlighting that softmax applied to `-inf` values produces NaNs rather than zeros, which can cause issues in attention mechanisms. Multiple discussions emphasize that softmax should ideally treat masked positions as zeros, and that current workarounds involve replacing `-inf` with large negative numbers (like `-1e9`) or patching softmax, but these are considered suboptimal or hacky solutions. Some suggest that the root cause is softmax behavior itself, not masking, and advocate for modifying softmax to handle masked positions gracefully. There are also unresolved questions about softmax's expected behavior, how masking interacts with different dtypes and device types, and whether softmax should be patched or softmax-like functions should be customized for better correctness and stability."
2023-04-13,pytorch/pytorch,"The discussions reveal multiple key issues: first, support for sparse CSR operations and their autograd support is limited, requiring extensions or workarounds such as converting to COO; second, there are concerns about the robustness and correctness of `load_state_dict`, especially regarding tensor contiguity and device consistency; third, the importance of ensuring compiler and runtime correctness when supporting features like `device_asserts`, mixed precision, and multi-interpreter sharing, with some proposals to improve API design and internal data management for shared storage ownership and refcounting; fourth, cross-platform build and compatibility issues, including GCC 13 compatibility and proper handling of profile/tracing tools; lastly, the need for clearer documentation, test coverage, and fixing regressions related to these areas before merging."
2023-04-14,pytorch/pytorch,"The discussions highlight several technical concerns: the implementation of a generic in-place parameter update method, with proposals for using a shared deleter in DataPtr or refcounted wrappers, aiming for efficient parameter sharing; the need to add comprehensive testing and documentation, especially clarifying behaviors like random seed effects, in relation to various PRs; addressing flaky or crash-inducing tests, often caused by environment or synchronization issues, and considering whether to disable or modify certain tests or barriers; and the ongoing challenge of supporting features like in-place mutations, checkpointing, and de-duplication in distributed and autograd contexts while ensuring correctness and minimal disruption. Unresolved questions include how best to extend APIs for flexible parameter management, ensuring safety in failure scenarios, and managing cross-platform or architecture-specific issues."
2023-04-15,pytorch/pytorch,"The discussions highlight ongoing technical concerns around CUDA support and stability, particularly relating to the transition to CUDA 12 and associated regressions in build and runtime stability, with suggestions to improve CI coverage for new hardware targets. Several contributors question the support of sparse-matrix operations on Apple Silicon and recommend porting or leveraging existing BLAS/LAPACK implementations, such as Eigen, to improve CPU support and performance. There are also issues related to the interaction of FSDP and process group destructors, with potential race conditions causing crashes, and suggestions to improve test coverage and debugging facilities for these components. Additionally, discussions address the need for more granular or informative error handling in kernel APIs to aid debugging, as well as considerations for modularizing complex test suites for better maintainability. Unresolved questions mainly involve the approach to ensuring correctness and stability across diverse hardware platforms while balancing performance optimizations."
2023-04-16,pytorch/pytorch,"The discussions highlight concerns about PyTorch's CUDA stream API, specifically regarding asynchronous execution and potential race conditions, exemplified by inconsistent reproducibility of `stream.sum()` assertions across different examples and hardware setups. Some contributors notice discrepancies between expected and observed resource utilization and kernel occupancy, questioning underlying kernel execution and overlap. Several issues relate to device support, especially on Apple's MPS backend, where complex data types like complex floats and bfloat16 are unsupported, causing runtime errors and fallbacks. Others involve deep internal features such as model serialization, sparse tensor support, and integration of third-party libraries like CUTLASS, with questions about future fixes, build configurations, and ongoing support efforts. Overall, many discussions revolve around ensuring correct, efficient GPU/accelerator utilization, compatibility, and debugging challenges stemming from hardware or software limitations."
2023-04-17,pytorch/pytorch,"The discussion covers multiple issues encountered during PyTorch development, including challenges with supporting features like spectral normalization resets, ONNX export/import for BatchNorm and complex numbers, and in-place tensor mutations, often highlighting the need for better error messages or documentation updates. Several threads address build failures or environment-specific bugs (e.g., ROCm, macOS, compiler settings), with suggestions such as patching macros, disabling problematic tests, or adjusting build configurations. There are also discussions on improving internal tooling, including logging architectures, autotuning caching strategies, and the integration of inductor and dynamo features, emphasizing the importance of compatibility, performance, and clarity. Some issues are marked as fixed or ongoing, with potential further enhancements proposed, such as adding positional-only arguments or refining error handling. Unresolved questions include handling complex number support in AMP, support for in-place custom ops, and managing multi-GPU or distributed training robustness."
2023-04-18,pytorch/pytorch,"The discussions encompass several core issues: the need for enhanced support and documentation for sparse tensor operations, especially in the context of FSDP and the introduction of new APIs like `to_sparse_csr`, with suggestions to use tensor operations directly or extend existing registration mechanisms; concerns about the proper handling of `align_corners` in `grid_sample`, debating its default behavior and potential bugs in 3D resampling, as well as how to clarify this in documentation; complexities around the logging and artifact system, advocating for more intuitive instrumentation (e.g., prefixing logs with function info or improving the API for artifact filtering); unresolved technical challenges with CUDA and ROCm compatibility, notably the `nccl` and `cufft` issues, and the necessity for better environment management and testing strategies across different hardware and software configurations; and performance trade-offs related to internal guard checks, reassigning responsibilities for guard logic, and the implications of changing function argument handling (positional-only vs keyword) to improve API correctness and maintainability."
2023-04-19,pytorch/pytorch,"The comments highlight several recurring issues including dependency management and import practices (e.g., unnecessary import of `caffe2.python` in `tensorboard`), compatibility problems across PyTorch versions, operating systems, and hardware (notably CUDA, ROCm support, and macOS/M1 Mac issues). Many discussions involve build failures, environment misconfiguration (e.g., CUDA driver/runtime mismatches, missing modules like Triton), and the importance of proper serialization, submodule handling, and backward compatibility, especially regarding experimental or newly added features such as `is_causal`. There are concerns around test stability and flakiness in CI, as well as the need for clearer documentation and guidelines on argument passing conventions, module registration, and API evolution. Several suggestions for code refactoring, adding tests, or improving user experience are proposed, but some are blocked by ongoing regressions or CI failures. Overall, unresolved questions primarily revolve around compatibility, dependency management, and ensuring stable, maintainable code and tests across diverse environments."
2023-04-20,pytorch/pytorch,"The comments encompass a variety of technical concerns, primarily related to PyTorch's build process, operator support, and internal APIs. Key issues include build errors with Caffe2 and OpenCV, support for new operators like LSTMCell in ONNX, and the handling of dynamic vs static shapes during export and runtime. Some discussions suggest improving extensibility for custom backends, refining the annotation and pattern matching system for quantization, and addressing platform-specific bugs (e.g., MacOS MPS or ROCm). Several questions revolve around stability, compatibility, and performance trade-offs—such as disabling certain kernels, improving test robustness, and ensuring code changes don’t regress existing behavior. Unresolved points include how to support certain operators in specific hardware contexts, managing build environment discrepancies, and whether to refactor or extend existing APIs for better flexibility."
2023-04-21,pytorch/pytorch,"The comments reveal ongoing challenges with integrating PyTorch's scripting and compilation features, particularly around scripting variable arguments, handling dynamic shapes, and ensuring compatibility across hardware and backends like CUDA and ROCm. Several issues involve the correctness and stability of the JIT and recursive scripting, including handling empty tensors, data-dependent operators, and special cases like multi-batch and nested operations. There are concerns about the impact of recent code changes on performance, correctness, and backward compatibility, along with the need for clearer documentation of implementation details, kernel limitations, and API behaviors. Additionally, some discussions focus on managing external dependencies, build issues, and proper testing, with suggested improvements including better test coverage and more explicit support for edge cases. Overall, these reflect a state of active development, bug fixing, and refinement in PyTorch’s compiler, scripting, and runtime infrastructure."
2023-04-22,pytorch/pytorch,"The discussions mainly revolve around handling unimplemented or partially supported CUDA/MPS operations on different devices, especially MPS, and finding ways to work around these issues—such as intercepting sparse tensor conversions or bypassing unsupported operators—though these approaches risk increasing memory footprint or may cause errors like overflow. Several comments address the challenges of ensuring model compatibility and correctness when dealing with device-specific functionalities, especially for complex models like Whisper or in the context of multi-GPU and multi-device setups. Some propose improvements to existing APIs, such as adding explicit operators like `associative_scan`, or modifying internal functions (e.g., `_apply`) to better handle sparse tensors, but these can lead to new bugs or performance regressions. There are ongoing efforts to update dependencies (PyTorch, Triton) for better device support, with questions about regressions, merge conflicts, and the impact on release timelines. Overall, the key concerns involve device compatibility, workaround strategies, and maintaining correctness and performance across diverse hardware configurations."
2023-04-23,pytorch/pytorch,"The discussions highlight ongoing challenges with checkpoint loading in PyTorch, particularly around handling buffers with `None` values and documenting this behavior clearly. There are proposed improvements such as enabling `load_state_dict()` to update `None` buffers and developing an explicit `associative_scan` operator for parallel scanning, which is especially relevant for advanced models like linear state-space networks. Other concerns include inconsistencies between CPU and CUDA behaviors, performance issues with memory allocators, and the need for better support and stability for distributed training, including environment and backend configurations. Several discussions involve proactive code modifications, such as deprecating legacy features, fixing specific operator behaviors, and enhancing toolchain compatibility to improve maintainability and user experience. Unresolved questions remain around the implementation details for some features (e.g., CUDA optimizations, new operators) and ensuring compatibility across hardware and software environments."
2023-04-24,pytorch/pytorch,"The discussions highlight various technical concerns including discrepancies and potential bugs in tensor serialization, stride consistency, and device-specific behaviors (e.g., for custom backends, PCH support, and device properties like UUID). Several questions focus on improving stability and correctness in autograd, kernel performance, and operator dispatching—such as fixing in-place gradient issues, enhancing deterministic or stable kernel generation, and expanding backend registration interfaces. There are also support and testing challenges related to environment dependencies, multi-device setups, and platform-specific build issues, notably in ROCm and M1 environments. Some proposals suggest refactoring or extending internal APIs (e.g., for tensor metadata, reproducibility, and device handling) while balancing backward compatibility and ease of integration. Unresolved questions remain around ensuring consistent behavior across different hardware, platform support, and interface abstractions for customized device types."
2023-04-25,pytorch/pytorch,"The comments reflect a wide range of technical issues and discussions within the PyTorch repository, including bug fixes, performance optimizations, and API deprecations. Significant concerns involve handling of tensor device types, especially regarding deprecated classes like `torch.cuda.{Dtype}Tensor`, and ensuring correctness and stability in features like inductor and compiler enhancements. There are recurring mentions of CI test flakiness and the need for better coverage and profiling, as well as coordination challenges like synchronizing updates for internal libraries (e.g., MKL, ideep). Several discussions highlight the importance of proper error messaging and robust handling of edge cases (e.g., numerical overflow, overlapping memory regions), alongside efforts to improve the maintainability and clarity of the codebase through documentation, test coverage, and code refactoring. Unresolved questions largely focus on fixing flaky tests, ensuring compatibility across hardware and software environments, and clarifying the intentions behind API changes or deprecations."
2023-04-26,pytorch/pytorch,"The comments reveal several recurring themes: first, issues related to multi-GPU training performance and correctness, especially how DataParallel affects speed and memory efficiency; second, challenges with hardware compatibility and version mismatches, particularly concerning CUDA/ROCm support, device capabilities, and related bugs or regressions; third, difficulties in ensuring proper model sharding, checkpointing, and serialization, especially with complex or non-contiguous tensors; fourth, problems with runtime errors or crashes linked to environment inconsistencies, such as mismatched dependencies or hardware-specific bugs; and lastly, efforts to improve test coverage, CI stability, and support for new features like DTensor and Transformer modules, alongside ongoing maintenance of build infrastructure and documentation updates."
2023-04-27,pytorch/pytorch,"The discussions highlight issues with the behavior of normalization layers (e.g., BatchNorm) in training versus eval mode, particularly involving differences in variance calculation methods (biased vs. unbiased). There are concerns about potential divergence in implementation correctness and whether this behavior is intentional or warrants documentation updates. Some comments suggest that the current handling may cause subtle numerical discrepancies, affecting model reproducibility or stability, especially in stacking layers. Additional topics include the need for clearer documentation, improvements in state management across multiple devices, and ensuring that CI tests reliably cover different configurations and environments. There are also technical questions about the correctness of certain code paths (e.g., copy strides, the implications of moving code to lower levels, and handling of custom device types), and uncertainty whether specific changes (e.g., in conversion or serialization) may introduce silent bugs or regressions."
2023-04-28,pytorch/pytorch,"The accumulated comments reflect several recurring themes in the PyTorch development community. Key issues include the desire for new features such as a `no_train` context manager, better device management utilities, and improvements to the serialization/deserialization process, especially for ONNX and custom ops. There are concerns about stability and correctness of features like in-place gradient operations, the impact of framework updates on reproducibility, and ensuring features like `functionalize` support complex or data-dependent operations robustly. Several discussions revolve around fixing flaky tests, addressing CI failures, and optimizing performance across various hardware setups, including CPUs, GPUs, and specialized accelerators, often involving compiler and operator support enhancements. Unresolved questions include how to better support mixed device types, improve error messaging for inconsistencies, and integrate new features smoothly without regressions or compatibility issues."
2023-04-29,pytorch/pytorch,"The discussions mainly revolve around issues with PyCUDA and CUDA context management in multithreaded scenarios, noting that PyTorch's recent versions (1.9 and 1.10) seem less affected, possibly due to fixed behavior or improvements. There are ongoing efforts to support flexible semantics—masked versus non-masked—for sparse tensors, with proposals for context managers to control these behaviors dynamically, but concerns remain about compatibility with autograd and the complexity of nested semantics states. Several issues relate to platform-specific limitations, such as missing operator implementations on MPS devices and CUDA driver incompatibilities, often with suggested workarounds like fallback modes or driver updates. Additionally, some bugs are observed in autograd and FX graph transformations, especially involving the AOTAutograd and FX trace correctness, with hints that certain errors may be due to shape mismatches or incomplete operator support in ONNX exports. Overall, the discussions highlight both functional gaps and ongoing proposals for flexible, robust semantics management, along with platform-specific troubleshooting."
2023-04-30,pytorch/pytorch,"The discussions highlight technical concerns such as missing or misconfigured library folders causing import errors in PyTorch, especially when building from source or running code in environments with conflicting folder names. Several issues involve operational or compatibility problems on specific hardware platforms (notably MPS and CUDA backends) and with different data types, like bfloat16 support or NCCL errors on V100 GPUs. There are ongoing discussions about improving API behavior, such as clarifying the behavior of `is_causal` in Transformer modules, and the need for better handling and fallback mechanisms for unsupported features or kernel dispatch failures. Additionally, some questions address debugging strategies, support for custom operators, and the integration of advanced features like `torch.compile()` with complex models or specific hardware. Overall, unresolved issues often relate to platform-specific support, API clarity, and ensuring deterministic, robust execution across various configurations."
2023-05-01,pytorch/pytorch,"The discussions highlight recurring issues related to missing or misnamed build folders for PyTorch, which often stem from running commands in directories containing conflicting 'torch' folders or incomplete builds. Several users encounter 'ModuleNotFoundError: No module named torch._C', which typically indicates an improper installation or environment conflict. There are suggestions to check import behavior outside of problematic directories, ensure proper PyTorch installation, and verify environment setup. Additionally, some comments point to ongoing instability or flaky behavior in CI and external dependencies like network connectivity or package repositories, which complicate troubleshooting. Overall, resolving these issues involves verifying clean, correct installations of PyTorch, ensuring environment consistency, and avoiding directory conflicts during import."
2023-05-02,pytorch/pytorch,"The discussions highlight several core concerns: issues with reproducibility and correctness of specific PyTorch operations and their interactions with features like FX tracing, symbolic graph transformations, or compilation pathways (e.g., torch.compile, inductor decompositions). Several mentions relate to handling specific cases such as view operations in vmap, deformation of tensors with functionalization, and inaccuracies or incompatibilities introduced by in-place updates or tensor strides, especially under complex modes or hardware (like MPS, ROCm, or custom kernels). There are also recurring questions about environment variability, CI failures, and the robustness of the export to ONNX process, especially regarding data types and graph modifications. Many suggestions involve fixing bugs in the operator decomposition, improving debugging signals, or refining API ergonomics, often with an emphasis on proper error signaling and correctness assurances during compilation or graph transformations. Unresolved issues include supporting more diverse operations (e.g., FFT, reduce ops, batch norm with functionalization), improving tracing and symbolic conversion, and ensuring runtime stability across hardware and software configurations."
2023-05-03,pytorch/pytorch,"The comments reflect a variety of issues and discussions related to PyTorch development, including transient environment and installation problems (e.g., CUDA, MPS, and package dependencies), build and compilation issues across different platforms and backends (notably Windows, ROCm, and various compiler configurations), and ongoing experiments and performance regressions in model optimization and inference. Several discussions suggest fixing specific bugs (e.g., support for new operations, handling of fake tensors, reparameterizations), or improving usability and debugging (e.g., better error messages, environment variable management, debugging modes). Some comments are about operational concerns like CI failures, build reproducibility, and changes in the internal infrastructure or third-party dependencies. Several unresolved questions and feature requests involve API design, backends support (e.g., for Triton, ROCm, and specific hardware), and testing coverage enhancements. Overall, ongoing challenges include environment stability, backend compatibility, performance regressions, and improving developer experience and documentation."
2023-05-04,pytorch/pytorch,"The discussions highlight several technical concerns, notably the need to extend implementations such as MaxUnpool1d/3d for ONNX export, and the importance of avoiding graph breaks in modules like `SubModule`. There are questions about supporting `torch._assert` correctly within the compiler, especially regarding automatic assertions and runtime fusion issues. Some discussions address performance regressions, such as unexpected slowdowns in TIMM models and the impact of specific code improvements or heuristic changes. Other topics involve clarifying the behavior of autocast on CPU versus GPU, handling custom autograd functions and private device types, and the stability of CUDA-related operations amidst library version incompatibilities. Finally, several comments seek guidance on testing, documentation, and merging strategies to ensure correctness and efficiency."
2023-05-05,pytorch/pytorch,"The discussions highlight several technical concerns: (1) inconsistencies between PyTorch's `result_type` and NumPy's implementation, with proposals to extend support via new overloads and JIT support; (2) compatibility issues with cuDNN and CUDA versions, especially when building from source or using different driver and library configurations on different systems; (3) runtime errors and regressions caused by certain operations in mixed-precision or dynamic shape contexts, with suggestions to improve guard logic and handling of special cases; (4) challenges in integrating third-party extensions like multipy, which require building within containerized environments and managing dependencies; and (5) various build, merge, and CI failures related to environment setup, code conflicts, and infrastructure, raising questions about standardization and test robustness. Overall, unresolved questions include how to extend support for new operators, improve compatibility across device configurations, and streamline CI workflows."
2023-05-06,pytorch/pytorch,"The discussions highlight challenges related to PyTorch model export, particularly with ONNX support for operators like Einsum and handling non-constant attributes in certain operators, which can cause runtime errors during export. Several issues also address the need for better support and stability in dynamic shape handling, CUDA/ROCM compatibility, and device-specific operator implementation, including dealing with bugs in inductor, CUDA kernels, and FP32 precision concerns. There are concerns about the limitations of Python features such as *args/**kargs with FX, as well as issues with the Python environment's configuration affecting distributed training and library recognition. Additionally, some discussions focus on improving test coverage, catching subtle bugs (e.g., NaN behaviors, dangling handles), and optimizing code generation to avoid redundant computations while maintaining correctness and performance. Overall, these conversations reflect ongoing efforts to enhance PyTorch's export, runtime, and build stability across multiple hardware backends and scenarios."
2023-05-07,pytorch/pytorch,"The discussions highlight ongoing development and limitations around support for Large Model Support, MPS backend operator implementation, and integration issues with newer Python versions (e.g., 3.11) and OS environments like Ubuntu 22.04. Several comments emphasize gaps in operator coverage for MPS, such as unimplemented ops and fallback mechanisms, as well as challenges in improving guard overheads and runtime validation. There are inquiries about the status and progress of features like `aten::random_`, `torch.compile`, and support for specific operators across different hardware backends, including AMD ROCm and Apple MPS. Some technical suggestions involve moving guard checks to C++, reworking the codegen for reductions and broadcasting, and addressing CI build failures. Unresolved questions include prioritization of backend support, long-term plans for string preprocessing, and compatibility with latest Python versions."
2023-05-08,pytorch/pytorch,"The comments reflect ongoing challenges with compatibility and support across various components of PyTorch, such as issues with exporting models to ONNX (notably operators like 'aten::unflatten'), support for specific data types (e.g., BFloat16 on M1/M2 chips), and operator coverage for different backends (e.g., MPS, CUDA, ROCm). There are frequent discussions about fixing or workarounds for operator support gaps, build failures due to compiler or infrastructure issues, and the need for better testing and validation, including manual and automated test cases. Several comments highlight the importance of supporting features like activation checkpointing with torch.compile, and the complexities of ensuring models, especially large or custom ones, are portable and performant. Overall, unresolved concerns include improving operator coverage, supporting new hardware (e.g., M2, CUDA architectures), addressing build/test infrastructure failures, and ensuring long-term support plans for features like TorchScript and string preprocessing."
2023-05-09,pytorch/pytorch,"The discussions highlight several key concerns: (1) limitations of exporting models to ONNX with unsupported operators like 'aten::unflatten', and workarounds with custom implementations or operator support requests; (2) challenges in handling specific operators in distributed or CUDA contexts, including unimplemented support for operators like 'torchvision::nms' and 'aten::_assert_async.msg', especially when dealing with different hardware backends like ROCm or MPS; (3) issues related to data type support (e.g., bfloat16), and ensuring consistency between CPU and GPU/autocast behaviors, alongside handling of potential overflows in integer summations; (4) difficulties in implementing custom autograd functions for complex operations such as DEQ layers due to current limitations in module-level backward overrides and the inability to input modules into autograd.Function; (5) considerations around testing strategies, including how to automate or isolate tests for privateuse1 features, and whether to expose internal compiler passes to users or keep them tightly controlled for reproducibility. These points reflect ongoing efforts to extend operator support, improve exportability, handle cross-backend compatibility, and refine testing and API design."
2023-05-10,pytorch/pytorch,"The comments reflect widespread challenges in achieving full reproducibility and debugging in PyTorch, especially when involving GPU operations, random seed settings, and engine-specific features like cuBLAS or the new compiler passes. Several users question the appropriateness of current behaviors, such as whether certain operations should support specific dtypes (e.g., float16 on CPU), or the need for more explicit error handling, test coverage, and deterministic options. There are ongoing discussions about better handling of custom modules with backward functions, serialization of compiled models, and ensuring consistent behavior across different hardware backends and build environments. Additionally, some comments highlight infrastructure issues like CI failures, merge conflicts, and the importance of internal documentation practices. Overall, unresolved questions include how to improve reproducibility, error reporting, and the extensibility of the compilation and dispatch systems."
2023-05-11,pytorch/pytorch,"The discussions highlight ongoing efforts to improve PyTorch's feature support and reliability, including issues with internal and external build dependencies, device selection, and reproducibility. Several concerns are about serialization of compiled modules, handling mixed precision and deterministic algorithms, and enabling custom kernel support for MPS/JIT backends. There are also technical questions on ensuring correct guard semantics, preventing unintended cloning, and handling model reserialization. Additionally, issues about CI stability, test flakiness, and environment configuration emerge, with suggestions to enhance tooling, logging, and validation strategies. Overall, key unresolved questions involve robustness of caching mechanisms, correct behavior under various hardware and software configurations, and improving developer tooling for debugging and deployment."
2023-05-12,pytorch/pytorch,"The discussions predominantly revolve around various technical issues and bug reports spanning PyTorch's CUDA, MPS, and distributed APIs, including kernel implementation bugs, compatibility and correctness issues with model serialization (especially in non-ASCII paths), and precise control of behavior under deterministic modes. Many concerns involve backend and hardware support, such as problems with multi-stream CUDA execution, ROCm support, and kernel linking issues in the build system. Several discussions also address the need for improved or clarified documentation on features like `torch.where` scalar arguments, `torch.utils.checkpoint`, and `torch.compile` interactions, as well as maintenance tasks like updating dependency links, test disabling procedures, and ensuring reproducibility. Unresolved questions include how to better support cross-platform, multi-device, and sparse gradients while maintaining BC guarantees, and how to structure certain APIs or internal mechanisms (e.g., custom operators, kernel registration, API deprecation)."
2023-05-13,pytorch/pytorch,"The discussions highlight several key concerns: a request for GPU-accelerated implementations of specialized optimizers like SQP, with community members seeking existing solutions or guidance; questions about correctly managing policies and dynamic adjustment for recomputation and offloading checkpoints to CPU; issues related to device-specific operator support, such as MPS and GPU compatibility, including bug fixes and performance regressions; challenges in model exporting (e.g., ONNX opset compatibility) and runtime errors caused by code changes or missing attributes; and broader performance considerations, including the impact of environment setups, threading, type promotion behaviors, and runtime warnings, with ongoing efforts to improve reliability, compatibility, and documentation clarity. Unresolved questions remain about timing of fixes reaching stable releases, handling of certain operations with mixed types, and compatibility of newer features across hardware platforms."
2023-05-14,pytorch/pytorch,"The discussions highlight several technical concerns: the necessity of ensuring tensor contiguity for distributed operations like `all_gather`, `reduce_scatter`, and their backward implementations; the presence of potential inconsistencies or errors introduced by recent PRs affecting NCCL and CUDA-related functionalities, including import errors and environment-specific failures; questions about the performance implications and proper management of weight tying and parameter management when using FSDP, especially with `use_orig_params=True`; and issues related to the proper setup, signing of CLA, and merge conflicts in various PRs. Additionally, some discussions emphasize the importance of verifying multi-GPU, multi-node configurations, and the importance of rebasing or merging PRs only after ensuring all checks and documentation are correct. Overall, the main themes revolve around correctness of distributed tensor operations, environment and environment-specific failures, and proper parameter handling in large-scale training setups."
2023-05-15,pytorch/pytorch,"The discussions cover several technical issues, including the need to update Docker images with newer versions of `git` to avoid submodule errors, and handling multiprocessing spawn issues on macOS by adding the `__name__ == '__main__'` guard. Some commenters express concerns about PyTorch build stability, especially involving CUDA and ROCm support, suggesting environment-specific adjustments and workarounds. Others discuss the complexity of extending PyTorch's autogenerated code, the support for `torch.compile()` with custom models, and performance considerations for features like sparse gradients and inductor optimizations, often requesting clearer documentation or alternative implementation strategies. In addition, there are discussions on build configuration issues, support for new hardware (like AMD Navi22 GPUs), and the need for more robust test and CI infrastructure, including handling flaky tests and supporting various platform configurations. Overall, unresolved questions revolve around balancing backward compatibility, performance gains, and developer ergonomics in evolving features."
2023-05-16,pytorch/pytorch,"The comments highlight concerns about the large number of CUDA kernels loaded during PyTorch initialization and how this impacts startup time, with suggestions to split kernels or use persistence mode for improvement. Several issues discuss the behavior of `torch.norm` defaults, deprecation, and default semantics, including how to properly update or flag deprecated functions. There are multiple reports of slow inference speeds on Apple M1/M2 devices and discussions on optimizing MPS performance, as well as questions about supporting specific ONNX operators like 'aten::unflatten'. Some comments focus on improving testing infrastructure, dependency management, and CI behavior, with attention to environment-specific issues, custom operator registration, and codebase stability. Overall, unresolved questions include detailed implementation strategies for context managers, operator extension points, reproducibility, and platform-specific performance tuning."
2023-05-17,pytorch/pytorch,"The discussions primarily revolve around managing environment and code compatibility issues, such as GitHub branch name changes (master to main), dependency versions (e.g., numpy, mkl, torch, torchvision), and platform-specific bugs (e.g., CUDA, ROCm, MPS, inductor). There are concerns about untracked regressions caused by code changes, especially in distributed training, autograd, and operator support, often requiring re-basing, re-trying, or reverting PRs. Several questions concern the implementation details of features like torch.compile, model checkpointing, autocast handling during graph partitions, and safe usage of APIs like torch.jit. Some issues highlight need for improved testing, clear documentation, and stable CI processes, especially with complex system integrations and custom operators. Unresolved questions include environment setup for specific hardware (GPU, MPS, ROCm), proper handling of symbolic/graph transformations, and ensuring backward compatibility with previous versions."
2023-05-18,pytorch/pytorch,"The discussions mainly revolve around PyTorch's memory management, especially related to CUDA graphs and fragmentation, with questions on how to make graphs work efficiently with shape variations and how to monitor recaptures. There are concerns about the implementation of `requires_grad` on non-leaf tensors and how to freeze or modify gradients of weights, especially in RNNs. Some issues address the support for new features like complex number operations, ONNX operator support, and type annotations in torch.jit. Additionally, there are questions on how to handle dynamism in graphs, such as shape variability and subgraph recomputation, and how particular internal APIs or behaviors (like `nn.Module` hooks or in-place tensor modifications) impact correctness and performance. Many conversations also involve roadmap considerations, including prioritization, the impact of higher-order ops, and handling legacy versus new features."
2023-05-19,pytorch/pytorch,"The discussions encompass multiple issues related to PyTorch/LibTorch, including memory leaks in multithreaded inference with libtorch, the implementation status of differentiable incomplete gamma functions, and the support of specific operations and features like _foreach_max, meta API, and CUDA graphs. Several reports indicate compatibility and stability concerns, such as failures during DLL unloading on Windows, problems with get_backend_meta_serialization, and the need for better handling of dynamic shapes and AutoGrad behaviors. Also, there are ongoing efforts to enhance dataset serialization with mmap, to improve implementation of PyTorch's internal builds (e.g., IWYU support, compiler flags), and to address internal CI failures. Overall, key technical concerns include ensuring stability and correctness in multi-platform environments, improving API support (e.g., for pytrees, serialization, quantization), and refining feature integration (e.g., CUDA graphs, dynamic shape handling)."
2023-05-20,pytorch/pytorch,"The comments highlight several technical issues and discussions, including resource constraints leading to DataLoader worker terminations, the desire for better support or documentation for negative strides in PyTorch, and efforts to improve memory allocators like mimalloc and jemalloc, with suggestions to prioritize tc_malloc. There are questions about reducing verbosity in the PyTorch profiler, discrepancies in ONNX model outputs, and the need for more user-friendly environment configurations. Multiple contributors are working on bug fixes, feature enhancements, and API improvements such as in-place tensor operations, quantization workflows, and profiling tools, often with emphasis on testing, documentation, and usability. Concerns about ensuring proper model loading, memory-mapped storage support, and the flow of torch.compile with quantization indicate ongoing efforts to streamline and stabilize core functionalities. Overall, the discussions focus on optimizing performance, user experience, and robustness of PyTorch's features and integrations."
2023-05-21,pytorch/pytorch,"The discussions encompass a range of technical concerns: from a GitHub issue questioning the correctness of the `MultiLabelSoftMarginLoss` implementation and its documentation, to runtime CUDA errors and system stability issues potentially caused by hardware or environment misconfigurations. Several comments address upcoming features, like 3D convolutions and max pooling on MPS, and the need for better profiler log level control. There are debates about code modifications, such as how to handle pytree modeling in Dynamo and whether to support in-place flip operations, along with suggestions for improving user experience, test coverage, and build stability. Unresolved questions include proper implementation of control flow operators to work seamlessly with autograd and subsystems, and handling performance regressions or errors introduced by recent PRs."
2023-05-22,pytorch/pytorch,"The discussions highlight issues with GPU memory management during validation, suggesting to use `torch.no_grad()` or `@torch.no_grad()` during evaluation to free activations. Several threads address deprecated or confusing functions like `torch.norm` and the importance of guiding users on updated usage. Multiple questions revolve around improving determinism, especially with CUDA and inductor optimizations, as well as ensuring proper function behavior on different devices (e.g., MPS, CUDA). Some threads emphasize the need for better testing, code organization, and handling of advanced features like FSDP, hooks, and pytrees, with suggestions to model certain behaviors or temporarily disable flaky tests. Overall, unresolved issues concern robustness, correctness, and usability enhancements in memory handling, debugging, and API guidance."
2023-05-23,pytorch/pytorch,"The discussions primarily revolve around improving robustness and functionality in data loading, training, and model handling within PyTorch. Key concerns include handling corrupted samples during dataset loading to prevent training shutdowns, with proposed solutions involving filtering `None` values in `collate_fn()` and addressing edge cases where all samples are corrupted. There are questions about managing batch-level errors like empty batches, batch size dependencies, and how to gracefully handle batches where all entries are `None`. Additionally, some discussions focus on code hygiene, such as avoiding using `static` in favor of anonymous namespaces, and addressing linker issues caused by static functions. Unresolved questions remain about optimal strategies for error handling in batch processing, the impact of certain code refactoring decisions on backward compatibility and performance, and ensuring test robustness against platform and version-specific failures."
2023-05-24,pytorch/pytorch,"The discussions primarily revolve around extending functionality or fixing issues in PyTorch: adding support for bounds in LBFGS, improving indexed tensor usage in C++, and handling dynamic/static shape inference with FX and torch.compile. Concerns include ensuring backward compatibility, avoiding regressions in existing features, and clarifying the intent of certain API changes (e.g., what ""opcheck"" should verify). Several comments highlight potential performance regressions, platform-specific failures, and merge conflicts that need resolution. The questions also touch on compatibility with external projects like ONNX, Triton, and CUTLASS, and how internal infrastructure changes impact the Python/C++ API. Overall, unresolved questions include validating the correctness of new features, avoiding regressions, and ensuring proper testing and documentation before merge."
2023-05-25,pytorch/pytorch,"The collected comments highlight several recurring issues and feature requests in the PyTorch repository. Notably, there is concern about the lack of standard, robust functionalities for checkpoint loading with buffers of unknown or dynamic size, with suggestions to improve buffer initialization, support for None-valued buffers, and better documentation. Multiple discussions involve debugging CI flakiness, support for new features like disk-backed tensors, large tensor handling, and interoperability with formatting standards such as ONNX schemas. There are ongoing debates around implementation details for custom operators, process group controls, and maintainability of schema-promoting mechanisms. Unresolved questions include how to extend core functionalities reliably, coordinate cross-framework schema handling, and improve build and testing robustness across platforms."
2023-05-26,pytorch/pytorch,"The comments highlight several technical concerns, primarily centered on stability, correctness, and usability of features across different hardware and software configurations. Key issues include reproducibility of bugs related to specific device support (e.g., MPS, ROCm), potential regressions caused by recent PRs, and the need for better handling or support of certain operations (like `torch.sort`, `max_pool3d_with_indices`) on specific platforms. Discussions also raise questions about utility functions, test stability (tracial, flaky), and API design choices (e.g., for custom devices, code generation, tensor slicing). Several comments suggest actively reverting or fixing problematic PRs, improving test coverage, or clarifying API behaviors with minimal changes, indicating ongoing debugging and validation efforts. Unresolved questions involve how to properly support out-of-core dtype, better handle exotic cases (like `float8`), and improve platform-specific support or stability."
2023-05-27,pytorch/pytorch,"The discussions reveal multiple technical concerns primarily related to hardware and software compatibility issues on Mac M1/M2 and other systems, including runtime errors with MPS and in-place operation semantics, and the need for debugging improvements. Several issues include runtime errors such as `RuntimeError: MPS does not support cumsum op with int64 input`, and memory leaks during repeated loading of checkpoints, which may be tied to device support complexities. There are also recurring questions about supporting third-party devices in distributed setups, compatibility with third-party libraries like TF, and build/debugging workflows that could streamline troubleshooting. Additionally, some issues stem from changes in underlying libraries, such as ArrayRef overflow checks or in-place operation handling in `torch.distributed`. Unresolved questions include how to improve cross-library compatibility, better support for exotic hardware, and simplifying debugging processes for low-level operations."
2023-05-28,pytorch/pytorch,"The discussions highlight several technical issues with PyTorch, including a bug in `torch.multinomial` where error handling is ambiguous, and support for AMD CPUs and ROCm, which remains incomplete or unsupported. Users also report problems related to `torch.compile`, especially when combined with activation checkpointing and device-specific behaviors (e.g., MPS, RNNs, and `transpose` issues), sometimes requiring workarounds or code modifications. Debugging and building PyTorch from source is also a concern, with suggestions to use contiguous clones or caching to troubleshoot stride or API-related issues. Additionally, there are ongoing efforts to improve system compatibility, such as supporting newer compiler versions, handling device defaults safely, and resolving multi-node communication bottlenecks, though some questions about profiling and critical path analysis remain."
2023-05-29,pytorch/pytorch,"The discussions primarily revolve around the need for an explicit, user-friendly primitive for associative scans (e.g., `associative_scan`), which would improve parallelization capabilities in PyTorch. Several issues concern build and compatibility challenges, especially with hardware like AMD CPUs, and specific features such as support for `bfloat16` or third-party devices. There are also many reports of flaky tests, CI failures, and performance regressions, often linked to specific platforms or recent changes, highlighting ongoing stability and testing concerns. Debugging complex issues like memory management, operator support, and backend limitations (e.g., Windows build errors with MSVC, operator support in ONNX scripting) remains a recurring theme. Overall, while some features and optimizations are progressing, significant questions remain about platform support, test reliability, and feature completeness, especially for advanced hardware and large-scale distributed setups."
2023-05-30,pytorch/pytorch,"The discussions cover a wide range of topics related to PyTorch development, including the behavior of tensor methods like `.size()` vs `.numel()`, differences in support for dynamic vs static shapes in `torch.compile`, and issues with ONNX export involving sparse tensors. Several technical concerns involve the correct handling of tensor views on different hardware backends (especially MPS) and ensuring proper tensor cloning or contiguity to avoid bugs, particularly in CUDA/MPS interactions. There's also ongoing discussion about improving internal APIs such as `ir.Reduction`, optimizing communication patterns in distributed training (FSDP, inter-node bandwidth), and fixing bugs in operator registration and gradient computation (e.g., autograd with in-place ops, autograd guards). Additionally, manual or automated testing workflows, CI stability, and the integration of custom or third-party hardware/LLVM features (e.g., support for opaque pointers or new Triton versions) are important unresolved issues. Overall, the focus is on stability, correctness, performance improvements, and better support for hardware heterogeneity."
2023-05-31,pytorch/pytorch,"The discussions cover a variety of issues including serialization bugs in learning rate schedulers, the behavior and semantics of `torch.multinomial`, and the support for BFloat16 on M1/M2 chips with the MPS backend. Several comments propose fixes, such as making methods static to avoid serialization issues, clarifying the API and behavior of multinomial sampling, and updating hardware support in PyTorch for BFloat16. Researchers also raised questions about performance regressions, potential bugs in distributed training setups (e.g., differences between DDP and FSDP, inter-node communication costs), and the importance of robust testing and CI stability. Unresolved core questions involve handling negative symbolic dimensions, improving API visibility and stability, and troubleshooting reproducibility and performance issues across different hardware and software configurations."
2023-06-01,pytorch/pytorch,"The discussions highlight ongoing issues with crashing and non-deterministic behavior in PyTorch, often related to specific hardware backends such as ROCm, MPS, and CUDA, with some failures caused by segmentation faults or flakes possibly related to environment inconsistencies, driver incompatibilities, or in-depth bugs (e.g., in layout optimization, split reduction, or in the integration of extensions like Triton). Several threads focus on improving testing stability, handling of distributed training (DDP vs FSDP), and ensuring that environment setup (like LD_LIBRARY_PATH) is correct. There are also concerns about the correctness of autograd, compilation, and graph transformation, including handling of views created in no_grad context, as well as the need for better logging, error messages, and more robust CI validation processes. Finally, some discussions involve patches and PRs with uncertain future workflows, such as fixing bugs, improving reproducibility, and integrating new features like custom C++ logging handlers and device support."
2023-06-02,pytorch/pytorch,"The comments reveal ongoing issues and discussions related to PyTorch's hardware support, particularly with AMD/ROCm and OpenCL, compatibility and build configurations for different architectures, and experimental features like sparse matrix inverse, guard mechanisms in tracing, and custom batched rules. Several reports discuss intermittent CI failures, flaky tests, and stability problems when deploying on specific hardware like GPUs (e.g., RTX, AMD MI250x, H100) or environments like MPS on Macs. There are also requests for feature enhancements such as better ONNX support, tensor subclassing, and guard handling, along with infrastructure concerns around test stability, build system modifications, and CLA signing requirements. Overall, the discussions focus on debugging runtime issues, improving hardware and software integrations, and streamlining testing and build workflows, with some tasks pending review or resolution."
2023-06-03,pytorch/pytorch,"The discussions predominantly revolve around the behavior and bug reports associated with BCEWithLogitsLoss and its numerical stability, particularly concerning how the loss handles log(0) conditions and underflow with the log-sum-exp trick. Several contributors question whether this is a bug or an expected implementation detail, with some proposing that the current handling might be a bug or a design choice needing clarification. Additional concerns include the performance implications of specific features, such as the management of cache lookups, and compatibility issues across different hardware backends (e.g., ROCm, MPS, CUDA). There are also reports of errors occurring in quantized and scripted models, especially with quantization-related operations in the quantized batch normalization, and in certain sparse tensor constructions, alongside varying performance benchmarks on different architectures. Overall, the key unresolved questions relate to the correctness and numerical stability of loss functions, handling of dynamic shapes, and consistency across hardware and quantization modes."
2023-06-04,pytorch/pytorch,"The discussions primarily revolve around hardware and software limitations: the neural engine in Apple Silicon supports only low-precision computations (FP16 and INT8), making it unsuitable for training deep models, and there are questions about leveraging MLCompute and CoreML for ML workloads on Apple devices. Several users face CUDA-related issues due to mismatched driver and library versions, particularly with PyTorch compatibility and GPU recognition, especially on different hardware configurations like A100s or older GPUs with insufficient compute capabilities. Other concerns include interoperability and support for sparse and dynamic shapes, with questions about API behaviors, such as the equivalence of calling RNNs once versus iteratively, and the need for better testing and release management, including CI failures and PR reverts. Additionally, some discussions address build failures, dependency issues (like missing magma libraries), and the necessity for clearer API annotations and better documentation practices."
2023-06-05,pytorch/pytorch,"The discussions cover several technical issues within the PyTorch project, including challenges with 1) properly stopping gradient flow for non-leaf tensors and how detaching or modifying requires_grad flags can impact backward, especially in complex graphs like RNNs; 2) integrating new features like reparametrization (`parametrizations`) and ensuring compatibility with diverse models and serialization; 3) support for distributed training strategies such as FSDP, especially multi-node sharding strategies like `HYBRID_SHARD`, including code patterns and potential stability issues; 4) handling of compatibility and reproducibility issues on different hardware backends (CUDA, ROCm, MPS), including driver and version mismatches, driver bugs, and hardware-specific limitations; and 5) improving testing infrastructure, including test flakiness, CI isolation, and ensuring that new code (e.g., custom operators, new dispatch keys) integrates smoothly without causing build failures or instability. Several discussions also highlight the need for clearer documentation, better failure diagnostics, and the importance of precise version control and environment setup to ensure reproducibility across platforms."
2023-06-06,pytorch/pytorch,"The discussions primarily revolve around existing bugs and behavior inconsistencies in PyTorch, especially related to CUDA/ROCm device handling, sparse tensor support, and the internal dispatcher architecture. Several issues mention problems with multi-node training, kernel irregularities, and performance regressions due to recent commits, with some workarounds involving environment settings or code modifications. The proposed solutions include restructuring the FX-to-ONNX export pipeline for better layering and clarity, improving the dispatch mechanism to handle all FX node types uniformly, and refining device-specific initializations. There are concerns about compatibility and correctness, especially regarding coalesced tensors and device context management, with suggestions to improve error messaging and code robustness. Overall, unresolved questions include how to best design the dispatcher API, handle device and layout invariants efficiently, and ensure stable multi-device training."
2023-06-07,pytorch/pytorch,"The discussions highlight ongoing challenges with checkpointing in PyTorch, especially for buffers with unknown sizes and dynamic shapes, with suggestions to support uninitialized buffers or lazy loading mechanisms. Several issues pertain to improving the API for buffers, such as BufferList/BufferDict, and clarifying behaviors around buffer registration, shape inference, and inference mode compatibility. There is also concern about the complexity and maintainability of the dispatcher pattern used in export pipelines, with debates about encapsulating dispatch logic into classes versus pattern-based approaches. Additionally, issues involving distributed training (FSDP, DDP), compile errors, and CI flakiness are mentioned, often resolving from recent fixes or requiring further investigation. Overall, the key unresolved questions center on API design for buffers, consistency of behavior across device types, and simplifying internal dispatching mechanisms."
2023-06-08,pytorch/pytorch,"The discussions cover various issues and feature requests related to PyTorch, including support for Windows and MacOS, CUDA and MPS device issues, and multi-process memory management challenges. Several bug reports highlight the need for better error messaging, improved handling of complex autograd and graph break scenarios, and the importance of proper environment setup for GPU/TPU/ROCm compatibility. Some discussions involve refactoring efforts, such as integrating dispatch logic into classes for cleaner code, and ensuring test stability across diverse platforms. Additional concerns include dependency updates on PyPI, maintaining consistent package dependencies for different platforms, and addressing flaky CI tests due to platform-specific issues. Overall, these conversations point to ongoing efforts to improve robustness, usability, and correctness of the PyTorch ecosystem across multiple hardware and software configurations."
2023-06-09,pytorch/pytorch,"The discussions highlight concerns about proper configuration and clarity in using PyTorch's CMake setup, especially regarding the environment variable `Torch_DIR` and the build process for libtorch, with user confusion about the proper usage of `CMAKE_PREFIX_PATH`. There are questions about default initialization schemes for neural network layers and how they affect training; suggestions include adopting Kaiming He initialization as the default and providing both backward compatibility and options for custom initializers, possibly via decorators. Several issues involve default or incompatible settings in distributed training, such as NCCL port configuration, firewall limitations, and the management of ephemeral ports, alongside performance regressions or flaky tests in CI, often due to platform-specific or infrastructure-related problems. Additional feedback points to the need for better documentation, explicit default handling in frameworks (e.g., sparse tensor support, operator registration, and default graphs), and the importance of robust testing, including end-to-end and regression tests for features like initialization schemes, initialization defaults, and deployment workflows."
2023-06-10,pytorch/pytorch,"The discussions highlight persistent issues with CUDA errors such as CUDA initialization errors, illegal memory access, and unpredictable failures during training, especially in multi-GPU or distributed settings, often triggered by specific combinations of weights and inputs or by system configurations like undervolted GPU profiles. Several threads question the compatibility and support for Windows (including PyTorch 2.0+ and CUDA 11.6), macOS with MPS, and other hardware/software environments, noting that certain features or dependencies (like Triton, system oneDNN) are either unsupported or cause conflicts, complicating installation and deployment. There are concerns about performance regressions caused by recent code changes, particularly with the `pyhpc_equation_of_state` model, and the need for transparency about caching, recompilation, and `torch.compile` behavior. Additional topics include improving documentation for missing files, handling of tensor operations and in-place modifications, and integrating experimental features like string tensor support or telemetry. Unresolved questions involve system-specific issues, like GPU profiling impacts, custom kernel development, and more explicit guidance/documentation for users."
2023-06-11,pytorch/pytorch,"The discussions highlight challenges related to networking issues in distributed PyTorch setups, particularly involving NCCL, NAT traversal, and API usage in different environments like Docker, WSL2, and Windows. There are concerns about modifying environment variables at runtime affecting device counts, with solutions involving patching or overwriting functions like `torch.cuda.device_count`. Several issues address discrepancies or bugs in specific features such as MPS operations on Mac, support for float masks in nested tensors, and performance regressions. The merge conflicts and unutilized code in ongoing PRs raise questions about code maintenance, while many comments focus on improving documentation, release notes labeling, and clarifications around caching and recompilation behaviors. Unresolved questions remain about optimizing small batch performance, backpropagation in object detection, and support for additional data formats like TIFF."
2023-06-12,pytorch/pytorch,"The comments reflect a range of issues encountered in the PyTorch repository, including build and compatibility problems, such as architecture-specific linker errors on M1 Macs and arm64, and Python version-related assertion failures in tests. Several discussions involve CI failures caused by flaky tests, flaky environment configurations, or outdated cache issues, with suggestions such as switching to `mamba` or creating dedicated testing environments. There are also questions about potential regressions, such as changes to BC-breaking APIs, or incompatible behavior with custom operators and backend registration, often with requests for better test coverage or validation methods. Additionally, some comments address platform-specific concerns, like CUDA support for large device counts, or memory usage in new versions, as well as proposals to improve user experience, such as better naming conventions and telemetry for deployment. Overall, unresolved issues include platform-specific build failures, flaky tests needing stabilization, and the necessity for clearer testing and validation strategies, especially around new features and backward compatibility."
2023-06-13,pytorch/pytorch,"The comments highlight technical concerns regarding specific implementation details, such as the appropriateness of includes in generated headers, naming conventions for parameters (e.g., `output_dtype` vs. `compute_precision`), and the correctness of certain algorithmic transformations, especially in quantization and tensor shape inference for models like LLMs and attention mechanisms. Several discussions involve potential bugs or inaccuracies in gradient computations, numerical stability, or type promotion behavior, often with suggested workarounds or the need for more precise fixes (e.g., in the `exponential_` function or `gradcheck` assumptions). Issues related to test flakiness, flaky CI, and the need for better or additional test coverage are frequent, especially concerning flaky test disabling and re-enabling logic. Requests for code review, better documentation, or clarifications on implementation choices, as well as how to improve performance profiling and telemetry, are also prevalent. Overall, the discussions focus on correctness, clarity, and robustness of PyTorch internals, especially around quantization, gradient correctness, test reliability, and internal APIs."
2023-06-14,pytorch/pytorch,"The comments encompass a wide range of technical issues, mainly focusing on correctness and implementation details in PyTorch, such as the behavior of `AvgPool2d` with `ceil_mode`, potential bugs in `gradcheck` with sparse tensors, and internal mechanisms for device-specific autograd support. Several discussions analyze the mathematical correctness of operations like pooling size calculations, the proper handling of sparse and dense tensor conversions, and the implications of promoting data types during operations. There are also questions about internal infrastructure improvements, like merging reduction operations, adding support for third-party devices, and fixing environment-specific build issues. Many comments are resolved or marked as miscellaneous, but key questions remain about correctness, performance trade-offs, and proper API design choices."
2023-06-15,pytorch/pytorch,"The comments reflect various issues related to PyTorch development, testing, and functionality, such as handling complex data types in exports, the implementation of custom backend functions, and GPU/memory management, especially on ROCm and MPS devices. Several discussions involve code rebase efforts, merging conflicts, CI flakiness, and the need for internal or external code review and testing. There are proposals to improve user experience, e.g., better warning messages, adding features like string support, and clarifications on behaviors like memory initialization and export serialization. Additionally, some comments request or suggest improvements in testing strategies, internal infrastructure, and documentation updates. Many issues are ongoing, with some requiring code review, testing, or internal approval before resolution."
2023-06-16,pytorch/pytorch,"The discussions primarily revolve around correctly enforcing operation ordering constraints, particularly emphasizing that current dependency token mechanisms (`dep_token`) may be insufficient by only preventing reordering of stateful operations and not accounting for the order of stateless ops following or preceding them. Questions are raised about whether dependency tokens should be replaced with direct tensor outputs or integrated with existing dependency graphs to better capture semantics like ""assert x > 0"". There's also debate about whether `dep_token` should address only GC-related dependencies or also enforce execution order to prevent incorrect reordering, especially in scenarios like assertions and data flow. Some suggestions include making dependency tokens propagate naturally through the graph or switching to explicit dependency edges, while concerns are expressed about the complexity and correctness guarantees of such approaches. The overall unresolved issue is how best to impose precise, reliable operation ordering constraints that align with execution semantics without overcomplicating the design or risking silent errors."
2023-06-17,pytorch/pytorch,"The discussions predominantly revolve around issues encountered during model export, TorchScript conversion, and deployment, specifically relating to dynamic attribute access with `getattr`, and concerns about deterministic behavior and reproducibility in certain operations like `cumsum`. Several comments highlight errors caused by missing or incompatible CUDA libraries, including suggestions like creating symbolic links for `libcuda.so` or adjusting environment variables, though these are cautioned as potentially unsafe. Questions also appear regarding extending PyTorch's API for equivalent type mappings to support extension libraries and model pattern matching, with proposed interfaces for customization and extension. Additionally, there are reports of broken CI workflows and merge failures due to unrelated test failures or missing CLA signatures, alongside feature requests such as adding support for certain sampling methods or improving API annotations with type hints. Overall, unresolved concerns focus on stability and correctness of model export/import, determinism, and extension API flexibility."
2023-06-18,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch on Mac, particularly related to GPU support and CUDA deprecation, with some users struggling to find solutions for CPU-only environments, especially on M1/M2 Macs. There are concerns about runtime errors like unspecified launch failures during CUDA operations, possibly caused by driver issues, hardware variability, or system configurations such as overclocked profiles. Several threads discuss potential fixes, including reinstallation, environment recreation, and hardware/driver adjustments, but the root causes remain complex and hardware-dependent. Additionally, some conversations focus on improving support for newer hardware and APIs, like C++ DDP, and addressing confusion around function overloads and dispatching in the codebase."
2023-06-19,pytorch/pytorch,"The discussions highlight several technical challenges and considerations within PyTorch development. Notably, there are issues related to CUDA memory management, especially with the interaction between the main thread's memory pool and child threads, leading to out-of-memory errors. There are debates on API behavior for in-place operations, like `nn.Embedding`, and the need for clearer documentation and behavior consistency, especially concerning in-place vs. out-of-place transforms. Additionally, stability and correctness concerns are raised regarding specific functionalities such as sparse tensor conversions, model weight loading/serialization, and dynamic shape support in TorchScript/Triton, as well as handling errors from dynamically loaded libraries. Unresolved questions include improving debug info for failures, managing model export flows with model checkpoints, and clarifying behaviors for new features or API changes."
2023-06-20,pytorch/pytorch,"The comments reflect various issues and feature requests across the PyTorch repository, including handling of in-place operations, proper support for serialization and checkpointing with complex state, and the need for improvements in data pipeline reproducibility (e.g., snapshotting dataloaders). Several discussions highlight the importance of supporting save/load mechanisms for stateful components like samplers and iterators, with some proposing enhancements or new protocols like `state_dict()` and `load_state_dict()`. There are also concerns regarding performance optimizations, such as reducing CUDA kernel dispatch overhead for small matrix multiplications, and the integration of low-precision types like FP8, which involve questions around scaling, custom kernels, and native dtype support. Additionally, interface consistency and robustness of functions like `item()`, `argsort()`, and the support for stable sorting on GPU are raised, along with the need for better debugging, error messages, and support for experimental features (e.g., FP8, sparse tensor handling). Several unresolved questions pertain to release timelines, integration plans for features like `torch.compile`, and addressing flaky tests or CI stability issues."
2023-06-21,pytorch/pytorch,"The discussions cover several topics including the handling of mismatched tensor weights during loading (with suggestions for better error reporting), softmax stability with -inf inputs, support for custom and extended op types in graph matching, and fixing specific bugs related to dynamic shape inference, sparse tensor operations, and NCCL multi-threading support. There are also ongoing efforts to improve CI reliability, testing strategies, and internal build configurations. Some issues involve clarifying internal design decisions, especially regarding compatibility, inference correctness, and framework extensibility, with proposed code adjustments and plans for future features. Unresolved questions include precise support timelines, robustness of certain features across backends, and how to effectively control testing scope and configuration."
2023-06-22,pytorch/pytorch,"The discussions cover several technical topics, mainly focusing on extending batched sparse matrix multiplications to support batched sparse matrices, and the need for upstream support in PyTorch for sparse matrix operations and batched CSR formats. There are also issues related to performance regressions in MPS/mps for M1/M2 hardware, which seem to be hardware or driver-specific, and the need for more robust testing and documentation for features like fp8 data types, inline parameter updates, and the handling of saved intermediates during autograd. Some discussions involve broader infrastructure concerns, such as build system configurations on different platforms, job reruns, and test flakiness across various CI environments. Unresolved questions include how to properly support and document these features for stable upstream use, managing platform-specific bugs, and improving diagnostics and testing coverage for new or experimental functionalities."
2023-06-23,pytorch/pytorch,"The comments indicate a range of issues related to ONNX export with control flow, quantization, and symbolic shape management, emphasizing the need for more robust handling of dynamic control flow, especially booleans and tensor conditions, since only tensor-based conditions are compatible with ONNX. Several discussions focus on improving the traceability and correctness of models with multiple control flow and quantization schemes, suggesting enhancements in how symbolic shapes, guards, and cached models are managed to avoid unexpected behaviors or failures, particularly with mixed precision and quantization backends like cuBLASLt and MKL. There are concerns about the stability and performance implications of these features, alongside requests for clearer documentation, better testing, and configuration policies—especially around supporting flexible control flow constructs, custom ops, and consistent numerics across different precision levels. Additionally, some comments highlight infrastructural considerations, such as managing CI flakiness, build dependencies, and cache invalidation strategies to ensure reliable testing and deployment. Overall, the key issues involve improving robustness, usability, and correctness in handling dynamic shapes, control flow, and quantization in PyTorch's compilation and export pipelines."
2023-06-24,pytorch/pytorch,"The discussions highlight several technical concerns, including challenges with loading object detection models into LibTorch C++, particularly support for torchvision ops like NMS in TorchScript, and workarounds involving source builds. Additionally, there are ongoing efforts to incorporate new FP8 data types and support for hardware like H100 and Graphcore IPUs, raising questions about compatibility, scale handling, and integration with existing tools such as torch.compile and FX graph transformations. Other issues involve device-specific behaviors, such as stride inconsistencies in tensor views across backends, and difficulties with large tensor reductions exceeding integer limits in CUDA kernels. There are also concerns about thread safety in CUDNN APIs, exception handling conventions in custom allocators, and maintaining consistency or reliability in features like dynamic shapes, quantization, and model export support in the evolving PyTorch ecosystem."
2023-06-25,pytorch/pytorch,"The discussions primarily revolve around clarifying and improving the behavior and API design of gradient and requires_grad management in PyTorch, emphasizing the confusing dependency of output.grad retention and requires_grad on input.requires_grad status, and proposing API simplifications such as deprecating direct requires_grad modification in favor of explicit retain_grad methods. There are also technical concerns about specific implementation issues, such as runtime errors in autograd with large tensor reductions, compatibility challenges with older CUDA, Python versions, and platform-specific build issues, as well as the need for better documentation, device polymorphism support in export, and handling of operators for new hardware backends like MPS and RISC-V. Additionally, some discussions address build system updates, compatibility matrices for older software environments, and specific bug fixes and test failures. Overall, the focus is on API consistency, usability, compatibility, optimization, and addressing platform-specific bugs for stable and clear PyTorch operation."
2023-06-26,pytorch/pytorch,"The discussions highlight several technical concerns including the nuanced behavior of `requires_grad_()`, `retain_grad()`, and gradient computation for non-leaf tensors, with suggestions that these could be simplified or clarified through API redesigns. Multiple issues pertain to CUDA and GPU memory management, such as out-of-memory errors, driver compatibility, and memory freeing challenges across GPUs, with potential solutions involving driver upgrades or tweaks to caching strategies. There are questions about reproducibility, especially regarding bugs involving MPS, CUML, and symbolic shapes, emphasizing the need for minimal reproducible code and clearer test conditions. Other topics include supporting symbolic shapes in certain functions, handling dynamic shapes, extending support for sparse tensor formats, and ensuring compatibility of custom registration, as well as managing infrastructure inconsistencies like CI flakiness. Lastly, there are suggestions for improving functionality (e.g., associative scans, packing boolean tensors) and cleanup of build/configuration issues, indicating ongoing efforts to streamline the system and improve robustness."
2023-06-27,pytorch/pytorch,"The discussions highlight ongoing challenges and community interest in integrating support for alternative Python implementations like PyPy, low-precision dtypes (e.g., FP8), and CPU-specific optimizations (e.g., OpenMP). Several issues relate to CI stability, flaky test flakes, and environment setup, requiring better documentation, handling of symbolic shapes, and infrastructure improvements. Specific technical concerns include ensuring correctness of operations like `torch.inverse` across versions, handling symbolic shape inference more efficiently, and enabling features such as `torch.cond` for training/inference mode switching in exported models. There are also ongoing efforts to improve support for specialized hardware and backend compatibility (e.g., ROCm, HIP, NVIDIA CUDA), as well as mitigation of memory leaks and performance regressions. Unresolved questions involve how to manage in-graph conditional execution, better support for distributed and runtime environments, and ensuring platform-agnostic stability for advanced features."
2023-06-28,pytorch/pytorch,"The comments reflect ongoing efforts to improve PyTorch's internal APIs, especially around handling shared memory and in-place modifications, with recent discussions on implementing or stabilizing support for `ShareableList` and in-place tensor value modifications. Several issues relate to distributed training, CUDA/memory errors, and platform-specific bugs, often requiring fixes or reverts of prior PRs (e.g., around `inductor_random`). There are concerns about the reproducibility of failures across different environments and the impact of certain operations on determinism or compatibility, such as the behavior of `resize_()` on quantized tensors or handling shape inference with symbolic shapes. Some discussions suggest adding tests, reverts, or extra validation, but unresolved questions include improving CI stability, platform-specific bugs, and better API support for advanced tensor operations. Overall, the main concerns center on stability, correctness, and reproducibility of features across different environments and internal APIs that may cause circular dependencies or platform-specific issues."
2023-06-29,pytorch/pytorch,"The collected comments from the GitHub issues reflect ongoing efforts to address feature requests, bugs, and integration challenges within PyTorch. Common themes include difficulties with CUDA and environment setup (notably with fabric manager and cuDNN), support for new hardware like FP8 data types, and enhancements to model compilation and graph tracing (e.g., torch.compile, inductor, dynamo). There are concerns about stability, reproducibility, and the correct handling of specific operations for various device types and data types. Several discussions also focus on improving tooling, debugging, documentation, and support for external integrations such as ONNX, third-party libraries, and custom backends. Overall, unresolved questions remain around compatibility, performance optimization, and expanding support for emerging hardware and data formats."
2023-06-30,pytorch/pytorch,"The discussions highlight several technical challenges related to PyTorch's build environment and features. Notably, issues involve platform-specific configurations, such as libc10/fixed GPU support, CUDA version dependencies, and cross-compilation problems on architectures like RISC-V and macOS, often requiring environment adjustments or building from source. There are ongoing concerns about the stability and flaky test failures in CI runs, with suggestions to improve test determinism, reduce compile times, and handle specific operators or modules more robustly. Some discussions focus on extending functionality (e.g., running modules in reverse order, supporting specific quantization/dataloader behaviors, and functionalization), often requiring deeper codebase modifications or better testing coverage. Overall, unresolved questions involve build environment consistency, improving CI reliability, and expanding feature support across hardware and software configurations."
2023-07-01,pytorch/pytorch,"The discussions highlight recurring technical issues with PyTorch, such as import errors (e.g., `_update_worker_pids`), and inconsistent behaviors across different platforms and environments, especially involving CUDA, MPS, and GPU-specific errors like `CUDAError: unspecified launch failure`. Some comments express concerns over regression regressions in performance, memory footprint, and accuracy after recent updates, introducing debates about the correctness and stability of certain features (e.g., `torch.compile` with activation checkpointing, CUDA to HIP mappings). There's also discussion about code maintenance and design choices, such as whether to allow user-defined functions for parameter initialization or rely on external context managers instead of core API additions. Additionally, some issues are platform-specific or environment-specific (e.g., Mac M1, CI failures), with suggestions to improve documentation, testing, and code clarity for better longevity and robustness."
2023-07-02,pytorch/pytorch,"The discussions center around several technical challenges, including issues with loading pretrained models due to zip archive read errors and file management, as seen in issue #31620. There are questions about dtype inference behavior, specifically whether `as_tensor` and `tensor` maintain default types when `torch.set_default_dtype()` is set, as addressed in issue #31689. Multiple reports highlight unimplemented operators and backend-specific errors on MPS devices, such as `aten::empty_strided` and `aten::fft_r2c`, raising concerns about coverage and support for various hardware backends (issues #77764). Further, there are ongoing efforts to improve autograd tracking, sequence number propagation, and model export reliability, with specific attention to ensuring correct execution order of backward operations and accurate data type representations across Python and Java. Lastly, internal review, code reverts, and PR management issues are noted, reflecting ongoing maintenance and collaboration challenges within the repository."
2023-07-03,pytorch/pytorch,"The discussions highlight several technical concerns, including the handling of CUDA and GPU compatibility, especially on macOS and with different GPU architectures like A100s, where users encounter errors related to deprecated or unsupported CUDA operations (e.g., device >= 0 && device < num_gpus assertions, unsupported operators on MPS devices). There are issues with CUDA stream capture modes (e.g., thread-local vs. global modes) affecting multi-threaded or multi-GPU programming, and potential implications for correctness and performance. Some automatic suppression of test failures or flaky tests is due to the dynamic nature of hardware or driver environments, requiring careful management of environment configurations and error handling. Several discussions also involve maintainers' reviews, bug reverts, and the importance of proper deprecation, warnings, and documentation to guide users through hardware-specific or precision-related limitations. Unresolved questions include how best to inform users about potential precision issues with float16/bfloat16, how to ensure correct environment setup for local and CI testing, and how to improve interoperability and error propagation across different languages and libraries."
2023-07-04,pytorch/pytorch,"The discussions highlight several technical concerns: the potential non-determinism of `adaptive_avg_pool2d_backward_cuda` when contiguous memory is used, prompting considerations of atomic operations versus deterministic paths; the need for a DDP-aware and versatile file download utility supporting configurable caching and cloud sources; challenges in supporting dynamic shapes during graph export, leading to proposals for better shape inference and handling in TorchDynamo; the occurrence of trunk CI flakiness across platforms necessitating flakiness detection and mitigation strategies; and issues with ONNX export for models with dynamic sequence lengths, especially for transformer architectures, indicating a need for improved handling of variable dimensions and shape inference. Proposed solutions include refining the deterministic implementation, enhancing download utilities in `torch.utils.data`, integrating warnings or documentation notices about numerical issues, and exploring per-backend or thread-local capture modes to address device-specific behaviors. Several unresolved questions remain about the best approaches for deterministic backward paths, handling of dynamic shapes, and ensuring consistent CI stability."
2023-07-05,pytorch/pytorch,"The comments highlight various technical issues, including segmentation faults and failures when using torch.compile() with dynamic shapes, especially on Windows and CUDA environments. Several discussions consider whether features like device polymorphism in export or support for bfloat16 on MPS are feasible, with suggestions to implement symbolic device concepts or conditional fallback mechanisms. There are concerns about correctness and performance trade-offs in optimizers, quantization, and inductor patterns, as well as questions about the support and testing of new features like torch.compile and quantized ops. Additionally, issues related to CI flakiness, internal internal failure regressions, and proper handling of function patterns and default argument normalization are proposed but unresolved. Overall, the discussions suggest ongoing work on stability, feature support, and performance enhancements that require careful review and testing."
2023-07-06,pytorch/pytorch,"The discussions highlight issues related to GPU memory management and reporting inconsistencies in PyTorch, including challenges with CUDA memory summation, fragmentation, and driver/library mismatches, especially on various hardware like A100s, Tesla P40s, and M1 Macs. Several users report discrepancies between Python's memory stats and nvidia-smi outputs or unexpected GPU memory retention after deleting models, often due to PyTorch's caching and context behaviors. Other concerns involve compatibility and stability for features like distributed training (including NCCL and NCCL retries), support for new hardware and data types (like float8), and the need for better tooling, reporting, and testing, especially for edge cases like near-singular matrices or custom hardware backends. Some discussions suggest workarounds such as disabling IOMMU, adjusting environment variables, or relying on nightly builds, but unresolved questions pertain to memory fragmentation, operator support across hardware, and ensuring consistency across different runtime environments. Overall, these issues underscore ongoing efforts to improve memory transparency, hardware support, and stability in PyTorch's GPU ecosystem."
2023-07-07,pytorch/pytorch,"The comments cover various issues and discussions related to the PyTorch codebase, including performance regressions, correctness in specific modules (e.g., attention, quantization, autograd), and features like multi-GPU batch size handling, custom allocator integration, and extensibility of API surfaces. Notable themes involve improving robustness and reproducibility (e.g., deterministic LSTM behavior, handling non-contiguous tensors), optimizing in-place operations via `torch._foreach`, and ensuring compatibility and correctness in various backends (CUDA, MPS, AMD, ROCm, IPU). Several questions focus on the design of API abstractions (e.g., unified autocast, parameter vs buffer type checks, support for new data types like fp8), plus decisions about test coverage and stabilization of dynamic shape and distribution workflows. Some discussion also involves managing complex code modifications, rebase and revert patterns, and the proper way to extend functionality (e.g., fusing quantization nodes, supporting third-party allocators). Overall, the conversations reflect ongoing efforts to balance performance, correctness, API consistency, and maintainability amidst evolving hardware and software features."
2023-07-08,pytorch/pytorch,"The discussions highlight several technical concerns including the support and design of asynchronous tensor classes like `AsyncTensor` in PyTorch and the challenges of integrating `__torch_function__` and `__torch_dispatch__` for custom tensor subclasses, particularly within Dynamo’s framework. There are questions regarding the management of fake tensor states, optimizer initialization, and behaviors during model tracing and compilation, especially with respect to lazy initialization and in-place modifications. Additionally, issues with handling specific operators on different devices (such as MPS) and the expected functionalities of softargmax versus softmax functions are discussed. Multiple threads emphasize the need for clearer documentation, consistent design strategies, and handling of third-party or custom optimizers and modules, with some proposals for API changes or workarounds. Unresolved questions remain about the best practices for optimizer state management during compilation/tracing and the support for dynamic shapes and device-specific operators."
2023-07-09,pytorch/pytorch,"The discussed comments primarily focus on ensuring reproducibility and proper seeding strategies within PyTorch's DataLoader, especially regarding third-party libraries like NumPy and augmentations, with solutions such as inside DataLoader modifications and user-managed seeding. Several issues involve performance and correctness challenges, including handling dynamic shapes with torch.compile, ensuring consistency in quantization and ABI configurations, and addressing potential bug regressions introduced by recent PRs in the inference and graph code. There are troubleshooting notes on environment setup, especially around CUDA memory configuration, environment variables, and hardware-specific issues like those with ROCm and AMD GPUs. Finally, issues with test failures, merge conflicts, and incorrect assumptions about internal mechanisms or API behaviors highlight ongoing maintenance and documentation improvements needed."
2023-07-10,pytorch/pytorch,"The dataset contains numerous issues and feature discussions related to PyTorch's internal implementations, especially regarding testing stability, reproducibility, and debugging (e.g., handling of nondeterministic algorithms, guard mechanisms, and shape inference). Several technical concerns revolve around proper handling of custom operations, CUDA and ROCm backend behaviors, and consistency in API design—for instance, how to implement or unify features like device mesh management, explicit shape guards, and quantized operations. There are questions about inference acceleration, model export efficiency, and the correctness of exported models, particularly with ONNX and tensor shape representations. Discussions also highlight the need for clearer documentation, better contributor onboarding, and strategic alignment between tools like TorchDistautograd, FX passes, and external libraries. Unresolved issues focus on fixing/troubleshooting flaky tests, improving reporting of compilation metrics, and designing more extensible, reliable APIs for debugging and development workflows."
2023-07-11,pytorch/pytorch,"The discussions highlight several technical concerns—including the behavior of inductor's support for custom CUDA and low-precision types, especially on ARM architectures, and the handling of tensor strides and device-specific kernels, which require careful build configuration and code changes. Multiple issues relate to CI flakiness, test failures, and the need for better documentation, test coverage, and management of unstable jobs. There are questions about ONNX export compatibility with TorchScript/FX graphs, as well as proposals for improving the clarity and robustness of hooks for recompilation detection. Some conversations also involve the potential refactoring of code for more modular, maintainable implementation, particularly around fusion, decomposition, and deprecation handling. Finally, several unresolved questions focus on the integration of new features with existing frameworks (like TorchGen), build setup, and how to ensure proper support across diverse hardware and software environments."
2023-07-12,pytorch/pytorch,"The discussions raised several technical concerns including: the building and verification of specific Docker images used in PyTorch, particularly regarding how they incorporate CUDA and NCCL components; issues with multiprocessing start methods and their impact on CUDA runtime reinitialization when using SDKs like IOMMU or in distributed training; challenges related to PyTorch's compilation and fusion strategies, especially in lowering composite functions like `affine_grid + grid_sample` for performance gains; compatibility and support for specific hardware architectures such as PowerPC and AMD ROCm, including build and configuration questions; and miscellaneous topics such as bug tracking, patch merging, and test stability, along with feature requests (e.g., support for multi-dimensional axis arguments in tensor operations). Unresolved questions pertain mostly to environment-specific build configurations, advanced operator fusion, and correct handling of scalars and dynamic shapes within TorchDynamo and ONNX workflows."
2023-07-13,pytorch/pytorch,"The collected comments reveal a mixture of technical issues and discussions around PyTorch internals, including the challenges of correctly handling runtime behaviors such as CUDA/hip stream capture modes, memory management, and operator registration across different hardware (x86, ARM, MPS, ROCm). Several contributors are concerned with ensuring proper debug, stability, and reproducibility, including problems where errors during forward passes can lead to inconsistent or invalid states, especially for features like CUDA graph capture and weight sharing. There are also ongoing efforts to improve the user experience with model exporting, custom operator registration, and code refactoring (e.g., replacing make_fx with other techniques to avoid compile overhead). Additionally, some comments involve removing redundant or fragile code, managing CI flaky tests, and addressing build configurations for specific architectures and dependencies (like MKLDNN or LAPACK). Overall, unresolved questions include how to better handle dynamic shapes and multi-parameter modules, ensuring reproducibility across versions, and improving debugging/logging for complex runtime interactions."
2023-07-14,pytorch/pytorch,"The discussions highlight several technical concerns including: the default behavior of DataLoader's `drop_last` and the potential benefit of a more nuanced option to drop only trailing batch of size one; the lack of implementation for `affine_grid` for older opset versions and the potential for custom `RFFT` support in ONNX, with some proposed code snippets and workarounds. There are ongoing issues with CUDA operation support on various devices (e.g., MPS, ARM), bottlenecks in fusion passes (e.g., with `unfold` and `as_strided`), and performance/behavior anomalies (e.g., inconsistent `layernorm` and `RMSNorm` implementations, memory leaks). Several PR reviews discuss code changes, testing strategies, and necessary build configuration adjustments, such as installing `lapack` on ARM or fixing external dependencies. Unresolved questions include how to uniformly support operations across different backends, enhance symbolic registration for custom ops, and improve CI reliability for flaky or platform-specific failures."
2023-07-15,pytorch/pytorch,"The discussions highlight significant concerns regarding the accuracy of profiling times in PyTorch, especially the reliability of CPU versus GPU timings due to asynchronous CUDA kernel launches, with suggestions to use cudaDeviceSynchronize for precise measurement. Multiple issues involve build environment inconsistencies, notably incompatibilities with different Visual Studio versions on Windows, system libraries like libcudnn, and the impact of dependencies such as LLVM, conda, and libstdc++ on reproducibility and correctness. There are questions about the integration of new features or operations, like adaptive pooling in specific ONNX opsets, and how to safely add or support new functionalities (e.g., approximation functions, sparse tensor conversions) within the core PyTorch codebase or via `@torch.compile`. Several discussions address existing or potential build failures, CI flakiness, and the need for robust handling of default device settings, particularly for tensor creation functions and their device/ dtype consistency. Overall, the unresolved issues revolve around improving profiling accuracy, environment reproducibility, proper support for feature extensions, and stability in build/deployment workflows."
2023-07-16,pytorch/pytorch,"The discussions highlight ongoing challenges with certain GPU operations, especially on GTX 1660 cards, where updating cuDNN versions helped mitigate ""nan"" issues, yet some problems persist, particularly with FP16 tensor cores. There are concerns about optimizing dynamic quantization, including understanding how it determines qscale and potentially registering custom ops or backends for improved flexibility and experimentation. Several issues involve inconsistencies or bugs in model compilation, such as handling namedtuples, mismatched tensor views, or specific operator support in different devices or data types, prompting suggestions for better documentation and testing. Additionally, there are operational concerns such as efficient model loading and initialization with FSDP, managing warnings during distributed training, and ensuring compatibility across different CUDA/cuDNN versions. Overall, the discussions emphasize fixing specific technical bugs, improving documentation and support for quantization and operator support, and optimizing distributed training workflows."
2023-07-17,pytorch/pytorch,"The discussions highlight concerns about PyTorch's internal API stability, such as the support for `Generator` instances being `save_for_backward`, and the handling of `bincount/histc` return types, with proposals to add explicit `dtype` arguments. Several issues relate to distributed training, including setup difficulties with `Gloo` and `NCCL`, and potential misconfiguration due to hostname resolution or environment variables. There are questions about the support and performance of new features like `RMSNorm` vs. `LayerNorm`, and the integration of on-device code, such as `torchvision::nms`, or fusion libraries like GMM, in different hardware architectures (CPU, GPU, ARM). Some discussions suggest workarounds or incremental fixes for compatibility issues, e.g., fixing `as_strided` behavior in graph lowering, and providing better support for differential operations like `jacrev` in `torch.compile`. Overall, the conversations reflect ongoing efforts to improve stability, performance, and extensibility of PyTorch's core features across diverse environments and use cases, with several unresolved questions about hardware support, API consistency, and build configurations."
2023-07-18,pytorch/pytorch,"The discussions highlight several technical concerns: (1) the potential impact and proper handling of graphs and in-place modifications when using torchcompile, especially for modules like `get_attr` that lack certain metadata; (2) the trade-offs between automatic behaviors (e.g., automatic detachment or detouring deep copies) and user control, alongside the need for customizable APIs or flags like `FakeCopyMode`; (3) the compatibility and correctness issues raised by specific operator support, such as `ChannelShuffle` on CUDA, which currently relies on dispatch rules that may mask the absence of a proper CUDA implementation; (4) the need for improvements in precision and correctness (e.g., float to int conversions, large-batch operations, operations like `jacrev` with torch.compile, and behavior consistency across devices); and (5) the importance of versioning, build configurations, and environment-specific issues (like ROCm build support, onnx support, or external dependencies) influencing the stability and correctness of various features under development."
2023-07-19,pytorch/pytorch,"The discussions largely revolve around CUDA and hardware compatibility issues, including specific errors like illegal memory access, kernel errors, and driver or driver API mismatches, often related to certain driver versions or GPU types. Several topics concern the stability and correctness of PyTorch's distributed operations and tensor interactions, especially when dealing with complex number support, sparse tensor formats, and dtype conversions (e.g., float8). There are also questions about the integration and functioning of PyTorch's compilation pipelines (like torch.compile, inductor, and export), especially regarding dynamic shapes, caching, and reusing compiled functions, as well as issues with model serialization and ONNX export. Additionally, some discussions focus on CI flakiness, test disablement strategies, and the impact of specific PR changes on internal and external features. Overall, unresolved questions about kernel support, operator compatibility, memory management, and stable release workflows persist."
2023-07-20,pytorch/pytorch,"The comments highlight several key issues, including the lack of implementation for rsample in mixture models and whether it can backpropagate gradients, as well as CUDA runtime errors and possible version incompatibilities that cause illegal memory access. There are discussions about performance bottlenecks and memory leaks in PyTorch's CUDA and CPU operations, along with suggestions for alternative allocators like tc_malloc and mimalloc. In addition, there are requests for new features such as support for complex number operations, sparse tensors, and better handling of dynamic shapes, often coupled with questions about implementation details and best practices. Several reports concern platform-specific bugs or flaky tests, especially on macOS, Linux, and ROCm, with some issues related to CI stability and configuration conflicts. Unresolved questions revolve around supporting advanced functionalities like gradient checkpointing, out-of-tree custom operators, and code generation, as well as fixes for existing bugs and performance improvements."
2023-07-21,pytorch/pytorch,"The comments mainly discuss issues related to PyTorch's internal development and testing, including CI flakiness, or specific feature behaviors like freezing, tensor shape handling, or support for certain hardware (e.g., ROCm, MPS). Several entries highlight the need for better error messages, bug fixes, or code modifications to improve stability and performance, such as handling zero-sized tensors, refining inductor or CUDA kernel behaviors, and clarifying tensor interaction semantics. There are ongoing discussions about contributions, code reviews, and testing strategies, as well as adjustments to test configurations and internal implementation details (like activation checkpointing, device type handling, and code rebase procedures). Many of the unresolved questions concern specific technical fixes, support status, and compatibility issues, with some discussions about how to enhance or automate certain processes (e.g., tuning, testing, or code modification). Overall, the discussions reflect active maintenance, troubleshooting, and incremental improvements across the PyTorch codebase."
2023-07-22,pytorch/pytorch,"The discussions reveal concerns about hardware interconnect limitations, such as PCIe topology and ACS settings affecting NCCL communication efficiency, particularly with A6000 GPUs and NVLink. There are issues related to model serialization and pickling, notably with `torch.compile()` and multithreaded state, which are expected to be addressed by upcoming PRs. Several merge conflicts and CI failures highlight ongoing integration challenges, often related to testing, rebase intricacies, or code conflicts, some of which are being handled via force merges. Additionally, there's interest in improving user control over compilation flags, enabling better performance tuning, and disabling features like dynamic compilation and cudagraphs testing. Several discussions also touch on code quality issues, such as the need to adopt `RUF013` for linting and handling optional parameters more cleanly, alongside proposals for better tooling and environment detection strategies."
2023-07-23,pytorch/pytorch,"The comments primarily address technical issues related to PyTorch's development, including a known bug with `torch.compile()` on complex numbers and potential workarounds, such as rewriting functions to avoid complex operations. There are discussions about CUDA compatibility, specifically the PTX ISA version mismatch between Triton and PyTorch, and how it may affect GPU support. Several issues involve build and environment verification, such as confirming correct CUDA and driver versions, and handling unhandled exceptions in multi-processing contexts. Additionally, questions on backward compatibility, especially regarding the use of native dicts versus `OrderedDict`, and the need for support in specific operators (`aten::lerp.Scalar_out`, `aten::_ctc_loss`) are raised. Finally, there are concerns about code revert impacts, CI failures, and merging processes, indicating ongoing maintenance and integration challenges."
2023-07-24,pytorch/pytorch,"The discussions encompass various topics related to PyTorch development, including addressing CI flaky tests, experiments with CUDA kernels and performance optimizations, and improvements to the distributed and autograd systems. Several comments suggest refactoring or extending current mechanisms for better maintainability, such as API reorganization for in-place operations, better handling of shape during tracing, and refining the auto-detection of hardware features. There is also interest in expanding support for features like sparse tensor operations, diverse CPU instruction sets (e.g., NEON), and porting fused kernels to improve performance across hardware. Unresolved questions include how to best integrate third-party tools, manage version compatibility for dependencies like ONNX and Triton, and coordinate API deprecations and experimental features to avoid disruptions."
2023-07-25,pytorch/pytorch,"The comments primarily reflect ongoing issues related to PyTorch's multi-processing and distributed training, such as potential integer overflow in the internal `grad_ready_order_indices_` vector, which may impair DDP logging but not core functionality. There's concern over support for various hardware and kernel configurations, especially support for complex numbers on MPS, and the need for better device management and support for custom devices via context managers. Several reports indicate test flakiness in CI, with suggestions to improve stability, and discussions about the correct handling of in-place operations, deprecated APIs, and serialization (e.g., `fuse_model()` removal, `deepcopy()` semantics). Additionally, a recurring theme involves improving user control over compilation strategies (like enabling/disabling certain optimizations) and addressing known bugs and performance regressions, often requiring workarounds or reverts of recent PRs. Overall, the main concerns center on system robustness, hardware support, API consistency, and CI stability."
2023-07-26,pytorch/pytorch,"The discussions highlight several technical issues including the challenges of integrating torch.compile with certain modules, particularly around the handling of meta tensors and attribute mutation within FSDP, which affects state dict loading and serialization consistency. Multiple reports of flaky CI tests caused by resource shortages, platform-specific bugs, or upstream issues (e.g., CUDA and ROCm compatibility, Kernel launch configurations, and system-specific constraints like PCIe atomic support) also recur. There are concerns about API design choices, such as the handling of device parameters, the default prefetch factors, and the need for better support for non-inplace tensor operations or custom kernel dispatching, especially on ARM/NEON architectures. Several developers seek guidance on best practices, workarounds, or code refactoring to improve kernel support, performance, and robustness across diverse hardware and software environments. Unresolved questions remain about proper API extensions, runtime checks for hardware features, and testing strategies for evolving platform-specific features."
2023-07-27,pytorch/pytorch,"The comments mainly revolve around issues with PyTorch's C++ frontend setup, especially related to include directories, configuration with CMake, and missing references when following official documentation examples. Several users report failures or discrepancies when building or using libtorch, notably on Windows and Linux, often linked to missing or misconfigured files, or related to support for features like CUDA 12.x or MPS fallback behavior. There are discussions about enhancing documentation snippets, search engine snippets, and improving support for features like the `torch.compile()` with dynamic shapes, as well as CUDA/PTX version compatibility issues on different hardware. Additionally, some comments mention internal CI flakiness, build failures, and specific bug reports needing fixes or further investigation, including support for new data types or deeper integration of certain features. Overall, key concerns include build environment configurations, compatibility across platforms and hardware, and improving robustness and correctness of PyTorch's compilation and runtime support."
2023-07-28,pytorch/pytorch,"The comments cover a broad range of issues: from low-level system interactions (glibc TLS limits, library loading order, and GPU driver incompatibilities), to performance concerns with specific functions (like `torch.cumsum`), and hardware support (Vulkan, AMD/ROCm, MPS on Mac). Several discussions involve debugging or reproducing errors (e.g., on MPS fallback, CUDA kernel issues, or runtime failures), often with proposed workarounds or requests for detailed logs. There are also ongoing efforts to improve PyTorch features, such as Vulkan support, custom op registration, deferred initialization, and tensor subclassing, along with infrastructure and test stability challenges (test flakiness and CI failures). While some bugs are resolved or marked as non-critical, unresolved questions remain about system compatibility, API extensions (e.g. torch.device handling), and potential performance impacts of new modes. Overall, the discussions reflect active development, debugging, and planning for system and feature enhancements."
2023-07-29,pytorch/pytorch,"The comments highlight issues related to the handling of tensor contiguity and gradients (Issue #47163), particularly in distributed training and with transpilation operations like transpose and permute, suggesting the use of .contiguous() to fix stride errors. Several discussions (Issues #61309, #67638, #87406) focus on the design and naming of different loss reduction modes, debating whether `batchmean`, `unweighted_mean`, or other names best describe the intended behavior, especially in soft label contexts. Issues also address platform-specific unimplemented operators on Metal/MPS devices (Issues #77764, #77764, #77764), and how to work around or extend support—namely environment variables or fallback modes. Additional concerns include GPU memory management and deterministic behaviors (Issues #105964, #105964), with suggestions for proper cleanup and avoiding non-deterministic results. Lastly, there are internal infrastructure challenges such as internal build dependencies on Windows (Issues #106163), and complexities in PR merging and code refactoring, with discussions around API design, circular imports, and the integration of new features like Fourier operators and complex number support."
2023-07-30,pytorch/pytorch,"The discussions raise concerns about GPU and CPU resource management, especially related to environment variables like `CUDA_VISIBLE_DEVICES`, environment-specific issues on clusters and Mac M2/M1 hardware, and memory overhead during model compilation with inductor and other compilers. Several issues involve the stability and correctness of distributed training, including potential integer overflow in the `grad_ready_order_indices_` vector and problems with overlapping communication and computation in multi-GPU setups. There are questions on hardware support for features like FFT, UVM, and Android builds, along with bugs in specific configurations such as mixed precision (FP16) and runtime errors with autograd, fusing, and kernel implementations. Some discussions explore product features and wrapper libraries, as well as the integration and stabilization of new optimizations and backend support, with unresolved questions about environment setup, performance regressions, and the review process timing."
2023-07-31,pytorch/pytorch,"The comments reflect numerous technical challenges and discussions regarding PyTorch development, including issues with package installation (notably Caffe2's setup.py missing 'tools.setup_helpers'), deep learning model optimizations, hardware compatibility (such as ROCm, CUDA, and M1/M2 Mac support), and build configuration complexities (e.g., environment-specific compiler support, compiler version mismatches, and build system modifications). Several discussions revolve around improving CI stability and flakiness, handling large model and memory issues, and refining internal functions like `print_tabular` and symbolic size representations in Dynamo. Some concerns include ensuring backward compatibility, performance regressions, and proper support for different backends and architectures. Overall, unresolved questions include the appropriate ways to handle environment variability, hardware-specific limitations, and the integration of features like UVM, along with managing CI stability and the implications of recent refactoring efforts."
2023-08-01,pytorch/pytorch,"The discussions primarily focus on certain internal API design and implementation details, such as the internal handling of operator fusion patterns, operator behaviors (e.g., normalization in BatchNorm), and the management of memory pools and allocator behaviors, especially in relation to CUDA memory and graph capturing. Several questions are raised about API stability and public access (e.g., whether to expose `_aot_export_function()` or `_aot_export_module()`) and about ensuring correctness (e.g., verification of communication overlap in distributed training). There are concerns about test flakiness, synchronization issues, and build environment configurations, often linked to specific internal or external dependencies like Triton, protobuf, or CUDA environment variables. Some suggestions involve improving message clarity in errors, adjusting macro definitions to reduce conflicts, or updating documentation to better reflect changes. Unresolved questions include how to handle complex cases like ambiguous overloads of `_foreach_mul`, and whether certain optimizations or low-level memory management adjustments can be safely integrated into the main codebase."
2023-08-02,pytorch/pytorch,"The excerpts reveal multiple ongoing technical concerns, including the incomplete support for scalar sources in scatter operations, issues with CUDA/CUDA-related errors (initialization errors, memory leaks, intersection with specific hardware like A100 GPUs, ROCm, and UVM behavior), and problems related to PyTorch's JIT, FX graph tracing, and DDP/FSDP multi-GPU training stability. Several discussions address improvements in API design, such as adding features like `torch.compile()`, better fx subgraph guard handling, and support for custom backend passes. There are also frequent mentions of flaky tests and CI stability, often linked to trunk flakiness and platform-specific issues, with proposed workarounds or re-enabling/disabling tests based on recent rerun statistics. Overall, unresolved questions include ensuring support for dynamic tensor shapes, clarifying intended behaviors for ambiguous cases (e.g., list of tensors), and stabilizing multi-GPU and hardware-accelerated workflows, while some solutions involve incremental API changes, better documentation, or internal code refactoring."
2023-08-03,pytorch/pytorch,"The comments reveal ongoing efforts to enhance PyTorch's tensor operations, such as supporting scalar sources in scatter, broadcasting improvements, and extending APIs like scatter_reduce_. There are also discussions around improving ONNX export support, handling dynamic shapes, and ensuring compatibility across hardware (CPU, GPU, ARM, ROCm). Several issues pertain to performance regressions, flakiness in CI tests, and build failures related to compiler support, kernel implementation, or environment setup. Notably, questions arise about defining shape dynamics, managing cached compile states, and clarifying design decisions around class usage, operator support, and interface consistency. Unresolved questions include how to handle shape semantics in export, balancing correctness vs. performance in kernel compilation, and ensuring support for various hardware backends."
2023-08-04,pytorch/pytorch,"The discussions cover a range of technical concerns including the interaction issues between open3d and PyTorch that lead to segmentation faults, the need to support uvm memory management in CUDA for better handling of oversubscription, and the importance of implementing deterministic behaviors for reproducibility. There is also focus on improving the profiling and logging of MKL and MKLDNN operators to better trace CPU performance, as well as the challenge of supporting complex operations like convolution and matrix multiplication efficiently across diverse hardware targets with code generation and micro-kernels. Additional topics include refining API design for dynamic shape and export phases, handling unsupported operators on specific backends such as MPS, and ensuring correctness and robustness through range checks and inputs validation to prevent memory corruption or non-determinism. Unresolved questions involve how to best expose these features via APIs, how to guarantee consistent profiling and logging, and how to incorporate new decomposition and optimization passes effectively."
2023-08-05,pytorch/pytorch,"The discussions highlight concerns regarding the correlation between tensor storage and block allocator concepts in PyTorch, as allocator tracing and potential performance overheads from UVM are explored. There is ongoing investigation into CUDA memory management, especially with UVM, its impact on memory statistics, and the behavior difference when over-allocating GPU memory, along with performance implications of cudaMallocManaged and page faults. Compatibility issues on Apple hardware (MPS backend) and specific operator support (e.g., `aten::upsample_linear1d`) on different hardware and PyTorch versions are also discussed. Additionally, questions about the stability and semantics of graph fusion, shape hints for compilers, and the handling of dynamic shapes and tensor views in core code, alongside requests for documentation and testing clarifications, are recurrent. Overall, unresolved questions center on the behavior and performance of advanced memory management techniques, hardware compatibility, operator support, and ensuring correctness with new features like `torch.compile`."
2023-08-06,pytorch/pytorch,"The discussions highlight challenges with correctly implementing multiprocessing with CUDA tensors, emphasizing the need to set the start method within the `if __name__ == '__main__'` block to avoid context errors. There is ongoing effort to support custom ONNX operators and shape inference, with questions about correctly registering operators and handling unsupported operations. Some issues involve compatibility and stability across different hardware, software, or Python environments, including the handling of tensor reshaping inconsistencies, known bugs on specific GPUs, and dependency-related errors (e.g., missing shared libraries). Efforts to improve code generation, shape inference, and config handling are also noted, alongside concerns about merge conflicts and CI failures—especially related to GPU support, performance, and stability. Overall, key concerns focus on ensuring correct multiprocessing behavior, extending operator support, maintaining compatibility, and resolving environment-specific bugs."
2023-08-07,pytorch/pytorch,"The comments reflect ongoing discussions about various PyTorch development topics, primarily focusing on whether certain features or bug fixes have been officially released or adequately implemented, such as support for quantized conv1D operations, faster compilation strategies, or support for new device architectures. There are concerns about correctness and consistency across CPU, GPU, and device-specific backends (e.g., ROCm, MPS), as well as performance implications of certain optimizations like unified memory or code generation choices. The maintainers frequently request updates on the status of specific issues, suggest improvements to APIs or testing practices, and review code merges or refactoring strategies, with some discussions about handling edge cases like NaNs, shape inference, or undefined behaviors in distributed or runtime environments. Additionally, there is emphasis on proper documentation, stabilization of tests, and effective contribution workflows, including private internal development, CI stability, and user-friendly APIs. Overall, the discussions highlight active troubleshooting, feature development, and community collaboration efforts to improve PyTorch's functionality, stability, and usability."
2023-08-08,pytorch/pytorch,"The discussions encompass various technical concerns including inconsistent API behavior for schedulers like ReduceLROnPlateau, challenges with GPU/CPU memory management (notably shared memory and transfer optimizations), and correctness issues in quantized operations especially on Windows platforms. There are questions about supporting flexible mask handling in tensor operations, improving documentation for new APIs, and evaluating performance impacts of code refactoring for compiler efficiency, notably in convolution and matrix operations. Several issues relate to CI flakiness, build environment compatibility, and the need for better user-facing APIs and documentation, especially regarding hardware-specific optimizations and support for features like tensor movement, dynamic shapes, and guard trees. Unresolved questions include how to properly handle scheduler API evolution in user code, extending support for certain hardware features on Windows, and the best practices for integrating new performance optimizations without introducing instability."
2023-08-09,pytorch/pytorch,"The comments span various issues including the removal and reintroduction of fixes in optimizers like Adam, concerns about the support of specific operations (e.g., `aten::unflatten`, `aten::nms`, `aten::fft_rfftn`, `aten::stft`, `aten::searchsorted.Tensor`) in different ONNX opset versions, and performance regressions or instability in PyTorch's build, compilation, or runtime behaviors across platforms. Several discussions highlight workarounds (e.g., wrapping in `torch.jit.ignore`, manual min scale enforcement for AMP, or using `torch.set_float32_matmul_precision`) and feature requests, such as improving torchscript, supporting negative strides, and better benchmarking tools. Many issues relate to CI flakiness, build errors, or unsupported operations on certain hardware backends (e.g., MPS, ROCm, Triton), with suggestions for early validation, better error handling, or incremental support to improve stability and compatibility. Some comments also focus on code quality hygiene, documentation updates, and potential API design improvements like making reduction operations more flexible and better differentiated. Unresolved questions include support status of certain operators, correctness of recent code changes, and platform-specific issues."
2023-08-10,pytorch/pytorch,"The discussions highlight several persistent issues: discrepancies between PyTorch's softmax input representations and correct distribution modeling, especially for deterministic distributions; performance regressions with certain tensor operations like `repeat_interleave` on CPU; support limitations in `torch.nn.functional.interpolate` for 3D bicubic (noting the need for fixes); and challenges with ONNX export involving complex types and dynamic shapes, requiring recent versions or specialized handling. Additionally, several experimental or flaky CI tests are repeatedly being disabled and need further stabilization. Overall, key concerns involve ensuring compatibility and correctness in tensor representations, performance optimizations, and improving robustness of export and testing workflows. These require targeted fixes, better documentation, and possibly rethinking API design for complex and dynamic modeling scenarios."
2023-08-11,pytorch/pytorch,"The discussions primarily revolve around disabling flaky or problematic tests in the PyTorch CI, often due to flaky infrastructure or unimplemented features on specific platforms (e.g., Windows, ROCm). Several suggestions include restructuring test sharding logic, deprecating or moving certain functionalities, or editing CMake configurations to better support platform-specific issues. There are also concerns about code stability, such as handling numpy-like behaviors with tensors, or the need to improve support for features like static shapes or certain backend integrations (e.g., NVTX, cutlass). Some issues highlight the challenge in ensuring consistent behavior across hardware and software configurations, especially regarding memory management, device compatibility, and API stability. Unresolved questions frequently involve how to better manage platform limitations, dependency updates, or the correctness/performance implications of ongoing patchwork fixes."
2023-08-12,pytorch/pytorch,"The discussions raise several key concerns: security enhancements in the RPC framework, specifically implementing OAuth or TLS support; handling of unsupported operators on MPS devices and related fixes; issues with complex number support in PyTorch including compilation errors and incorrect computations; performance regressions and kernel optimization strategies in torch.compile, particularly for tensor memory access patterns and loop order heuristics; and various internal merge conflicts, build failures, and CI stability issues that hinder development progress. Additionally, questions about the reproducibility of bugs not seen in CI, and the need for support for newer CUDA versions and onnx opset support, are discussed. Overall, unresolved technical questions include improving security protocols, fixing operator support and correctness bugs, optimizing compilation performance, and ensuring stable CI workflows."
2023-08-13,pytorch/pytorch,"The discussions revolve around improving PyTorch's parallelization and kernel implementations, including the feasibility of CUDA-based quickselect algorithms for efficient top-k operations, and concerns about the support for larger device indices and addressability of extensive GPU clusters. There are questions about supporting newer Python versions, Windows compatibility issues with certain backends, and the challenges of conditional compilation related to ONNX runtime versions, especially concerning FP8 support. Other topics include the design of device management abstractions to enable third-party device integration, ensuring seamless model loading and training with FSDP, and optimizing performance benchmarks for specific tensor operations. Unresolved issues include internal build failures, merge conflicts, and compatibility constraints with certain compiler or runtime features, indicating ongoing development and refinement efforts."
2023-08-14,pytorch/pytorch,"The discussions cover a range of issues including the limitations in exposing certain tensor operations (like in JIT scripting and advanced indexing), API gaps (such as `__setitem__` exposure, and the need for safe `slice_scatter` functions), and device/architecture-specific implementation concerns (e.g., support for ARM NEON, oneDNN fusion, FP8 support, and CPU instruction set compatibility). Several threads mention existing support or workarounds (like `worker_init_fn` for DataLoader seed randomness, or `torch._dynamo.reset()` for CUDA memory issues), but many highlight the need for further development, testing, or rework (e.g., improving `autocast` behavior, ensuring correctness in fused models, or fixing test flakiness). Some conversations revolve around internal infrastructure, CI stability, and build system adjustments, particularly around flaky tests or environment setup. Overall, ongoing feature enhancements are balanced with bug fixes, API improvements, and infrastructure stability efforts."
2023-08-15,pytorch/pytorch,"The discussions predominantly concern PyTorch's CI flakiness, especially on various GPUs and platforms, with many failures attributed to unrelated test issues or external dependencies. Several issues highlight failures caused by memory management, NCCL errors, or build-related problems, some of which are mitigated by rebases or workarounds. There are questions about the correctness and behavior of specific features, such as the handling of masks in attention modules and the support for different `Tensor` and numpy behaviors, with some bugs identified (e.g., NaN treatments, memory format consistency). Multiple discussions suggest the need for better diagnostics, more robust tests, and clarifications in documentation regarding features like CUDA support, feature fallbacks, and API behavior. Overall, unresolved questions relate to platform-specific bugs, correctness of certain operations, and improving stability and documentation clarity."
2023-08-16,pytorch/pytorch,"The comments collectively highlight several technical issues and questions related to PyTorch's interoperability with various hardware and software environments. Key concerns include developing effective mechanisms for serializing and transferring arbitrary Python objects or compressed neural networks in distributed settings—particularly using the RPC framework, with challenges such as pickling custom classes. There are discussions around hardware support and performance optimization for AMD GPUs (via ROCm), Intel's oneDNN patterns, and Nvidia's cublasLt, including handling specific kernel failures and architecture-specific requirements, especially in the context of newer CUDA and ROCm versions. Additionally, some comments focus on improving JIT scripting behaviors, especially with custom types, and fixing existing test failures or flaky tests across platforms, sometimes due to environment misconfigurations or compiler issues. Overall, the discussions center on enhancing hardware compatibility, optimizing performance, and making distributed or serialization features more robust and flexible."
2023-08-17,pytorch/pytorch,"The discussions highlight issues related to thread management in PyTorch, deadlock solutions involving process forking and threading, and the need for warnings about potential deadlocks when forking processes. Several comments focus on adding or improving support for RMSNorm, including its API and implementation details, contrasting it with LayerNorm. There are recurring flakiness issues in CI tests on Linux, macOS, and Windows platforms, often resolved by rerunning tests many times, prompting suggestions to better isolate flaky tests or improve test stability. Specific technical questions include how to handle device-side assertions (DSA) and the support for ATen operator features like scatter_reduce, as well as implementing more flexible or modular optimizer components. Overall, key concerns involve handling threading/forking safety, enhancing norm layer support, ensuring CI robustness, and expanding operator and optimizer functionalities."
2023-08-18,pytorch/pytorch,"The discussions address a variety of technical concerns including the support and stability of distributed backends, especially MPI and NCCL NCCL errors, NCCL internal errors, and NCCL version compatibility issues. Several issues highlight the need for better error handling, including more informative error messages, preventing internal NCCL errors like bootstrap socket failures and internal checks failing. There are suggestions to revert or improve recent changes in NCCL error handling and support for features like device-side assertions and mixed-precision operations. Discussions also mention the necessity for more robust testing, including device-specific tests, and some questions about specific implementation details (e.g., the use of UVM, dynamic shape handling, or kernel support on different architectures). Unresolved topics include supporting Windows ROCm, fixing build failures, and ensuring new kernel features or error handling mechanisms do not break existing workflows or checkpoints."
2023-08-19,pytorch/pytorch,"The discussions highlight various technical issues including memory and resource constraints when running data loaders with multiple workers, and potential configuration adjustments like increasing memory or reducing worker count. There are specific concerns about subtle bugs related to hardware details—for example, GPU-specific issues with tensor core uses, TF32 mode, and kernel behaviors on different NVIDIA cards, which affect model outputs and stability. Several internal development updates involve the challenge of ensuring backward compatibility in APIs (notably with FX and optimizer signatures), handling rebase conflicts, and managing complex CI/merge processes amid flaky tests and external dependency issues. Unresolved questions include how to better support hash/equality comparison for dynamic backends in JIT, how to handle uncertain merges due to approval or CLA requirements, and how to improve debugging and reproducibility of specific failures. Overall, the focus is on resolving hardware-specific bugs, maintaining API compatibility, and improving CI reliability."
2023-08-20,pytorch/pytorch,"The discussions highlight several technical issues: the absence of a `torch::autograd::set_detect_anomaly` function in the C++ frontend; persistent problems with running Stable Diffusion on certain NVIDIA GPUs due to Tensor Cores and FP16/FP32 precision assumptions; ambiguities in variance estimation during BatchNorm training versus eval mode; inconsistencies and potential bugs in NCCL's support for aborts across different threads; and challenges in extending or modifying operator implementations like `mul` in the PyTorch native functions due to missing codegen support. Some conversations suggest workarounds, such as disabling TF32 or switching memory formats for accuracy, while others identify ambiguous or missing documentation on device-specific behaviors or operator behavior, especially in complex heterogeneous or distributed environments."
2023-08-21,pytorch/pytorch,"The comments highlight various technical issues and questions related to PyTorch features, including memory management, dataset loading strategies, softmax quantization, softmax fallback implementations, and performance regressions associated with changes in CUDA, ROCm, and graph compilation. Several discussions revolve around improving the robustness and correctness of JIT/tracing, handling of mixed precision (FP16/FP32), and compatibility with different hardware architectures and software versions. There is interest in better error handling, API improvements, and performance benchmarking, alongside ongoing problems like flaky CI tests, system hangs, and invalid tensor states. Some proposals involve API adjustments, deeper hardware-aware configurations, and better tooling or diagnostics for debugging. Overall, key concerns focus on stability, correctness, performance, and ease of development and testing."
2023-08-22,pytorch/pytorch,"The discussions cover several technical issues in the PyTorch repository, including the ambiguity and correctness of gradient computations, particularly in complex scenarios like parameter reparametrizations and re-implementations of gradient functions. There’s emphasis on improving documentation clarity regarding eigenvalue sorting, the support for various data types (e.g., complex, uint8, bool) in operations like nn.MHA and scatter_reduce, and handling of device-side assertions to prevent silent errors. Discussions also highlight the need to stabilize and optimize code involving CUDA kernels, internal build processes, and integration with external tools like Triton, ONNX, and TorchXLA, along with ensuring CI robustness. Some efforts are aimed at maintaining compatibility, fixing regressions, and improving test coverage to support upcoming release deadlines, with recurring concerns about CI failures being unrelated to code changes."
2023-08-23,pytorch/pytorch,"The comments reveal ongoing discussions about enhancing the PyTorch codebase and its testing framework, including improving error messaging, supporting complex tensors in C++, and fixing bugs related to shape inference, kernel support, and graph serialization. Several issues pertain to build and runtime failures on specific hardware platforms, notably ROCm, MPS, and internal environments, often requiring patches, reverts, or environment adjustments. There are also concerns about and efforts toward better test coverage, reproducibility, and stability across different versions and configurations, with some discussions about fixing underlying bugs, or changing default behaviors (e.g., opset versions, tensor behaviors). Additionally, some comments address infrastructure and maintenance questions such as prioritization, adapting to external library updates, and repository management. Overall, the conversations focus on debugging, feature support, build stability, and improving developer experience in testing and deployment contexts."
2023-08-24,pytorch/pytorch,"The discussions highlight ongoing issues with CUDA and PyTorch compatibility, such as errors when exporting models with operators like `aten::scatter_reduce` and `aten::fft_rfft`, which are expected to be supported by upcoming `torch.onnx.dynamo_export` improvements scheduled for PyTorch 2.1. There's concern about maintaining and testing extensive internal and external benchmarks, including ensuring compatibility with different hardware configurations, CUDA versions, and PyTorch builds. Some discussions also address the need for better test integration into CI pipelines, managing internal dependencies, and handling flakiness in trunk tests across multiple platforms. Additionally, questions arise regarding ABI compatibility, proper support for complex tensor types on various backends, and how to evolve internal APIs and testing strategies effectively."
2023-08-25,pytorch/pytorch,"The discussions mainly revolve around debugging and improving the PyTorch softmax implementation for integer tensors, with considerations about type support and in-place tensor modifications affecting tracing and inference behavior. Several issues highlight the need for clearer testing, especially for complex features like FSDP, HSDP, and quantization, and questions about compatibility with specific hardware (e.g., ROCm, AMD GPUs). There are concerns about the correctness of certain implementation details, such as tensor contiguity, sharding, and aliasing, which could impact reproducibility and performance. Some discussions focus on refining the API design, including better support for custom device meshes, fallthrough mechanisms, and explicit user toggles for advanced features. Lastly, multiple issues indicate ongoing CI flakiness and the importance of ensuring feature stability through targeted tests and proper documentation."
2023-08-26,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's handling of modules with slots, ensuring proper serialization (`__getstate__`) and compatibility especially with `nn.Module` subclasses defining slots, which impacts pickle behavior and state management. Contributors are attempting to implement support for slots, with considerations for performance benchmarking and backward compatibility, and may address handling of `__getstate__` to include slot attributes. Some issues involve unimplemented operators or features (e.g., `cumprod.out`), while others focus on expanding support for new optimizers like Lookahead, adding support for operators like `aten::masked_scatter_`, and fixing errors in distributed or onnx export workflows. Several PRs are undergoing rebases, merges, or revert workflows due to CI failures, conflicts, or infra issues, with ongoing efforts to resolve Build/CI-related problems and improve stability. Unresolved questions include proper implementation of slot serialization support, operator support for new device backends, and managing complex merge conflicts or CLA signing issues."
2023-08-27,pytorch/pytorch,"The discussions primarily revolve around compatibility and environment setup issues, such as conflicts between PyTorch versions and CUDA/ROCm hardware/software compatibility, which can cause errors like missing shared libraries or runtime failures. Several comments suggest solutions like downgrading or upgrading specific libraries (e.g., torchvision, torch), reinstalling correct versions, or disabling certain features like cuDNN to mitigate performance issues or errors. A recurring concern is ensuring environment consistency and compatibility, especially for GPU-enabled setups, to prevent runtime errors and performance bottlenecks. Additionally, there are notes on ongoing development tasks and review processes for various PRs, with some discussions about code structure and testing strategies. Overall, the focus is on resolving environment-related issues, enhancing compatibility, and improving stability across diverse hardware and software configurations."
2023-08-28,pytorch/pytorch,"The comments encompass various technical concerns, including challenges with quantization (e.g., inserting QuantStub/DeQuantStub and handling backend compatibility), device and dtype casting enhancements, and batch device device behavior in distributed training. There are reports of specific model export issues (e.g., ViT to ONNX, transformer models), and discussions on framework support for TPU, CUDA graphs, and handling link-local IPv6 addresses. Several PR reviews highlight errors due to code changes, deprecated behaviors, and build/test flakiness, with suggestions for more robust testing and better handling of edge cases such as tensor aliasing, deterministic behavior, and internal feature flags. Several unresolved questions ask for clarifications on PR intentions, build configurations, and design choices for future features like static address tensors, device priority, and backend abstraction layers. Overall, the discussions reflect ongoing efforts to improve framework robustness, compatibility, and performance, alongside troubleshooting build/test failures and platform-specific issues."
2023-08-29,pytorch/pytorch,"The collected GitHub comments reflect multifaceted issues spanning performance optimizations (e.g., sparse tensor operations, batched gather, inductor-based speedups, conversion to ONNX, and CUDA context memory management), framework consistency (e.g., support for new operators like `torch.mode`, proper handling of dynamic shapes and symbolic outputs), and environment-specific challenges (e.g., ROCm GPU support issues, device selection abstraction, platform-specific bugs, and transition to newer PyTorch versions). There are ongoing efforts to improve quantization workflows, operator support, and debugging tools, often involving internal refactors, re-approvals, and test retries due to flaky failures. Several issues relate to CI/CD stability, test flakiness, and compatibility patches, with some discussions emphasizing proper API changes, feature deprecations, and better dev tooling (like subgraph rewriters and pass hooks). Unresolved questions include how to streamline device abstraction, support new operators reliably, and improve community/developer workflows for maintainability and performance benchmarking."
2023-08-30,pytorch/pytorch,"The comments highlight several technical concerns: difficulties in supporting in-place and in-place mutation verification during graph decomposition and export, especially across different frameworks and with dynamic shapes; the need to properly handle various data types (e.g., float8, uint8, symints) in custom ops, fusion, and dispatch mechanisms; potential issues with runtime library dependencies and environment configurations affecting performance and correctness; and discussions about the proper way to support or improve the model export process, especially for quantization and optimized kernels. Several inquiries seek clarification on the best practices for integrating third-party libraries like Cutlass, and there are considerations about build infrastructure, dependencies, and version compatibility. Unresolved questions include how to best implement flexible, reliable checks for in-place mutation and shape support, and how to properly manage API evolution for export and translation between training and inference modes."
2023-08-31,pytorch/pytorch,"The discussion raises multiple issues related to PyTorch's development, such as memory leak problems when dynamically changing input shapes in convolutional models, and the need for more robust testing of features like ransomware, replication, and the new inductor API, especially under different hardware and software configurations. Several reports highlight build failures, flaky tests, or performance regressions, some stemming from external dependencies like CUDA, ROCm, or third-party libraries such as cutlass, with ongoing efforts to improve build processes and dependency management. There are concerns about backwards compatibility and proper handling of model serialization, particularly around endianness and model export formats like ONNX, with suggestions to improve default behaviors and user configurability. Additionally, issues with non-deterministic behavior, API consistency, and upcoming feature support (e.g., HSDP, device mesh, and model parallel features) are discussed, often requiring reverts or incremental fixes while awaiting more comprehensive solutions."
2023-09-01,pytorch/pytorch,"The discussions predominantly revolve around persistent bugs and operational issues in PyTorch, including performance bottlenecks, memory leaks, and correctness for features like sparse semi-structured tensors, quantization, and MPS support on Apple Silicon. Several comments suggest that certain failures are due to incomplete feature support, deprecated functionalities, or testing infrastructure, with some issues linked to specific layers like BatchNorm and normalization in quantization workflows. There are ongoing efforts to improve the reliability of distributed operations, device compatibility, and automatic kernel tuning, often with suggestions for better testing, validation, or fallback mechanisms. Many of the unresolved questions concern compatibility with new hardware (e.g., Apple M1/M2), big-endian systems, or internal API limitations, indicating active work on robustness and correctness in these areas."
2023-09-02,pytorch/pytorch,"The discussions primarily revolve around implementation gaps and compatibility issues within PyTorch, notably the lack of support for certain operators on MPS and other devices, leading to errors like 'torchvision::nms' not being implemented. Users seek support for additional linear algebra operations such as `aten::linalg_cholesky_ex.L`, and question the registration of hooks and the interaction with AOTAutograd, highlighting the complexity of handling intermediate hooks and custom graph transformations. Several issues involve build and test failures due to conflicts, missing dependencies (e.g., `packaging`), and environment setup challenges, especially for heterogeneous hardware like ROCm, AMD GPUs, and different system configurations. There are also discussions on improving documentation, clarifying configuration for various backends, and extending testing coverage while managing merge conflicts. Overall, the core concerns focus on enhancing device support, operator implementation, debugging tooling, and streamlining development workflows amidst ongoing infrastructure updates."
2023-09-03,pytorch/pytorch,"The comments encompass a broad range of technical issues and discussions related to PyTorch, including website accessibility problems, CUDA multiprocessing errors, potential low-bit tensor implementations, onnx export errors involving shape inference, and inconsistencies with dynamic shapes and fake tensors. Specific concerns involve fixing particular bugs (e.g., shape inference failures in ONNX conversions, CUDA runtime errors in distributed training), and improving infrastructure and API design—such as more flexible handling of symbolic integers and optimizing kernel selection. Many questions revolve around bug reporting and resolution, with some discussions on clarifying the semantics of certain API functions and handling shape-related errors, especially with symbolic or dynamic shapes. Several comments also relate to ongoing merge conflicts, testing issues, and documentation delays, reflecting active development and debugging efforts within the codebase. Overall, the main concerns are fixing specific bugs, improving API consistency and performance, and clarifying documentation for new features."
2023-09-04,pytorch/pytorch,"The discussions primarily revolve around expanding PyTorch's capabilities and support across various platforms and functionalities. Key issues include implementing QR with pivoting for rank-deficient least squares problems, improving PyPy support, addressing MPS operator limitations, and enabling features like affine_grid for ONNX export. Contributors seek guidance on ongoing efforts to fix these problems, such as handling complex gradients, updating supported operators, and fixing bugs in specific use cases like FSDP, torch._C._profiler, or specific operators. Several discussions mention waiting for fixes in upcoming PyTorch releases, nightly builds, or external dependencies like onnx or compiler behaviors. Unresolved questions focus on compatibility, test coverage, and fixing specific bugs that hinder features or integration, highlighting a demand for ongoing development and testing across different systems and use cases."
2023-09-05,pytorch/pytorch,"These GitHub comments discuss a variety of technical issues related to PyTorch, such as extending support for operations like `affine_grid` to ONNX opset <= 16 and addressing `torch.norm` deprecation and internal restructuring, with proposals to migrate internal code to `torch.linalg.norm` and improve consistency via internal assertions. Others cover performance and stability concerns, including handling CUDA and ROCm device issues, optimizing compilation and autotuning times in `torch.compile`, and preventing regressions and flaky tests in the CI pipeline. There are also discussions about supporting advanced features like `torch.onnx.export` compatibility with `torch.compile`, improving automatic graph handling for in-place mutations, and managing internal development workflows with proper test coverage, tracking, and merge blocking resolutions. Unresolved questions include how to better handle complex or higher-order operations in dynamic graphs, and how to ensure stability and performance across different hardware setups. Overall, the main concerns are ensuring robustness, compatibility, and efficiency of PyTorch features amid ongoing internal and external integration challenges."
2023-09-06,pytorch/pytorch,"The discussions highlight several ongoing development concerns, including the need for implementing advanced linear algebra support such as QR with pivoting for least squares problems, especially for rank-deficient matrices, with accepted PRs to extend CPU support. There is interest in improving multi-GPU and distributed training performance, including potential auto-tuning and fusion heuristics, as well as addressing performance regressions from fusion (e.g., epilogue fusion issues) to balance runtime efficiency and compile time. Multiple issues related to trunk stability, flaky CI tests, and environment-specific problems (e.g., CUDA visibility, compatibility with WSL, or ROCm support) are noted, requiring investigation or re-licensing efforts. Additionally, there is discussion about API changes and internal refactors, such as the PyTree API, log system improvements, and handling sparse tensor operations within the autograd and compilation frameworks. Overall, unresolved technical questions include optimizing fusion strategies for large models, ensuring stable multi-GPU autotuning, and maintaining backward compatibility amid ongoing internal API updates."
2023-09-07,pytorch/pytorch,"The discussions highlight several technical issues including the need for providing precise versioning (preferably via commit hashes) for reproducibility, especially when referencing nightly builds or dependencies like pytorch-triton; discrepancies in CUDA kernel performance and behavior across different PyTorch versions, possibly due to environment variables controlling TF32 execution (e.g., `TORCH_ALLOW_TF32_CUBLAS_OVERRIDE`); ambiguities around CUDA link paths and driver compatibility on systems, which affect runtime stability and performance; challenges in extending PyTorch's support for advanced features like dynamic rendezvous with flexible `MASTER_ADDR` settings; and the importance of refining testing practices, such as adding comprehensive tests for new features or edge cases (like complex tensor behavior, sparse matrix operations, or frozen evaluation modes) to ensure robustness before merging."
2023-09-08,pytorch/pytorch,"The discussions highlight various technical concerns, including the need for more accurate register usage estimation in GPU kernels, the importance of preserving API consistency especially for operations like `reduce_op` and `scatter_dim`, and ensuring test cases accurately reflect expected behaviors, particularly under dynamic or edge conditions such as large tensor sizes or packed sequences. Several contributors question the necessity of certain checks or design choices, such as the placement of `check_kwargs` and whether `out` parameters should be modified directly, emphasizing the importance of maintaining API stability and correctness. There are also concerns about test flakiness caused by system state (e.g., network interface zone indexes, CI environment issues) and build failures due to environment or merge conflicts, which need careful handling. Overall, the discussions underscore the need for clearer API design, improved validation, and more reliable testing strategies to handle edge cases, performance regressions, and external system dependencies."
2023-09-09,pytorch/pytorch,"The discussions highlight ongoing challenges with eigenvalue computations and sparse eigensolvers in PyTorch, noting that specialized algorithms like those in #29488 or external repositories could improve efficiency. Several issues relate to performance degradation and memory leaks, especially when using weight decay in optimizers or running large benchmarks, with some suggesting code reimplementation or infrastructure adjustments. Hardware compatibility concerns are raised about multi-GPU setups, PCIe slot configurations, and AMD GPU support, emphasizing the need to verify PCIe connections and driver support for optimal performance. Additionally, development workflows involving PR rebasing, merge conflicts, and CI failures are frequent topics, with suggestions to improve automation and debugging tools. Unresolved questions include when NVIDIA will release updated binaries, how to better integrate fused computation for training, and how infrastructure or hardware configurations affect software functionality."
2023-09-10,pytorch/pytorch,"The discussions primarily revolve around the absence or incomplete support of specific features in PyTorch, such as sparse eigensolvers, RMSNorm implementation, and advanced support for sparse or structured matrices' backward semantics. There are concerns about performance issues and inconsistencies, especially regarding optimizer behaviors, CPU thread utilization, and potential regressions in recent updates. Some threads suggest incorporating or improving features like a `powersum` function, supporting sparse systems in eigenvalue solvers, or enhancing the `tile` API, alongside discussions about build dependencies and compatibility (e.g., with different compiler versions or hardware). Additionally, there are ongoing investigations into performance regressions, CUDA kernel issues, and the integration of third-party libraries like Cutlass, with strategic decisions on reverts and internal builds to ensure stability and compatibility."
2023-09-11,pytorch/pytorch,"The comments reflect ongoing discussions and troubleshooting related to various deep learning model implementations, build system issues, and CI pipeline flakiness within the PyTorch ecosystem. Key concerns include handling complex custom modules and ensuring correct serialization/deserialization, addressing errors arising from specific hardware or backend configurations (e.g., MPS, Vulkan, CUDA issues), and managing build environment dependencies like CMake versions. Several discussions also focus on improving tooling for profiling, debugging, and supporting sparse or specialized operations (e.g., sparse matrix solves, inductor codegen, etc.), as well as streamlining the merging process amidst flaky CI results and CLA signing issues. Unresolved questions often involve verifying correctness of certain features (like dynamic shape handling, operator dispatch behaviors, and compiler-integration nuances) and enhancing robustness against flaky test failures or environmental setup problems."
2023-09-12,pytorch/pytorch,"The discussions highlight ongoing efforts to improve default weight initialization in PyTorch, including proposals for framework-wide changes, customization decorators, and backward compatibility. Several issues relate to default initializations, custom layer initializations, and handling initialization schemes upon framework upgrades (e.g., PyTorch 2.0). There are also concerns about the default behavior of specific operations (e.g., `multinomial`, `select`) and the need for better API support for custom initializations and out-of-core or multi-backend strategies. Additionally, threading and concurrency issues (e.g., GIL management in Triton, CUDA synchronization, and subprocess behavior) are discussed, with suggestions to improve performance and stability. Unresolved questions include precise mechanisms for resetting thread-local states, fixing race conditions related to CUDA kernel launches, and ensuring backward compatibility after operator signature changes."
2023-09-13,pytorch/pytorch,"The comments reveal multiple ongoing concerns: configuring shared memory (`--shm-size`) on Google Cloud ML jobs to prevent SIGBUS errors in multi-GPU training, implementing efficient packbits CUDA kernels for model quantization, supporting native Python iteration over sparse tensor indices, and fixing bugs related to CUDA/GPU resource management and distributed training (e.g., GIL issues, device consistency). Several discussions involve verifying model accuracy, performance regressions, and ensuring compatibility of API behaviors (like `torch.linalg.lobpcg`, `torch.tensor` creation, and `torch.nn.functional.scaled_dot_product_attention`). There's also attention on handling dynamic shapes in Torch FX and inductor, as well as improving test robustness, CI stability, and correct rebase procedures. Proposed solutions include native kernel implementations, better device support in API wrappers, and more detailed benchmarking, with some issues awaiting further investigation or review before merging."
2023-09-14,pytorch/pytorch,"The comments reference numerous issues related to PyTorch's internal development process, such as merging conflicts, rebase failures, and CI process concerns, rather than specific technical questions. Several discussions highlight the need for better documentation, testing, or review of particular features like shape inference, tensor serialization, or IR rewriting. Others note performance regressions or platform-specific problems that may require further investigation or bug fixes, such as CUDA kernel behavior, library compatibility, or deterministic ops. Some comments involve proposals for API/design improvements, such as handling shapes, guard trees, or operator registration, but many of these are ongoing discussions without definitive solutions. Overall, the main concerns involve merge conflicts, CI stability, platform differences, and clarity of design, rather than stand-alone technical implementation issues."
2023-09-15,pytorch/pytorch,"The discussions highlight concerns about the differences in behavior between cuSPARSELt and CUTLASS for sparse-dense matrix operations, especially regarding the conversion to dense, and the impact on numerical accuracy and performance. Several contributors suggest that the current implementation's handling of sparse tensor memory formats, whether in creation, transpose, or conversion, leads to inconsistent results and potential correctness issues, notably with non-contiguous tensors. There is an emphasis on understanding whether the user-facing API should enforce contiguous tensors or handle non-contiguous tensors properly within cuSPARSELt, possibly by coupling the operation's descriptors with the tensor's memory format. Additionally, some discussions indicate plans to clarify and rename the related conversion functions for transparency and to accommodate future improvements, including better support for custom ops and memory format invariants. The overarching unresolved questions relate to ensuring numerical stability, consistency, and performance, especially when integrating cuSPARSELt into existing workflows and supporting a broader set of tensor operations and memory formats."
2023-09-16,pytorch/pytorch,"The discussions highlight ongoing issues with numerical precision and implementation consistency between different GPU backends, notably between cuSPARSELt and CUTLASS, affecting the correctness of sparse tensor operations and their reverse conversions. There are concerns about the impact of memory formats and non-contiguous tensors on the accuracy of sparse-dense matrix multiplications, especially in the context of pruning and model accuracy. Additional challenges include managing GPU deadlocks when NCCL errors occur, as well as ensuring proper synchronization and memory management during compile and run-time, particularly for dynamic or fused kernels. Some discussions also focus on how to properly integrate features such as stream abstraction, model serialization formats, and moving certain passes to later phases of the compilation or export pipeline to improve robustness and maintainability. Unresolved questions remain about optimizing autotuning decision points, handling overflow in low-precision computations, and ensuring that advanced features like pre-grad passes do not adversely affect training or inference workflows."
2023-09-17,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's robustness and flexibility in various areas. Key concerns include implementing features like shape mismatch information in `load_state_dict`, improving the support for `torch.jit.script` with new Python syntax, and ensuring backward compatibility when operator signatures change (e.g., in `aten::split`/`chunk`). There are also discussions on the support and integration of optional dependencies like `optree`, handling CUDA and device-specific operator implementation gaps for MPS, and addressing numerical and stability issues in distribution constraints and new kernel implementations. Additionally, there are questions about best practices for model serialization, versioning, and handling experimental or internal-only features, along with requests for better user-visible testing and release notes documentation. Unresolved questions include ensuring backward compatibility with operator changes, supporting more advanced scientific computing tools like slepc, and coordinating internal infrastructure and CI workflows."
2023-09-18,pytorch/pytorch,"The discussions mainly revolve around handling the softmax masking issue in MultiHeadAttention, criticizing the prevalent practice of replacing `-inf` with a large epsilon like `-1e9`, and proposing more accurate, numerically stable solutions such as specialized masking or intrinsic softmax modifications. Several comments express frustration that softmax yields NaNs when masking out all elements, and emphasize that softmax should handle empty or fully-masked rows gracefully. There are concerns about the correctness of current fixes, the need for API changes in functional.py (e.g., `_canonical_mask`), and questions about the desired semantics of masking behavior, especially in the context of training versus inference. Unresolved questions include how to properly modify or override softmax to avoid NaNs, how to implement safe masking for fully masked rows, and how to ensure these changes maintain compatibility and correctness across various configurations."
2023-09-19,pytorch/pytorch,"The comments raise a variety of technical issues related to PyTorch, including concerns over the correctness and stability of specific kernel implementations and operator behaviors (e.g., fused kernels with register spills, sparse matrix operations, and device support for operators). Several discussions focus on the correctness, reliability, and consistency of the autograd and JIT/autograd/export pipelines, especially regarding handling dynamic shapes, attribute mutations, and graph normalization strategies. There are ongoing efforts to improve performance, caching, and reproducibility (e.g., kernel autotuning, caching mechanisms, and tracing handling), with recommendations to split APIs for better clarity and maintainability. Many issues involve compatibility, especially on non-CUDA devices like MPS and AMD, and questions about proper support, fallback mechanisms, and ongoing support for features like float8 and cross-platform compatibility. Overall, the discussions aim at balancing correctness, performance, and usability, often with active debates on API design and internal architecture choices."
2023-09-20,pytorch/pytorch,"The discussions highlight issues around system stability and resource management during training, such as disk I/O bottlenecks linked to large batch sizes, dataloader worker configurations, and disk caching strategies. Several questions concern runtime performance optimizations, including memory efficiency (e.g., inplace operations, nan operators), and the correctness of numerical outputs across different hardware (e.g., cudnn batch norm vs. native). There are concerns about compatibility, such as support for Python 3.11 wheels, CUDA and ROCm backend interactions, and cross-platform CI failures, with suggestions for more resilient error handling (e.g., signal handlers, synchronization). A few technical proposals involve extending debugging and observability features, introducing explicit APIs or new modules for custom operators, and ensuring backward compatibility in core APIs. Unresolved issues include stability in distributed and multi-process setups, and hardware-specific discrepancies affecting training outcomes."
2023-09-21,pytorch/pytorch,"The discussions highlight several technical concerns, including the necessity of supporting and efficiently handling in-place and custom CUDA Triton kernels within the PyTorch ecosystem, with proposals to wrap such kernels as functional ops to enable better integration with inductor and Dynamo. There is an ongoing debate about whether Triton kernels should be integrated as core PyTorch operators via the dispatcher or supported through custom ops, with considerations about aliasing, reordering, and performance implications for compilation and optimization. Additionally, many issues involve improving or fixing the correctness, consistency, and stability of features like `torch.export`, handling of meta tensors, inlining behaviors, and backward trapping, while maintaining ecosystem compatibility. Certain support gaps, especially around Windows ROCm, and the need for better abstractions or APIs (like the separation of export and normalization steps or a standard backend registry) are recurrent themes. Overall, the discussions reflect a push towards more robust, modular, and integrable mechanisms for supporting custom kernels, export workflows, and heterogeneous hardware support, with unresolved questions about optimal APIs, transformations, and performance trade-offs."
2023-09-22,pytorch/pytorch,"The discussions highlight multiple technical challenges and questions spanning different areas of PyTorch development. Key issues include debugging and fixing ML-specific kernel problems (e.g., register spilling, fused kernels, batch norm numerical divergences), improving the reproducibility and correctness of various models and input handling, and managing the complexity of stream abstractions on CPU/GPU. There are also questions about expanding optimizer support, particularly Adafactor, and ensuring consistency and stability in dynamic shape handling, especially with respect to symbolic tracing, decomposition, and relay mechanisms. The discussions emphasize the delicate balance between performance, correctness, BC-compatibility, and ease of maintenance, often requiring targeted follow-up PRs or refactoring. Unresolved questions mainly concern the best design strategies for stream abstraction, kernel optimization, and optimizer integration without introducing regressions or breaking existing workflows."
2023-09-23,pytorch/pytorch,"The discussions highlight several key concerns: the handling of nested lists in functions like `torch.stack`, with suggestions to improve error messaging or support; limitations of FP16 support on CPU, with clarifications on device dispatching and potential for patches; issues with CI stability and flaky tests around inductor and nvfuser, including possible test removals; challenges with operator decomposition and face with control-flow assertions (like `_assert_async`) during tracing and export workflows; and questions around code organization, such as the separation of COW logic into multiple libraries versus a unified approach. Unresolved questions include whether to extend support for nested list inputs, how to properly handle assertions in exported graphs, and verifying the correctness of device-specific or backend-related behavior in various operators."
2023-09-24,pytorch/pytorch,"The discussions highlight a need for clarification and improvements in various areas of PyTorch, including the support for higher-order derivatives in operations like `grid_sample` and the implementation of type handling with `astype`. Several issues pertain to the stability and correctness of features such as distributed training with `ProcessGroup`, and the challenge of enabling RISC-V support, requiring custom operator replacements and backend modifications. Troubleshooting merge failures and CI issues also feature prominently, along with suggestions to better document and test new features, especially around automatic dynamic shapes, guard guards, and code inlining behaviors. Unresolved questions include how to best implement or improve these features without breaking existing functionality, as well as practical steps for debugging and deploying on different hardware architectures."
2023-09-25,pytorch/pytorch,"The comments reveal ongoing discussions about various PyTorch features and issues, including challenges with CPU offloading in FSDP, kernel decomposition and functionalization for Triton, stability of small singular values in autograd, and bug fixes related to GIL handling, CUDA context memory, and model export mechanisms like torch.jit and torch.export. There is emphasis on ensuring proper test coverage, handling small numerical singular values, and maintaining the integrity of operator decompositions during transformations. Questions are raised about long-term plans for Triton kernel integration, the stability of gradient computations when singular values are tiny, and the handling of data-dependent assertions like `_assert_async`. Some discussions also involve CI flakiness, merge strategies, and ensuring backward compatibility and correctness across runtime and export workflows."
2023-09-26,pytorch/pytorch,"The discussions encompass several technical concerns, including addressing intermittent CI test flakiness on various platforms (e.g., trunk flakiness, test time limits, environment inconsistencies), and handling specific failures such as segmentation faults, invalid tensor operations, and environment setup issues (e.g., CUDA, NCCL, ROCm). There are questions about improving documentation and tooling, such as clarifying the arch or implementation details for guard execution, profiling the compilation process, and designing better support for features like `torch.export`, `SyncBatchNorm`, and dynamic shape handling. Several proposals involve refactoring or relocating code to enhance robustness, correctness, or performance, e.g., moving guard computations, fixing codegen bugs, or expanding support for certain primitive operations. Unresolved questions include how to structure guard trees, how to maintain reproducibility, and how to ensure test stability while accommodating ongoing feature development."
2023-09-27,pytorch/pytorch,"The discussion primarily revolves around the deprecation of certain features and their impact on API consistency and documentation, such as the removal of `nvfuser` support in favor of supporting `torch.jit.script` or `torch.compile`. Concerns include ensuring backward compatibility, especially for educational materials and older models, while providing clear traceability and documentation of deprecated functionalities. There are mentions of technical challenges with integration, such as handling in-place tensor mutations during module reparametrization, and the importance of carefully managing guards and shape inference to prevent reference cycles and ensure correct graph execution. Additionally, questions have been raised about runtime support for custom kernels (like Triton) and associated dispatcher mechanisms, suggesting a broader need for design considerations around custom operators, autograd, and the long-term coexistence of multiple codegen pathways. Overall, the key unresolved issues involve balancing deprecation vs. documentation clarity, maintaining backward compatibility, and designing flexible, performant integration paths for custom kernels and operators."
2023-09-28,pytorch/pytorch,"The discussions predominantly revolve around troubleshooting various PyTorch issues including tensor shape mismatches, CUDA and environment configuration problems, runtime errors, and performance regressions. Notable concerns include dimensions alignment in loss functions, CUDA driver compatibility, and specific bug fixes that may impact internal caching or kernel execution. Several community members seek clarifications on recent code changes, bug regressions, and whether certain features or APIs (like tensor quantization, custom operators, or support for Apple Neural Engine) are supported or need updates. There is also ongoing effort to improve debugging tools, logging, and tracing support for complex environments and multi-GPU setups. Unresolved questions involve timing of feature releases, configuration workarounds, and detailed performance impacts of recent changes."
2023-09-29,pytorch/pytorch,"The discussions highlight several issues related to PyTorch development: challenges in implementing and testing new features like tensor-to-bitmap conversions, which involve handling data formats (e.g., grayscale masks and RGBA images); ongoing efforts to improve CUDA and ROCm backend stability, including fixing regression bugs, reworking inductor kernel fusion, and managing CI flakiness across various platforms; and infrastructure concerns such as build configuration, dependency management (e.g., static vs. shared libs), and ensuring proper support for different hardware and driver versions, notably for GPU and AMD ROCm. Some technical questions remain about supporting FP16 in attention masks, the semantics of custom operator fusion, and the proper API design for extensions. Overall, the conversations focus on stabilizing and enhancing PyTorch’s features, backend compatibility, and CI reliability, with specific attention to detailed implementation issues and testing workflows."
2023-09-30,pytorch/pytorch,"The discussions primarily revolve around issues related to PyTorch's internal implementation and build processes, such as inconsistencies in autograd registration for scalar operations, handling environment variables affecting OpenMP and LAPACK/BLAS backends, and potential segmentation faults caused by build configurations (e.g., shared library linking, missing dependencies). Specific concerns include ensuring consistency between Python and C++ sizes, improving the handling of fake tensors and symbolic shapes, and resolving build errors on different architectures (e.g., macOS, Windows, AArch64). There are questions about proper integration with external tools like Triton, ONNX, and framework-specific features like Deepcopy for weight normalization, as well as build strategies involving CMake, Ninja, and Python packaging workflows. Unresolved issues include determining the correct autograd registration for scalar functions, fixing build failures due to shared library or dependency misconfigurations, and verifying runtime stability across diverse hardware and OS environments."
2023-10-01,pytorch/pytorch,"The discussions highlight several technical issues, primarily focusing on padding methods for images with high aspect ratios, where reflection padding can lead to artifacts or errors, prompting suggestions for alternative padding strategies like zero padding. Multiple questions concern the compatibility and behavior differences between PIL and tensor inputs, as well as inconsistencies between CPU and GPU memory management, especially on heterogeneous hardware like Mac M1 and Linux systems. There are ongoing discussions about optimizing large model training with FSDP and CPU offloading, alongside efforts to improve build processes involving CMake, Ninja, and wheel packaging workflows. Additional concerns are raised about kernel build failures, repository dependencies, and ensuring proper environment setup across different systems. Finally, there are some updates about features and fixes in the PyTorch documentation, versioning, and the handling of specific tensor operations for scalar or scalar-like inputs."
2023-10-02,pytorch/pytorch,"The discussions highlight several technical concerns: uncertainties about supporting IPC functionality without TCC mode on Windows for parallel inference; challenges with reading model weights on multiple GPUs connected via specific motherboards and PCIe configurations; performance disparities and potential issues related to the memory format and conversion between cuSPARSELt and CUTLASS, especially when using sparse tensors and their dense conversions; questions about the support of complex operations on MPS devices and their failure modes; and considerations around re-compilation behavior when `keep_graph` changes, as well as handling dynamic ragged dimensions in symbolic representations. Several suggestions include improving the robustness of CUDA device handling, ensuring proper environment setup for multi-GPU operations, clarifying memory format impacts on sparse-dense matmul accuracy, and refining precompilation and caching strategies. There are also ongoing discussions about codebase modifications, build tool updates, and the need for better test coverage and documentation for upcoming features. Unresolved questions mainly focus on system-specific behaviors, compatibility issues, and the best approaches to extend support for new operations or hardware configurations."
2023-10-03,pytorch/pytorch,"The discussion covers multiple technical concerns including the complexities of linking static libraries with CUDA and handling CUDA dependencies, especially supporting CPU-only versus CUDA-enabled builds. Several issues relate to specific PyTorch functionalities failing or exhibiting inconsistent behavior across platforms or versions, such as Transformer encoder output truncation and operator support (e.g., torchvision ops like `nms`). There are also ongoing discussions about compiler behaviors, including view handling, inductor graph optimizations, and the impact of graph breaks on performance. Additional concerns involve build and test infrastructure, CI flakiness, and dependency management, notably around eigen, numpy, and external package compatibility. Several proposed fixes, workarounds, or follow-up steps are suggested, but unresolved questions remain around backward compatibility, proper registration of operators, and ensuring performance without introducing regressions."
2023-10-04,pytorch/pytorch,"The comments cover a range of issues in the PyTorch repository, including discussions about tensor flipping performance techniques and support for negative strides, proposal and implementation status for inheritance support in modules, and serialization concerns in models like Whisper and Detectron2. Several technical problems are highlighted, such as slow compile times for optimizers, CUDA kernel fusion limitations, and validity of symbolic expressions. There are ongoing debates about the best way to handle model shape inference with symbolic or unbacked sizes, including the need for fallbacks and guard management. Many issues involve testing flakiness, CI failures, and review processes, with suggestions for better debugging, more robust handling of operators and graph transformations, and improvements in the build and test infrastructure."
2023-10-05,pytorch/pytorch,"The discussions involve several key issues: (1) Some errors stem from IR invalidation in JIT and IR passes, particularly around alias analysis and in-place operations; (2) There are ongoing challenges with distributed training, including memory leaks, process cleanup, and multi-node scalability, with various proposed fixes like using jemalloc or modifying sharding semantics; (3) Support for specific features like nested tensors, sparse tensor types, and dynamic shape handling in compilation and export workflows is being improved, with considerations for backward compatibility and API stability, e.g., ensuring BC does not break onnx export or Python scalar support; (4) Performance concerns are raised around compilation bottlenecks for optimizers or large models, with suggestions to move code into init routines or optimize passes; (5) Some issues are environment-specific, such as failures on macOS, ROCm versions, or MPS, requiring targeted fixes or reverts, while others deal with test flakiness and CI stability."
2023-10-06,pytorch/pytorch,"The discussions highlight issues related to PyTorch's build and development environment, such as missing header inclusion when linking against CUDA libraries, especially for static builds or in cross-architecture scenarios (amd64 vs ARM). Multiple comments address the need for better build practices, including potentially offering a minimal header package alongside binary distributions, and leveraging existing API functions with code-generation rather than new APIs. Trunk and CI stability concerns are mentioned, with frequent references to flaky tests, CI failures, and the need for better diagnostic and reproducibility tools. Some discussions involve code correctness, such as ensuring deterministic behavior, handling of special cases like complex tensors or nested functions, and the importance of precise testing. Several planning and workflow suggestions are also exchanged, including fixing build dependencies, updating documentation, and refactoring for maintainability."
2023-10-07,pytorch/pytorch,"The discussions highlight ongoing technical concerns regarding PyTorch's feature support, API stability, and performance. Notably, issues include difficulties in converting models to TorchScript and ONNX, inconsistencies and bugs in specific modules like transformers and torch.nn.functional, and performance regressions due to backend implementations (e.g., in log, sigmoid, and inductor tensor fusion). Several comments suggest workarounds—such as disabling nested tensors, setting flags, or using alternative APIs—and mention ongoing fixes or improvements being merged or tested in different branches or versions. There are also questions about reproducibility, debugging complex errors (e.g., in CUDA, ROCm, or code generation), and ensuring proper environment setup across different hardware architectures and software configurations. Overall, unresolved questions involve ensuring model correctness, stability across diverse hardware and software setups, and optimizing performance in the face of these issues."
2023-10-08,pytorch/pytorch,"The discussions encompass several key technical issues in PyTorch development, including the need for enhanced `torch.unique` functionality such as stable sorting and stable inverse indices across different PyTorch versions, and an extended `uniqueXT` function to handle diverse use cases, which is seen as a temporary workaround pending backend support. Concerns about MPS GPU precision limitations highlight discrepancies in inference results on Apple hardware, with a solution involving the latest nightly builds; the discussion also notes hardware constraints like lack of 64-bit float support in Metal. There are issues related to PyTorch distributed training, notably FSDP memory fragmentation and optimizer state sharding, with suggestions for APIs and load strategies, and a bug in `randperm` generator device handling for MPS that causes runtime errors. Other discussions address internal infrastructure, such as the need for a stream abstraction on the Python side, handling of virtual functions, guard insertion and correctness, and build configuration problems like missing or incompatible binaries and compiler issues. Overall, these discussions reflect ongoing efforts to improve core functionalities, hardware support, tracing, and build stability in PyTorch."
2023-10-09,pytorch/pytorch,"The discussions cover a variety of technical issues related to PyTorch development, such as the need for proper handling of specific operator export/workaround strategies (e.g., max_unpool), operator support for various device types (like M1 Macs, MPS backend), and enhancements to device mesh APIs for distributed training. There are concerns about correctness and performance regressions introduced by recent changes (e.g., in distributed state dict handling, shape inference, or optimizer behaviors), as well as API stability and deprecation impacts (e.g., in `set_` methods, `torch_function` behavior). Several feature requests are noted, such as support for new padding modes, expanded operator support for different backend devices, and improved usability for dynamic shapes, especially in ONNX exporting and JIT/tracing contexts. Some discussions highlight the importance of testing, reversions, or workarounds for regressions, along with questions about future support timelines, binary distribution strategies, and integration with third-party tools. Overall, the community is balancing ongoing feature improvements, stability concerns, compatibility issues, and backward compatibility while planning upcoming releases."
2023-10-10,pytorch/pytorch,"The comments reflect technical concerns surrounding various issues in PyTorch development, including recent bugs, build problems, and debugging challenges. Notably, there are discussions about supporting specific features like `maxunpool` conversions, handling dynamic shapes in `torch.compile`, and fixing regressions in `onnx`/`torchscript` exports, with proposed solutions like code patching, API adjustments, and better testing. Several issues involve build environment compatibility, especially on ROCm, CUDA, and ARM architectures, and there are questions about support timelines, capabilities, and prioritization. Multiple discussions highlight the need for improved error handling, testing, and documentation, especially regarding support for newer hardware and software versions. Unresolved questions include how to reliably support dynamic shape support, handle specific bugs, and streamline build and deployment workflows."
2023-10-11,pytorch/pytorch,"The comments cover multiple issues related to PyTorch, including support for conversions of max_unpool operators to ONNX, discrepancies in quantization accuracy and performance across different hardware and versions, and potential bugs introduced by recent updates. Several discussions address the support status of features like static quantization, changes in operator behavior (e.g., the 'out' variant of ops), and challenges in building and integrating PyTorch with specific hardware configurations (e.g., Jetson, ROCm, MPS). Some comments point out existing regressions, flaky tests, and technical limitations in the current implementations, with a few proposing fixes or workarounds, such as updating submodules, protecting against specific compiler issues, or adjusting test configurations. Additionally, there's ongoing concern about test stability, CLI and API consistency, and the need for clearer documentation and better support for features like tensor subsclass handling and dynamic shape resolution. Unresolved questions remain about enhancing feature support, fixing regressions, and ensuring compatibility across diverse hardware and software setups."
2023-10-12,pytorch/pytorch,"The comments cover a range of topics, primarily involving integration and compatibility issues, including patching third-party dependencies like ONNX and glog, and handling environment-specific build failures (e.g., CUDA, ROCm, WSL, and mobile builds). Several discussions address debugging and stability challenges, such as flaky CI tests, internal build failures, and discrepancies between eager and compiled execution, especially with dynamic shapes, fake tensors, and subclass support. There are also improvements proposed for PyTorch's internal APIs, like `__hash__` in `nn.Module`, handling of views in JIT/functionalization, and support for new operator variants or attributes to ensure backward compatibility and better operator coverage. Overall, the main concerns involve enhancing robustness, compatibility across environments, and extending support for new features like subclass tracing, quantization, and distributed training while managing ongoing internal refactoring and regression testing."
2023-10-13,pytorch/pytorch,"The discussions highlight ongoing challenges with PyLint configuration in VS Code, particularly addressing false member no-member errors for torch and other libraries, often requiring `pylintArgs` modifications. Several issues concern ONNX export compatibility, such as handling `new_zeros` with dynamic shapes, adding support for additional argument patterns, and ensuring proper input expectations, sometimes leading to test failures or the need for explicit `xfail`s. There are active debates on improving the robustness of PyTorch’s dynamic and subclass support, including the design of custom operators like `move_`, better handling of `Tensor` subclasses such as DTensors, and supporting complex or piecewise symbolic shapes. Discussions also involve build and compatibility issues, like updates to ROCm versions, the impact of CI flakiness, and handling of dependencies like Triton, NCCL, and compiler toolchains. Overall, the focus is on improving developer tooling, inference and export reliability, shape inference, and build stability amidst rapidly evolving backend support and features."
2023-10-14,pytorch/pytorch,"The discussions reveal issues with device consistency in optimizers like Adagrad, indicating that model parameters should be moved to the GPU before optimizer creation to avoid device mismatch errors. There are ongoing efforts to improve the tracing of tensor operations in Dynamo, such as handling proxies correctly and refining constant folding logic, with some concerns about fundamental structural changes and test coverage. Several merge conflicts and CI failures are noted, often related to missing release notes labels or infrastructure issues, and there is a recurring emphasis on better handling environment metadata and compatibility, especially regarding ROCm versions and hardware support. Additionally, previous efforts to modify checks for Triton and Windows compatibility are discussed, with suggestions to localize certain checks within backends rather than at the top level. Overall, the main concerns involve ensuring device correctness, improving tracing and graph fidelity, managing infrastructure and environment dependencies, and maintaining robust testing and documentation."
2023-10-15,pytorch/pytorch,"The discussions highlight persistent challenges with environment and runtime compatibility issues, particularly related to OpenMP libraries (`libomp.dylib` vs `libiomp5.dylib`) on macOS, which cause segmentation faults and errors like `OMP: Error #15`. Many users report that setting the environment variable `KMP_DUPLICATE_LIB_OK=TRUE` temporarily circumvents crashes but can lead to instability, indicating a deeper conflict between multiple linked OpenMP runtime libraries. Additionally, there are issues with CUDA architecture specifications in CMake, requiring manual adjustments to `CMAKE_CUDA_ARCHITECTURES`, and compatibility/migration concerns with ROCm versions, especially regarding PCIe atomic support. Some discussions focus on improving test coverage, especially around guard conditions in `torch._dynamo`, and on fixing errors related to unsupported operations on certain hardware backends or configurations. Overall, unresolved questions relate to managing library conflicts, improving cross-platform robustness, and ensuring compatibility with evolving hardware and software environments."
2023-10-16,pytorch/pytorch,"The comments predominantly revolve around challenges in correctly loading and syncing model and optimizer states with respect to device placement, particularly when checkpointing in multi-GPU or CPU/GPU environments. Several users discuss solutions like using `map_location`, loading checkpoints in the correct order, or moving model and optimizer states onto the same device to prevent device mismatch errors. There are also technical issues highlighted about model exporting with ONNX, handling custom modules like MaxUnpool2d, and support for unsupported operators or backends (e.g., MPS, ROCm, or certain quantization schemes). Other discussions involve debugging memory leaks in serialization, handling unsupported operations during graph compilation/tracing, and test/CI flakiness issues across various platforms. Overall, unresolved questions relate to device synchronization, operator support in various backends, and ensuring robustness across different hardware and software configurations."
2023-10-17,pytorch/pytorch,"The discussions highlight issues related to thread safety when loading and sharing modules in C++, and the recommendation to use a single shared module loaded once, rather than multiple load calls, to prevent threading problems. Several comments address support and feature gaps for operators on MPS and ROCm backends, including missing operator implementations and fallback mechanisms, alongside potential workarounds like setting environment variables. There are also reports of bugs in specific PyTorch functionalities such as `linear_backward` on MPS, and performance regressions tied to CUDA environment variables like `CUBLAS_WORKSPACE_CONFIG`. Some discussions focus on testing and reproducibility challenges, for example, with model exporting, automatic differentiation, and compiler behavior, along with requests for better environment configuration, build workflows, and detailed error reports. Unresolved questions include the technical details of dispatch implementations, cross-platform issues, and the need for clearer error messaging and documentation improvements."
2023-10-18,pytorch/pytorch,"The comments primarily revolve around troubleshooting compatibility, performance, and stability issues in PyTorch across various platforms and versions. Several discussions address build failures or runtime errors related to CUDA/ROCm support, MPS on Apple Silicon, and specific ops like BCEWithLogitsLoss, tensor device handling, and inductor autotuning. There are questions about the stability and support status of experimental features like NestedTensor, torch.compile constraints, and prototype APIs, with suggestions for potential future improvements such as improving subgraph rewriting with matcher support, handling out-of-place tensor operations during functionalization, and expanding test coverage for dynamic shapes and backends. Many issues involve CI flakiness, slowdowns, and memory leaks, with recommendations to disable or revisit tests, or to modify environment configs. Unresolved questions include whether certain deprecated/experimental features should be removed or stabilized, and how to properly support new types or backends in a backward-compatible manner."
2023-10-19,pytorch/pytorch,"The comments highlight issues related to the need for proper placement and handling of the `if __name__ == '__main__':` guard in scripts, especially around multiprocessing and spawning processes in PyTorch, with solutions that involve reorganizing code blocks for compatibility with `spawn` start method. Others discuss import conflicts, such as naming conflicts with libraries like 'onnx.py' or 'einops.py', which cause import errors or shadowing issues. Several reports concern reproducibility, numerical discrepancies, or failures in distributed or CUDA-related operations, often linked to environment configuration, version mismatches, or device-specific behaviors, with suggested workarounds involving environment variable adjustments, reinstallation, or code modifications. Some discussions involve fixing regressions or bugs, like memory leaks, etree traversal issues, or compatibility problems with hardware/software configurations (ROCm, CUDA, specific platforms), often requiring conditional guards, code restructuring, or additional API support. Overall, these conversations emphasize the importance of correct script structure, careful environment management, precise device handling, and targeted fixes for hardware/system-specific issues to ensure reliability and reproducibility."
2023-10-20,pytorch/pytorch,"The discussions highlight various technical issues and questions such as ensuring proper CUDA and library compatibility, especially with ROCm versions, addressing memory leaks in sparse tensor operations, and handling device contexts correctly in the internal implementation. There are concerns about the correctness of specific operations like `Tensor.__eq__`, the implications of skipping or modifying context managers during tracing, and potential regressions due to recent code changes in the compiler and runtime. Some issues relate to increasing introspection and debug capabilities, as well as ensuring performance regressions are understood and mitigated before release. Overall, the conversations focus on fixing correctness bugs, improving tooling robustness, and managing complex interactions between runtime components and system-specific configurations."
2023-10-21,pytorch/pytorch,"The discussions encompass several technical concerns, including the implementation and routing of PyTorch's internal operators (_VF.py and aten::), debugging CUDA vs CPU differences, and issues with consistency of tensor devices and data types during operations. There are ongoing questions about specific implementation details, such as CUDA kernel locations for GRU cells, tensor aliasing and storage management especially in relation to XLA devices, and handling large variables exceeding memory limits. Several entries request clarification or propose solutions for known bugs, performance gaps, or the effects of recent PR merges, such as changes to inductor optimizations and guard frameworks. Unresolved issues include platform-specific bugs, device conversions, and compatibility with various backends, alongside requests for testing and review of proposed fixes."
2023-10-22,pytorch/pytorch,"The discussions address various technical challenges in PyTorch development, such as unimplemented operators on specific hardware (e.g., 'c10d::broadcast_' on MPS), build failures due to missing dependencies (e.g., fmt/format.h), and performance issues with CPU-based mixed precision (BF16/FP16) due to incorrect kernel selection and core utilization. There are concerns about the correctness and robustness of backend APIs (e.g., `is_autocast_enabled`), and difficulties in debugging compiled functions, especially with Dynamo and TorchCompile, due to frame eval hook conflicts and bytecode decompilation limitations. Some issues highlight potential refcounting bugs in Torch XLA and problems with device aliasing and storage resizing, suggesting the need for codebase fixes or better API clarity. Overall, the discussions point to ongoing efforts to fix backend support, improve build stability, and enhance debugging and performance profiling robustness."
2023-10-23,pytorch/pytorch,"The discussions highlight several key concerns: the ambiguous origin of the 'c10' name, clarifying that it is unrelated to CUDA 10, with historical context provided; issues related to out-of-bounds indices in indexing operations, likely due to incorrect targets or discontinuous ground truth labels, and approaches to identify and resolve this error; challenges with deep kernel argument limits, suggesting kernel splitting or refactoring to reduce argument count; performance regressions on new hardware or software versions, especially relating to matrix multiplication on CPUs without hardware acceleration, along with considerations for proper configuration of thread and register usage; and the importance of better documentation, test coverage, and API design to manage complexity, ensure compatibility, and improve maintainability across various components like PyTorch internals, ONNX export, and distributed training."
2023-10-24,pytorch/pytorch,"The comments span various issues related to PyTorch's inductor/backend mechanisms, distribution (e.g., DDP, FSDP), and testing workflows. Common themes include difficulties with ensuring correctness and stability across features such as complex dtype support, shared parameters, dynamic shapes, and multi-node sharding strategies. Several discussions highlight the ongoing need for better debug tooling, test robustness, and API reliability—particularly around false dependencies, opaque kernel fusion behaviors, and minor bugs like version counters or fake tensor deduplication. Some issues also reflect CI flakiness, platform-specific inconsistencies, and the transition to new features or codebase restructuring (e.g., optimizations, codegen, pattern matching). Overall, unresolved questions revolve around improving correctness guarantees, build/compatibility, and test coverage in complex, high-performance scenarios."
2023-10-25,pytorch/pytorch,"The discussions primarily revolve around issues related to PyTorch's model export and inference workflows, including correct placement of `torch.no_grad()`, compatibility of torchscript models with ONNX, and handling of special cases like custom `ProcessGroup` classes and quantized tensors. Several comments address performance discrepancies, particularly discrepancies in GPU timing and memory usage across versions, suggesting potential impacts of environment setup or backend configurations like `CUBLAS_WORKSPACE_CONFIG`. There are also considerations for improving robustness and clarity in the code, such as refactoring for better modularity, handling dynamic shapes, and implementing more reliable mechanisms for fake tensor deduplication. Additionally, some discussions note ongoing flakiness in CI tests and the necessity for clearer documentation and testing to prevent regressions. Overall, unresolved questions include ensuring environment consistency for performance, maintaining backward compatibility with exported models, and structuring code for easier maintenance and debugging."
2023-10-26,pytorch/pytorch,"The collected comments highlight several key technical concerns within the 'pytorch/pytorch' repository discussions:

1. **Implementation and Integration of Mathematical Functions:** Several stakeholders suggest implementing specialized functions like Lambert W or complex-valued operations in PyTorch-friendly libraries (e.g., torchlambertw) rather than in core, emphasizing the importance of GPU efficiency and domain-specific libraries incubating research.

2. **Distributed Autograd and Gradient Correctness:** Multiple discussions revolve around the silent wrong gradients in `torch.distributed.all_reduce`, with considerations on semantics in distributed backprop, the correctness of gradient calculations across nodes, and the safety of the current semantics.

3. **Compiler and Tracing Limitations:** Issues with TorchScript and `torch.jit` conversion, especially with nested dictionaries, data-dependent shapes, and dynamic shapes, are raised, alongside recommendations to switch to `torch.fx` or to improve tracing APIs and shape inference mechanisms.

4. **Kernel and Backend Support:** Several comments concern support and optimization for specific hardware backends like ROCm, CUDA, AMD GPUs, and the inclusion of new operators (e.g., Cholesky, LU). There are proposals to extend support, refactor kernels, and optimize for specific device architectures.

5. **Testing Flakiness and CI Stability:** Many comments address flaky tests, CI failures, and flaky performance issues, with suggestions to improve test robustness, better reporting, and more precise control over test skips and merges, often requiring better tracking of test dependencies and environment consistency.

These discussions also include proposed solutions like code refactoring, domain-specific libraries, better shape inference, and internal support for new hardware features, with several unresolved questions about implementation details, compatibility, and testing strategies."
2023-10-27,pytorch/pytorch,"The comments revolve around several technical concerns in the PyTorch repository, including the subtle semantics of distributed gradient backpropagation, especially regarding `.all_reduce`, with debates on interpretation for gradients across nodes. There are issues related to internal implementation details like the mechanics of guard reimplementation, handling of custom context managers, and concerns about graph break conditions in Dynamo, especially for complex or flaky cases. Several discussions focus on the correctness and stability of features such as the `aot_compile` API, proper handling of data-dependent tensors, and ensuring that optimizations like fusion or specialized kernels do not introduce regressions. Additionally, some remarks address external build and environment issues, like CUDA library detection, system compatibility, and signing CLA requirements. Overall, the key unresolved questions include how to robustly handle side effects (like `set_grad_enabled`), improve reproducibility and correctness, and manage complex dependency and build issues in the CI environment."
2023-10-28,pytorch/pytorch,"The comments highlight ongoing challenges with PyTorch's data loading performance differences across OSes, suggesting potential threading solutions to replace multiprocessing due to GIL limitations. Discussions also address subtle correctness and gradient issues in distributed all-reduce operations, emphasizing the importance of proper semantics and backward operation understanding. Several issues involve build system errors, such as handling position-independent code, signing CLA compliance, or build failures with specific hardware/software configurations, indicating ongoing maintenance and compatibility concerns. There's also mention of debugging reproducibility, particularly in internal tests and CI runs, with some failures traced back to code conflicts, missing documentation, or flaky tests. Overall, these discussions point to core concerns around build stability, correctness in distributed/async operations, and performance optimization, along with the need for better testing and documentation clarity."
2023-10-29,pytorch/pytorch,"The discussions highlight ongoing efforts to enhance PyTorch's functionalities, including implementing multivariate normal CDFs (potentially in CUDA), and handling complex derivatives requiring numerical integrations. Several issues involve build and compatibility challenges, such as resolving dependency conflicts, compiler intrinsic support, and enabling Metal or CUDA in various environments, with solutions like building from source or customizing build flags. There are discussions on improving the pytree registration system to enforce namespace hygiene and lifetime safety, aiming to prevent bugs by contextualizing registrations and avoiding duplicate entries. Additionally, questions around supporting sub-interpreters, debugging runtime errors (e.g., segmentation faults, incorrect operator implementations), and integrating new features like `fori_loop` for JIT performance are raised. Overall, unresolved questions about build stability, namespace management, and ensuring feature completeness remain, with some solutions in progress or requiring further attention."
2023-10-30,pytorch/pytorch,"The comments highlight ongoing feature requests and technical concerns within the PyTorch project. Notably, there are discussions about improving `grid_sample` interpolation operators, CUDA compatibility issues on Mac M2, and efforts to document and clarify the algorithms used in `pca_lowrank`. Some users report performance regressions, especially related to `CUBLAS_WORKSPACE_CONFIG`, and suggest requiring explicit user control over certain behaviors to improve reproducibility and robustness. There are also concerns about internal CI test flakiness, proper namespace registration for pytrees, and the handling of dynamic shapes and mutation detection in AOTAutograd. Unresolved questions include how to safely extend namespace registration, improve test skipping strategies, and refine internal user API behaviors for stability."
2023-10-31,pytorch/pytorch,"The discussions highlight several technical concerns including handling of input mutations during export, especially in the context of `torch._dynamo` and `torch.export`, with suggestions to add explicit guards or annotations to prevent unintended re-executions. There's debate over whether support for `dicts` with kwargs should be generalized, with some advocating for a simple, buffer+metadata approach for data structures such as TensorDict. Compatibility issues with specific GPU hardware, driver versions, and compiler behaviors are also recurrent, emphasizing the need for better testing and environment validation. Additionally, efforts are ongoing to improve the stability of internal CI flakiness, rebase conflicts, and the MBart/Falcon model support across various platforms, along with proposals to streamline backend support (e.g., fused Adam) and improve serialization and operator dispatch mechanisms. Unresolved questions include the best way to handle dynamic shapes, input constancy, and ensuring a consistent user experience across different build environments and backends."
2023-11-01,pytorch/pytorch,"The discussions largely revolve around improvements and issues affecting PyTorch's internal and external workflows, including the handling of optional arguments (suggesting using pointers instead of booleans), the correctness and performance of dynamo-traced graphs, and the proper support for features like FSDP, ONNX export, and CUDA kernel dependencies. Key questions include whether to introduce new APIs alongside deprecated ones, how to better manage dynamic shape marking, and ensuring correctness of graph transformations such as inlining or inplacing. Several issues relate to the stability and correctness of compiled/traced graphs, especially with regard to mutable state, gradient handling, or external kernels, with some suggesting targeted bug fixes or better API design. Unresolved concerns include ensuring consistent behavior across different backends, managing compatibility and build configurations, and validating that transformations or optimizations don't cause regressions or failures in specific workloads or environments."
2023-11-02,pytorch/pytorch,"The comments reveal a variety of ongoing challenges and discussions within the PyTorch ecosystem. Major issues include handling of sparse tensors during multi-GPU training, with workarounds like registering indices and making sparse tensors only when needed. There are concerns about the stability, correctness, and precision of operations such as matrix square root and trace, sometimes linked to implementation details like pairwise summation. Several discussions revolve around improving debugging, profiling, and traceability, including better error messages, backtraces, and profiling tools like kineto and nvtx. Additionally, there are technical debates about API design (e.g., argument naming for pytree registration), implementation details (e.g., in-place ops, functionalization), and internal infrastructure, especially regarding testing, CI flakiness, and build compatibility across platforms and Python versions."
2023-11-03,pytorch/pytorch,"Several discussions concern maintaining compatibility and correctness in PyTorch's codebase, such as addressing issues with include dependencies (e.g., circular dependencies with funcol), handling optional function arguments (considering pointer vs boolean), and unifying in-place operation mechanisms across operators and collected tensors. There are also multiple reports of flaky CI tests that are being resolved by reruns or disabling flaky tests, often related to environment setup or platform support limitations. Additionally, some discussions recommend improvements in code modularity, such as replacing reliance on side-tables with more serializable or functional representations, and suggestions for better testing strategies, such as adding more comprehensive OpInfo cases or reproducer scripts. Overall, key unresolved questions relate to refining in-place and functional behavior, reducing flaky CI, and improving test coverage and stability for various backends and features."
2023-11-04,pytorch/pytorch,"The discussions primarily address the need for improved debugging tools and performance diagnostics in PyTorch, with recommendations to utilize functions like `autograd.detect_anomaly` despite their cost, and suggestions to expose more detailed memory usage metrics. Several issues consider enhancing fallback or custom implementations (e.g., Python-based `BCEWithLogitsLoss` with label smoothing, handling `FakeTensor` in autograd, and better support for optional parameters via pointers) to improve flexibility and user control. Multiple threads discuss infrastructural and compatibility concerns, such as building for ARM64 on macOS, supporting ROCm on Windows, and integrating benchmarking/testing workflows (`OpBench`, `TorchBench`) into CI pipelines. There are also ongoing efforts to fix bugs and regressions (e.g., ops like `aten::unique_consecutive`, unsupported operators in ONNX exports, and handling graph breaks), with some workarounds and patches being proposed. Unresolved questions include optimization of autograd tracing, handling nested control flow, and improving the build and deployment process across diverse platforms."
2023-11-05,pytorch/pytorch,"The comments reveal multiple ongoing technical concerns related to PyTorch's development, including issues with operator implementation on specific hardware devices like MPS, problems with gradient computations involving gradient of gradients, and the handling of attention masks in multihead attention modules to avoid inference breakage. Several discussions involve updating dependencies (e.g., Triton) for performance and compatibility, addressing build system and documentation support, and ensuring stability in complex features like graph optimization and inlining. There are also questions about supporting specific data types (like float8), improving testing strategies, and managing environment dependencies in Docker images. Overall, the discussions highlight efforts to fix device-specific operator support, improve performance and robustness, and enhance testing and CI infrastructure for future features."
2023-11-06,pytorch/pytorch,"The comments reveal ongoing discussions about PyTorch's support and implementation details across multiple features and experiments. Major concerns include ensuring consistent behavior during model export and retracing (particularly shape and shape inference behavior), handling of in-place mutation in autograd and symbolic graphs, and supporting complex operations or data types (e.g., handling complex tensors, multivariate distributions, or in graph break scenarios). There are questions about fallback strategies when specific functions or operations (like complex-specific implementations or certain hardware features) are missing, and how to gracefully handle or detect these cases. Additionally, several issues involve CI instability, flaky tests, and build configuration questions, indicating active efforts to improve testing reliability and support for new hardware or features. Unresolved questions focus on the best approach for API robustness, shape invariance during export, and consistent support for advanced features like scatter or sparse tensor layouts in various backends."
2023-11-07,pytorch/pytorch,"The discussions revolve around challenges related to extending and debugging PyTorch's compiler and operator support in various contexts. Key technical concerns include ensuring proper integration of new operators and IR patterns, handling dynamic shape and data-dependent shape guards (e.g., in inductor and dynamo), and managing the behavior of symbolic expressions like SymInts during graph compilation. Several issues highlight the difficulty in making certain functions (like `torch.abs` or `Tensor.clone`) compatible with symbolic or in-place operations, especially when data-dependent guards or device/shape-guarding are involved. There are ongoing efforts to improve the robustness of the IR, operator registration, and shape inference, with suggestions to better handle data-dependencies, support for new operators like QA in ONNX, and to fix bugs that cause crashes or incorrect results in specific device contexts. Unresolved questions include how to properly propagate symbolic shape info, reliably re-construct symbolic nodes after compilation, and improve the general infrastructure for caching, inlining, and guard placement."
2023-11-08,pytorch/pytorch,"The comments span a variety of technical issues related to PyTorch development:

- Several discussions focus on ONNX export support issues, especially handling unsupported operators like `aten::unflatten` in certain opset versions, and potential workarounds including replacing operators or fixing symbolic support.
- There are recurrent mentions of flaky CI tests across different platforms; many are resolved after numerous reruns, suggesting instability in test environments.
- Multiple issues involve compatibility, reproducibility, or precision concerns, particularly on Apple Silicon (MPS) devices, M1 hardware, and floating-point discrepancies.
- Some discussions address backend support problems, such as NCCL hangs on multi-GPU setups, device-specific bugs, or implementation details for features like gradient checkpointing, FSDP, or kernel support on various hardware.
- Overall, core challenges include operator support gaps (e.g., unsupported ONNX ops), platform-specific bugs (particularly on Apple and ROCm), and testing stability, with many proposals for operator replacements, code fixes, or improvements in testing and debugging workflows."
2023-11-09,pytorch/pytorch,"The discussions highlight several technical concerns, mainly regarding unexpected behaviors and failures in PyTorch's distributed, autograd, and JIT functionalities, often related to shape/memory mismatches, compatibility issues, or incomplete support for specific hardware/backends (e.g., ROCm, ARM, CUDA versions). Some issues involve the need for improved error handling, validation, and robustness—such as better shape validation for send/recv operations, handling of shape inference errors, and proper support for certain tensor operations in compiled and custom autograd functions. Others address code style and documentation consistency, as well as infrastructure considerations like build dependencies and tooling integration. Unresolved questions frequently relate to ensuring BC-compatibility, managing platform-specific limitations, and extending support for new hardware or features (e.g., fake tensors, torch.compile). Overall, the discussions suggest ongoing efforts to stabilize and improve correctness, performance, and user experience in complex, edge case scenarios."
2023-11-10,pytorch/pytorch,"The comments reflect diverse ongoing issues and discussions: a longstanding bug in `torch.multinomial` validation checks that hasn't been fixed despite being documented as such; repeated unreliable CI flakiness reports on Linux platforms that are eventually deemed false positives; difficulties in GPU utilization and performance discrepancies on NVIDIA GPUs (notably, low GPU usage and potential hardware or driver issues, with troubleshooting suggestions like `nvidia-smi` and `CUDA_LAUNCH_BLOCKING=1`); internal build and environment setup challenges, such as linking flags for inductor, issues with PyTorch version support on Windows, and compatibility concerns with libraries like `libcudnn`; multiple refactoring, CI, and documentation update requests, with some requiring minor fixes (e.g., linting, signatures, docstrings) and others involving complex features like `TensorDict`, export API support for complex ops, and handling symbolic shape mutation in `torch._inductor`. Many unresolved questions concern correctness, backward compatibility, build configurations, and tooling support for advanced features."
2023-11-11,pytorch/pytorch,"The discussions highlight several common concerns: a memory leak issue in DataLoader's __getitem__ (Issue #13246), inconsistencies and confusion around the use of unbiased vs biased estimators in BatchNorm during train/eval (Issues #77427, #1410), and potential discrepancies in how datasets/loading methods should be optimized or designed for efficiency and correctness. Other recurring themes include the need for clearer documentation on certain behaviors (e.g., Bessel's correction in BatchNorm, dropout behavior during export), technical challenges in handling compiled or JITed models with dynamic shapes and complex operators (Issues involving TorchDynamo, inductor, ONNX export), and questions about proper validation, testing, and CI workflows (e.g., flaky tests, merge failures, linting). Several discussions probe the appropriateness of certain code conventions, configurations, or internal API design choices—such as the correct handling of const data pointers, tensor mutation visibility, and operator support for advanced features like higher-order ops or multi-device sharing. Overall, unresolved questions center on balancing correctness, performance, and clarity—particularly in complex areas like autograd, model serialization, memory management, and hardware-specific kernels."
2023-11-12,pytorch/pytorch,"The discussions highlight ongoing challenges in extending PyTorch's support for various operators on specific devices like MPS, requiring further implementation of missing ops such as `aten::upsample_bicubic2d.out`. Multiple issues relate to CUDA and GPU/multiprocessing compatibility, with solutions emphasizing proper start methods (`spawn`) and environment configurations to prevent runtime errors. Several bug reports concern guard installation in the JIT/graph compilation pipeline, suggesting more robust guard management and guard propagation strategies to handle constant lifting and nested graph breaks. Additionally, there are extensive efforts to improve code quality via automated linting and documentation standards, and some issues involve build environment inconsistencies, especially on Windows. Overall, unresolved questions focus on proper operator support for diverse hardware, guard correctness in JIT, and maintaining code quality amidst ongoing refactoring and build system changes."
2023-11-13,pytorch/pytorch,"The discussions highlight several key technical concerns: (1) limitations in current CUDA kernel support, especially for non-x86 architectures like ARM (e.g., AArch64), due to incomplete unwinder implementation; (2) issues with non-compositional tensor operations such as `as_strided()` and `resize_()`, which impact functionalization and differentiation accuracy; (3) the need for proper handling and tracking of parameters within `torch.compile()`-optimized modules, especially for models with parameterized or in-place operations, to ensure correctness during backward passes; (4) debugging and resolving flaky or failing CI tests that often relate to environment setup, bug regressions, or hardware-specific behaviors; and (5) aligning best practices for automatic guard management, module attribute handling, and support for advanced features like nested tensors, higher-order ops, and support for non-standard backends, with some suggestions for potential long-term fixes or workarounds."
2023-11-14,pytorch/pytorch,"The comments reveal several key technical concerns: the complexity of understanding the interactions between views, detaching, and requires_grad in PyTorch's autograd, especially when views do not require gradients; challenges around caching datasets, with arguments about API simplicity, existing solutions, and worker memory leaks; performance implications of depthwise convolution implementations and the slow backward pass, with benchmarking data for FP16/FP32; issues related to CUDA NCCL group synchronization, error handling for mismatched send/recv sizes, and the need for better validation, error messages, and asynchronous communication support; and documentation, code style, and API consistency improvements across multiple modules. There are also unresolved questions about bug regressions, support for new features like quantized models on Vulkan, and integration plans for tools like TensorDict, torch.fx, and distributed training with FSDP and DDP. Proposed solutions include API refactoring, improved logs and validation, and postponing non-critical features until stability and clarity are achieved. Many discussions highlight ongoing work or need for further investigation, with some issues suggested for tracking via new issues or raising to core teams."
2023-11-15,pytorch/pytorch,"The discussions primarily revolve around PyTorch's internal mechanics for handling optimizers, parameter management, and model state consistency, with particular emphasis on set-based parameter grouping, backpropagation behavior with in-place operations, and compatibility issues with external modules like torchvision. Several issues highlight the importance of proper model-optimizer synchronization, correct serialization of model states (including custom or out-of-tree components), and maintaining behavior compatibility across different hardware backends and PyTorch versions. Concerns are raised about the robustness of debugging tools, error messaging, and testing for edge cases such as dynamic shapes and sparse or jagged tensors. Additionally, proposals include improving API discoverability for backends, better error handling for missing dependencies like NCCL, and more precise control over runtime behaviors like deterministic algorithms and floating-point precisions (e.g., bfloat16). Unresolved questions include how to manage model modifications after optimizer loading, ensuring correctness when models are loaded or reconstructed, and integrating support for more hardware backends and tensor representations in a consistent and user-friendly manner."
2023-11-16,pytorch/pytorch,"The discussions highlight significant concerns about the discoverability, management, and stability of third-party backends in PyTorch, emphasizing that current methods like `torch._C._dispatch_has_kernel_for_dispatch_key` are internal and fragile. Contributors suggest maintaining a curated, externally maintained list (e.g., in a YAML file) within core, accessible via `torch.compiler.list_backends`, with clear criteria for inclusion and user-friendly error messages guiding installation. There's consensus that the current proposal to remove non-official backends risks making them less discoverable, which could harm ecosystem adoption. Moreover, explicit handling and user notifications about missing backends or support status are recommended to improve user experience, while ensuring that the core remains unbiased and does not endorse or judge external developers. Lastly, some discussions point to the need for gradual, well-documented improvements rather than abrupt removals or opaque internal checks."
2023-11-17,pytorch/pytorch,"The discussion highlights issues related to multiprocessing and threading locks, especially under 'fork' versus 'spawn' contexts, and potential workarounds like disabling internal threading with `torch.set_num_threads(1)`. Several comments mention CUDA memory management, GPU compatibility, and the challenges of ensuring correct device-specific behavior, such as support for quantized ops on different architectures or backends. Some discussions focus on test flakiness due to environment configurations or hardware variability, and the need for better discoverability and management of external backends in PyTorch. Conflicts and errors during model conversion, export, or runtime misbehavior are also discussed, including concerns about serialization, shape inference, and deep integration with native code or compiler frameworks. Unresolved questions include how to improve test robustness, handle threading/memory issues safely, and support evolving backend features, especially in the context of JIT, FX, or quantization workflows."
2023-11-18,pytorch/pytorch,"The discussions encompass a variety of technical concerns and questions related to PyTorch development. Key issues include challenges with package installation (notably slow downloads and environment solver performance), support for quantization and device-specific operators (e.g., MPS support and operator implementation gaps), and reproducibility and correctness of models when executing across different backends, devices, and shape configurations. Several discussions focus on the behavior and stability of features like dynamic shapes, CUDA graph interactions, and guard mechanisms in JIT compilation and runtime, often considering potential fixes, workarounds, or API improvements. Additionally, there are ongoing efforts around model export compatibility, handling of sparse tensors, and maintaining backward compatibility in the context of evolving internal representations. Unresolved questions involve how to better integrate user feedback, improve logging and debugging, and develop more robust, predictable runtime behaviors for heterogeneous hardware and advanced model configurations."
2023-11-19,pytorch/pytorch,"The discussions encompass several key technical concerns, including challenges in building and installing PyTorch on specific hardware architectures (e.g., ppc64le), issues with kernels and operator compatibility across different CUDA versions, and discrepancies in numerical precision and trace correctness during model export and tracing, especially with dynamic or custom models. Several issues focus on improving the robustness and correctness of PyTorch's JIT and ONNX export workflows, addressing runtime errors, and ensuring full support for various backends including MPS, CUDA, and CPU. Other topics involve enhancing the user experience with API consistency, guard conditions, and debugging capabilities, along with infrastructure-related concerns such as merge conflicts, CI failures, and licensing considerations. Across these discussions, unresolved questions remain about handling complex trace scenarios, ensuring backward compatibility, and optimizing performance overhead introduced by abstraction layers."
2023-11-20,pytorch/pytorch,"The discussions primarily revolve around improving PyTorch's code stability, usability, and developer experience. Key concerns include handling and documenting edge cases with tensor operations (like autograd, view, and sparse tensors), ensuring backward compatibility, and reducing internal and external API surface inconsistencies—especially regarding registered functionalities, submodule naming, and the visibility of supported backends. There's also a focus on debugging and fixing specific issues related to NCCL errors, missing features (like GRU support on MPS), and correctness of shape inference and serialization processes. Proposals include adding warning mechanisms, better API discovery (via list_backends with documentation), and careful handling of feature deprecation and BC-breaking. Unresolved questions involve best practices for API evolution, managing external dependencies (like packaging), and ensuring reproducibility and stability across different hardware and software configurations."
2023-11-21,pytorch/pytorch,"The discussions highlight several technical issues: the potential mismatch and leakage in the `torch.compile` autograd pipeline, especially in handling in-place mutations and side effects, which may lead to correctness or memory issues; the complexity of serializing or supporting custom Triton kernels in model export, raising questions about feasibility and current limitations; and the need for better support for dynamic shapes, guard mechanisms, and better logging/debugging tools to diagnose compilation or runtime errors. Some suggest enhancements like better in-memory layout handling for batch computations, explicit API support for tensor-to-bytes serialization, and more robust handling of autograd functions and graph breakpoints. Several unresolved questions concern how to extend or unify support for heterogeneous or dynamic input attributes, manage kernel support in export, and improve reproducibility and debugging across different environments and hardware. Overall, the main concerns revolve around correctness, performance, and extensibility of the compile/export infrastructure in complex, real-world models."
2023-11-22,pytorch/pytorch,"The comments highlight various issues related to PyTorch's development, including the need for improved support for specific data types (e.g., uint16), correctness and stability of numerical results (e.g., gradcheck failures, in-place operations, large exponentials), and handling of dynamic shapes, especially with subclasses and data-dependent tensors. Several discussions focus on backends like inductor, aot_autograd, and compiler flags, with concerns about correctness, performance, and test flakiness, often requiring patching, rebase, or experimental workarounds. There are also questions about the behavior of specific features like autocast, gradient hooks, and distributed collective operations, alongside ongoing efforts to improve test robustness, memory management, and internal infrastructure. Unresolved questions include how to support non-static or data-dependent shapes, how to fix certain internal bugs and their impact on performance, and how to streamline testing/execution strategies across different configurations."
2023-11-23,pytorch/pytorch,"The discussions highlight challenges in supporting and discovering backends (such as PowerPC, CUDA, ROCm) and the impact of project policies on their accessibility, with concerns about discoverability, maintainability, and licensing. There are recurring issues related to build failures, compatibility, and runtime errors, often linked to environment configuration, missing dependencies, or system-specific limitations. Several threads involve fixing specific crashes or performance regressions, sometimes requiring reverts or manual workarounds, and questions about build tooling, test coverage, and error diagnostics are common. Some propose improvements in backend registration, documentation, and testing practices to prevent future issues. Unresolved questions involve how to better automate environment validation, handle data-dependent guard conditions, and improve the robustness of compile-time and runtime support across diverse hardware."
2023-11-24,pytorch/pytorch,"The discussions highlight persistent compatibility and performance issues with PyTorch across various hardware and software configurations, such as CUDA, MPS, and Apple Silicon, often related to backend operator support and kernel compilation failures. Several reports mention specific bugs or regressions introduced by recent commits, with some suggesting potential workarounds or the need to disable certain features (e.g., nvfuser). There are ongoing efforts to improve dynamic shape support, subgraph caching, and code generation, but challenges remain due to complex interactions between PyTorch's JIT, FX graph transformations, and hardware-specific kernels. Compatibility problems with packaging, dependencies like Triton, and non-standard build environments (e.g., ppc64le) are also discussed. Overall, many issues are tied to kernel support, build configurations, and performance regressions, with active investigation and patching underway."
2023-11-25,pytorch/pytorch,"The discussions primarily revolve around debugging and optimizing PyTorch features, with particular emphasis on issues related to reproducibility and numerical precision (e.g., effects of autocast, fp32 parameters, and precision handling in FSDP). Several users report errors with specific models or operations, such as CUDA memory access errors, DLL load failures, or mismatched behavior in Jupyter notebooks, often suggesting build or environment misconfigurations. There are ongoing efforts to improve serialization (pickle, FX codegen), memory sharing (e.g., device placement, tensor views), and dependencies (e.g., packaging and wheel consistency), alongside addressing internal compiler or runtime bugs (e.g., handling in structured_avg_pool2d_backward and in-place mutation safety). Unresolved questions include the proper management of device placement flags, effects of custom build options on consistency, and how to better document or enforce correct usage patterns for distribution and distribution-related classes. Overall, the discussions reflect active troubleshooting, feature development, and environment management challenges within the PyTorch community."
2023-11-26,pytorch/pytorch,"The discussions highlight several technical concerns: ensuring CUDA driver versions match the installed PyTorch CUDA libraries to prevent runtime errors, especially when dealing with specific wheel support issues and platform compatibility; addressing build failures caused by deprecated or incompatible ffmpeg versions, requiring patching or reconfiguration; managing complex interactions between FSDP, autocast, activation checkpointing, and gradient scaling to avoid out-of-memory errors and ensure proper state dict saving; and resolving limitations in symbolic shape inference (such as `_constrain_as_size`) during torch.export and inductor compilation, which can lead to shape-related errors and guard failures. Additionally, questions about version support for CUDA on various systems and the necessity of certain type annotations point to ongoing efforts to improve compatibility and code clarity."
2023-11-27,pytorch/pytorch,"The discussions reflect multiple technical concerns in the PyTorch codebase including issues with sparse tensor operations, GPU and CPU backend consistency, and the handling of in-place tensor and attribute mutations. Several reports mention intermittent CI test flakiness, often requiring re-disabling tests or rebase efforts, highlighting flakiness difficulties. Some issues relate to the integration of third-party tools like Triton, CUDA, or third-party libraries, posing challenges for stable functionality and build reproducibility, especially across different environments or hardware. There are concerns about backwards compatibility (e.g., attribute mutation semantics, deprecated APIs), as well as specific issues with the correctness of code motion, tensor shape handling, and performance optimizations (e.g., in inductors). Unresolved questions include better debugging tools for sharding strategies, handling third-party dependencies, and ensuring consistent behavior across platforms and backends."
2023-11-28,pytorch/pytorch,"The comments address various issues including installation problems with Caffe2, discrepancies in distributions of batch normalization, and support for custom operations or backends in PyTorch, often involving low-level kernel or IR representations. Several threads discuss the challenges of implementing specific functionalities (e.g., statistical functions, dynamic shapes, CUDA compatibility, deterministic modes) and the associated performance and correctness considerations, sometimes requiring reorganization of code or detailed debugging. There are also discussions about CI flakiness and test disabling strategies, alongside questions about support for non-standard hardware architectures (e.g., ppc64). Overall, the main concerns are ensuring correctness, compatibility across platforms, and efficient integration of new features or fixes, with some pending work on long-term solutions or infrastructure adjustments."
2023-11-29,pytorch/pytorch,"The collected comments highlight several ongoing concerns and discussions within the PyTorch community, notably about feature support and correctness in distributed training, custom operators, and the behavior of specific functions like `torch.compile`, `autograd`, and `onnx`. There are issues related to support for newer features such as fp8, dynamic shapes, and Triton kernels, as well as questions about proper API design, input mutation handling, and compatibility across PyTorch versions. Many reports involve flaky tests in CI, regressions, or unimplemented operators, with some suggestions for workarounds and future improvements, such as better documentation, explicit API supports, or rethinking certain design decisions (e.g., attribute mutation handling, in-place ops). Several comments indicate progress in fixing bugs or refactoring components, but unresolved questions remain about support for specific operators, performance regressions, and correctness guarantees in edge cases. Overall, the discussions reflect active development, ongoing bug fixes, and planning for feature expansion, with an emphasis on stability, correctness, and API consistency."
2023-11-30,pytorch/pytorch,"The comments reveal ongoing discussions about environment variable dependencies (e.g., `CUDA_VISIBLE_DEVICES`, `NCCL_P2P_DISABLE`) affecting NCCL/NCCL performance and error handling, with plans to improve NCCL dependency checks and exception messaging. Several issues involve performance and correctness bugs in complex models or operators (e.g., deterministic support for ops, shape inference with prim view ops, or all-gather/fusion with mixed fp8/bf16 dtypes), with plans for bug fixes and better testing. There are concerns about internal development costs, backend discoverability, and the complexity of supporting features like fp8, sparse tensors, or custom backends in PyTorch’s core/auto-derivations, leading to proposals for policy changes and better API abstractions. Multiple PRs aim to address bugs, improve API consistency, and enable new features (e.g., quantization, meta tensors, torchfunc integration), with some discussions about the necessity of maintaining external extension mechanisms versus core support and community contributions."
2023-12-01,pytorch/pytorch,"The issues span various topics, including challenges in reproducing and debugging backend behaviors (notably in inductor and Triton code execution), the need for improved test robustness and coverage (such as GPU compatibility and accuracy assessments), and concerns about build stability and environment consistency (e.g., CUDA version mismatches, environment setup, and CI flakiness). Several comments request detailed reproductions, benchmark analyses, and potential code fixes for performance regressions and correctness issues, often suggesting more granular testing or specific code edits (like adjustments to `baddbmm`, `reshape`, or `scatter`). There are also discussions around architecture evolution (FSDP device management, CUDA support, and combination with CPU offloading), as well as documentation, build, and environment setup considerations (e.g., support for older hardware, deprecation strategies, and CI infrastructure). Unresolved questions mainly concern debugging discrepancies between CUDA and CPU runs, ensuring compatibility across environments, and refining API or implementation details to improve stability and performance."
2023-12-02,pytorch/pytorch,"The discussions highlight ongoing challenges related to building and compiling PyTorch, particularly with dependencies such as rocblas and compiler compatibility issues (e.g., GCC version concerns). Several issues involve merging or re-landing PRs with merge conflicts, build failures, or flaky test results, which require careful management of CI workflows and code conflicts. There are also technical debates around optimization and performance, such as the use of `torch.compile()`, CUDA support, and memory management during checkpointing and shared optimizer states in FSDP. Some discussions focus on tooling, including lintrunner configuration and the importance of unit tests, as well as feature enhancements like parallel audio decoders and safe tensor manipulation. Overall, unresolved questions remain about CI stability, compatibility with hardware/software environments, and feature implementation details."
2023-12-03,pytorch/pytorch,"The discussions highlight concerns over adding Adafactor as a new optimizer, considering its memory benefits for large models, with questions about whether it should be integrated into core or labs, and how it compares to existing optimizers like Adam. There are technical challenges in making Adafactor compatible with FSDP, especially with different sharding strategies, and analyses suggest that FSDP with Adam becomes more efficient beyond 2 GPUs. Another key topic involves potential speedups in compile times via function caching or higher-order ops, with plans to explore those options. Several issues relate to build/test stability and clarity, such as handling large PRs, test disabling, and reproducibility concerns on specific hardware platforms."
2023-12-04,pytorch/pytorch,"The discussions primarily revolve around issues related to shared memory limitations in DataLoader workers, particularly in non-Docker environments, and solutions such as adjusting the multiprocessing start method or reducing shared memory usage. Multiple threads highlight diagnostics and troubleshooting steps for TensorBoard package conflicts, including duplicate installations and environment cleanup, especially involving TensorFlow and TensorBoard versions. Several issues concern CUDA and hardware-specific problems, such as support for certain GPU architectures (e.g., ROCm on AMD GPU, NVIDIA 7900XTX, or A100 with specific library versions), along with driver and kernel parameter configurations. There are also questions about PyTorch's internal functionalities, such as the behavior of tensor sharding with small dimensions, handling of aliasing inputs in operations, and compatibility of advanced modules like FSDP and Adafactor with new sharding techniques. Lastly, some discussions address testing reproducibility, performance regressions, and the impact of recent code changes on stability and correctness across different system configurations, including macOS with Apple Silicon."
2023-12-05,pytorch/pytorch,"The shared comments span various topics, including persistent issues with DataLoader freezing on Linux OS and Jupyter Notebook setups, especially related to multiprocessing and threading options; challenges in data loading performance and API flexibility, like switching between threading and multiprocessing workers; bugs in PyTorch's handling of in-place tensor operations, especially with advanced indexing, cloning, or shape transformations, which can cause incorrect graphs or inaccuracy in fused kernels; compatibility and support issues for newer hardware features such as cuSPARSELt 0.5.0, including build requirements and internal library support, along with questions about user experience improvements and how to better abstract or integrate these features at the PyTorch level; and ongoing efforts to stabilize CI tests, review internal patching, and handle dynamic shape support, with considerations on API design, bug fixing, and documentation for new capabilities like semi-structured sparsity and distributed sharding."
2023-12-06,pytorch/pytorch,"The comments highlight several issues and discussions around improving PyTorch's features and deployment workflows. Key concerns include enhancing support for complex data structures (e.g., `Dict[str, Tensor]`, lists of tensors), fixing bugs in loss computations for negative values, and ensuring compatibility and correctness of model export and optimization (notably with `torch.onnx.export`, shape inferences, and handling negative indices). There are notable technical challenges in enforcing consistent behavior across different backends and hardware (e.g., cuSPARSELt support for int8, handling `Work` classes in distributed communication, and issues with in-place operations and serialization). Additionally, discussions focus on the maintenance and stability of critical features like sharded optimizer states, the impact of recent PR regressions, and ensuring build compatibility on various platforms. Unresolved questions include how to safely import distributed modules without build dependencies, improving test robustness, and whether certain patches and feature enhancements can be merged into upcoming releases like 2.2."
2023-12-07,pytorch/pytorch,"The discussion highlights several technical concerns across the threads. Notably, users question the implementation and correctness of second derivative support in grid sampling autograd, especially on distributed data parallel (DDP) and with higher-order derivatives, with some reports of unimplemented backward kernels. There are issues related to CUDA kernels, such as supporting tensors of boolean type, unsigned integer types, and handling floating-point inaccuracies for INT8 quantization, which may involve private API usage or limitations of cuSPARSELt. Additionally, several discussions focus on improving distributed API usability, debugging GPU hangs (like NCCL timeouts), and maintaining compatibility and stability during system updates or feature rollouts. Lastly, there are ongoing efforts in hardware support, backend configuration, and runtime behavior, emphasizing the need for stability, comprehensive testing, and better error messaging in complex AUTOGRAD/FX workflows."
2023-12-08,pytorch/pytorch,"The comments highlight several recurring technical issues and discussions in the PyTorch repository: 

1. CPU core utilization and affinity problems in DataLoader, which solutions like `taskset` address; there's also discussion on default process affinity behavior and how Pytorch masks CPU affinity by default, affecting CPU core usage across forks.
2. Compatibility and correctness issues with quantized models, such as biases not propagating or timing discrepancies, often linked to data types (e.g., int8 vs float32 biases) and underlying library support (e.g., cuSPARSELt, CUTLASS versions).
3. Problems with the torch.onnx.export process, especially when using `dynamo_export`, source code inconsistencies, or unsupported operators (e.g., `math.ceil`), with suggestions to fix by improving operator support, documentation, and function interface constraints.
4. Flakiness in CI tests across platforms (e.g., dynamo, torchbench), leading to issues being temporarily disabled and discussions on better test strategies, such as adding unit tests or switching to more reliable infrastructure.
5. Broader discussions on kernel support, kernel registration, and API design, including handling special cases in collective communication (like batched P2P vs all2all), with suggestions to streamline and unify APIs and improve internal consistency."
2023-12-09,pytorch/pytorch,"The discussions highlight several technical issues: concerns about building libtorch with Link-Time Optimization (LTO), particularly its compatibility with dependencies like protobuf and ONNX; questions regarding customization of pip packages for different GPU architectures (Kepler, Maxwell) and how to implement such variations; investigations into persistent memory leaks during CUDA FFT operations, especially in relation to CUDA 12.2 and cufft cache management; clarifications on internal behaviors such as memory reservation/reservation versus active allocation, the handling of workspace allocations in cublas, and limits on GPUs or MIG devices. Several technical uncertainties involve implementation details—such as the handling of `saved_result` in autograd, the negative overflow issue for `INT64_MIN`, and the impact of code modifications on backward compatibility and performance—some of which remain unresolved."
2023-12-10,pytorch/pytorch,"The collected comments highlight several technical concerns, including: (1) the lack of DistributedDataParallel support for C++ API and training support on non-Python frontends, (2) implementation gaps and regressions related to specific operators (e.g., bicubic upsampling, FFT, complex tensor operations) and the need for better support for dynamic shapes and shared checkpoint loading in distributed training, (3) issues with compiling large or complex models in torch.compile, particularly with handling deep graphs, heavy models like stable diffusion, or custom modules like heapq, (4) integration challenges with external tools such as ONNX, Triton, and debugging distributed models, often due to unsupported operators or incomplete feature coverage, and (5) maintenance and review bottlenecks, including stale PRs, merge conflicts, and missing release notes. Proposed solutions include increasing limits (e.g., CUDA device limits), separating refactors from feature additions, and improving documentation and testing, but several questions remain about operator support, regression impacts, and best practices for advanced debugging."
2023-12-11,pytorch/pytorch,"The discussions cover a range of issues related to PyTorch internals, plugin support, and deployment workflows. Key concerns include incomplete API support for hooks (forward, backward, tensor hooks), especially on platforms like MPS and in certain functions like `register_full_backward_hook`. There are multiple reports of unsupported operations in various devices (e.g., MPS, ROCm with specific operators, CUDA backend), with suggestions to implement fallbacks or extend support, sometimes requiring kernel or operator deprecation, or API adjustments. Additionally, challenges involve shape and tensor metadata handling in tracing and serialization, especially with symbolic shapes and fake tensors, highlighting the need for more robust automatic verification and formalization. Finally, some discussions concern software engineering practices: API stability (e.g., device specification defaults), build system integrations, and test coverage for nuanced features like inlining, node canonicalization, or multi-device configurations."
2023-12-12,pytorch/pytorch,"The discussions mostly revolve around handling unsupported or experimental operations in PyTorch's compilation and export workflows, such as custom operators, complex number support, or operations like resize_ and relu_backward, often requiring either stubbing as no-ops or implementing custom kernels. Several issues concern the stability, performance, and correctness of these features, especially when using dynamic shapes, custom ops, or certain hardware (e.g., MPS, cuSPARSELt). There are requests for enhanced debugging, better documentation, and infrastructure improvements such as explicit guards or shape guards to catch shape-related errors early. Some proposals aim at improving model diagnostics, testing coverage, and managing codebase complexity via automated issue triaging or IR canonicalization. Unresolved questions include the best way to support operations with complex or dynamic shape inputs, and how to safely extend or integrate these unsupported features into the core PyTorch stack."
2023-12-13,pytorch/pytorch,"The discussions highlight ongoing challenges with protobuf symbol visibility in libtorch, linker behaviors affecting protobuf inclusion, and linker errors on Linux, especially related to protobuf and linker scripts usage. Several comments discuss build configuration nuances, such as `@CAFFE2_LINK_LOCAL_PROTOBUF@` and protobuf dependency exposure. There are also questions about the handling of protobuf symbols since recent protobuf versions (3.22+), linker/linker scripts for Linux, and differences between Mac and Linux builds, particularly in relation to protobuf symbol visibility. Additionally, there is interest in the status ofexperimental features like fp8 support, protobuf serialization issues, and build infrastructure concerns. Overall, a key concern is managing protobuf symbols and build configurations for consistent, portable libtorch binaries, alongside addressing protobuf version compatibility and linker errors across platforms."
2023-12-14,pytorch/pytorch,"The comments reflect ongoing discussions and concerns related to PyTorch's features and internal workflows, including potential breaking changes from argument parsing updates, compatibility issues with various hardware (especially AMD and NVIDIA GPUs), and stability/regression regressions in profiling, compilation, and distributed training workflows. Several issues address the need for clearer documentation, better support for system configurations (e.g., MIG devices, CUDA versions), and improvements to testing and internal dev tools for scale and reliability. Notably, there are efforts to revert broken patches, improve multi-GPU and distributed correctness, and address flaky CI tests related to trunk stability. Many questions focus on debugging, performance regressions, and whether certain features or patches should be integrated for broader usability across hardware and software environments."
2023-12-15,pytorch/pytorch,"The discussions primarily revolve around improving tooling and testing infrastructure within PyTorch, such as adding more reliable GPU memory profiling tools, implementing an API for peak memory monitoring, and developing formal verification methods for ops using constraint-based input generation. There are concerns about maintaining backward compatibility, particularly with schema versions, and how to best manage user workload to support both forward and backward validation. Several suggestions include introducing new APIs, reworking existing mechanisms like OpInfos, and improving CI reliability via better failure detection and artifact handling. Unresolved questions include how to scale input generation efficiently, whether certain features can be integrated without breaking compatibility, and how to handle flaky or platform-specific CI failures."
2023-12-16,pytorch/pytorch,"The discussions highlight several key technical concerns, including the difficulty of implementing automatic tuning in PyTorch due to its loosely coupled design compared to TensorFlow, with a current focus on autotuning `num_workers` in DataLoader. Other issues involve support and performance regressions on specific hardware (e.g., Windows, MPS, ROCm), with efforts to improve stability, compatibility, and benchmarking clarity. Several bugs are identified, such as input validation errors, memory copying inefficiencies, and support limitations for certain operators like `Conv3d` on MPS, often accompanied by suggested fixes or workarounds. Additionally, there are ongoing discussions about testing, reproducing crashes, and ensuring build stability across platforms, as well as refinements in internal mechanisms like autograd's handling of saved variables and compiler optimizations. Overall, the conversations reflect active troubleshooting, feature enhancement, and validation challenges in maintaining and advancing PyTorch's robustness and portability."
2023-12-17,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's support on Apple M1/M2 devices, particularly issues with operators like `aten::_weight_norm_interface` and `aten::upsample_bicubic2d.out` not being supported on MPS, leading to errors such as ""Conv3D is not supported on MPS."" There is also concern about performance slowdowns when running inference on Apple silicon, with reports of significant slowdowns (e.g., 100x slower) and memory issues, especially with operations like SVD and large batch processing. Additionally, some discussions mention the limited GPU support for AMD ROCm on Linux and the complications of using WSL2 on Windows, with performance trade-offs compared to native environments. There are technical suggestions to set environment variables (like `PYTORCH_ENABLE_MPS_FALLBACK=1`), and ongoing efforts to improve support through patches and upstream fixes; unresolved questions include how to efficiently leverage GPU capabilities on various architectures and whether upcoming features like `torch.compile` and `torchtrt` will help mitigate current performance and support gaps."
2023-12-18,pytorch/pytorch,"The comments encompass a diverse set of issues related to PyTorch's development, including hardware-related challenges (e.g., GPU power management, compatibility with Apple Silicon), functionality gaps (e.g., missing operator implementations on MPS, support for certain operations like `_fft_r2c`), and build or stability concerns (e.g., trunk flakiness, build errors, rebase management). Several discussions focus on ensuring correctness and performance, such as fixing bug regressions, improving compilation and evaluation benchmarks, and expanding test coverage. Topics also include API design considerations (e.g., handling tensor subclasses, reshaping, and serialization) and build system nuances (e.g., handling protobuf, compiler flags, and internal infrastructure). Many unresolved questions involve ensuring backward compatibility, addressing platform-specific issues (e.g., ROCm, Windows, macOS), and maintaining stable CI workflows amid ongoing development changes."
2023-12-19,pytorch/pytorch,"The discussion centers around issues related to PyTorch's internal implementation and behavior, including potential bugs, performance concerns, and API support. Several technical points include the handling of special cases in quantization and operator registration (`torch_dispatch` behavior, relaying of `clone`, `view`, `index_put` operations), assembly-level concerns on overflow or compiler errors, and the proper way to extend or customize operator implementations (e.g., for `matmul`, `cudnn_convolution`). There are also ongoing efforts to improve features like higher-order ops lowering, distributed tensor sharding, and decompilation usability, with questions on ensuring correctness, compatibility, and performance. Some issues involve stability and flakiness in CI or across hardware platforms, and require systematic troubleshooting or code modifications. Overall, the conversation reflects active development, bug fixing, and feature enhancement needs, along with strategic API, implementation, and testing considerations."
2023-12-20,pytorch/pytorch,"The discussion covers multiple technical concerns related to PyTorch’s distributed communication, CUDA memory management, symbolic shape handling, and ONNX export compatibility. Key questions include how to serialize arbitrary objects in distributed settings, how CUDA preallocated workspace impacts memory overhead, and how to support dynamic shapes and arbitrary functions during export. There are also questions about specific code behaviors, such as the effect of dtype conversions on memory and whether certain regressions are caused by recent commits. Some suggested improvements involve wrapping functions to prevent unnecessary memory reallocations and clarifications around support for CPU processing and specific operators. Unresolved issues include ensuring backward compatibility with Triton, handling shape and function serialization cleanly, and fixing build failures linked to certain commits or configurations."
2023-12-21,pytorch/pytorch,"The discussions reveal concerns about numerical accuracy in tensor operations like trace and matrix square roots, as well as support for batching in trace. There are recurring issues with runtime errors, compatibility problems, and flaky CI tests, often linked to specific commit regressions or hardware (e.g., ROCm, CUDA versions, architectures). Multiple comments suggest verifying or fixing issues related to specific operations (e.g., `copy_`, `expand`, `reshape`) and enhancing debugging tools or support for advanced features like associative scans, inductor lowering, and autodiff. Support for non-CUDA backends, device compatibility, and environment setup (e.g., automatic detection, building from source with specific configurations) are also prominent topics. Overall, unresolved questions include fixing specific kernel regressions, improving support for dynamic shapes and batch operations, and ensuring stability across diverse hardware and software environments."
2023-12-22,pytorch/pytorch,"The discussions highlight multiple issues related to PyTorch's code correctness, compilation behavior, and environment compatibility. Several comments address the handling of padding, overflow, and memory behavior in quantization and sparse tensor operations, proposing changes to ensure consistent and safe execution across CPU and GPU. Environment configuration challenges, especially around ROCm, HIP, and dependencies like ONNX, are also noted, with suggestions to improve detection and integration mechanisms. Some discussions involve merging request processes, test flakiness, and the need for proper test coverage and documentation updates. Overall, key concerns include ensuring correct and portable behavior in quantization and sparse GEMM, improving CI reliability, and clarifying environment and dependency management."
2023-12-23,pytorch/pytorch,"The discussions primarily revolve around stability and compatibility issues with PyTorch, including OS/hardware-related errors such as OSError when writing to mounted drives, and cuDNN execution failures on GPU training. Several comments address challenges with large sparse matrices in CSR format, notably exceeding int32 max size, and potential workarounds like chunking or building PyTorch with MKL ILP64 support. Users also inquire about CUDA and Python version compatibility, especially with newer Python releases like 3.12, and concerns about build failures on Windows due to missing external symbols and MSVC configurations. Some comments highlight ongoing PR reviews, stale issues, and questions about release plans for new CUDA versions and Python support. Overall, the key issues involve improving stability, large tensor handling, and ensuring compatibility with hardware and software updates."
2023-12-24,pytorch/pytorch,"The discussions primarily revolve around persistent issues with NVIDIA kernel errors (e.g., IMA errors, long stalls during CUDA operations), and the need for non-blocking GPU event queries (e.g., using `c10::Stream::query()` or `poll()`) to reduce CPU usage during synchronization. Several comments highlight difficulties with specific GPU architectures, driver/kernel compatibility, or support for certain operations like Conv3D on MPS, indicating ongoing compatibility and stability challenges on Apple Silicon and AMD hardware. Additionally, there is concern about the proper integration and extension of serialization methods (`deserialize_conv`) across different backends, and the importance of proper testing, dependency management, and community communication regarding recent changes and regressions. Unresolved questions include how to effectively handle asynchronous waiting for GPU tasks across diverse hardware, ensuring correct operation of advanced features like `torch.linalg` functions, and managing dependencies to avoid runtime or installation issues."
2023-12-25,pytorch/pytorch,"The discussions highlight ongoing issues related to performance bugs and inefficiencies, notably the slower speed of grouped convolutions on certain GPUs (e.g., GTX 3090, A100) and the need for better documentation of these performance characteristics. There are concerns about specific framework functionalities, such as the handling of cudnn algorithm choices, autocasting behaviors, and kernel implementations like special operators and SVD accuracy, especially on different hardware (e.g., Apple MPS, CUDA). Several issues also address API stability and deprecation, such as the removal of `aten::_record_function_enter`, and the need for clearer control over algorithm selection and backend options. Some discussions involve test stability and flakiness, particularly in CI pipelines and distributed training, with efforts to update or disable flaky tests. Overall, unresolved questions remain about performance optimization, hardware-specific behaviors, and API deprecation strategies."
2023-12-26,pytorch/pytorch,"The comments highlight several recurring issues and discussions in the PyTorch community, including sporadic CUDA and memory-related errors during training, especially with large models and distributed setups, often linked to environment or system configurations (e.g., driver versions, API behaviors, CUdNN workspace limits). There are concerns about the effectiveness of current memory offloading strategies like FSDP with CPU offloading, and questions about the stability and support of features like `torch.vmap`, `torch.inductor`, and mixed-precision training for quantization and large models. Some discussions suggest potential improvements such as adding tests, better diagnostics for connection errors, and clarifications on behaviors like `softmax` tensor types or scatter/gather semantics. A few issues relate to system-specific behaviors (e.g., MPS device, multi-GPU setups) and architectural aspects like dtype propagation, export interoperability, and build tooling, with embedded questions about the future of certain APIs and optimization strategies. Overall, the discussions reflect ongoing troubleshooting, feature requests, and plans for stability and performance enhancements in the framework."
2023-12-27,pytorch/pytorch,"The discussions highlight ongoing efforts to address various technical challenges in PyTorch, including implementing complex statistical functions like the T distribution CDF with backpropagation support, developing efficient graph transformations such as lowering 2D convolutions to symbolic kernels, and improving compiler features like dynamic shape handling, recompile strategies, and profiling. Several issues concern ensuring compatibility and correctness across hardware backends (e.g., MPS, CUDA, ONNX runtime), especially regarding operator support and shape discrepancies in models and export behaviors. There are also questions about expanding the PyTorch ecosystem with features like dataframes and better quantization support, as well as enhancing build and testing infrastructure for stability across different platforms and configurations. Finally, some entries relate to maintenance practices such as update validation, rebase procedures, and CLA signing, emphasizing a focus on robustness and contributor onboarding."
2023-12-28,pytorch/pytorch,"The discussions primarily revolve around PyTorch's ongoing development, including dependencies, feature implementations, and bug fixing. Several issues address build and environment setup challenges, especially on Windows or macOS with specific hardware, and dependency versions like CUDA, cuDNN, and compiler tools. There are concerns about porting support and consistency across hardware backends (e.g., MPS, ROCm, custom kernels), as well as ensuring features like quantization, sparse GEMMs, and dynamic shape handling are correctly implemented and tested. Some threads suggest potential improvements, such as caching checks, enhancing error messaging, and consolidating out variants to improve performance and correctness. Unresolved questions include specific bug reproductions, support for certain features (e.g., float64 steps or specific backends), and coordination for merging PRs or selective test disabling."
2023-12-29,pytorch/pytorch,"The discussions highlight challenges with enhancing `autograd.Function` to support keyword arguments, including API design questions and maintaining simplicity. Several technical issues involve implementing custom operations in C++ and CUDA (e.g., torch.mm inaccuracies on MPS, enabling flexible argument passing, and kernel fusion complexities). There are ongoing efforts to improve cross-device exportability, reproducibility, and handling dynamic shapes, with specific concerns about kernel support, compatibility, and debugging. Additionally, some discussions address reliability and flakiness in CI testing, along with maintaining backward compatibility and build infrastructure. Overall, core themes include API improvements for custom functions, hardware-specific kernel correctness, and reliable, portable model export workflows."
2023-12-30,pytorch/pytorch,"The discussions primarily revolve around enhancing PyTorch's functionality and stability, including the addition of (log) beta functions for numerical stability purposes, support for various unsigned integer tensor types to improve memory efficiency, and handling of operator implementations on different hardware backends like MPS and ROCm. Several comments address the need for proper testing, bug fixes, and code refactoring to ensure correct behavior, especially in edge cases such as tensor contiguity, device-specific operators, and compile-time feature detection. There are also concerns about CI failures, build system brittleness due to reliance on environment version checks (e.g., ROCm), and the implementation of features related to Torch Compile, autograd, and symbolic shape analysis. Unresolved questions include how to properly handle missing operator implementations on certain devices, how to improve build robustness against environment-specific issues, and suggestions for better feature detection APIs rather than version-based heuristics."
2023-12-31,pytorch/pytorch,"The discussions primarily focus on enhancing reproducibility in PyTorch, with suggestions like setting seed values and noting issues with global dtype changes that affect test consistency. Several engineering challenges are raised, including supporting arbitrary tensor arguments in custom autograd functions (both Python and C++), and making TensorIterator const-correct to prevent unnecessary tensor copying. There are ongoing efforts to improve hooks API support, especially backward hooks, and integrate more support for device-specific operators on platforms like MPS and ROCm. Additionally, questions about improving distributed training, such as support for combining FSDP with Adafactor or supporting distributed tensors by default, are discussed. Unresolved issues include handling non-compositional operations like `as_strided`, ensuring deterministic behavior across compile and eager modes, and maintaining backward compatibility amid API changes."
2024-01-01,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for C++ API support for passing arbitrary numbers of tensors to custom `autograd.Function`, and potential enhancements for dtype representations like `uint16`. There are questions about implementing graph transformations at the IR level for decompositions and lowering, and the difficulty of handling dynamic and scalar shapes, including the possibility of introducing functions like `torch.mark_dynamic` for scalars. Issues related to backend compatibility, such as Windows+Triton support, ROCm GPU issues, and CUDA memory management, are also prominent. Additionally, concerns about the stability and correctness of features like `torch.compile`, `fx.symbolic_trace`, and the impact of nondeterministic sets on reproducibility are discussed."
2024-01-02,pytorch/pytorch,"The discussions address challenges with dynamic shape support and static kernel export limitations in PyTorch, notably impacting ONNX exporting and tensor operations. Several comments highlight issues with implementing dynamic kernel size inputs for ops like `avg_pool`, and suggest workarounds such as static size inference or code restructuring, along with concerns over tracing warnings and performance implications. Other points focus on enhancing custom operator support via `torch.library`, improving robustness for mixed device architectures (e.g., GPU, NPU), and addressing build issues like missing header files or outdated third-party libraries. There is interest in refining CUDA/HIP decompilation and heuristics for optimizing kernel behavior, along with considerations for proper testing, reproducibility, and documentation of these features. Unresolved questions include how to reliably detect device-specific bugs, how to manage mixed deterministic/nondeterministic modes, and whether to introduce new APIs or modules for better extensibility and performance."
2024-01-03,pytorch/pytorch,"The discussions highlight various technical issues and feature requests in the PyTorch repository, including tests (such as distributed, CUDA, and MPS backends), compatibility and implementation gaps (e.g., support for complex numbers on MPS, support for specific operators like `aten::_fft_r2c`), and performance/architectural concerns (e.g., deterministic behavior consistency between eager and compiled modes, unifying API designs, and refining guard mechanisms). Several complaints involve flaky tests, missing support in backend kernels, and compatibility issues with hardware (like older CPUs or GPUs). Proposed solutions include fixing specific kernel implementations, improving CI and testing infrastructure, clarifying or updating documentation, and enhancing support for features like dynamic shapes, custom ops, and meta kernels. Overall, unresolved questions concern how to ensure hardware support, testing reproducibility, backend extensibility, and API consistency in the evolving PyTorch ecosystem."
2024-01-04,pytorch/pytorch,"The discussions highlight multiple technical concerns including the compatibility and registration of custom kernels and operators with different backends (notably ROCm, XPU, CUDA), the need for out-of-place variants and out variants to support APIs efficiently, and handling of complex data structures like nested tensors. Several issues involve ensuring that the build and deployment process correctly recognizes and manages dependencies such as cuDNN versions, and that the runtime supports these features without performance regressions or correctness issues. There are also questions on how to properly integrate custom functions or operators into the PyTorch dispatch and guard systems, as well as the need for more extensive testing and validation to prevent flaky CI failures. Overall, the key suggestions involve expanding support for custom kernels, improving backend registration and guard handling, and ensuring correctness across diverse hardware and software configurations."
2024-01-05,pytorch/pytorch,"The discussions encompass several technical concerns, including support and implementation status of backward hooks in PyTorch, particularly within torch.compile, and issues with operator support on MPS devices (e.g., aten::bucketize, aten::grid_sampler_backward). There are questions about the behavior and limitations of certain operators in distributed training, CUDA and ROCm support, and kernel support for different hardware configurations. Some discussions address the need for clearer documentation of feature support (e.g., attention kernels, kernel parameters) and guidance for integrating custom kernels, especially for complex or hardware-specific operations. There are also reports of intermittent CI flakiness, failure handling improvements, and PR review/merge workflows, as well as specific bug reports, like segmentation faults on ARM architectures and precision errors in quantization. Overall, unresolved questions focus on operator support completeness, kernel fallbacks, hardware compatibility, and CI stability, with suggestions for better feature detection, documentation, and robust support strategies."
2024-01-06,pytorch/pytorch,"The discussions cover a range of technical issues, including reproducibility of specific bugs, build system intricacies, and performance regressions. Several questions inquire about how to handle edge cases or improve existing functionalities, such as diagnostics for `scaled_dot_product_attention`, handling of singleton dimensions, and kernel compatibility with different memory formats. There are also requests for clarifications on ongoing PR statuses, merging processes, and code behaviors, especially around the stability and correctness of CUDA/ROCm operations, graph inlining, and ONNX export performance. Overall, the comments reflect active troubleshooting, validation efforts, and feature requests aimed at improving PyTorch's robustness, usability, and internal tooling."
2024-01-07,pytorch/pytorch,"The discussions primarily focus on improving documentation for attention kernels, including support parameters like kernel type, head size, CUDA compute capability, and `attn_mask`, to reduce trial-and-error. There are questions about the support and gradient flow of `attn_mask` in different attention implementations. Several issues involve handling system shared memory for PyTorch on AMD APU hardware, and optimizing memory allocation and model serialization formats, such as avoiding nested ZIP files for model storage. Additionally, there are concerns about stability and correctness of certain PyTorch features (e.g., `torch.compile`, `fullgraph=True`), and efforts to unify and streamline related code modules (e.g., CPU quantization and inductor rules). Unresolved questions include how to support specific tensor transformations (like `view` and `reverse` for int16 tensors), and whether certain features like unsigned bitwise operations will be natively supported."
2024-01-08,pytorch/pytorch,"The discussions primarily revolve around issues related to PyTorch's internal operations, such as CUDA kernel support, operator behavior (e.g., `scaled_dot_product_attention`, `matmul`, `scatter`), and the handling of fake tensors and symbolic shapes within Dynamo. Many comments suggest potential fixes like refining kernel constraints, modifying guard logic, or adjusting layout and shape protocols while considering backward compatibility and performance implications. There are also concerns about performance regressions, flakiness in CI tests, and the need for better documentation and validation of features like static build support, inductor support for external libraries, and multi-device synchronization. Some proposals include re-evaluating the inclusion of certain operator implementations in core, refining the handling of layout/contiguity, and improving testing strategies to catch regressions early. Unresolved questions involve the best practices for extending support across diverse hardware architectures, managing operator invariants for ONNX exports, and ensuring test robustness in complex build environments."
2024-01-09,pytorch/pytorch,"The discussions cover a range of technical concerns including: compatibility issues with hardware (e.g., missing support for certain CPU instruction sets or GPU features, such as AVX512vnni and specific CUDA kernel bugs), the need for more granular control over custom primitive registration (e.g., registering custom kernels with correct dispatch keys or layout invariants), handling of symbolic shapes and dynamic shape guards in graph export, and stability issues related to code execution (including silent failures like nan values, illegal memory accesses, and inconsistent test results in CI). There are suggestions for improving user control over layout fixedness, clarifications on API behaviors (such as for torch.remainder), and discussions about the handling of out-of-place operations and graph breakpoints. Several unresolved questions involve whether to support specific features (e.g., dedicated `need_fixed_layout` tags, explicit dtype representations like int4), and how to ensure reproducibility and correctness across different hardware and software versions. Overall, the main concerns revolve around compatibility, control, robustness, and correctness of the PyTorch codebase and associated tooling."
2024-01-10,pytorch/pytorch,"The discussions revolve around enhancing low-bit and bit-packed tensor support in PyTorch, such as proposing a uint1 (bit) dtype for efficient bool packing, with considerations on how to implement packing/unpacking, indexing semantics, and integration with existing kernels and dispatch systems. There are concerns about ensuring consistent and intuitive API design, including dtype behaviors, and how to handle cases like non-divisible shapes and endianness. Several issues address correctness and stability challenges in complex areas such as multi-head attention precision, Nvidia NCCL backend compatibility, and compiler pass interactions, often with possible workarounds or debugging suggestions. Questions about code exports, guard mechanisms, and device-specific issues indicate ongoing efforts to ensure robustness, correctness, and usability across different hardware and software configurations. Lastly, issues of build environment setup, version mismatches, and CI reliability are also discussed, highlighting the need for careful testing and infrastructure stability."
2024-01-11,pytorch/pytorch,"The comments reveal ongoing challenges with PyTorch's distributed training, especially in setting up and resolving errors in DDP/torch.distributed init, such as `dist.init_process_group` hanging, and issues with process synchronization, desynchronization, or crashes like segmentation faults and NCCL deadlocks. A recurring concern involves ensuring proper device placement and synchronization (e.g., moving models to CPU before quantization, dealing with mixed device buffers), as well as handling compatibility (e.g., CUDA, ROCm, MPS, backend-specific operators). Several discussions highlight issues with specific operators unsupported on certain devices or backends (like MPS), kernel bugs, and dependency conflicts, often leading to flaky tests or internal failures. Strategies such as modifying the build, adjusting environment variables, and refining the internal handling of shape guards or partial operations are proposed, but many errors remain unresolved or require careful debugging and validation. Overall, the main concerns focus on stable, synchronized execution across devices, reliable initialization in distributed contexts, and managing compatibility/unsupported operators to prevent crashes and flaky tests."
2024-01-12,pytorch/pytorch,"The discussions reveal concerns about the correctness and stability of PyTorch's dynamic shape handling, particularly in relation to symbolic shape inference, deprecation of certain APIs (e.g., sparse tensor operations, native_layer_norm), and the impact of various optimizations and compilation strategies (e.g., inductor/decomposition changes, graph caching, auto-retry for CUDA errors, and DAG inlining). Several issues also highlight the need for better error reporting, user guidance, and test reproducibility, especially around complex features like sparse tensors, custom operators, and distributed communication. Unresolved questions include how to extend support for deprecated or missing operations, improve robustness against hardware-specific issues (e.g., ROCm), and incorporate user-level performance and correctness benchmarks. Overall, the discussions emphasize ongoing work to improve correctness, robustness, and usability in PyTorch's dynamic and compilation frameworks, with many details still under active development or review."
2024-01-13,pytorch/pytorch,"The discussions highlight several technical concerns including the optimization of low-level bitpacking operations for quantized tensors, which are expected to be implemented in Python with eventual low-level hardware support; the recent issues with memory leaks in PyTorch's Adam optimizer under weight decay; and compatibility challenges with building and running PyTorch on various platforms, especially regarding compiler support (e.g., MSVC, C++ standards, SSE4.1) and GPU backends (CUDA, ROCm, MPS). There are questions about the performance overhead of `torch.compile` vs eager execution, cache strategies, and the expected behavior of models with dropout in exported states. Many discussions also focus on build system issues, environment setup, and configuration, including handling dependencies like oneDNN, Triton, and compiler flags across different hardware and OS configurations. Unresolved questions involve the support for advanced control flow, custom operators, and the handling of specific hardware features, as well as clarifications on test behaviors and CI processes."
2024-01-14,pytorch/pytorch,"The discussions predominantly revolve around several technical issues: (1) Proper usage patterns of `torch.multiprocessing.spawn`, including function signatures and inter-process CUDA initialization errors, with workarounds like hardcoding process counts. (2) Limited support for certain operations (`svd`, `nms`, `scatter_reduce`, etc.) on MPS devices, and efforts to improve backend support for Apple Silicon, along with related error handling improvements. (3) Compatibility and build issues on Windows, especially related to compiler configurations (MSVC) and environment variables, with suggested fixes like setting `CXX=cl`. (4) Challenges with fake tensors and symbolic shapes in `torch.compile` and Dynamo, particularly regarding `__index__` calls and constant handling, with potential solutions involving avoiding `__index__` on fake tensors. (5) General concerns about merge failures, CLA signing, test failures, and build infrastructure, alongside feature requests for functionality like `eye_like`, `torch.cuda.device_count()` reliability, and arch-specific support. The discussions highlight ongoing efforts to stabilize multiprocessing, front-end/backend support, and build environments across platforms."
2024-01-15,pytorch/pytorch,"The discussions highlight various complex technical issues encountered in PyTorch, including hardware-specific bugs (e.g., kernel register issues on ROCm, AMD CPU limitations), memory leaks in containerized environments, and the need for better support and patches for specialized operations like inductor, Triton, and custom kernels. Several enhancements are suggested, such as improving kernel registration methods, enabling dynamic and distributed tensor support, and providing more flexible API hooks for extension and profiling. There are unresolved questions about backward compatibility, build configuration adjustments for ARM/AMD architectures, and the integration of new features like tensor parallelism, graph tracing, and custom operator support. Overall, the conversations underscore ongoing efforts to enhance robustness, performance, and extensibility of PyTorch across diverse hardware and software configurations."
2024-01-16,pytorch/pytorch,"The discussions cover various technical concerns in PyTorch development, including issues with reproducibility, numerical accuracy, and platform-specific bugs, especially affecting CUDA and ROCm backends. There are questions about the stability and correctness of kernel implementations, with some considering patching or re-implementing operations like `addmm_` or `upsample_nearest2d`, and others debating the impact of specific optimizations on correctness and performance. Several entries highlight the need for better testing, reproducibility, and handling of dynamic shapes or in-place operations, as well as progress on features like quantization, fusing, and support for new data types. The core questions include how to improve reliability, backward compatibility, and performance metrics, especially where the current implementation may cause memory leaks, sampling, or runtime errors. Overall, the focus is on fixing platform-specific bugs, improving robustness, and planning future architectural enhancements."
2024-01-17,pytorch/pytorch,"The discussions revolve around multiple issues primarily related to PyTorch's internal implementation, CI infrastructure, and serialization: concerns include the consistency and correctness of serialization formats (e.g., torch::jit::load vs save), handling of dynamic shapes and symbolic shapes in tracing (with suggestions to explicitly specify size constraints via mark_dynamic), and performance regressions with certain kernels and test flakiness across platforms. Several questions focus on the implementation details of features like inlining modules, collective communication correctness, and the semantics of graph outputs in compilation workflows (e.g., whether fused outputs need to be explicitly returned for backward). There are also discussions about improving developer experience, such as clearer error messages, reducing code duplication, and handling dependencies and environment setup. Some issues highlight platform-specific bugs or limitations, especially on ROCm and MPS devices, or build failures due to internal or external toolchain mismatches, with potential workarounds suggested. Overall, the discussions indicate ongoing efforts to improve robustness, clarity, performance, and platform support in PyTorch's tooling and internal workflows."
2024-01-18,pytorch/pytorch,"The comments reflect several technical issues and discussions in the PyTorch repository: inconsistent behaviors in FP16/FP32 computations causing numerical divergences in models like Whisper, particularly with compile and autocast; challenges in specifying dynamic shapes and constraints in TorchDynamo/FX exports, including handling complex inter-shape relationships; memory management concerns related to tensor cloning and reference lifetimes affecting memory release timing; support and stability issues with features like 'torch.xpu' and collective communication operations in distributed setups; and periodic CI flakiness, especially on ROCm and Linux, sometimes requiring workarounds like preloading custom malloc implementations. Many questions revolve around reproducing issues, handling shape constraints, or improving code robustness, while some are about merging and testing workflows. Certain regressions are attributed to recent PRs, with ongoing efforts to confirm or mitigate the problems, and some discussions involve the need for better API design or support for features like sparse tensors, mixed precision, and multi-stream execution."
2024-01-19,pytorch/pytorch,"The discussions highlight several recurring concerns, including challenges with multi-stream memory management in PyTorch, especially around how tensor cloning and deletion affect GPU memory deallocation and whether explicit stream handling is necessary. There are technical questions about the correctness and safety of manipulating tensor references, particularly regarding `del` and how it affects memory release. Several issues pertain to CI flakiness, test disabling, and the need for more systematic handling of flaky or platform-specific tests. Additional technical considerations include the internal handling of dynamic shapes and symbolic dimensions in TorchDynamo, the need for explicit user control over op mutability semantics, and the complexity of preserving API consistency and backward compatibility while enabling features like constant inputs for custom kernels. Unresolved questions involve how best to integrate external MLIR analysis, manage test invariants and shape guards, and coordinate between internal and user-visible API behaviors."
2024-01-20,pytorch/pytorch,"The discussions highlight challenges with PyTorch's CUDA extension development, particularly regarding support for relocatable device code linking and dynamic parallelism, with users requesting features to automate extra linking steps. Several issues address large matrix eigenvalue computations, memory management, and system-specific configurations, often requiring improved error messaging or manual workarounds. Recurrent concerns involve correct handling of large tensors, in-place modifications, and shape inference during compiles, including specific API-related bugs such as in `split_with_sizes` and `abs` operator support. Systematic integration, such as with Triton, requires refactoring for API changes, and there are ongoing efforts to clean up code structure, improve documentation, and resolve build/test failures across platforms. Additional unresolved questions include performance regressions, proper handling of environment-specific quirks (e.g., WSL permissions, CUDA versions), and ensuring reliability of features like autograd, autograd's interaction with Python finalization, and distributed memory operations."
2024-01-21,pytorch/pytorch,"The discussions encompass several key topics: issues with installing and importing PyTorch-related packages, particularly moduleNotFoundError for 'torch._C' and DLL load failures when using sentence-transformers in a virtual environment; challenges with exporting ONNX models using PyTorch Dynamo, including reconstructing correct graph structures and handling scope in graph breakpoints; performance overhead concerns with torch.compile and caching mechanisms; and specific PR reviews focusing on code correctness, such as Proper management of scope during graph inlining, correctness in the inductor backend, and handling large matrix operations or custom data types. Several discussions highlight the need for clearer documentation, better error messages, and API improvements for user control. Unresolved questions include the cause of certain build failures, the correct way to handle large matrices or custom data types, and ensuring compatibility across different hardware and software configurations."
2024-01-22,pytorch/pytorch,"The discussions cover various technical concerns including handling of large tensors and memory formats (e.g., blocked dtype, col32), reworking of FSDP's sharding and resharding behaviors, and nuances in PyTorch's dispatch, CUDA/ROCm features, and runtime range analysis. Several questions involve implementation details—such as supporting custom dtypes, managing scope in code generation, and ensuring correctness with different modes (AMP, inductor, etc.)—and whether certain features (like fallback mechanisms or specific optimization strategies) should be introduced or improved. There are also ongoing efforts to address internal CI flakiness, test failures, and build issues, along with proposals for clearer documentation and better configuration management (e.g., for nested modules or environment checks). Unresolved questions concern how to best support user-defined data layouts, improve robustness in complex code transformations, and handle specific hardware/software discrepancies. Overall, the discussions reflect a need for more precise control, better extensibility, and clearer communication of behavior in advanced PyTorch internals and APIs."
2024-01-23,pytorch/pytorch,"The comments highlight compatibility issues and experimental features in PyTorch, such as support for quantized/unsupported datatypes like uint8 and bool in certain ops, and the need to better handle unsupported dtypes or deprecated warnings. There are technical discussions on improving the integration and debugging of dynamic shapes, tensor subclasses, and internal IR representations, especially in relation to inductor, FX, and lowering passes, including proper handling of SymInt bounds and graph reconstruction. Several reports concern CI flakiness, test failures, and build issues related to specific backends, ROCm versions, and external dependencies (e.g., Triton, fsspec), with suggestions on how to improve testing coverage, error messages, and configuration management. Notably, there are ongoing discussions about refactoring and improving the internal codebase, including migration strategies for legacy APIs, deprecating old support, and enhancing diagnostics for better stability and correctness in compilation and runtime. Finally, administrative and infrastructural concerns such as repository merging, CLA signing, and CI automation are also recurrent themes."
2024-01-24,pytorch/pytorch,"The discussions encompass various topics, primarily centered on PyTorch's internal development and debugging processes. Key concerns include handling unsupported data types in functions like interpolate (e.g., integers, booleans), optimizing interpolation methods for integer tensors, and clarifying behavior for bitwise representation in tensors. Several issues address performance regressions, flaky tests, and CI test failures, with specific technical suggestions such as adjusting Triton kernel configurations, improving model state dict compatibility, fixing eradication of frame local references, and refining the handling of asynchronous operations. Questions also arise on API stability, features like functools.partial support in torch.export, and detailed internal behaviors—particularly around CUDA, ROCm compatibility, and finalization order—highlighting ongoing efforts to improve robustness, reproducibility, and compatibility across hardware and Python versions. Unresolved points include ensuring consistent behavior of non-contiguous tensors, stability of caching and optimizer states, and testing specific configurations or system environments."
2024-01-25,pytorch/pytorch,"The discussions cover several technical concerns in the PyTorch repository, including handling of multiprocessing errors and device consistency, especially related to NCCL and distributed communication patterns, with suggested fixes and debugging strategies. There are questions about supporting new data types like complex or bfloat16, and the necessity of adding new dtype support versus workarounds, along with discussions on tensor device management, such as the implications of using meta tensors, device inference, and early device assignment. Several issues involve test flakiness and CI stability, with some being resolved after multiple reruns, and others requiring internal infrastructure updates. Overall, key themes include improving robustness and clarity of device and dtype handling, ensuring correctness and reproducibility of large models (e.g., with protobuf protocols), and stabilizing CI tests across platforms and configurations."
2024-01-26,pytorch/pytorch,"The discussions involve multiple technical concerns including improving device management APIs in PyTorch (e.g., adding support for `device` args, explicit APIs for backend device tracking), addressing incomplete MPS device operator support on Apple Silicon, and handling large tensor serialization with proper protocol versioning. Several issues highlight the need for better handling of distributed initialization, NCCL communication (non-blocking mode, version detection), and runtime behaviors such as CUDA stream synchronization, kernel launch timing, and memory management with checkpointing/autograd. There are also ongoing efforts to improve robustness against trunk flakiness, clarify CI test disabling procedures, and enhance documentation and API consistency, especially around exporter/ONNX compatibility and user-facing configuration flexibility. Additionally, some discussions tackle low-level plugin conflicts, internal API stability, and interface design for debugging, profiling, and model serialization—all with unresolved questions around API stability, performance impacts, and correctness assurances."
2024-01-27,pytorch/pytorch,"The discussions highlight various technical concerns including GPU memory management during multiprocessing, with suggestions such as process recreation (`maxtasksperchild=1`) and cautious use of `empty_cache`, noting both potential leaks and context-related issues. Several issues address implementation details and correctness, such as ensuring proper handling of device properties during serialization, the potential need for device guard guards, and the importance of supporting operations on specific hardware backends like MPS, ROCm, and XPU, with some reported missing operator support.

Others focus on code quality and compatibility, such as fixing typos in `register_state_dict_pre_hook` documentation, addressing merge conflicts, and improving type safety or class design (e.g., refactoring `LazyGraphModule` to use composition). Several discussions concern stability and performance regressions, with suggestions to improve profiling, dynamic tuning, and guarding to avoid issues with non-differentiable points or misaligned meta data during graph tracing or serialization. Technical questions about the impact of certain changes (e.g., the necessity of specific dependencies, the behavior of `torch.save`, or the interface of `torch._dynamo.export`) suggest ongoing uncertainties in implementation details, especially across different hardware and runtime configurations."
2024-01-28,pytorch/pytorch,"The discussions highlight concerns about enhancing the efficiency and stability of `pdist()` by adding a `metric=""sqeuclidean""` option, addressing auto-grad issues at zero, and enabling auto-vectorization over arbitrary axes. There are ongoing investigations into PyTorch's support for FP16 tensor cores on GTX 1660 series cards, with suggestions to improve compatibility and performance without relying on `--no-half`. Multi-node distributed training setups face challenges with startup rendezvous, suggesting adjustments to `rdzv` backend configuration and environment variables like `SLURM_PROCID`. Support for MPS on Apple Silicon, especially for operators like `nms` and `linalg_solve_ex`, remains limited, with workarounds such as fallback to CPU being common. Several issues involve merge failures or CI failures related to build configurations, support for dynamic shapes in ONNX export, and code refactoring proposals for unlifting modules, emphasizing the need for API stability and consistency."
2024-01-29,pytorch/pytorch,"The discussions primarily revolve around issues with specific hardware support, such as the limited CUDA/FP16 cores on GTX1660 and the need for compatible PyTorch versions and configurations, including environment flags or nightly builds. Several reports indicate runtime errors, NaNs, or NaN-related issues on M1/M2 Macs, ROCm, and certain GPU architectures, suggesting ongoing compatibility challenges, especially with the MPS backend and large model tensor handling. There are concerns about ensuring deterministic behavior, such as consistent function naming and guard checks during model tracing and re-compilations, and about the proper registration of backend meta implementations and process groups. Some suggestions involve disabling or improving support for certain operators, adding custom hooks or APIs, or refining the build and environment setup. Finally, there's interest in expanding support for ragged and nested tensors, as well as performance tracking for specific models like stable diffusion or large transformers."
2024-01-30,pytorch/pytorch,"The comments reflect multiple issues and discussions concerning PyTorch's development, including challenges with custom autograd support and JIT compilation (especially for quantized modules), CUDA and ROCm compatibility (e.g., memory management, kernel compatibility, and driver issues), and build/deployment process intricacies (such as package dependencies, installer compatibility, and build system updates). Several entries address performance regressions (e.g., changes from `mm` to `bmm`, kernel reimplementations, and autotune blacklists), operational bugs (like memory leaks, mismatched metadata during save/load, and timeout failures), and test management (disabling flaky tests, improving test coverage, and better error diagnostics). Additionally, there are discussions on infrastructure stability (merge failures, CI flakiness, and release notes automation) and API design questions (such as the support for multi-dimensional ragged tensors and device mesh management). Proposals include conditionally enabling features based on hardware, reworking certain guards or internal mechanisms, and infrastructure adjustments to improve portability and robustness. Many issues remain unresolved or require deeper investigation, especially around hardware-specific bugs, build environment consistency, and support for advanced features like mixed-precision and multi-ragged tensor operations."
2024-01-31,pytorch/pytorch,"The comments reveal recurring issues with PyTorch's support for large tensors (>4GB), especially related to serialization with pickle protocols, and with the correctness of tensor operations like `torch.bmm` and `torch.nn.functional` in specific hardware contexts (e.g., Metal on macOS, older GPU architectures). Several discussions consider whether to implement custom operators and meta support for certain functions, or to enforce graph breaks to handle edge cases, especially for operations involving views, in-place modifications, or dynamic shape scenarios. There are ongoing efforts to improve the robustness of guard logic, handle tensor views and side effects consistently, and support new features like `torch.compile` with dynamic shapes and distributed setups. Many unresolved questions concern the compatibility and performance implications of new kernel implementations, the proper handling of large tensors and hardware-specific optimizations, and how to gracefully handle flaky tests and CI failures that sometimes result from platform or environment-specific conditions."
2024-02-01,pytorch/pytorch,"The discussions highlight multiple technical concerns including compatibility and build issues with protobuf and C++ extensions, platform-specific differences in integer handling, and the need for better test coverage and stability in distributed and dynamic shape scenarios. Several comments mention regressions caused by recent PRs, such as changes in guard logic, tensor specialization, and optimizer handling, with suggestions to revert or refine these changes to prevent performance and correctness regressions. There are ongoing questions about support for advanced features like async CPU ops, model parallelism, and ONNX opset versions, with some proposals for API enhancements (e.g., submesh functions, guard handling, and dynamic shape limits). Unresolved issues include flaky test failures in CI, the proper handling of FakeTensor metadata, and the integration of new features across multiple platforms, particularly Windows and macOS. Overall, most discussions seek to improve stability, compatibility, and performance, with specific attention to fixing regressions and ensuring correctness in complex scenarios."
2024-02-02,pytorch/pytorch,"The discussions highlight significant issues with the current implementation of various PyTorch features, including the support and reliability of extension building (notably CUDA extensions on different hardware/OS), especially with recent PyTorch versions like 2.2.0. There are concerns about the correctness of certain internal APIs, such as `to_local` for DTensors, which involve complex handling of device-specific and attribute-specific logic. Flakiness and intermittent failures in CI tests are attributed to environment-dependent factors, including platform-specific behaviors, driver versions, and unsupported operations on certain devices (e.g., MPS). Some discussions also propose improvements like more flexible submesh APIs for DeviceMesh, better debugging tools (`get_python_cpp_trace`), and handling of special tensor types. Overall, key unresolved questions involve ensuring compatibility across diverse hardware/software environments, cleaning up internal APIs with rigorous correctness, and improving testing and debugging infrastructure."
2024-02-03,pytorch/pytorch,"The discussions highlight inconsistencies in handling 'same' padding behavior across frameworks like PyTorch, SciPy, and TensorFlow, especially with non-unit strides and even kernel sizes, with various code snippets and implementations proposed for consistent padding functions. There's concern about whether to support the recent array API cross product signature standard, with suggestions to place such functions in dedicated linear algebra namespaces, and cautiousness about changing core functions like `torch.cross` based on standardization debates. Several technical issues relate to device-specific operator support, such as unimplemented operators on MPS and CUDA, and compatibility with different hardware, driver versions, and environments, often with suggested workarounds like fallback modes or environment variables. Other concerns involve improving type safety and API clarity, for example in dataset batch annotations and custom device tensor type registration, to better leverage static typing and plugin extensibility. Overall, the conversations focus on balancing framework consistency, compatibility, extensibility, and performance across different hardware, with ongoing work to refine padding behaviors, API standards, and device support issues."
2024-02-04,pytorch/pytorch,"The discussions mainly revolve around configuring BLAS/LAPACK libraries for PyTorch installations, particularly highlighting the use of environment variables like `OpenBLAS_HOME` and handling non-standard library locations. Several issues concern build and compatibility challenges on different environments, such as M-series Macs, ARM architectures, and platform-specific compiler setups (e.g., C++ standards, MSVC, clang). There are ongoing efforts to improve CUDA and GPU support on CPUs, especially regarding BF16/FP16 performance, and to ensure proper registration and support for custom device tensor types. Additionally, some threads address regression detection, performance benchmarking, and debugging compile errors, as well as discussions on expanding API support for custom devices and tensor types. Unresolved questions include the timeline for new features, system-specific build fixes, and ensuring consistency between eager execution and `torch.compile()` behavior."
2024-02-05,pytorch/pytorch,"The comments reveal a broad concern about production-level data loading, distributed training, and scaling in PyTorch, with calls for prebuilt, scalable data loaders. Several issues involve improving or fixing existing features, such as synchronization in distributed training, handling of dynamic shapes in ONNX export, and better performance benchmarking. There are discussions around ensuring compatibility and correctness in various environments, including M1, ROCm, Windows, and different hardware accelerators. Suggestions for API improvements include better control over tensor type registration for custom devices and more flexible hooks, along with fixes for bugs like NaNs in certain inductor/FP16 scenarios. Unresolved questions focus on testing strategies, performance profiling, and ensuring robust, user-friendly error handling and documentation updates."
2024-02-06,pytorch/pytorch,"The discussions highlight several technical concerns: First, the need for a way to store and infer input shapes in models, especially for deployment systems like libtorch, with suggestions to encode shape info as module attributes or parameters; second, support for operators like `pad` beyond 2D (e.g., 3D or non-2D padding), with current solutions involving workaround implementations; third, handling of complex functionalities such as `fft`, `stft`, and dynamic shape inference in export workflows, including issues with complex number handling and onnx operator support; fourth, challenges with FSDP and compile interaction, specifically regarding proper gradient and storage management, and the importance of supporting various device-specific APIs and behaviors; lastly, the overarching need for API stability, support for deprecated or legacy functions, and managing build and platform-specific errors, with ongoing efforts to address CI flaky tests and build configuration issues."
2024-02-07,pytorch/pytorch,"The comments highlight several recurring concerns in the PyTorch repository, including documentation clarity (e.g., not clearly specifying `optimizer.step()` and `.step()` signatures), CUDA memory management issues, and the support for features like DistributedDataParallel (especially C++ API support), dynamic shapes, and in-place operations during compilation. Several discussions focus on handling complex features such as tokenizing/compression of quantized models (Q/DQ IR), sophisticated guard mechanisms for symbolic tracing, and explicit API support for custom backends and data migration (e.g., HuggingFace model serialization). There's also concern about experimental status (e.g., flaky tests, unimplemented ops for specific backends or hardware platforms like MPS/ROCm) and the need for better error messaging, user guidance, and incremental support (e.g., support for sub-byte quantization, better error handling for mixed in-place and out-of-place ops). Overall, the proposals range from improving robustness and documentation to extending framework capabilities like cross-backend support, better handling of dynamic shapes, and more transparent, user-friendly error reporting."
2024-02-08,pytorch/pytorch,"The discussions raise several technical issues: the need for clearer documentation and error messages for `torch.compile` and `torch._logging`; handling of device-specific dispatch, especially regarding ROCm and CUDA compatibility; the proper way to support custom tensor subclasses and override attributes or methods (e.g., via `__torch_function__` and associated logic); concerns on handling tensor metadata such as storage size, padding, and strides during save/load; and the challenge of splitting large code reviews for better review process. Some discussions also focus on performance considerations for kernel dispatch, enabling more flexible device support, and ensuring consistency in exported models. Unresolved questions include how to handle certain tensor or model invariants, or how to improve user experience with API restrictions or error feedback. Overall, these conversations highlight ongoing efforts to improve documentation, robustness, and flexibility of PyTorch's execution and serialization mechanisms."
2024-02-09,pytorch/pytorch,"The comments largely revolve around handling CUDA out-of-memory (OOM) issues, performance optimizations, and correctness verification, often suggesting specific environment variable tweaks, memory management strategies, or code refactorings. Several discussions address challenges with specific operator implementations on different backends (MPS, ROCm, etc.), including request for better logging and debugging capabilities, and potential code splitting for reviews. There are also mentions of integration issues with pyre, fake tensor serialization, and the need for better user-facing diagnostics for compiler fallback failures and graph break reasons. Additional concerns involve API deprecation, managing runtime device consistency, and the effects of code restructuring on stability and performance benchmarking, with some discussions advocating for more granular or flexible testing and logging improvements."
2024-02-10,pytorch/pytorch,"The discussions highlight concerns about environment-specific issues affecting PyTorch's functionality, especially related to CUDA, MPS, and platform compatibility. Several problems involve incomplete operator support on certain devices or backends, which can be mitigated via fallbacks or environment variables but impact performance and reliability. There are ongoing questions about build configurations, including the inclusion of deprecated functions, linking options, and handling of internal structures like the FX meta dict, which affects model tracing and export. Performance benchmarking discrepancies and correctness of operations like `polygamma` at boundary values are also discussed. Unresolved issues include ensuring compatibility across diverse hardware environments, fixing operator support gaps, and managing build process complexities for optimal performance and usability."
2024-02-11,pytorch/pytorch,"The discussions highlight persistent bugs and feature limitations in PyTorch, such as incomplete fixes for issues like #40972 and #119648, and unsupported operations on certain devices like MPS (#77764). There are concerns about the correctness of gradient calculations when using `vjp` with `vmap`, particularly involving BatchedTensor objects, and about the need for improved handling of tensor properties like `.item()` within `vmap`. Several discussions involve ongoing development efforts, including improving distributed training tools (e.g., replacing `DDPOptimizer`), enhancing support for specific backends (e.g., ROCm, AMD GPUs), and addressing performance regressions. Many issues are marked as stale or waiting on internal upgrades and CI considerations, raising questions about testing strategies, resource availability, and ensuring compatibility across different hardware and software configurations. Overall, contributors seek clarification on unsupported features, bug regressions, and the integration of new features, emphasizing the importance of thorough testing and clear documentation."
2024-02-12,pytorch/pytorch,"The discussions highlight ongoing technical concerns related to PyTorch's stability, correctness, and integration. Multiple comments address discrepancies in output or performance features, such as inconsistent behavior in `_C/_nn` stubs, or issues with `torch.load` in fake tensor mode, often tied to specific configurations or hardware (e.g., ROCm, CUDA versions). Several threads discuss ongoing PR reviews, including the impact of recent code changes on features like `torch.compile`, or potential regressions in inductor or triton kernel support, with questions about test coverage, CUDA device support, and backward compatibility. There are also references to CI failures, flaky tests, and the need for better test coverage or environment setup, especially when handling dynamic shapes, in-place operations, or multi-GPU scenarios. Overall, many comments seek clarification, validation, or guidance on the stability and correctness of features and integrations, as well as coordination for maintenance and testing workflows."
2024-02-13,pytorch/pytorch,"The comments highlight ongoing challenges with PyTorch's JIT and backend compilation behaviors, particularly related to activation checkpointing, graph optimizations, and operator support, especially for CUDA, MPS, and dynamic shape features. Several discussions suggest the need for better pattern matching, support for newer operators, handling of storage resize operations, and compatibility across different backends or device types. There are concerns about correctness, performance regressions, and stability when enabling features like inductor, torch.compile, and their interaction with existing functionalities such as DDP or checkpointing. Some threads also involve infrastructure issues, build inconsistencies, or clarifications about API behaviors and future plans. Overall, the primary issues involve enhancing robustness, compatibility, and performance of the compilation and runtime infrastructure, while unresolved questions revolve around testing strategies, support scope, and how to safely extend current features."
2024-02-14,pytorch/pytorch,"The discussions highlight persistent issues with softmax masking using -inf, proposing solutions like replacing -inf with large negatives (e.g., -1e9) and adjusting softmax behavior accordingly, as well as modifying the attention functions to handle masked rows as zeros explicitly. There's concern over softmax NaNs when entire rows are masked, and approaches such as treating fully masked rows as zero and ensuring softmax does not produce NaNs are discussed. Additionally, some suggest implementing in attention modules checks or special cases for all-masked rows and making softmax masking more robust. Unresolved questions include how softmax should behave on all-masked inputs, handling cases where entire rows are masked, and ensuring changes are made in core implementations rather than hacks."
2024-02-15,pytorch/pytorch,"The comments reveal ongoing discussions about PyTorch features and bugs, particularly regarding the implementation of circular padding in `grid_sample`, the behavior of softmax with `-inf` inputs (highlighting issues with NaN handling and the need for proper masking), and the handling of complex tensor operations on MPS, especially with `view_as_complex`. There are concerns about the correctness and completeness of custom CUDA kernel pattern matching, the stability and flakiness of CI tests across platforms, and the need to improve error messages for unsupported operators like `quantize_per_tensor`, especially when working with meta tensors and dynamic shapes. Some discussions also involve the integration of NCCL, FSDP, and DTensor, with suggestions to fix resource management and synchronization issues, and better API support clearances. Overall, the main concerns center on improving consistency, correctness, and debugging support for advanced features and platform-specific limitations."
2024-02-16,pytorch/pytorch,"The discussions reveal recurring issues with PyTorch's compatibility and implementation regarding SSL verification workarounds, CUDA/GCC version dependencies, and specific operator support (notably on MPS and ROCm devices). Several threads indicate problems with distributed operations involving DTensors and nested tensors, particularly around correct module wrapping, state dict handling, and collective communication support. There are concerns about test flakiness, CI stability, and the impact of recent code changes on build integrity and performance, including the behavior of torch.compile, fuser patterns, and graph break points under inductor. Some discussions focus on surface-level API design questions, like handling optional arguments in function signatures and the need for better documentation or explicit error messaging for certain API usage scenarios. Overall, unresolved questions center on ensuring proper support for new features across diverse hardware backends, maintaining backward compatibility, and refining the expressiveness of PyTorch’s compilation and transformation tools."
2024-02-17,pytorch/pytorch,"The discussions primarily revolve around improving and clarifying the handling of `torch.Generator`, including supporting CUDA equivalents, using generators as context managers, and enabling pickling of Generator objects. Several issues address the support for `List[Tensor]` outputs in object detection, suggesting restructuring inputs for batching purposes. There are concerns about operator support and device-specific implementation gaps, such as missing NCCL collectives, `avg_pool` shape discrepancies in ONNX export, and operators unsupported on MPS devices. Other technical topics include refining distribution and synchronization correctness, fixing timeout errors in NCCL collectives, and updating device macros for consistency. Overall, the debates focus on enhancing robustness, usability, and completeness of multi-device support, deployment flexibility, and runtime behaviors."
2024-02-18,pytorch/pytorch,"The discussions highlight several technical concerns including compatibility issues with numpy versions during YOLO v7 training, errors in PyTorch's inductor backend related to complex tensors and control flow, and performance regressions caused by recent commits affecting tensor cloning and graph optimizations. Additionally, there are questions about the stability and support of certain functionalities like `torch.cond`, `torch._amp_foreach_non_finite_check_and_unscale_`, and the handling of GPU memory during CUDA graph capture, with proposed solutions involving CUDA user objects and refactorings to ensure proper lifetime and ownership of buffers. Other topics include debugging lengthy compilation times, handling regression after specific code changes, and issues with distributed state dictionaries and device indexing validation. Overall, unresolved questions pertain to improving test scalability, ensuring compatibility across hardware and software versions, and refining internal mechanisms like graph capture and error messaging for better robustness."
2024-02-19,pytorch/pytorch,"The discussions highlight various ongoing issues and feature requests in the PyTorch repository, including the need for better support for random number generators (supporting the `generator=` argument and pickling), handling of distributed training across different environments (e.g., GPU issues in HPC and cloud environments, P2P communication, and robustness of state_dict saving/loading), and bug reports related to specific models or operations (e.g., numerical discrepancies with CUDA graphs, deprecation of APIs, and compatibility issues with different hardware/software configurations). Several comments address the challenges of integrating new features such as SVE support on ARM, device-specific kernel implementations, and ecosystem compatibility, often emphasizing the importance of thorough validation, backward compatibility, and proper CI testing. Some discussions also involve API design concerns, such as whether to internalize certain features or keep them out-of-tree, and the need for clear documentation or migration guides for upcoming changes. Overall, unresolved questions include how to unify support for complex distributed workflows, optimize runtime performance and stability, and systematically manage deprecations and internal API transitions."
2024-02-20,pytorch/pytorch,"The discussions highlight several core concerns regarding recent experimental and implementation issues in PyTorch. Key points include the challenges of implementing query masking in `nn.MultiheadAttention`, particularly for cross-attention scenarios, and the difficulty in accurately capturing and guarding against data-dependent size expressions in symbolic shape analysis, which affects both correctness and performance. There's also ongoing debate about the proper handling of `DTensor`s and their interactions with autograd, especially in multi-GPU and FSDP contexts, with suggestions to centralize or restrict mutable data pointer access to avoid concurrency hazards. Additionally, issues with backward passes, kernel fusion, and support for new hardware or APIs (like CUDA 12.4, NVIDIA P2P, and SVE on ARM) reveal the need for more robust error reporting, better tooling, and targeted code restructuring. Overall, unresolved questions remain about handling complex shape dependencies, ensuring CI stability amidst regressive or flaky tests, and integrating new hardware/software features smoothly into the PyTorch stack."
2024-02-21,pytorch/pytorch,"The comments reflect ongoing discussions about enhancing PyTorch's checkpointing and state-saving capabilities, particularly around snapshotting and resuming training with complex data pipelines, such as datasets wrapped in sharded or distributed tensors (e.g., DTensor). There is interest in developing a `state_dict()` implementation for `DataLoader` iterators and dataloaders, as well as the ability to snapshot and load the internal state of data preprocessing pipelines, especially for complex data types like DTensors, which involves capturing sharding information and copying tensor data during load. Compatibility issues and bug fixes related to system-specific libc differences, such as assertions, and platform-specific issues, notably on musl-based systems or with different CUDA/NCCL versions, are also discussed. Some proposals include reworking the `tree_iter` function for efficient traversal, handling the shape change errors in FSDP with CPU offloading, and ensuring correct tensor serialization across different Python and hardware setups. Overall, the discussions focus on making PyTorch’s state management more robust, flexible, and system-compatible, while also addressing performance regressions and platform-specific bugs."
2024-02-22,pytorch/pytorch,"The discussion covers several technical issues and feature updates, including efforts to align `torch.cross` with numpy, support broadcasting, and improve deprecation handling. Multiple reports highlight model compatibility, performance regressions, and support gaps in various configurations, such as for MPS on Mac, ROCm, and CUDA versions, with suggested workarounds like environment variables or driver downgrades. Several discussions focus on improving API design and exposing finer-grained control over distributed and tensor-level policies, particularly for FSDP, offloading, and sharding modes. Other issues involve debugging performance regressions, threading behavior, and CI flakiness, with ongoing work to enhance diagnostics, support new hardware, and streamline build processes. Unresolved questions remain around the best API abstractions for tensor offloading policies, handling of dynamic shapes in compilation and tracing, and ensuring consistent and efficient behavior across diverse hardware and software environments."
2024-02-23,pytorch/pytorch,"The comments primarily discuss the need for improved support and correctness in various PyTorch features, including efficient handling of in-place vs. out-of-place operations (e.g., torch._assert_async), support for dynamic shapes and multi-device use cases (e.g., FSDP, NCCL), and the stability of modules with respect to distributed synchronization and collectives. Several proposals involve adding or refining APIs (e.g., support for `torch.compile()` to handle kernel fusion, improved module swapping, and better range/shape inference) and ensuring correctness and performance for emerging hardware and features (e.g., MPS, Habana, Triton kernels). The potential for regressions or flaky behaviors in trunk or with specific hardware configurations prompts numerous monitoring and rebase activities. Additionally, resolving build/test failures and addressing changes in external libraries or internal infrastructure—such as missing support for certain data types, proper cache invalidation, and support for newer NCCL versions—is frequently cited. Overall, the focus remains on improving stability, performance, and feature support across diverse hardware and use cases."
2024-02-24,pytorch/pytorch,"The discussions primarily focus on optimizing dataloader performance on Windows, particularly concerning multi-worker data loading bottlenecks and GPU utilization, with a user-provided custom solution involving preloaded datasets. Additionally, issues regarding device-specific errors (e.g., MPS and CUDA support), non-deterministic behaviors due to mixed precisions, and the need for proper tensor device allocation are recurring themes. Several conversations address merging and backporting bug fixes or improvements, highlighting challenges in ensuring reproducibility, correctness, and compatibility across hardware backends. Finally, some discussions involve technical improvements or fixes in the PyTorch codebase, such as enhancing guards, handling compiler-related discrepancies, and ensuring stable, deterministic training results."
2024-02-25,pytorch/pytorch,"The discussions primarily focus on various technical issues and feature requests related to PyTorch, such as environment configuration for distributed debugging, compatibility issues with MPS devices, and support for newer CUDA versions. Several PRs are in progress or stalled due to CI failures, requiring further review, rebase, or missing labels like release notes. There are concerns about implementation specifics, such as hash function behavior on Windows, or re-dispatching logic for `as_strided` in functionalization. Some threads highlight discrepancies in performance profiling or potential bugs in dependencies like Poetry or libstdc++. Overall, the comments reflect ongoing debugging, feature development, and infrastructure challenges, with unresolved questions about performance impacts and compatibility."
2024-02-26,pytorch/pytorch,"The discussions highlight various ongoing issues and feature requests in PyTorch, such as the need for batched pairwise distance functions (e.g., `parwise_dist`) and support for new distance metrics (e.g., squared L2). Several issues address bugs or performance regressions related to distributed training (FSDP, DDP), memory leaks, and inaccuracies caused by specific upgrades like oneDNN and ROCm. There are debates about implementation details, such as handling of property setting in `nn.Module`, mechanisms for offloading tensors during training, and the integration of new CUDA or hardware features. Additionally, many discussions involve debugging failures, flaky tests, or CI inconsistencies, often involving internal or flaky hardware/CI environments, with occasional guidance toward upstream PRs or workarounds. Overall, the main concerns involve bug fixes, feature enhancements, stability improvements, and infrastructure adjustments."
2024-02-27,pytorch/pytorch,"The discussion encompasses various issues related to PyTorch, including default initialization schemes and the potential need for default scheme updates, layer initialization support, and the default weight initialization change in PyTorch 2.0. Several issues concern framework internals such as improving the default setup for weight initialization, support for custom initializations, backward compatibility, and managing default behaviors across versions. Hardware-related issues like CUDA, ROCm compatibility, and hardware-specific bugs are also discussed, with suggestions for debugging, hardware support, and vendor-specific fixes. Some discussions cover CI flakiness, stability of reproducibility, and export/import of models across heterogeneous hardware environments, with proposed workarounds or future improvements. Overall, unresolved questions include framework default schemes, support for user-defined initializations, hardware-specific bugs, and reliable reproducibility verification."
2024-02-28,pytorch/pytorch,"The discussions revolve around optimizing PyTorch build sizes and dependencies, particularly regarding CUDA and CUDA-dependent libraries such as NCCL and cuDNN, with suggestions for reducing image size and linking dependencies more efficiently. Several questions pertain to debugging and reproducing errors in diverse environments, including CI failures, distributed training inconsistencies, and hardware-specific issues like MPS and ROCm support. There are also technical concerns about PyTorch's internal APIs and features, such as the handling of tensor state_dicts for model sharing and compatibility, as well as issues with the TorchScript, dynamo, and compiler passes that impact correctness and performance. Some discussions address the need for better test coverage, error handling, and documentation, especially for new or experimental features (e.g., AOT compilation, device support, complex number support). Unresolved questions remain about specific bug fixes’ effects, cross-platform/architecture support, and smooth integration of third-party hardware or software extensions."
2024-02-29,pytorch/pytorch,"The collected comments encompass a wide range of technical issues, questions, and discussions related to PyTorch's development, including CUDA and ROCm GPU backend stability, specific bug fixes, performance regressions, and features like deterministic reduction and in-place operations. Several comments highlight difficulties reproducing errors locally versus CI, inquiries about support for complex or dynamic shapes, and suggestions to improve debugging with extended logs or documentation. There are recurring concerns about flaky tests, build times, and the need for better testing and benchmarking of new features such as CUTLASS kernels or advanced memory management strategies like FSDP with meta tensors. Overall, key unresolved themes involve ensuring stability and correctness across diverse hardware (CPUs, GPUs, different vendors), improving developer experience with better debugging tools, and addressing CI flakiness and performance regressions."
2024-03-01,pytorch/pytorch,"The comments encompass a wide range of technical concerns, including issues with documentation links, the need for API improvements (e.g., returning JIT-compiled models, exposing internal store access), and performance regressions caused by recent code or library upgrades (such as oneDNN updates, cuDNN version mismatches, or specific kernel behaviors). Several discussions highlight the importance of proper testing, profiling, and correctness validation, especially after modifications like custom operators, kernel rewrites, or shape inference adjustments. There are questions about enabling or disabling certain features (like guard modes, JIT returns, or fusion passes), and suggestions for API safe-guards and backwards compatibility measures. Unresolved issues often involve complex interactions between autograd, compilation, runtime behavior, and hardware-specific constraints, requiring careful investigation before further merging or deployment."
2024-03-02,pytorch/pytorch,"The discussions encompass a range of technical concerns in the PyTorch ecosystem. Several threads address driver and environment issues, such as restoring GPU driver states after suspend, CUDA driver bugs, OpenMP support on macOS, and mismatched cuDNN versions, suggesting workarounds or system configuration fixes. Others focus on API and feature behaviors, like the handling of complex spectral norm, gradient computation with out parameters, and support for sub-byte quantization types. There are questions about correctness and API consistency, including the semantics of gradient penalties, the design of schedulers, and the expected relationships between different gradient functions. Additionally, there are enhancements proposed such as making certain behaviors opt-in, improving documentation, and expanding testing coverage for various backends and platforms."
2024-03-03,pytorch/pytorch,"The discussions highlight ongoing challenges related to memory management and device support in PyTorch, such as memory leaks when loading datasets as attributes, and support for complex and FFT-based spectral norms. There are considerations about extending functionality, like adding `nn.Buffer`, improving skeleton scheduler interfaces to handle arguments more flexibly, and refining device-specific operator implementations (e.g., for MPS and OpenMP support). Several issues involve handling platform-specific problems, such as MacOS compatibility with OpenMP and compiler configurations for AVX/AVX512 support, as well as ensuring internal CI workflows and testing are correctly configured. Unresolved questions include how to best dispatch mixed data types (float, half, bfloat), how to improve backward compatibility and BC-breaking changes, and how to handle device-specific kernels and operator implementations, especially on non-Linux systems."
2024-03-04,pytorch/pytorch,"The discussions primarily focus on clarifying internal PyTorch naming conventions (e.g., ""c10"" and ""ATen""), fixing specific implementation issues, and enhancing functionalities such as support for complex numbers, sparse tensor formats, and new device backends (e.g., ROCm, MPS). Several threads involve bug investigations, including compatibility bugs (e.g., for `__cuda_array_interface__`, operators on Apple M1, and specific hardware like MI250x), as well as performance and correctness regressions after recent code changes. There is interest in extending APIs for device offloading, mixed precision, and custom device support, often with suggestions for better testing, documentation, and modular API design. Many issues involve bug fixes, code reverts for internal build stability, and discussions on feature scheduling, approval procedures, and code review processes. Unresolved questions include the design for tensor-level policies, handling legacy operator behaviors, and coordination between internal and external release cycles."
2024-03-05,pytorch/pytorch,"The discussions across these GitHub comments highlight issues related to PyTorch's internal functionality, such as implementation of DataParallel, operator support, and shape inference. Several comments express concerns about inconsistent behaviors across hardware platforms or software versions, like CUDA, ROCm, and MKL, as well as the need for better error messaging, module registration, and support for custom operators or data types (e.g., UINT4, BFLOAT16). There are recurring suggestions for upstreaming changes, improving test coverage, and reworking certain mechanisms (like grad hooks and graph operators) to enhance stability, performance, or usability. Some issues relate to build configurations, dependency management, or compatibility, with questions about regression fixes, performance benchmarking, and handling dynamic shapes or custom device integrations. Overall, the core concerns revolve around robustness, correctness, and efficiency of PyTorch's evolving features and their porting to various hardware/software environments."
2024-03-06,pytorch/pytorch,"The discussions highlight several technical issues, including the need for proper handling of `__len__` in subclasses (Issue #120139), and the importance of ensuring correct `device` and `dtype` propagation during module modifications and quantization (Issues #121071, #121092, #121094). Several patches involve re-implementing or re-decomposing operators (e.g., `nll_loss2d_backward`, `convolution`) for different platforms like ARM and ROCm, sometimes requiring reversion or extensive rework (Issues #121150, #121187, #121188). Additional concerns include handling of CUDA/HIP flags, compiler backends, and performance regressions, with proposals for improving build configurations, testing coverage, and diagnostics (Issues #120794, #120984, #121150). Compatibility issues with library dependencies (protobuf, NCCL, oneDNN) and platform-specific constraints (Mac, ARM, ROCm) are also recurring, requiring careful environment management and potential reversion of incompatible patches. Lastly, several remarks emphasize better testing, documentation, and stability improvements in the evolving codebase."
2024-03-07,pytorch/pytorch,"The discussions highlight several technical concerns, including compatibility issues with specific hardware/software configurations (e.g., Triton inference server with Triton 24.02, CUDA, cuDNN, MPS with mixed types), and the need for improved error handling, especially in the context of dynamic shape support and nested tensor operations. There are questions about the applicability of certain operator support (e.g., `aten::take`, support for `aten::scatter_reduce.two_out`, and `aten::nanmedian.dim`) and suggestions for better global state management, API consistency, and the handling of mixed dtype scenarios. Additionally, issues related to system-specific builds, environment setup, and stability concerns (e.g., trunk flakiness, CUDA graph capturing, and synchronization) are noted. Lastly, there are discussions on the integration of features like `torch.compile`, CUDA graph support, and the handling of custom modules within frameworks like Pyro, with recommendations to improve robustness and user experience."
2024-03-08,pytorch/pytorch,"The discussions encompass various issues related to PyTorch's internal and external functionalities. Several concerns involve handling edge cases, such as empty sequences, zero-length tensors, and how to manage tensor shapes and device states efficiently without significant overhead. There are questions about support and performance of specific operators on different hardware backends (e.g., MPS, ROCm, CUDA), and the need for more extensive testing and validation of new features, especially around fusion, shape pragmas, and custom kernel support. Some proposals suggest using tensor subclasses, changing the order of fusion passes, or avoiding certain pattern matches at fusion time to optimize performance and correctness. Unresolved questions focus on the best approaches for device consistency, kernel registration, handling dynamic shapes, and compatibility with third-party libraries like Pyro and torchvision."
2024-03-09,pytorch/pytorch,"The comments highlight ongoing challenges with CUDA support, particularly regarding support for newer versions (e.g., CUDA 12.4) and hardware-specific issues such as NEON in the inductor backend, which may require reverting certain PRs or adding new hardware flags. Several reports detail compatibility and stability issues on various platforms, including ARM, macOS, and Apple M1, often tied to specific hardware capabilities or software configurations. There are also discussions about improving code generation and fusion strategies in the compiler, especially related to outer loop fusion and dynamic shape handling, with suggestions to shift certain logic to earlier fusion stages. Additionally, multiple discussions focus on CI process concerns, such as test flake re-enablement, merge failures due to checks, and clarifying support status for features like MHA and scaled dot-product attention. Unresolved questions include how to better handle dynamic shapes, reduce build friction on different architectures, and implement platform-agnostic support for operators and codegen."
2024-03-10,pytorch/pytorch,"The discussions highlight several technical concerns, including the lack of support for complex numbers in automatic mixed precision (AMP) in PyTorch, with a specific reproducible error when using complex64 tensors. There are requests for expanding operator support (e.g., upsample_bicubic2d.out, aten::_linalg_solve_ex) and support for dynamic shapes in Triton fusion, especially regarding shape matching and fallback mechanisms. Compatibility issues with CUDA versions and GPU support, especially CUDA 12.4 and newer hardware like the RTX 4080, are noted, alongside challenges with building and installing PyTorch on different architectures like AWS Graviton. Additionally, some issues involve internal PyTorch functionalities, such as handling sparse tensors, random number generator behaviors across versions, and autograd tracing with SavedVariables, as well as external integration challenges with executorch and onnx export."
2024-03-11,pytorch/pytorch,"The discussions highlight several technical concerns, including the challenge of maintaining synchronization when adding a new extended generator to CUTLASS versus upstream integration, suggesting upstreaming changes may be preferable. There's debate over the semantics of `event_dim` and how `StackTransform` should handle batch versus dependent event dimensions, indicating ambiguity in its intended use and behavior. Additionally, issues with inconsistent log behaviors—such as arithmetic on complex numbers, zero-length tensor handling, and numerical discrepancies between float32 and float64—raise questions about expected behavior and correctness. Some conversations also address the complexity of integrating new features like PagedAttention and support for hardware-specific optimizations across different modules or repositories. Finally, numerous discussions involve CI reliability, approval workflows, and platform-specific build issues, emphasizing ongoing challenges in maintaining robustness and consistency of the development pipeline."
2024-03-12,pytorch/pytorch,"The discussions primarily focus on technical challenges and proposed solutions related to PyTorch's inductor and compiler features. Key concerns include handling complex guard conditions, especially with dynamic shapes and in-place tensor modifications, where some patterns (e.g., reshape, view) cause graph breaks or incorrect behavior in tracing and compilation. Several issues address performance regressions and correctness bugs, with suggestions such as compiling certain operations, improving guard placement, and supporting more robust pattern matching with external backends like Triton and oneDNN. There are ongoing efforts to enhance the compatibility and efficiency of custom ops, guard generation, and explicit kernel configurations across hardware (e.g., ROCm, AMD, Intel). Unresolved questions include how to robustly support dynamic shapes, in-place operations, and cross-compilation scenarios, and whether certain features (like runtime assertions) should be configurable or automatically optimized."
2024-03-13,pytorch/pytorch,"The discussions primarily revolve around debugging and improving PyTorch internals, including handling of symbolic shapes, layout and dtype casting, and kernel support (particularly for int8, uint8, and various hardware architectures). Several issues concern the correctness and stability of AI model serialization, tensor transformations, and distributed training (e.g., DDP, FSDP, NCCL, DTensor). There are questions about specific kernel implementations (e.g., Triton, CUTLASS), compatibility of build environments, and runtime behavior, especially for complex models like GPT or attention mechanisms. Unresolved questions include the support for certain dtype conversions, multi-architecture builds, and incorporating custom kernels into PyTorch's core or Torchao frameworks. Overall, the discussions highlight ongoing efforts for robust model export, runtime efficiency, and cross-platform support."
2024-03-14,pytorch/pytorch,"The comments reveal multiple technical concerns related to PyTorch features and compatibility issues. Key topics include the unsupported `padding='same'` in `nn.Conv1d`, limitations with bfloat16 support on various CUDA devices, and challenges in exporting models with `torch.compile` and `torch.export`, especially involving FX graphs and tracing. Several discussions highlight ongoing platform-specific problems (e.g., MPS operator gaps, rocM flakiness, CUDA/CUDNN version mismatches), alongside requests for new APIs and best practices for model exporting, profiling, and multiprocessing. There are also issues with library support (e.g., sparse tensors, dataclass handling, complex number DDP support) and infrastructure/developer tooling, such as CI flakiness, documentation updates, and merge workflows. Many unresolved questions concern progress updates, planned fixes, and strategies for enhancing cross-platform and feature compatibility in evolving PyTorch versions."
2024-03-15,pytorch/pytorch,"The discussions mainly revolve around optimizing and fixing various kernel and backend issues in PyTorch, including orthogonalization speedups, spectral norm convergence controls, and implementation details of custom or fused kernels such as upsample_linear1d and attention mechanisms. Several reports highlight performance regressions, correctness concerns (like discrepancies in half-precision and sparse tensor support), and functional correctness bugs (e.g., handling of in-place tensors, device consistency, and data-dependent guards). There are ongoing efforts to improve CUDA and HIP kernel support, handle dynamic shape and control flow complexities, and integrate new features like native CUDA kernels and device-side assertions. Some issues are related to CI flakiness, build failures, or infrastructure configuration problems, which are being addressed through reverts and retries. Unresolved questions include how to better handle tensor layouts to maintain performance, managing tensor device and memory semantics during tensor transformations, and improving robustness and testing for complex dynamic behaviors."
2024-03-16,pytorch/pytorch,"The discussions highlight various technical issues such as inconsistent or hard-coded behaviors in PyTorch's handling of download links, CUDA/NVTX support, and device-specific operations like MPS complex tensor support. Several concerns involve build and dependency management, including package compatibility, build failures on specific platforms like Windows/MSVC, and header-only library support (e.g., NVTX). There are questions about the correctness and stability of recent code changes, including performance regressions, potential memory leaks, and correctness of custom kernel implementations. Additionally, some discussions focus on API design choices, such as whether to unify or separate related functions, and on the usability of type hints and debugging strategies in complex distributed or compile-time contexts. Unresolved issues include build failures, inconsistent behavior across platforms, and the need for further investigation and testing."
2024-03-17,pytorch/pytorch,"The discussions highlight various technical issues and feature requests within PyTorch, including support for 5D tensors in SSIM/MS-SSIM metrics, handling multi-GPU device allocation errors, and improving device compatibility on macOS MPS and Linux. Several issues pertain to backend and driver compatibility, such as NVTX header support, CUDA version management, and kernel functions on different hardware (e.g., A100 GPUs, complex tensors on MPS). There are ongoing efforts to enhance build systems, dependency management (like optional dependencies such as networkx), and enabling features like CUDAGraphs and torch.compile with checkpointing. Many questions remain about merging status, driver and framework support, and configuration ambiguities, particularly around debugging and runtime failures. Overall, the discussions reflect active development, troubleshooting, and feature enhancement efforts with a focus on broad hardware/software compatibility and usability."
2024-03-18,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch's performance and correctness across various backends and hardware, including CUDA, MPS, ROCm, and CPU. There is concern about bugs in specific operators (e.g., `index_copy`, `matmul`, `eigh`), and efforts include patching, rebase strategies, and backend-specific fixes, often involving modifications to lower-level code or APIs. Some conversations address API design choices (like separating or unifying multi-GPU group functions, layered configuration flags, and serialization improvements), as well as infrastructure stability questions, e.g., CI flakiness, test disabling, and build dependencies. Several topics involve debugging environment or hardware-specific issues, such as CUDA driver errors, GPU device resets, or memory leaks, often suggesting reproductions, environment checks, or workarounds. Unresolved questions remain about API changes, hardware support, and ensuring correctness and stability in distributed or compiled execution modes."
2024-03-19,pytorch/pytorch,"The discussions highlight several core concerns including the stability and correctness of operations in specific backends (e.g., issues with libtorch builds, CUDA/MPS interactions, NCCL communication errors, and codegen bugs in inductor). Many comments suggest that certain behaviors (like tensor view casting, operator support, or graph capturing mechanisms) need fixes or clearer documentation, often proposing alternative implementations or feature flag controls. There is ongoing debate about API design choices—such as API layering for device mesh configurations or serialization strategies for views—to improve usability and debugging. Several reports involve regression or flakiness in CI tests, indicating stability issues, especially with hardware-specific or environment-sensitive features. Overall, unresolved questions focus on improving robustness, correctness, and developer tooling for backend support, serialization, and handling dynamic or out-of-bound tensor operations."
2024-03-20,pytorch/pytorch,"The discussions revolve around various topics including challenges with disabling Caffe2 builds via environment variables, flaky CI tests especially under Linux or ROCm, and issues related to specific external dependencies like CUDA/ROCm versions causing build failures. Several comments highlight problems with specific PyTorch functionalities, such as symbolic shape handling, TorchScript export, or operator behavior inconsistencies across devices, often suggesting potential fixes or workarounds. There are questions about improving user experience with API behaviors, merging processes, and compatibility considerations for external components like ONNX, TorchMLIR, or third-party libraries. Unresolved or ongoing concerns include improving test stability, handling dynamic vs. static shape constraints, and ensuring backward compatibility across different hardware and software configurations."
2024-03-21,pytorch/pytorch,"The discussions primarily revolve around addressing various technical issues and regressions in PyTorch, including stability problems with the ROCm backend, performance regressions associated with recent PRs, and correct handling of symbolic shapes during compilation and export. Notable suggestions include: implementing smarter guard guards and shape range checks to prevent constraint violations; porting or reworking existing kernels (like `to_sparse_semi_structured`) with CUTLASS or porting code from xformers for improved efficiency; and fixing bugs related to data-dependent shape guards, unbacked symbolic integers, and operator support. Some issues are being fixed via revert and rebase workflows, while others require adding tests, changing build configurations, or delayed fixes involving compiler and runtime internals. Major unresolved questions include how to properly handle symbolic shape guards for dynamic dimensions, ensuring consistent guard serialization, and improving performance or correctness in backend kernels."
2024-03-22,pytorch/pytorch,"The comments highlight ongoing concerns with ensuring correct and efficient behavior of PyTorch features, such as parallel model training, kernel support, and autograd graph consistency, particularly in complex scenarios like FSDP, FakeTensor, and dynamic shapes. Several issues relate to performance regressions, memory leaks, and correctness bugs post-optimizations or code changes, with some suggesting reverts or patches; others focus on improving tooling, test coverage, and documentation. There are also discussions about proper handling of device-side memory, device-agnostic APIs, and enabling features like cudaGraph or mixed-precision training, often with questions about implementation details and best practices. Unresolved questions include how to best fix specific bugs, manage cache/metadata, and ensure compatibility across different hardware/software configurations, especially with new features or external dependencies. Overall, the conversations indicate active debugging, feature development, and some regressions in complex areas requiring careful review and iterative fixes."
2024-03-23,pytorch/pytorch,"The discussions primarily revolve around PyTorch's autograd limitations for second derivatives, especially in bilinear interpolation; solutions involve modifying gradient files and autograd graph handling. Several issues concern improving the design and flexibility of the learning rate schedulers, including their interfaces and multi-scheduler compositions. Multiple concerns about CUDA, ROCm, and backend support highlight challenges with hardware compatibility, driver dependencies, and kernel performance optimizations, often seeking community or team guidance. Some discussions focus on bug fixes and feature support in specific modules such as `grid_sample`, `Unique`, and `torch.fft`, with investigations into errors related to shape, data mutation, and operator support. Overall, unresolved questions include how to better handle graph break situations, support for in-place metadata mutation, and compatibility across various hardware and software versions, with community contributions and improvements being sought in these areas."
2024-03-24,pytorch/pytorch,"The discussions highlight concerns about proper handling and documentation of multihead attention weights, as well as support for complex data types in ONNX export, especially related to FFT operations. Several issues involve test failures, build system regressions, and hardware-specific bugs such as segmentation faults and CUDA errors, suggesting underlying infrastructure or compatibility challenges. There are debates on default configurations like CUDA graph support and distributed tensor use, with suggestions for policy-driven approaches. Regressions caused by rebases and merge conflicts are frequent, indicating ongoing integration difficulties. Overall, key unresolved questions include ensuring correct support for complex types, GPU-specific bugs, and maintaining robust CI/CD processes amid rapid development."
2024-03-25,pytorch/pytorch,"The comments highlight several recurring themes: a desire for more flexible API support for memory format manipulations (such as `memory_format=torch.channels_last`), with discussions around adding user-accessible APIs versus internal API design choices; ongoing work and improvements related to ONNX export compatibilities, including handling unsupported ops and memory size limitations; concerns about CUDA initialization when using FakeTensor modes, and the need for careful API design to avoid conflicts between user convenience and system robustness; challenges in handling specific ops like `repeat_interleave` and upsample in the context of ONNX, with suggestions to improve decomposition and support; and issues related to correctness and stability of the internal compilation and profiling tools (e.g., the memory leaks, handling of tensor strides, and profiling flakiness). The discussions also revolve around managing complex backend-specific configurations, safe API evolution, and necessary testing strategies to ensure robustness amid these changes."
2024-03-26,pytorch/pytorch,"The discussions highlight several technical challenges: 
1. Efficiently reimplementing large-scale sampling methods such as `multinomial()` for datasets with over 2^24 categories, including splitting weights or using repeated sampling, to bypass hardware or utility limitations.
2. Improving support and testing for new backends and hardware features, like SVE on ARM and ROCm, including proper detection, build configuration, and performance benchmarking.
3. Enhancing the robustness of distributed training, such as handling model sharding, memory management in DDP/FSDP, and reproducibility of randomness across different devices and modes.
4. Addressing issues with PyTorch's compilation, tracing, and operator support, especially for complex functions (e.g., scaled dot-product attention, upsample ops), to ensure correctness, performance stability, and interoperability.
5. Deployment and extension mechanisms, such as standardizing plugin architectures via entry points, handling auto-differentiation issues (e.g., in nested or graph break scenarios), and maintaining support for legacy or deprecated API patterns."
2024-03-27,pytorch/pytorch,"The comments predominantly revolve around troubleshooting and improving PyTorch's model exporting, compilation, and optimization workflows, especially regarding ONNX export issues, dynamic shape handling, and performance tuning. Notable concerns include ensuring correct guard handling with FX and Dynamo, managing inlining and guard scoping for accurate graph capture, and addressing specific operator support or shape mismatch bugs (like in flatten/concat or certain convolutions). Several suggestions involve refining API design for user clarity, such as default opset versions, precision control flags, and environment setup, as well as adding more comprehensive tests. There are also infrastructure and compatibility issues, including flaky CI, build errors on specific hardware or environments, and the need for better internal support for distributed/parallel models and custom operators. Overall, many unresolved questions focus on fixing bugs, improving robustness, and ensuring consistency across dynamic/static model workflows."
2024-03-28,pytorch/pytorch,"The collected GitHub comments reveal ongoing concerns about handling corrupted images during data loading, with proposed solutions involving returning `None` and filtering in `collate_fn()`, but issues arise when entire batches are corrupted, leading to potential index errors. Discussions similarly highlight challenges with supporting dynamic shape and memory format transformations during model export to ONNX, especially when shape inference produces ""unknown"" tensor dimensions, and some suggest using `torch.export.export` before `dynamo_export`. There’s attention to performance regressions introduced by recent PRs, especially in the context of inductor and Triton kernel generation, and methods to diagnose and mitigate them, such as adjusting codegen parameters or compiler passes. Several comments focus on build and compatibility issues—such as CUDA/ROCm version mismatches, NCCL and MAGMA linking, and environment configuration—often suggesting environment verification or patching. Finally, management of CI flaky tests, merge procedures, and configurations around distributed setup, environment variables, and signing CLA are raised as operational concerns."
2024-03-29,pytorch/pytorch,"The discussion centers on handling corrupted images in data pipelines and the implications for batching with `None` values in `collate_fn()`, highlighting issues when entire batches are corrupted, leading to errors in `default_collate()`. Another key concern involves managing numerical and hardware-related issues, such as NaNs in L-BFGS, precision control APIs, and performance regressions after code changes—especially around Triton kernel optimizations, memory coalescing, and code generation performance. Several questions pertain to deprecations, API design, and backward compatibility, emphasizing the need for clearer API semantics and robust code review practices. There’s also ongoing motivation to refine distributed training workflows, device plugins, and support for heterogenous compute devices, with discussions about fast export, ONNX support, and environment configuration complexities. Unresolved issues include error handling for batches of `None`, performance impacts from code refactoring, and ensuring compatibility and correctness in complex distributed and mixed-precision training setups."
2024-03-30,pytorch/pytorch,"The discussions highlight several key issues, including the need for enhanced support for advanced tensor operations such as associative scans, max pooling with indices, and fft functions on different hardware backends like MPS and MPSGraph, with proposals for API changes and performance improvements. There are ongoing concerns about the correctness and efficiency of distributed training patterns, especially with model parallelism, pipeline parallelism, and data parallelism, and how to properly handle batch sizes, rank identification, and synchronization across devices. Implementation challenges such as CUDA context initialization, NCCL behavior, and proper handling of autograd and functional storage mutations in distributed setups are also discussed. Questions about the clear definition of local versus group ranks, the impact of environment variables, and potential renaming for clarity suggest a need for precise API design. Additionally, there are administrative issues related to release notes tagging, CLA signing, and merge failures due to internal checks."
2024-03-31,pytorch/pytorch,"The comments mostly revolve around troubleshooting and improving PyTorch functionalities, such as resolving DLL issues with Intel OpenMP, support for MPS backend (especially for aten::fft_r2c and aten::max_unpool2d), and addressing FP16 normalization overflow errors that seem fixed in newer versions. Several discussions focus on compatibility problems, like CUDA/cuDNN/ NCCL version mismatches and environment setup, with suggested solutions including upgrading environments, installing nightly builds, or using specific frameworks like Triton. There are also merge conflicts and build failures due to internal changes or conflicts in files like `BlasImpl.h`, along with proposals for better testing practices, environment handling, and code refactoring. Additionally, some discussions touch on performance considerations of stochastic rounding and the need for clearer documentation or high-level explanations of certain PyTorch features."
2024-04-01,pytorch/pytorch,"The comments highlight ongoing challenges with PyTorch's compatibility across different system and hardware configurations, especially related to proper CUDA, ROCm, and ONNX support, as well as issues arising from multiprocessing and threading contexts (e.g., fork vs spawn). Several discussions focus on improving or replacing legacy components like `torch.onnx.export` with newer APIs (`torch.onnx.dynamo_export`) for better robustness, especially on complex models or dynamic shapes. There are concerns about performance regressions, the integration and benchmarking of custom kernels (e.g., sparse semi-structured matrix operations), and ensuring correct device and memory handling across diverse hardware (e.g., MPS, AMD, ROCm). Additional topics include bug reports related to distributed training, tensor serialization, and documentation testing, with calls for better tooling, test coverage, and clarity in API behaviors. Overall, the key unresolved issues involve ensuring compatibility, performance, and correctness across evolving hardware and software stacks, and improving user experience via tooling and documentation."
2024-04-02,pytorch/pytorch,"The discussions mainly revolve around addressing specific bugs, regressions, and performance issues in PyTorch, often linked to internal or external changes such as operator support, Triton kernel differences, and compiler behaviors. Several comments request clarification, suggest patching or reverting problematic PRs, or propose new approaches like patching `from_numpy`, improving CUDA/cuDNN/other hardware support, and managing serialization formats. Some issues highlight CI test flakiness, build inconsistencies, and environment-specific bugs, especially on ROCm, Apple Silicon, or Windows, requiring targeted workarounds or fixes. Additionally, there are concerns about ensuring correctness (e.g., in gradient/memory leak tests) and maintaining compatibility across different PyTorch versions and deployment setups. Overall, unresolved questions center on fixing bugs without regressions, improving build and runtime robustness, and clarifying technical details for future support and performance improvements."
2024-04-03,pytorch/pytorch,"The discussions cover various topics such as reproducibility and test coverage, with concerns about missing or flaky tests, especially on specific platforms like ROCm and ROCm CI failures, often related to new code or regressions introduced by recent PRs. Multiple comments mention the need for better test coverage, regression tests, and integration of new APIs (like `torch.onnx.dynamo_export`) with proper examples and documentation. There are also technical issues raised about specific implementation details, such as GPU kernel optimizations, compatibility with different hardware architectures, and handling of data-dependent shape operations in tracing/auto-export workflows. Several comments indicate ongoing investigations, bugs, or the need for better support for features like mixed precision, sharded training, or dynamic library loading, with some suggestions for API improvements, test enhancements, or environment setup fixes. Overall, the main concerns are ensuring stable, portable, and optimized behavior across diverse platforms, APIs, and models, alongside improving test coverage and developer tooling."
2024-04-04,pytorch/pytorch,"The discussions predominantly revolve around handling legacy GPU support, particularly for GeForce GTX 770/770M, where PyTorch's minimum CUDA capability support has become a barrier, with some users providing detailed instructions for building PyTorch with architecture patches. Several issues concern NCCL backend errors, especially system errors related to driver/library mismatches, which have been mitigated by updating NCCL versions and reconfiguring CUDA installations. Other topics include the stability and implementation of MPS support for models like GRU, with evidence that compatibility is limited or unstable, and suggestions to improve CUDA graph handling, runtime diagnostics, and operator support for ONNX export. There are also recurring mentions of build flakes, test infrastructure, and the need for better testing and logging around compilation, dependencies, and kernel generation. Some issues remain unresolved or require further investigation, including input mutation handling, linking breaks, and ensuring robust support across various hardware and software configurations."
2024-04-05,pytorch/pytorch,"The discussions highlight multiple technical concerns including compatibility issues during model export to ONNX, particularly with unknown tensor shapes and complex operations, leading to limitations in supported operators and symbolic graph representations. There are questions about the stability and correctness of dynamic shape handling, especially regarding range constraints and shape invariants during export. Several comments point to runtime errors, such as unsupported CSR tensor operations, and move towards fixing these through code changes or custom operator support. Additionally, there are infrastructure issues, like flaky CI tests and failures caused by environment mismatches or build system bugs, which complicate development and validation. The participants are exploring solutions such as better operator registration, improved diagnostic logging, and careful handling of tensor layout and memory management to ensure both correctness and performance."
2024-04-06,pytorch/pytorch,"The discussions highlight ongoing challenges and improvements in PyTorch, particularly regarding performance optimizations for tensor copying and memory operations on specific hardware (e.g., skylakes, AMD). Key questions concern the stability and correctness of mathematical operations like `acos` near singularities, especially on CPU vs GPU, and the need for robust solutions or safer implementations. Several issues relate to compatibility and support for advanced features such as fp8 training, sparse tensor operations, and support for specific hardware backends like MPS and ROCm, with some fixes already addressed and others still unresolved. There are also administrative concerns about documentation accuracy, build failures, and ensuring proper testing coverage as new features are integrated. Overall, unresolved issues include performance regressions, hardware support limitations, and the need for clearer, more reliable validation and documentation."
2024-04-07,pytorch/pytorch,"The comments reflect various ongoing challenges with PyTorch’s internal components, including performance regressions after recent updates, issues with deterministic behavior and test flakiness, and complexities in the compilation and optimization pipelines such as loop ordering heuristics and shape constraint handling. Several technical questions pertain to improving the robustness and efficiency of features like device rank caching, tensor layout support, and kernel generation, with suggestions for more standardized or automated approaches (e.g., benchmarking heuristics, caching attributes). Additionally, there are dependency issues linked to CUDA and cuDNN versions, and API support gaps such as missing operators or problematic default behaviors in quantization and tensor constants handling. Overall, the discussions indicate a focus on fixing regressions, enhancing performance, and clarifying design decisions around shape, layout, and device management."
2024-04-08,pytorch/pytorch,"The comments reflect ongoing efforts to address various issues in PyTorch, including implementing native masking operations to fix NaN gradients in attention mechanisms, handling Python multiprocessing caveats, supporting new features like torch.compile() on Python 3.12, and extending operator support for different hardware backends like MPS, ROCm, and XPU. Several discussions highlight fixing edge cases such as sparse tensor formats, numerical stability in special functions, and operator coverage for hardware-specific operators. There are also mentions of improving build efficiency, adding test coverage, and addressing CI flakiness or merge conflicts. Unresolved questions include the support timelines for torch.compile() on newer Python versions, compatibility with hardware accelerators, and ensuring that internal APIs are properly typed and documented."
2024-04-09,pytorch/pytorch,"The comments reveal recurring concerns about implementation details and stability in PyTorch's development, such as the support for `torch.compile` on different hardware architectures, breaking internal tests, and the need for better documentation of dynamic shape ranges and API behaviors. Several discussions highlight issues with specific operators (e.g., `aten::grid_sampler_2d_backward`, `ctc_loss`) not being supported or giving inconsistent results across backends like CUDA, MPS, or ROCm, often requiring fallback mechanisms or bug fixes. There are questions about the proper way to handle API changes, such as the deprecation of `async_save`, and how to improve testing and debugging workflows for new features. Some comments concern community tooling, such as improving linters and formatting tools, and managing release notes and dependencies, especially with CUDA and NCCL versions. Unresolved questions include how to better support multithreading, dynamic shapes, and feature compatibility across hardware and software configurations."
2024-04-10,pytorch/pytorch,"The discussions primarily revolve around handling multiple outputs in libtorch (C++) modules, issues with profiler and cupti support in various environments, and the behavior of tensor operations like `view_as_complex` with regard to strides and accuracy. Several comments mention specific failures caused by shape inference errors, mismatched expectations for autograd and sharding behaviors, and the need for better testing, especially for quantized, tensor subclass, and dynamic shape scenarios. Some discussions concern migration towards more robust API behaviors (e.g., replacing `set_()` logic to occur in the graph) and improving build and testing infrastructure, including container images and reproducibility. Overall, unresolved questions include ensuring correctness of shape inference, backward differentiation of distributed ops, and making system-level caching and resource management more reliable."
2024-04-11,pytorch/pytorch,"The discussions primarily focus on the challenges and nuances of implementing and testing various PyTorch features, particularly related to loss functions, serialization, dynamic shapes, and operator support. Several comments highlight issues with operator support in ONNX export, deterministic behavior and reproducibility, and compatibility with different hardware backends like MPS and XPU. There are concerns about the stability and correctness of features like `torch.to()` with tensor subclasses, CUDA/NCCL setup complexities, and the need for better documentation and test coverage. Some discussions suggest potential API changes or workarounds, such as replacing deprecated operators, enabling deterministic algorithms, or adding flags to control behavior. Overall, unresolved questions include how to better handle dynamic shapes, operator support in various backends, and ensuring robustness in distributed and mixed hardware environments."
2024-04-12,pytorch/pytorch,"The discussions primarily revolve around handling large tensor operations (such as random sampling, model scripting, and sparse tensor creation) efficiently and reliably in PyTorch, addressing issues like crashes, slowdowns, or failures due to shape or dtype constraints. Several comments suggest modifying or extending existing APIs—like better environment detection, passing meshes for distributed operations, or improving guarding and aliasing assumptions—to enhance robustness, performance, or compatibility across hardware and software configurations. Some discussions identify specific bugs, such as missing symbol definitions or incorrect assumptions around data types and shapes, with proposed fixes or workarounds involving code reordering, extra checks, or reimplementation of certain functionalities. There are also ongoing efforts to improve CI automation, testing coverage, and stability, especially for distributed/parallel modes and specialized hardware backends like ROCm or XPU. Unresolved questions include how to best manage guard logic, optimize runtime checks, and generalize features for broader hardware and deployment scenarios while maintaining correctness and performance."
2024-04-13,pytorch/pytorch,"The discussions highlight several technical concerns: support for float16 sparse matrix multiplication in PyTorch has known issues, potentially due to bugs in CuSparse algorithms like CUSPARSE_SPMM_CSR_ALG1, which manifest as incorrect results, especially in float16. Revisions to internal functions, such as `_copy_state_dict`, reveal ambiguities about whether copies are made, emphasizing the need for clearer copy semantics. There are complexities and bugs in specific backends like Triton, where assumptions about assertion support and kernel compilation cause failures—some fixes are linked, but workarounds like disabling debug assertions are suggested. Additionally, integration points such as passing mesh information to FSDP methods and managing device-specific behaviors (e.g., MPS issues, Linux/Windows discrepancies) require careful handling to prevent errors. Overall, unresolved questions include ensuring correctness and performance in low-precision sparse and batched operations, managing backend-specific bugs, and aligning internal API assumptions with user-facing expectations."
2024-04-14,pytorch/pytorch,"The discussions primarily revolve around CUDA and cuDNN compatibility issues in Docker-based PyTorch environments, with users seeking minimal solutions that do not bloat container size; some users highlight path and library loading challenges related to different CUDA versions and Anaconda environments. There are concerns about specific bugs or regressions, such as in `torch.save` file serialization and in the `torch.onnx.export` support for opset 19/20. Several threads mention ongoing PRs, their review status, conflicts, or merge failures due to CI checks or internal policy, with some discussions about the reusability and control over FP8 and custom CUDA extensions. Additionally, issues related to internal testing (e.g., test failures, flake discrepancies, and rebase conflicts) are noted, with some requests for help or clarification on build/test failures. Lastly, a few discussions are about CLA signing, PR rebase efforts, and the importance of integrating lower-level APIs and features into PyTorch for enhanced customization and performance."
2024-04-15,pytorch/pytorch,"The discussions highlight several key concerns: the absence of certain local convolution and spatial-aware layers in PyTorch, and the need for native hardware support (e.g., FP8, GEMM optimizations, and new tensor types like UINT4). There are requests for features such as native popcount, improved CUDA/mixed-precision support, and better handling of sparse tensor operations, alongside ongoing efforts for kernel-level enhancements and API extensions. Several issues involve CI flakiness, reproducibility, and internal build inconsistencies, requiring targeted fixes or re-enabling tests. Additionally, discussions address the broader development challenges of native support for named tensors, heterogenous device integration, and robustness in distributed/async workflows. Overall, unresolved questions include backward compatibility, API design clarity, and performance optimizations for emerging hardware and data types."
2024-04-16,pytorch/pytorch,"The discussions highlight several technical issues: (1) the persistent problem of `RuntimeError: context has already been set` when setting multiprocessing start methods after certain modules like `datasets` are imported, with suggested workarounds such as forcing start_method='spawn'; (2) problems with CUDA initialize slowdowns, inconsistent GPU behaviors, including the need for pre-warming CUDA, and complex interactions with in-graph operations like dropout and in-place modifications after model quantization, alongside experimental proposals to improve graph pattern matching; (3) internal CI flakiness and test instability, especially around distributed and inductor-based tests, some of which are due to prerequisites like environment variables, packaging, or synchronization issues; (4) ongoing maintenance concerns, such as the removal of deprecated macro guards for ROCm, the need for better validation checks for sparse tensor invariants, and updates to external dependencies like Triton. Questions remain about the best strategies to improve multiprocessing flexibility (e.g., rename APIs or enforce validation), handling inconsistent CUDA/mps behaviors and leveraging new compiler APIs or test frameworks better, and clarifying internal CI/test flakiness sources."
2024-04-17,pytorch/pytorch,"The comments reflect a range of technical concerns including the stability and reliability of PyTorch features across different hardware platforms (e.g., MPS, ROCm, XPU), interactions with CUDA and CUDA-related features like cuBLAS, and specific issues with kernel implementations such as `inductor` and `triton`. Several discussions point to the need for better benchmarking, performance analysis, and consistency checks—especially related to autotuning, memory management, and numerical stability. There are questions around the support for non-tensor inputs, custom ops, and the implications of changing internal APIs or configurations (e.g., deterministic algorithms, device mesh, constant propagation). Some comments highlight the importance of reducing flaky tests, improving CI reliability, and handling platform-specific bugs; in many cases, workarounds or reverts are used to address regressions or flaky behaviors. Lastly, issues such as build reproducibility, environment compatibility, and proper test coverage are recurrent topics needing further investigation."
2024-04-18,pytorch/pytorch,"The discussions cover various technical issues including performance regressions caused by recent PRs, particularly affecting models like YOLOv3, ConvIT, and several NLP models, often linked to specific commits such as #122135 and #1104e07. Several reported problems involve flakiness in CI, especially on platforms like Linux, ROCm, and macOS, often attributed to environmental inconsistencies or driver/kernel issues, with some regressions traced back to specific changes in the codebase (e.g., removal of macro guards, unaligned memory access). There are also recurring concerns about correctness and stability, such as inaccurate outputs, memory leaks, and failures in certain operators or operator bindings, notably with MPS on Apple Silicon and CUDA/HIP on Linux/ROCm. Proposed solutions include refactoring code (e.g., moving device guard logic, improving caching and re-mathematization strategies), disabling or fixing failing tests, and aggregating static analysis tools like nm for better binary validation. Overall, unresolved questions involve prioritization, robustness of environment-dependent regressions, and the need for clearer APIs or mechanisms to control dynamic shapes, resource management, and code rewriting strategies."
2024-04-19,pytorch/pytorch,"The comments cover a range of issues, including proposals to improve PyTorch's C++ API auto-generation via YAML files, and discussions about advancing support for loop operators like torch.scan, with notes on potential benefits for kernel generation and inference optimization. Several reports highlight bugs related to NCCL, CUDA memory leaks, and device-specific operator support (e.g., aten::isin.Tensor_Tensor_out on MPS), often with workarounds such as environment variable adjustments or reverting to earlier library versions. Other comments focus on stability concerns in trunk (e.g., flaky tests and CI failures) and API deprecations or enhancements, including considerations for less-breaking API updates. Overall, key themes involve optimizing kernel performance, expanding operator support across devices, refining APIs for better user workflows, and stabilizing CI/test reliability amidst ongoing development."
2024-04-20,pytorch/pytorch,"The discussions highlight several technical issues and questions, including support for specific operations like `aten::isin.Tensor_Tensor_out` in the MPS backend, and the impact of backend support limitations on functionality such as `aten::topk` and `aten::isin`. There are concerns about performance and memory overhead in optimizers like AdamW, especially when integrating new features or modifications to the backward hooks for gradient accumulation. Additionally, there are challenges related to debugging and testing in diverse hardware environments, such as Macs with M1/M2 chips, and cross-platform build issues on Windows and Linux, including dependency resolution and proper linking of CUDA or CPU libraries. Proposals suggest adding support for new ops, deprecating or modifying public APIs carefully, and improving CI coverage to detect platform-specific bugs early. Overall, the discussions reflect ongoing efforts to enhance backend support, stability, and usability across diverse hardware and software configurations."
2024-04-21,pytorch/pytorch,"The discussions encompass various technical concerns, including issues with performance and correctness on specific hardware like MPS and Apple Silicon, memory management and tensor formatting in PyTorch, and challenges with profiling, debugging, and ensuring deterministic behaviors. Several comments address the need for better tooling and testing, especially for open-source contributions and customized operators, as well as improvements in code maintainability and documentation. There are unresolved questions regarding default device-specific generator usage, tensor memory formats, and the integration of new features like ConvTranspose3D on MPS. Merge conflicts and build failures due to continuous integration issues are frequent, along with requests for code review and guidance on contributing to specific subsystems like optimizers and graph inlining. Overall, the discussions highlight ongoing efforts to improve stability, hardware support, performance profiling, and community engagement in the PyTorch project."
2024-04-22,pytorch/pytorch,"The comments reveal multiple technical issues and discussions, including the need for enhanced testing (such as unit tests for `torch.autograd.grad()`), handling of integer parameters in distributed training (notably in FullyShardedDataParallel), and specific bug fixes (like proper stride handling for `view_as_complex` and deterministic behavior of `aten::sum`). There are ongoing efforts to improve performance (e.g., FP8 matrix multiplication, kernel vectorization, and compiler optimizations), with some concerns about accuracy trade-offs when using fused FMAs. Several discussions involve CI stability, flaky test disabling, and merge management, while internal fixes and refactors (e.g., for JIT, tracing, or new APIs) are also prevalent. Overall, the focus is on bug fixing, enhancing test coverage, performance tuning, and ensuring correctness in complex features like model quantization, custom backends, and distributed training."
2024-04-23,pytorch/pytorch,"The discussions raise several technical concerns including the need for a `BufferDict`-like feature for better buffer parameter management, especially for TorchScript compatibility, and the potential for unified device placement and device switching utilities. Support for RISC-V vector ISA in PyTorch, particularly around operator support and performance, is questioned, along with the possibility of automatic RVV utilization via compilation. Several issues highlight flakiness in CI testing, often related to platform-specific or resource limitations, and the importance of proper test disabling protocols. There's also ongoing debate on API stability versus BC-breaking changes, particularly regarding tensor slicing, view semantics, and deprecation strategies. Lastly, multiple PRs related to compiler, backend, and operator support are either delayed or require careful handling to avoid regressions, especially with dynamic shape support and cross-architecture compatibility."
2024-04-24,pytorch/pytorch,"The discussions highlight several key technical concerns, particularly around the support and implementation of custom autograd functions in Python versus C++, with some noting that support remains limited in Python and only readily available through C++, accompanied by references to complex workarounds. There are recurring issues with build and deployment, such as discrepancies between pip and conda wheels, and problems with specific hardware configurations (e.g., MI300, Apple M2 Max), including related errors and performance regressions. Multiple discussions involve the stability and performance tradeoffs of advanced GPU kernel features like Cutlass epilogues, with debates on their utility versus complexity, especially in the context of whole-model performance gains. Additional concerns involve the stability of distributed interconnects (e.g., NCCL), profiling and debugging tools (e.g., kineto), and the challenges of integrating features like private memory pools across different backends and architectures. Lastly, there are questions about build infrastructure issues, CI flakiness, and the organization and naming of features such as pipeline parallelism, reflecting ongoing operational and engineering clarifications."
2024-04-25,pytorch/pytorch,"The comments cover a wide range of technical concerns including the need for improved API support (e.g., multi-dimensional masks in `nn.MultiheadAttention`, handling of in-place ops and quantized tensors), handling of device and memory management—especially in distributed, CUDA, and ROCm contexts, and robustness of features like CUDA Graphs, inductor's kernel generation, and support for various hardware backends (e.g., MPS, ROCm, XLA). Notable questions involve ensuring correct tensor device/device-compatibility handling, fixing build environment issues, managing third-party dependencies, and ensuring reproducibility and correctness of performance benchmarks. There are suggestions for API changes (e.g., in `TensorIterator`, allocators, and in handling extra states in modules), and concerns about flaky tests, CI stability, and regressions caused by recent code changes. Additionally, the discussions include proposals for architectural improvements such as supporting pluggable allocators and pools, as well as enhancing testing and verification strategies to prevent regressions."
2024-04-26,pytorch/pytorch,"The discussions raise concerns about handling in-place and view-returning operations for proper BC enforcement, suggesting the need for explicit APIs or annotations (e.g., marking mutating arguments) and revising the related warnings. There are questions about ensuring consistent behavior across different tensor formats, especially for sparse and meta tensors, and how to extend or improve re-inplacing mechanisms, potentially via a registration API for linked in-place/out-of-place operators. Some conversations focus on performance trade-offs when fusing operators, the complexity of generating safe and stable epilogues, and how to support custom ops and sharding strategies for advanced use cases like DTensors. Additional suggestions involve better detection and handling of unsupported or non-contiguous memory formats, and carefully managing the impact on functionalization, propagations, or maintaining correctness while reducing overhead. Unresolved issues include the implementation of comprehensive warning triggering, support for multiple conditional operations, and coordination of metadata of operations in the context of BC and functionalization."
2024-04-27,pytorch/pytorch,"The comments reflect a range of issues and discussions primarily related to PyTorch's development and usage, including implementation gaps on certain hardware devices (notably MPS and ROCm), and challenges with model conversion to ONNX and OpenVINO formats due to data type and shape handling, especially with functions like `F.fold`. Several reports concern runtime errors, such as unsupported operators on specific devices (e.g., MPS's `aten::_embedding_bag`), and deprecated or broken features following API changes or version upgrades. There are ongoing efforts regarding infrastructure, CI stability, and build environment issues across different platforms and CUDA versions, as well as discussions about code formatting and tooling (e.g., `ruff`, `black`). Some issues involve fixing bugs, improving compatibility, or enhancing features like double backward support, with a need for regression tests, better documentation, and addressing specific build or import errors on various hardware and software configurations."
2024-04-28,pytorch/pytorch,"The discussions highlight ongoing challenges with data initialization in PyTorch's DataLoader, especially involving shared state and performance issues across workers, with suggestions like delaying initialization to `__getitem__` or using pandas DataFrames to mitigate memory leaks. There are technical concerns regarding incomplete operator support on MPS devices, leading to `NotImplementedError` exceptions, and potential workarounds such as environment variables or fallback methods. A significant portion addresses API stability and backward compatibility, including handling fused layers with DTensor, and evolving support for heterogeneous hardware and interop interfaces like DLPack. Other topics involve deep technical details about inductor codegen, operator mutability in custom ops, and ensuring correctness amid code refactoring and PR merges. Overall, the discussions reflect active efforts to improve performance, compatibility, and robustness across hardware backends and advanced features."
2024-04-29,pytorch/pytorch,"The comments reveal a range of ongoing issues and development efforts within the PyTorch repository. Several threads discuss bug fixes and regressions, such as regression caused by PRs, flaky tests, and specific bugs with inductor or CUDA kernels, some requiring reverts or targeted debugging. Others focus on feature requests or improvements, like better support for low-bit quantization, low-bit access, fusing fused layers, and enhancing distributed training (FSDP, MoE, DTensor). There are also discussions about infrastructure issues, such as build time increases, CI test flakiness, and issues with external dependencies or documentation updates. Overall, unresolved questions include fixing regressions, improving hardware support, optimizing performance, and streamlining development workflows."
2024-04-30,pytorch/pytorch,"The discussions encompass various technical topics including the implementation of advanced linear algebra functions like CPU support for rank-deficient least squares solving, and improvements in PyTorch's autograd and documentation support, such as documenting overloads and enhancing inline docstrings. Several issues highlight the need for better support for deterministic algorithms, especially for operations like reflection padding, bilinear resize, and non-deterministic GPU kernels across different hardware platforms like M1/M3 Macs, AMD W7900, and NVIDIA CUDA with different driver versions. There are ongoing efforts to improve build and integration stability, including handling large models with protobuf, addressing trunk flakiness, and ensuring correct handling of complex, heterogeneous systems like multi-node distributed training and custom operator support. Additionally, there’s interest in more robust support for exporting models to ONNX, especially with dynamic shapes and advanced features like QAT, requiring newer APIs and better handling of model size limitations. Overall, the key concerns revolve around expanding hardware and feature support, improving deterministic behavior and reproducibility, and ensuring stable, maintainable build/test workflows."
2024-05-01,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch's ONNX export support, especially regarding unsupported operators like `prim::layout`, `aten::_scaled_dot_product_attention`, and sparse tensor operations, with workarounds or new exporters like `torch.onnx.dynamo_export` being suggested. Several entries focus on compatibility concerns, such as matching CUDA/cuDNN versions, and the need for better documentation and test coverage for specific features like auto mix-precision, device interoperability, and dynamic shapes. There are also technical concerns around the proper handling of device-specific behaviors, memory management, and edge cases like serialization, privacy, and correctness of in-place operations within model tracing and scripting. A subset of discussions relates to infrastructure issues, including merge failures, build caches, and environment consistency, pushing for better dev workflows and versioning practices. Unresolved questions mainly involve expanding support for newer operators, handling complex models with advanced features, and ensuring robustness across different hardware and software configurations."
2024-05-02,pytorch/pytorch,"The discussions primarily revolve around issues and feature requests related to PyTorch's ONNX export, with challenges in dynamic shape handling, custom ops, dictionary support, and compatibility with newer opset versions. Several reports concern runtime errors and incomplete feature support in the current maintenance mode of `torch.onnx.export`, prompting suggestions to use the new dynamo/torch-onnx exporter, which is still experimental and Linux-only. There are also technical concerns about the correctness and robustness of specific operators, such as `tensor_split`, `depthwise convolutions`, and potential need for custom shape inference or additional attributes for better model representation. Additionally, questions about reproducibility, CUDA kernel alignment issues, and dealing with PyTorch's internal memory management behaviors are noted. Unresolved questions include support for advanced features like non-static dictionaries, nested user-defined module hooks, multi-process mesh recovery, and the integration of features like real tensor propagation in Dynamo, indicating ongoing development and experimentation."
2024-05-03,pytorch/pytorch,"The comments highlight various technical concerns including reproducibility issues with model loading and state_dict handling, especially around quantization and quantized model compatibility across different PyTorch versions. Several discussions point to potential trunk flakiness and flaky test failures across platforms, often resolved after multiple reruns. There are questions about interface stability, such as ensuring fake tensors are correctly registered for autograd and pytree processing, and concerns about patching or extending existing APIs like torch._dynamo or FSDP. Additionally, some comments address build and hardware compatibility issues, such as CUDA/ROCm support, CUDA kernel compile performance, and dependencies like MKL, as well as test disabling or CI infrastructure adjustments. Unresolved questions include how to best automate or streamline these fixes, handle regressions, and improve test stability and reproducibility."
2024-05-04,pytorch/pytorch,"The discussions highlight several key technical concerns: issues with distributed object reduction and serialization, especially when passing complex Python objects or custom classes via RPC frameworks; challenges in supporting complex data types like complex numbers and large sparse tensors, including ONNX export limitations; and performance regressions due to kernel rebuilding times, notably in CUDA/ROCm kernels for specific operations like flash attention. There are ongoing efforts to improve usability, including adding naming conventions for tensors, better support for runtime quantization, and refining scheduling and graph export mechanisms. Several questions revolve around supporting specific operators (e.g., `aten::isin.Tensor_Tensor_out`) on different devices, fixing build and performance regressions, and enhancing tooling (like `dcp_load`) for model checkpointing. Despite existing solutions and proposals, unresolved issues remain around efficient, flexible serialization, kernel build optimization, and comprehensive device and data type support."
2024-05-05,pytorch/pytorch,"The discussions highlight challenges with serializing PyTorch objects like `EasyDict` and custom classes for tracing or exporting, emphasizing the need for user-defined flatten/unflatten functions or alternative approaches. There are ongoing efforts to refactor or handle these user-defined or external data structures properly within PyTorch's pytree utilities, with frustrations expressed over the complexity of such integrations. Some issues relate to device-specific operator implementation (e.g., MPS, GPU kernels), and others concern CI stability, test flakiness, and handling of external dependencies like Triton or MKL DLLs. Several PRs involve merging or reverting changes, with attention to build failures, platforms, and long-term maintainability, especially around backend support and reproducibility. Overall, key unresolved questions include how best to extend PyTorch's export mechanisms to custom or external data types and ensuring stable, performant device-specific operator support."
2024-05-06,pytorch/pytorch,"The comments reflect a range of issues, primarily revolving around build system inconsistencies, environment/configuration problems, and regression concerns. Notable topics include reproducibility of results after model loading, especially for quantized or finetuned models; performance regressions and optimizations for specific hardware (e.g., ROCm, MPS, CUDA, x86, ARM); and reliability of CI/dynamic shape guards, flaky tests, and build environment setup. Some discussions involve fixing or refactoring code for better maintainability (e.g., module tracking, custom ops, deprecations), while others focus on performance fixes (e.g., Triton kernel overhead, mixed-precision challenges). Several issues relate to internal infrastructure, such as build failures, test flakiness, or external dependencies like MKL and network access, with suggestions to improve automation, testing, or documentation. Overall, unresolved questions include deterministic reproducibility of models, environment consistency across sessions, and stability of CI tests in diverse hardware/software setups."
2024-05-07,pytorch/pytorch,"The discussions cover various technical issues in PyTorch, including problems with AMP training leading to NaNs under FP16 configurations, implementation details of torch.cuda.amp.GradScaler, and how to properly support device-agnostic autocast. Concerns are raised about the correctness and safety of handling in-place tensor modifications, especially with custom in-place ops and their integration into autograd and re-inplacement mechanisms. Several issues relate to the stability and reproducibility of CUDA, ROCm, and MPS backends, including specific bugs, driver/Torch version incompatibilities, and platform-specific behaviors like subnormal number support. Some discussions suggest refactoring or extending API support, such as adding `pin_memory_device` or more general backend registration, as well as potential code movements or architecture changes to better support features like nested tensors and custom kernels. Unresolved questions include ensuring backward compatibility, handling dynamic shape support, and proper error handling and debugging mechanisms across different platforms and backends."
2024-05-08,pytorch/pytorch,"The comments cover a wide range of issues and discussions related to PyTorch development, including deprecated features and migration concerns (e.g., `cpp_custom_type_hack`), profiling and performance improvements (e.g., tensorboard integration, nvprof overhead, Cupti counters), build and packaging challenges (e.g., CXX11 ABI support, CUDA/ROCm compatibility, multigpu test failures), and functionality enhancements (e.g., nested tensors, complex number support, FSDP improvements). Several discussions focus on debugging, correctness, and stability of profiling tools, as well as code refactoring and API design considerations (such as precision control flags, serialization, and API layer improvements). Unresolved questions include compatibility with upcoming CUDA versions, handling of guard suppression and boundary checks in ACL, and the impact of deprecations on workflows. Overall, the comments highlight ongoing efforts to enhance robustness, performance, and usability, along with some pending regressions and architectural decisions to address."
2024-05-09,pytorch/pytorch,"The comments reveal ongoing discussions about PyTorch's internal implementation and development process, including the potential deprecation or modification of hashing for tensors, handling of in-place and view operations during BC-breaking changes, and the need for supporting factory functions and dynamic shapes. Several issues relate to platform-specific builds, especially concerning macOS and Apple Silicon, and the need for better documentation or API adjustments. There are concerns about the stability and testing of features like torch.compile, profiler integrations, and CUDA graph serialization, often tied to environment differences or platform-specific limitations. Many comments also highlight the importance of code refactoring, fixing bugs, and improving build/test workflows, with unresolved questions around compatibility, performance, and API consistency across different hardware and software configurations."
2024-05-10,pytorch/pytorch,"The discussions highlight several technical concerns, including the appropriateness of class-based versus functional API designs, especially in interpolation and spline representations, emphasizing simplicity and discoverability for PyTorch users. There's a recurring theme about the maintenance and future deprecation of `torch.onnx.export`, alongside questions about support for specific opset versions and internal APIs like `prims.inductor_random`. Issues regarding runtime support and device-specific failures on various hardware (e.g., MPS on Apple Silicon, ROCm/ROCM build inconsistencies, and GPU driver compatibility) also arise, with some providing potential workarounds like forcing barriers. Several discussions focus on CI flakiness, test stability, and build caching strategies that may impact the reliability and performance of CI pipelines and build reproducibility. Unresolved questions include support for platform-specific features, the timeline for fixes (e.g., Triton integration failures), and how to evolve API patterns for device and memory management abstractions."
2024-05-11,pytorch/pytorch,"The discussions primarily focus on technical issues and feature requests within the PyTorch repository, including device support (e.g., MPS, ROCm), compatibility challenges (e.g., with complex tensors, numpy scalar indexing), and performance bugs (e.g., NaNs with scaled dot-product attention, graph break issues in Dynamo). Several discussions address build and environmental concerns, such as ensuring consistent CI testing, managing dependencies like MKL, and streamlining multi-platform support (e.g., iOS, Windows). There are also suggestions for API improvements, bug reverts, and ongoing feature developments (e.g., multi-process groups with NCCL, complex number support). Many unresolved questions relate to bug fixes (e.g., dynamic shape handling, distributed communication), stability of tests, and better tooling for internal/external developer workflows."
2024-05-12,pytorch/pytorch,"The discussions primarily focus on enhancing PyTorch's build configuration and backend support, specifically regarding BLAS library selection, CUDA/CuDNN compatibility, and handling of various tensor formats and layouts. Several comments highlight issues with the dependency finding logic in `Dependencies.cmake`, especially around Eigen, LAPACK, and BLAS combinations, proposing updated conditional logic to better respect user choices and preselected options. There are questions about device-specific issues like NCCL errors, GPU hardware misalignments, and CUDA version compatibility, alongside suggestions for improving code clarity and documenting expected behaviors in release notes. Additionally, users discuss extending PyTorch's tensor abstractions, such as support for wide/different dtypes, layout tagging, and in-place type modifications, as well as recommendations for debugging and profiling GPU-related crashes and performance bottlenecks."
2024-05-13,pytorch/pytorch,"The comments primarily discuss technical challenges in PyTorch related to GPU memory management, ONNX model export issues, and support for nested/tensor structures in various contexts. Key concerns include CUDA out-of-memory errors during model inference, especially with large models or complex operations like vision transformers, which might be mitigated by reducing batch sizes or leveraging nightly builds with fixes. There are questions about improving debugging, traceability, and test coverage for features like nested tensors, control collectives, and dynamic shape support, with suggestions to better detect dynamic tensor states or support more operations in compilation modes. Some discussions focus on environment-specific issues such as ROCm pipeline compatibility, compiler support, or build configuration for various hardware, and considerations on how to handle versioning and deprecation for API consistency. Overall, unresolved questions pertain to ensuring broad platform support, fixing specific bugs, and enhancing the robustness of export and compilation workflows."
2024-05-14,pytorch/pytorch,"The discussions highlight several issues including the need for better testing strategies, notably for complex operations like n-dimensional indexing, support of dynamic shapes, and handling of non-contiguous tensors, with suggestions for explicit decompostion or more robust state management. Concerns around maintaining backwards compatibility when changing behaviors (e.g., in advanced indexing) and the importance of clear documentation are emphasized. Multiple reports indicate CI/test failures due to bugs, unsupported operations (e.g., in onnx or with specific data types like complex or bfloat16), or environmental/setup inconsistencies, requiring targeted fixes, re-runs, or configuration changes. Several PRs and internal fixes are underway to address these issues, but ongoing work involves handling edge cases in distributed training, memory management, and runtime support across platforms and hardware. Overall, there's a need for clearer guidelines, better tooling, and incremental testing to prevent regressions and improve robustness."
2024-05-15,pytorch/pytorch,"The comments primarily revolve around common issues related to DataLoader worker signals, memory cache management, multitasking and process spawning strategies, and GPU device configuration in PyTorch. Several users report errors due to high `num_workers`, overlapping input/output tensors, or misaligned device settings, with suggested workarounds like reducing `num_workers` or setting environment variables early. Others mention compatibility problems with different PyTorch versions, CXX11 ABI issues, and model serialization using DCP, indicating some incompatibilities or missing features. Some discussions focus on the stability and performance of specific features such as nvFuser, FSDP, and the use of deterministic algorithms, with hints at ongoing fixes and improvements. Overall, unresolved questions include handling of GPU resource management, binary reproducibility, and ensuring reliability across diverse environments and versions."
2024-05-16,pytorch/pytorch,"The discussions involve several technical topics in PyTorch development, including the need to add box-constraints support to LBFGS-B optimizers, handling of dynamic shapes and auto-differentiation with higher-order operators, and improving the support for Named Tensors in core primitives. There are also issues related to exporting models to ONNX, especially with newer opset versions and decomposition passes, as well as challenges in distributed training, such as device guard management and FSDP's memory optimizations. Some comments address stability concerns with CUDA versions and driver interactions, along with the importance of testing and continuous integration pipelines. Several suggestions include refactoring APIs to reduce complexity, supporting non-tensor inputs in auto-diff and export workflows, and handling edge cases in model serialization and parallel execution."
2024-05-17,pytorch/pytorch,"The discussions cover various technical concerns, including the need for upstreaming Android-related CMake modifications, deprecation notices for ShardedTensor in favor of DTensor, and the handling of CUDA device guards and their performance implications. Several issues address API compatibility, such as differences in behavior after model export, changes in operator schemas, and support for custom ONNX operators, with suggestions for adding tests and clarifying environment configurations. There are also problems related to build dependencies (e.g., MKL or libc version mismatches), usage of private or internal classes (e.g., torch._C.Generator), and conditional logging behavior in XPU fallback mechanisms. Some threads discuss potential BC breaks, the importance of robustness in checkpointing, and the need for detailed profiling and testing, while unresolved questions include how to appropriately extend or monkeypatch internal C++ classes and whether certain code patterns should be supported or disabled."
2024-05-18,pytorch/pytorch,"The comments highlight several technical challenges and ongoing developments within PyTorch. Key concerns include ensuring API updates maintain backward compatibility, particularly with features like `pad_packed_sequence` and learning rate schedulers; addressing performance issues with fused layers and tensor parallelism in large language models; and improving dynamic shape and guard management for ONNX export, especially with data-dependent operations and symbolic shape constraints. There are also unresolved questions related to the reusability of bytecode with variable ordering, debugging runtime errors on specific hardware (e.g., A100/L4), and integration complexities in compilation, inlining, and distributed training components. Some discussions suggest requiring additional guard logic, robustness to data-dependent shapes, and careful handling of internal bytecode and variable ordering to preserve speed and correctness. Overall, these reflect active efforts to enhance stability, usability, and performance in PyTorch’s evolving ecosystem."
2024-05-19,pytorch/pytorch,"The discussions highlight various technical issues with PyTorch, such as the challenge of tracing dynamic behaviors in graphs (particularly with floating-point and shape-dependent operations), the need for more expressive type annotations (e.g., in `torch.nn` and tensors), and problems with ONNX export involving data-dependent shapes and unsupported nodes. Several posts concern internal build and compatibility issues, including link errors with MKL on Windows, compiler bugs on Linux, and runtime GPU errors on Apple Silicon, often related to layout, memory contiguity, or hardware limitations. There are proposals for improving the usability and robustness of features like `torch.compile`, better type annotations, and inlining of certain functions, alongside questions about future releases and internal framework modifications. Overall, many discussions focus on improving PyTorch's compatibility, traceability, and deployment robustness across different hardware and software environments."
2024-05-20,pytorch/pytorch,"The discussions highlight various technical concerns including the reproducibility of results in notebooks, the support and compatibility of functions like `load_lua` across different PyTorch versions, and the handling of non-contiguous tensors when using `.view()` (which can cause runtime errors). Several issues relate to the correctness and stability of model exporting (especially with custom ops like `erfc`, and handling dynamic shapes in ONNX export), as well as performance regressions and benchmarking for different precisions (float32, bfloat16, AMP) on various hardware setups, including AMD, NVIDIA, and ROCm. There are also discussions about improving the debugging and testing infrastructure (e.g., better test coverage, failure retry mechanisms, and CI annotations). Lastly, some concerns relate to the internal development workflow, like merging policies, dependency management, and API support for custom backends or hardware-specific features."
2024-05-21,pytorch/pytorch,"The discussion highlights several technical concerns, including the automatic lifting of parameters and buffers in torch.export and its implications for ONNX serialization, particularly with models like MONAI’s FullyConnectedNet, and the need for explicit APIs to handle such cases. There is debate over how in-place operations and inlined functions (e.g., user-defined functions or custom ops) should be treated within the graph (e.g., whether they should be re-inplaced or if users should define explicit in-place variants). Several issues involve handling dynamic shapes, device-specific behaviors (notably CUDA register usage on different GPU architectures), and error reporting—especially around CUDA errors, inference correctness, and unsupported operators. There are proposals for better observability (e.g., reporting, logging, or annotation improvements), and ongoing efforts to improve CUDA graph debugging, serialization, and runtime support. Unresolved questions remain about the best practices for user-facing APIs, backward compatibility, and automated testing/deployment strategies across various hardware/software environments."
2024-05-22,pytorch/pytorch,"The discussions highlight several technical concerns, notably inconsistencies and behaviors in PyTorch functions like `np.linalg.solve` and `torch.linalg.solve` that depend on input dimensions and broadcasting rules, with calls for updating to NumPy and array API behavior. There's interest in improving `torch.onnx.export` support, especially for newer opset versions and specific features like antialiasing in Resize, with suggestions to transition to `torch.onnx.dynamo_export`. Several issues address stability and performance regressions, such as potential performance regressions from specific PRs, and internal bug fixes, like handling dead tensors or in-place operation behaviors, with calls for better documentation and testing. Compatibility concerns with various hardware (Apple Silicon, ROCm, AMD GPUs, etc.) and environment-specific build issues are raised, alongside proposals for API improvements, such as enabling user annotations for sharding, communication overlap, and side-effect marking. Overall, unresolved questions include API support for in-place ops, better handling of distributed loading, and ensuring stability across different hardware and software configurations."
2024-05-23,pytorch/pytorch,"The discussions reveal several recurring issues: a CUDA out-of-memory problem often mitigated by batch size reduction, and an ongoing need for better error handling and profiling tools in PyTorch, including support for exception propagation, visualization, and operator support across devices. Multiple comments suggest that deployment and debugging are hindered by inconsistent environment setups (e.g., MKL versions, system libraries, CUDA/ONNX interoperability), and some developers express concerns about bifurcations between CPU/GPU/backends, particularly with integrations like oneDNN and custom kernels. There are also technical debates about the correctness of certain PyTorch operations (e.g., `aten::threshold`, `FloorDiv`) on different hardware backends, as well as issues with testing and validation for specialized hardware such as AMD ROCm, Apple Silicon, and XPU. Several discussions focus on refactoring, re-architecting debugging tools, and improving support for new hardware and operator behaviors to ensure robustness, performance, and usability."
2024-05-24,pytorch/pytorch,"The discussions mainly revolve around enhancing PyTorch build and runtime features, such as providing debug symbols for wheels, supporting more loop operators like torch.scan, and addressing build compatibility issues with CUDA, cuDNN, and ROCm, especially on different hardware and driver configurations. Several conversations highlight the need for better documentation, API stability, and handling of device-specific behaviors (such as GPU memory management and hardware flags). There are concerns about test flakiness, CI failures, and the correct integration of external libraries (e.g., NCCL, NCCL tests, third-party libs like amdsmi). Some issues relate to ensuring compatibility with custom or third-party hardware, and addressing code complexity, especially around datatype conversions and low-level device interactions. Unresolved questions include how to manage GPU flags safely during compilation and whether certain proposed API or feature additions, like list support in tracer, should be delayed or carefully integrated."
2024-05-25,pytorch/pytorch,"The discussions cover multiple technical issues with PyTorch including data initialization timing in datasets, memory leaks possibly caused by DataFrame or LMDB usage in `__getitem__`, and CUDA-related errors linked to specific GPU architectures and NCCL versions. Several users raise concerns about compatibility with Python 3.11 on macOS and dependency management when building from source, especially with CUDA and cuDNN versions. A recurring theme involves the stability and correctness of the profiling API when using schedule and repeated steps, alongside certain operator support and device-specific bugs, especially on Apple M1/M3 and Nvidia A10G GPUs. Some discussions suggest workarounds like using Ray or spawn on multiprocessing, or reverts and rebasings of PRs to address regressions. Overall, unresolved issues include memory leaks, operator support, CUDA NCCL discrepancies, and build process robustness, with some requests for bug reports and detailed diagnostics."
2024-05-26,pytorch/pytorch,"The discussions raise concerns about memory issues and crashes in native extensions and dynamic modules (e.g., C++ regex example) potentially due to dlopen flags or binary compatibility, with suggestions to modify dlopen flags or adjust sharding schemes. There are questions about the future of CUDA-based dynamic quantization and whether the existing APIs should be maintained or migrated to newer frameworks like torchao, along with considerations for backward compatibility. Several issues highlight model sharding, tensor layout, and distributed training challenges, such as FSDP parameter recombination and DTensor sharding strategies, indicating ongoing work to address data layout inconsistencies across different sharding approaches. Other topics include improving warnings suppression, handling large kernel regressions, fixing inductor/optimizer issues, and porting or refactoring code to support newer PyTorch features, with some discussions about CI infrastructure, test flakiness, and code hygiene. Unresolved questions involve API deprecation or migration, especially around quantization and export workflows, as well as ensuring stability on various hardware and software configurations."
2024-05-27,pytorch/pytorch,"The discussions highlight several issues including performance discrepancies when installing PyTorch and dependent libraries via different methods (pip vs conda), which can cause runtime errors related to OpenMP and CUDA libraries. There are ongoing concerns about selectively disabling flaky tests in CI workflows, often due to platform-specific or flaky environment issues, and suggestions to improve test robustness and diagnostics. Several PRs and code changes focus on enhancing compiler and backend support (e.g., quantization, flash attention, and symbolic inference), with particular attention to maintaining accuracy and compatibility across different hardware and software configurations. Some issues address build system inconsistencies, including proper linking and environment variable management, especially for custom extensions and external libraries. Unresolved questions include how to better detect and prevent environment or dependency mismatches and how to automate or streamline disabling flaky tests without masking underlying problems."
2024-05-28,pytorch/pytorch,"The discussions cover several key issues, including the need for clearer handling of dispatch modes in Dynamo (particularly regarding FakeTensorMode during compilation), concerns about accuracy differences between CPU and MPS devices (notably in transcendental functions), and the importance of maintaining proper tensor attributes and metadata during autograd and graph transformations. Some suggest avoiding in-place modifications or relying on cloning/deepcopy logic to prevent in-place errors and preserve custom attributes across backward passes. There are also repeated concerns about flaky tests, CI stability, and the necessity of sign CLA for contributions, as well as discussions on improving code robustness, test coverage, and documentation clarifications. Overall, unresolved questions involve better cache strategies, safer handling of dispatch modes, and ensuring compatibility across hardware and software versions."
2024-05-29,pytorch/pytorch,"The comments showcase a variety of technical concerns including GPU compatibility issues, notably with different CUDA versions and hardware (e.g., RTX 2080 Ti, A100, Mac M1/M2), which often require environment adjustments like matching CUDA versions or recompiling libraries. Several discussions address accuracy and performance tradeoffs, such as the imprecision of fast math libraries on MPS (Apple Silicon) and the potential benefits of custom kernel implementations or alternative math libraries like sleef. Issues surrounding API deprecation, API compatibility across devices and platforms, and the need for better tooling, testing, and documentation are recurrent. There are also concerns about build stability, merge failures, and CI flakes, alongside feature requests for better API support (e.g., for quantization, module handling, and static vs dynamic shapes). Unresolved questions include the justification for specific implementation choices (e.g., using fewer CPU threads), support for newer tools/libraries, and how to improve user guidance and migration paths for deprecated or unsupported features."
2024-05-30,pytorch/pytorch,"The comments reveal several recurring technical concerns and questions: (1) there is ongoing discussion about the imprecision of MPS transcendental functions and whether providing explicit, fast math kernels would improve results or performance; (2) issues with CUDA kernel performance and correctness, especially related to specific hardware (e.g., A100, H100, AMD GPUs), are noted, with some suggesting kernel splitting or hardware-specific optimizations; (3) ensuring proper handling of tensor alignment, memory addresses, and device-specific behaviors in both inductor and runtime contexts is a challenge, including cache correctness and guard guards; (4) updates to infrastructure scripts (e.g., Docker, TorchRock, Rockset ingest) are needed, along with questions about test stability across platforms and consistent environment configurations; (5) several issues involve testing stability, correctness of model serialization, and handling of special cases like complex types, quantization, or dynamic shape graphs, often requiring further investigation or new test cases."
2024-05-31,pytorch/pytorch,"The discussions span multiple issues concerning PyTorch's development, including environment and build dependency management (e.g., CUDA and cuDNN versions, container images), code support on different hardware backends (e.g., MPS, XPU, Triton), and feature stability (e.g., TorchScript, ONNX export, operator implementation). Concerns about performance, correctness, and hardware compatibility are frequently raised, often with suggestions to improve documentation, add tests, or revert problematic changes. Several issues involve deep dives into low-level kernel behaviors, hardware specifics, or build configurations, highlighting ongoing efforts to stabilize and optimize the framework across platforms. Questions about support timelines, contributions, and upstreaming modifications are common, as well as maintaining backward compatibility and handling internal infrastructure challenges. Overall, the discussions reflect active troubleshooting, feature enhancement, and ensuring robustness across diverse environments."
2024-06-01,pytorch/pytorch,"The comments highlight ongoing development and bug fixing efforts in the PyTorch repository, including adding new optimizer and function support, improving runtime graph handling (such as FSDP wrapping and DDP graph partitioning), and addressing internal and external stability issues, especially with CUDA, cuDNN, and hardware compatibility. Several discussions concern potential regressions, flaky tests, and internal bugs related to gradient and buffer management, as well as environment-specific concerns like CPU affinity and environment reproducibility. There are also administrative notes about merging workflows, stale PRs, and dependency updates, alongside reproducibility and correctness verification needs—such as unit tests, profiling logs, and documentation clarifications. Overall, unresolved questions remain about reproducibility of certain failures, the stability of in-progress features like hierarchical compilation and inductor optimizations, and the correctness of graph inlining and serialization, often tied to environment-specific issues or internal refactoring."
2024-06-02,pytorch/pytorch,"The comments cover multiple technical issues including the need for defining `PY_SSIZE_T_CLEAN` before including `Python.h` to handle string lengths properly, and discussions around fixing bugs related to CUDA and PyTorch features such as `torch.compile`, JIT optimization disabling, and model tracing/exporting. Several comments address build and integration problems, such as resolving header include cycles, build failures on different platforms, and CUDA-related errors with nvToolsExt and CMake configurations. In addition, there are concerns regarding correctness and robustness of the IR transformations, handling of graphs at the IR level, and ensuring proper reporting and detection of internal compiler errors or regressions. Unresolved questions include how to improve error reporting, handling of specific code generation bugs, and better management of issue staleness and community feedback."
2024-06-03,pytorch/pytorch,"The discussions highlight challenges related to implementing and supporting advanced features in PyTorch, such as custom operators, device compatibility (e.g., HIP and xPU support), and in-place operation handling, which require careful API design and potential refactoring. Several issues involve addressing bugs or regressions introduced by recent changes or external dependencies (like cuDNN, ONNX, or third-party extensions), necessitating fixes, regressions tests, or documentation updates. There are also concerns about CI stability, warning management (e.g., related to unsupported extensions), and the need for better support for in-place and mutation operations, along with improvements to infrastructure, testing, and user experience. Some discussions suggest architectural enhancements, such as proposing a dedicated CPU stream for asynchronous operations or improving the graph export system to handle control flow and dynamic shapes more robustly. Unresolved questions remain around the best approaches for device handling, custom operator support, backward compatibility, and test reliability enhancements."
2024-06-04,pytorch/pytorch,"The discussions reveal various technical concerns, including supporting unsupported modules in `torch.compile` (e.g., wrapping third-party C++ extensions into custom operators), and issues with shape inference and shape guards, especially in the context of Dynamo and graph break handling. Several comments suggest potential solutions, such as wrapping unsupported modules as custom ops or modifying shape guard guards, but there are unresolved questions about the stability, correctness, and impact on performance. Additional concerns involve managing CUDA memory pools, supporting in-place operations, and addressing regressions caused by recent PRs, as well as CI stability and reproducibility issues, especially on different hardware architectures or due to environmental mismatches. Overall, the need for more robust support for custom modules, shape inference, and better testing/verification mechanisms in Dynamo and compilation pipelines remains prominent."
2024-06-05,pytorch/pytorch,"The discussions primarily address configuration issues with CUDA, cuDNN, NCCL, and environment variables, emphasizing the correct variable names (e.g., `NCCL_ROOT`, `CUDNN_LIBRARY`) for successful build and detection. Several reports highlight build failures, flaky tests, and inconsistencies in internal CI versus public checks, often linked to environment mismatches or dependency conflicts, especially regarding NCCL and cuDNN versions. There are suggestions to improve inductor's handling of guard conditions, kernel caching, and compatibility with various GPUs and backends, including addressing issues with `torch.compile` interactions, symbolic shape guards, and runtime optimizations. Some discussions involve the integration of external tools like oneDNN Graph, Triton, and handling of model serialization, while others deal with internal infrastructure tasks like Rebase errors and CLA signings. Overall, unresolved questions include environment setup correctness, build configuration nuances, and ensuring robustness of model export and runtime behavior across diverse hardware and software configurations."
2024-06-06,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's custom operator support, especially regarding supporting Python-only implementations, custom op decompositions, and maintaining performance. There are concerns about correctness and compatibility in complex scenarios like mixed in-place operations, dynamic shape handling, and guard mechanisms, with suggestions to improve guard propagation and module attribute tracking. Significant attention is given to build-related issues, such as resolving failures due to protobuf, CUDA version mismatches, and environment dependencies like MKL, with proposals to simplify dependency management and error reporting. Additionally, several issues involve debugging and optimizing deep learning models for specific hardware backends (e.g., MPS, ROCm, XPU), and managing CI flakiness by refining testing strategies, caching, and resource usage. Overall, the core questions revolve around improving operator support, debugging accuracy, build stability, and performance consistency across diverse hardware and software configurations."
2024-06-07,pytorch/pytorch,"The comments reveal ongoing concerns about NaN or instability issues when switching between eval() and train() modes, especially for models with fp16 and autocast, and why eval() mode often produces NaN outputs which train() does not. Several discussions suggest potential causes include stochasticity in testing conditions, effects of quantization, or specific CUDA/CuDNN behaviors and environment mismatches. Some troubleshooting efforts involve re-implementing certain quantized ops with `torch.nn.quantized.FloatFunctional()` or adjusting kernel configurations, and there are questions about proper handling of scalar/tensor view semantics and whether certain operations or API behaviors (e.g., buffer version bumping, `wrap_output_with_input_device_`) are correctly implemented. There is also concern about instability introduced by environment variables like cudnn version updates and how to reproduce or mitigate these issues systematically. Lastly, some unresolved questions involve validating environment correctness, testing on different hardware/software setups, and whether recent regressions are caused by code changes or system-level mismatches."
2024-06-08,pytorch/pytorch,"The discussions highlight issues around packaging and linking with CXX11 ABI wheels, especially on Linux and macOS, and the difficulty developers face in hybrid C++/Python solutions due to ABI mismatches. Support for MPS on Mac and ROCm on WSL2 remains limited or unimplemented, delaying hardware acceleration improvements. Several bug reports concern stability and correctness issues with PyTorch's JIT, FSDP, and various compilation passes, often linked to improper handling of tensor shapes, device contexts, or tied embeddings, requiring careful debug and robust testing. There are ongoing efforts to improve compile-time diagnostics, suppress guards, and validate the decomposition of ops in Torch export, but regressions and flaky tests complicate progress. Overall, unresolved questions focus on better tooling, bug isolation, test coverage, and ensuring stability across platforms and hardware."
2024-06-09,pytorch/pytorch,"The discussions highlight issues with PyTorch's support for quantized tensors, particularly the unsupported 'empty_strided' operation on quantized tensors, and the need to switch to `torch.nn.quantized.FloatFunctional()` to fix certain tensor addition errors. There are ongoing challenges with MPS backend support, such as missing operations like `aten::_ctc_loss` and handling of tensor strides, which are being addressed through support status updates and test additions. Several users inquire about enabling Arm64 builds, with some PRs successfully rebased or merged, but others failing CI checks due to missing approvals or test failures. Additionally, there are concerns about PyTorch's environment variable documentation, the handling of backpropagation with certain tensor views and backend metadata, and support for new features like `vmap` registration for external packages like PyTorch3D. Unresolved questions include whether certain features or support are integrated into stable releases, how to properly register custom backward functions for batched operations, and how to manage the complexity of code export and tracing issues."
2024-06-10,pytorch/pytorch,"The comments reveal recurring themes around stability, correctness, and usability of PyTorch features such as model export, autograd, and backend support. Several discussions focus on fixing specific bugs—such as shape inference issues with symbolic shapes, handling of data-dependent guards, or missing operator support in ONNX—and often relate to ongoing work, regressions, or the need for better testing. There are also concerns about API design (e.g., making certain functions private, support for custom ops and gradient computation) and environmental dependencies (e.g., CUDA/cuDNN versions, compiler compatibility). Additionally, multiple comments address stability issues like flaky tests, trunk flakiness, and CI failures, often with suggestions to improve error reporting, testing coverage, or stability mechanisms like ref cycle detection or guard improvements. Overall, unresolved questions include how to better handle data-dependent guards, support dynamic attributes with autograd, and improve documentation and test coverage for complex features."
2024-06-11,pytorch/pytorch,"The discussions highlight recurring issues related to PyTorch's dynamic and static shape handling, including normalization over variable dimensions, and the need for generalizing fused kernels to support various normalization types. Several threads focus on improving ONNX export reliability, especially with dynamic shapes, and ensuring operators like attention and FFT are correctly supported across different hardware and software versions, including CUDA, cuDNN, and device-specific backends. There are concerns about environment misconfigurations, runtime errors, and compatibility with hardware features such as AMX and BF16, requiring either API adjustments or fallbacks. Also noted are ongoing efforts to improve debugging, reproducibility, and documentation clarity to assist users and developers in troubleshooting and contributing. Unresolved questions include handling partial device capability coverage, better integration of profiling tools, and ensuring correctness and performance across diverse hardware targets."
2024-06-12,pytorch/pytorch,"The comments reveal ongoing challenges with custom operators and their decomposition, especially regarding performance and tensor/shape handling, such as the difficulty in decomposing certain correlation or correlation CUDA code into Python implementations. There's discussion about improving model export/import robustness, especially with dynamic shapes, control flow, and error handling (e.g., try/except, warnings). Reproducibility and caching issues are raised, including slow reinitializations in torch.compile, CUDA graph memory overhead, and bugs caused by device mismatches or shape invariants. Additionally, various build, CI, and integration issues are discussed, often related to ROCm, compiler configurations, or missing dependencies, highlighting the need for better diagnostic tools and workarounds. Unresolved questions include how to support weak references during tracing, handling nested/dynamic shape guards efficiently, and improving informativity and stability of cache and debugging facilities."
2024-06-13,pytorch/pytorch,"The compiled conversations reveal ongoing challenges related to PyTorch's computational graph handling, especially concerning in-place operations, tensor faking, and device synchronization. Multiple discussions address issues such as autograd support for WeakRef objects, FSDP's `requires_grad` constraints, and the management of dynamic versus static shape recompilations impacting performance and cache efficiency. There are also engineering considerations around extending API support for features like `clip_grad_norm_`, improving CUDA graph memory management, and refining operator behaviors for various data types and devices. Several entries highlight the need for better tooling, testing, and documentation updates to support these complex, low-level backend features. Unresolved questions pertain to correct handling of device inferences, cache inspection, and safe API modifications amid ongoing inductor fixes and feature extensions."
2024-06-14,pytorch/pytorch,"The comments highlight ongoing challenges in obtaining debug symbols for PyTorch builds, especially for cameras like GDB/lldb and local debugging with DEBUG=1, due to missing symbols or build configuration issues. Several discussions relate to improving and debugging distributed training issues, including DDP memory sharing, handling of global state like RNG, and the impact of certain compiler flags (e.g., -ffast-math) on numerical accuracy and determinism. There are also questions about existing support or planned support for specific features such as custom halide kernels, GPU support, and new operator functionalities (e.g., `aten::isin.Tensor_Tensor_out`). Some comments involve critical internal workflows, rebase conflicts, and CI/test failures that need resolution, as well as discussions on code quality checks (like pydocstyle). Overall, key unresolved issues concern better tooling, debugging capabilities, build configurations, and support for advanced features in PyTorch."
2024-06-15,pytorch/pytorch,"The discussions highlight several key issues: the need for fused kernels in certain APIs like RMSNorm and `aten::_ctc_loss`, and whether tensor operations (e.g., `mul_`) support specific compute data types like bfloat16 in various models and hardware configurations. There is a recurring concern about compatibility and performance, especially on new hardware (e.g., H100 vs A100) and with different frameworks or backends (e.g., HIPBLAS, cuDNN, XLA). Issues related to build regressions, such as missing or incorrectly ordered codegen calls, and how to optimize or debug caching mechanisms also feature prominently. Additionally, there are questions about support for features like TF32/BF32 in oneDNN, and the implications of API changes or internal API deprecations. Overall, the discussions focus on improving performance, compatibility, and debugging tools across diverse hardware and software environments."
2024-06-16,pytorch/pytorch,"The discussions highlight concerns about improving model debugging and visibility, such as checking `.grad` status for parameters, and displaying `.training` states for modules in `model` printouts to promote better training mode awareness. There are requests for new operator support, like `aten::upsample_bicubic2d.out`, and issues with symbolic shapes guarding in FX, including data-dependent guard errors. Performance evaluations question why small shape operations are slow, suggesting block size tuning, and there are requests for enhanced string support for data-loading and tokenization vocabularies. Additionally, several issues relate to CI failures, merge conflicts, environment setup, and access permissions, implying ongoing maintenance and integration challenges."
2024-06-17,pytorch/pytorch,"The comments encompass a wide array of technical concerns, such as: mechanisms for detecting and handling broken links in documentation, especially regarding external links and anchors; API exposure gaps around tensor slicing and advanced indexing; issues with quantized tensors and support for operations like `empty_strided` on quantized types; efforts to improve model tracing, profiling, and logging, including handling dynamic shapes, submodule annotation, and profiling tools; and infrastructure issues like CI failures, rebase conflicts, and dependency management, especially related to PyTorch's internal build and testing processes. Several discussions focus on API usability and stability improvements, including clarifications for custom autograd functions, handling of distributed setups, and bug fixes for specific features like FSDP or Transformer modules. Unresolved questions remain about better ways to track internal state (like final callbacks in autograd graphs), support for new tensor modules, and ensuring features work across different hardware and backend configurations (e.g., ROCm, Windows). Overall, the discussions mainly revolve around debugging, API enhancement, build stability, and ensuring correct functionality for experimental or complex features."
2024-06-18,pytorch/pytorch,"The discussions highlight issues with PyTorch's DataLoader iteration, such as the StopIteration error when using `next(iter(data_loader))` only once before training, which is expected behavior. Several questions seek guidance on improving kernel fusion, dtype precision handling in RMSNorm, and the support for different DDP/FSdp configurations, with some proposing technical solutions like passing pointers or inlining modules. There are concerns about correctness and stability in various features, e.g., handling of local variables in dynamo, model save/load consistency, and handling of special cases like `torch.triu` on different devices. Multiple issues address CI failures, flaky tests, or specific bugs (e.g., torch_geometric, jaxtyping, Triton-based kernels), often asking for clarification, workarounds, or targeted code changes. Overall, unresolved questions focus on stability, correctness, performance optimizations, and better developer guidance/documentation."
2024-06-19,pytorch/pytorch,"The discussions highlight significant performance and implementation challenges within PyTorch, including a 300x speed discrepancy potentially due to inefficient half-precision atomic adds in functional padding, and the need to adapt quantile functions for larger tensors on CPU. There are ongoing issues with compatibility and support, especially for AMD GPUs, ROCm versions, and the integration of features like ONNX export, with some PRs being reverted or blocked due to build failures or internal constraints. Several bugs and experimental features (e.g., support for inductor, dynamic shapes, and kernel auto-tuning) are under investigation, with proposals for improved diagnostics, automatic detection, and better experimental flag management. Additionally, there’s concern over the stability and correctness of certain autograd and caching mechanisms, along with the need for more comprehensive testing, especially for multi-GPU and distributed setups. Overall, key unresolved questions involve performance optimizations, cross-platform GPU support, bug fixes, and enhancing testing infrastructure for experimental features."
2024-06-20,pytorch/pytorch,"The discussions highlight persistent bugs in PyTorch, particularly involving complex operations like ONNX export of STFT, bug fixes that may affect performance or correctness, and issues with device-specific behaviors (e.g., CPU vs GPU discrepancies). Several problems relate to code stability, such as handling of mutability in custom ops, support for arbitrary functions in torch.compile, and the stability of certain configurations like HIPBLASLT and dynamic shape handling. There are questions about testing coverage for different devices and for specific features like sharding strategies, as well as concerns about CI failures and how to reliably reproduce and fix flaky tests. Unresolved issues also include handling of deprecated or legacy behaviors, and the need for improved tooling or tests to verify correctness and performance improvements across diverse hardware and workload scenarios."
2024-06-21,pytorch/pytorch,"The discussions highlight issues with PyTorch's handling of tensor cloning, shared memory, and distributed training workflows, particularly around the correctness and performance of lazy cloning and tensor sharding (e.g., FSDP). Several suggestions involve fixing or improving the in-place and out-of-place behaviors of `_lazy_clone`, ensuring compatibility with `share_memory_()`, and refining sharding strategies for tensor parallelism and FSDP usage. There are ongoing concerns about deep integration with Triton cache management, CUDA/NCCL errors, and how certain decorators and codegen can be optimized or fixed to prevent runtime errors and memory issues. Additionally, some questions address how to better test, verify, and document features like hybrid distributed training, tensor size handling, and environment configurations across various hardware and software setups. Several unresolved questions relate to performance impacts, compatibility with different hardware (e.g., AMD GPUs, XPU), and the correctness of gradient computations when applying modifications to tensor workflows."
2024-06-22,pytorch/pytorch,"The discussions highlight technical challenges such as handling zeroed parameters in loss calculations, managing `torch.no_grad()` use with accelerate, and limitations of operator support on MPS devices, especially for MacBook Pro M1. Several issues relate to build compatibility, especially with Clang 17+ and different GPU architectures like AMD ROCm, as well as the need for better documentation of internal dependencies and the implementation of tensor flattening methods for traceable subclasses. Some comments involve troubleshooting specific errors, such as CUDA-related crashes, and managing CI failures due to linting or platform-specific failures. Additionally, there are discussions about performance optimizations, such as minimizing overhead in gradient reduction and improving unsharding procedures with DTensors. Unresolved questions focus on implementation details for flattening tensor subclasses and timing for features like `unbind()`, as well as handling of legacy or incompatible hardware and compilers."
2024-06-23,pytorch/pytorch,"The discussions highlight issues related to PyTorch's JIT compilation, including potential workarounds and the use of environment variables like `TORCHDYNAMO_INLINE_INBUILT_NN_MODULES=1`, with suggestions to deprecate or improve the handling of such environment flags. Several comments focus on enhancing the implementation and testing of spectral norm, especially on convolutional layers, and ensuring consistent and broad applicability across different layers and models. There are concerns about build and installation complications, such as permission issues when installing via CMake, and the desire to support XPU devices in CI environments. Additionally, discussions touch on the need for better documentation, testing CUDA compatibility (including in non-CUDA devices), and the potential to migrate kernel compilation offline for efficiency. Unresolved questions include when certain features (like XPU support) will be integrated into CI and how to adapt the codebase for newer Python versions (>=3.12)."
2024-06-24,pytorch/pytorch,"The discussions highlight concerns about incomplete or inaccurate support for specific features or workloads in PyTorch, such as JIT/torchscript, ONNX export, distributed training, and custom operator support, often due to underlying implementation assumptions or dependency issues. Several issues address bugs or regressions, with some related to internal changes like shape analysis, CUDA/cuSPARSELt support, or the behavior of symbolic shape inference, and often involve questions about correctness, compatibility, or performance implications. There are also ongoing discussions about improving usability, documentation, and robustness of features like mixed precision, gradient handling, or memory tracking, with suggestions for better testing, explicit fallbacks, or more flexible APIs. Some points emphasize the need for clearer error handling, test coverage, and maintenance for evolving Python versions or third-party dependencies. Overall, the main concerns revolve around ensuring correctness, compatibility, and performance amidst rapid development and multi-platform diversity."
2024-06-25,pytorch/pytorch,"The discussions highlight various technical challenges, including the inefficiency of `itertools.cycle` leading to OOMs in data loading, and the need for native support in `DataLoader` for a `repeat=True` feature. There are ongoing concerns about adding symmetric padding, symmetric padding default, and support for specific operations like `nnz` on sparse tensors to improve usability and performance. Several issues relate to complex codebase interactions, such as maintaining correctness of `torch.compile()` with dynamic inputs, bias in upstream third-party dependencies, and the need for better testing, documentation, and build stability across architectures and platforms (e.g., Windows, ARM, XPU). Proposals include introducing new APIs, improving the serialization/deserialization workflows, enabling more efficient kernel compilation strategies, optimizing inductor/cudagraph handling, and refining the management of global/global-like states, especially for custom kernels and multi-device computing. Several unresolved questions concern implementation details, such as modifications to runtime behavior, exact kernel code generation, and integration patterns for new features, as well as ongoing CI stability and feature support considerations."
2024-06-26,pytorch/pytorch,"The discussions cover several technical issues: the absence of a robust `sqrtm` implementation in PyTorch and potential backward computation approaches; the addition of a `popcount` operator similar to NumPy's in PyTorch, including support for uint64 types; ongoing work on evaluating and ensuring new in-place functionalization (e.g., for FSDP, sequence parallelism, and other features) handles tensor strides and device placements correctly; concerns over profiling inaccuracies due to timestamp truncation in the autograd profiler; and challenges with exporting models involving `nn.MultiheadAttention` or certain custom layers to ONNX, especially when dealing with dynamic shapes and operator decompositions. Several discussions suggest accepting contributions or improvements for these areas, while unresolved questions focus on implementation details, testing coverage, and API behavior clarifications."
2024-06-27,pytorch/pytorch,"The discussions primarily revolve around memory management issues in PyTorch, especially related to dataset leaks when using pandas DataFrames or polars, as well as problems with the MPS backend on Apple Silicon Macs and CUDA kernel inconsistencies, such as NaN generation in flash attention. Several comments address the need for proper and efficient custom operators, especially for vision augmentations and image processing, suggesting potential graph breaks or new operator implementations. There are concerns about ensuring compatibility and correctness in model export, serialization, and tracing—particularly handling non-tuple outputs, dynamic shapes, and support for mixed dtypes, which may impact forward compatibility and runtime performance. Additionally, the ports of certain features like RISC-V kernels and support for non-standard data types are discussed, with some proposing gradual improvements or fallback mechanisms. Persistent issues also include test failures, CI inconsistencies, and permission-related delays that hinder merging, alongside proposals for better API design and test coverage."
2024-06-28,pytorch/pytorch,"The collected comments highlight ongoing challenges with PyTorch's compilation and export workflows, including handling non-tuple outputs from `torch._dynamo.export`, the need for clearer API support for dynamic shapes and guard conditions, and ensuring correctness when dealing with tensor subclasses and views in TorchScript and inductor compilation. Several threads discuss improving C++ code handling, integrating better error messages for unsupported patterns, and addressing compatibility issues such as duplicate function names, serialization of tensors with diverse dtypes, and cross-process tensor sharing. There are technical debates around implementation strategies, such as whether to duplicate class structures or refactor shared code, and concerns about robustness and user experience, including handling incomplete trace data or memory leaks. Unresolved questions involve specific API behaviors (e.g., effect tokens, `broadcast_from_rank0` on the Meta device), and how to ensure reliable guard conditions and graph correctness in the presence of dynamic shapes and advanced features like nested tensors."
2024-06-29,pytorch/pytorch,"The comments highlight ongoing issues related to complex number support in PyTorch, with the main workaround involving manual conversion between complex and real representations due to lack of native support in NCCL. Several discussions revolve around the limitations of certain operations on specific devices (e.g., aten::isin.Tensor_Tensor_out on MPS) and the need for better handling or fallback options. There are multiple references to PRs, merge failures, and the need for additional unit tests or code rebase, indicating active development and ongoing integration challenges. Some concerns focus on performance implications of using _foreach operations versus tensor lists, and the potential safety risks in autograd related to donating buffers during backpropagation. Overall, unresolved questions include progress on complex number DDP support, safety mechanisms for autograd optimization, and the integration of new APIs and features like unshard/reshard actions in parallelized training contexts."
2024-06-30,pytorch/pytorch,"The comments highlight a range of technical issues surrounding PyTorch's functionality, including system-specific problems like module restart post-suspend, driver/firmware mismatches, and CUDA/cuDNN compatibility, especially across different versions and hardware setups. Several discussions concern the correctness and safety of graph compilation and optimization strategies, such as donation of buffers during backpropagation, proper handling of tensor contiguity, and the interaction of in-place operations with autograd's saved tensors, raising questions about runtime safety checks versus compile-time assumptions. There are also frequent concerns about development workflows, including handling of merge conflicts, test failures, and the need for sign-offs like CLA signatures, alongside evaluations of features like `torch.compile`, FX graph transformations, and distributed parallelization plans. Issues also relate to integration with external tools (e.g., Hugging Face models, profiling tools) and the stability or support for newer hardware features or software versions (e.g., SM90 support, CUDA 12.5, PyTorch nightly). Unresolved questions revolve around ensuring correctness of autograd buffer donation, handling of garbage collection for cached code entries, and the proper configuration for distributed module parallelism, indicating areas needing further debugging, testing, or design clarification."
2024-07-01,pytorch/pytorch,"The discussions mainly revolve around issues with loading and restoring model and optimizer states across different devices and formats, such as CUDA and CPU, with particular attention to the correct use of `map_location` during `torch.load()`. Several comments highlight device mismatch errors, device-specific code variations, and differences in behaviors between CUDA and CPU, especially when loading optimizer states or models saved in different environments. Some suggestions involve explicitly transferring optimizer states to the correct device tensors after loading, or restructuring loading code to avoid device mismatch errors during `optimizer.load_state_dict()`. There are also concerns about specific function bugs, such as deprecated arguments, reconciling Float16/BFloat16 precision, and ensuring correctness when using lower-level operations like `avg_pool2d_backward`. Overall, unresolved questions include why certain `map_location` configurations don't solve device mismatch issues, and how to reliably load optimizer and model states with device consistency."
2024-07-02,pytorch/pytorch,"The discussions reveal ongoing challenges with PyTorch’s inductor, especially related to CUDA and ROCm device support, compile times, and numerical accuracy, with several issues involving backend-specific bugs, performance regressions, and build failures. There’s interest in enabling support for more hardware and data types (e.g., complex, BFloat16, SM90a), along with procedures for managing third-party dependencies like composable kernels and Cutlass, either via submodules or environment variables, but clarity on the correct usage patterns and impact remains incomplete. Some threads address API improvements, particularly around scope-aware checkpointing and tensor lifecycle management, with suggestions for user-facing APIs that specify tensors' recomputation or save/restore behaviors more explicitly. Several issues concern build stability, test failures, and the need for better error messaging, handling, and reproducibility, especially across different platforms and Python versions. Overall, unresolved questions about API semantics, dependency management, and ensuring correctness/performance trade-offs persist in the discussions."
2024-07-03,pytorch/pytorch,"The discussion highlights several technical concerns including the development and integration of advanced features such as depthwise convolutions upstreaming in PyTorch, handling shape mismatch issues during loading weights, and the need for better support and testing of dynamic shapes and non-contiguous tensors, especially on different devices like MPS. Several issues relate to CI stability, environment setup, and compatibility with various hardware/software combinations, such as CUDA, MKL, Triton, and operating system specifics, often requiring workarounds or additional code changes. There are recurring questions about the proper design and maintenance of APIs, especially around the handling of fake tensors, shadow storages, and the interaction between different components like FX IR, inductor, and backend compilation. Some discussions also emphasize improving developer experience through better debugging tools, test coverage, and documentation, with several proposals for refactoring or adding new features to address these challenges. Overall, unresolved questions involve ensuring robustness, compatibility, and correctness across diverse configurations, while ongoing efforts aim to enhance performance and developer workflows."
2024-07-04,pytorch/pytorch,"The discussions highlight ongoing issues with PyTorch model conversion, such as failures in JIT tracing and ONNX export due to unsupported operators (e.g., `aten::fill_`, `quantized::batch_norm2d`, complex type support in `stft`). Several users report runtime errors and internal bugs, with efforts to fix specific operators, improve compatibility, and add support for new ops. Regressions, especially on CPU performance and precision in AMP, are noted, alongside infrastructure-related concerns like environment setup and device synchronization. There are also technical questions about proper initialization code, backend registration mechanisms, and the correct way to handle process groups. Overall, unresolved questions revolve around operator support coverage, correctness of model conversion workflows, and stability in multi-device, quantization, or quantized operator contexts."
2024-07-05,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for clearer documentation and consistent terminology around device management (e.g., `torch.cuda.set_device`, `torch.device`). Multiple issues address feature support and bug fixes, such as supporting complex types in ONNX export, TorchDynamo's compatibility with subinterpreters, and the implementation of `unshard()` and `reshard()` APIs for better FSDP and pipeline parallelism. Performance and compilation time optimizations are also central, with discussions on kernel fusion, loop ordering after fusion, and Trito n/inductor compilation efficiency. Several unresolved questions pertain to implementation details, user API design choices, and ensuring stability and correctness in various hardware and software configurations."
2024-07-06,pytorch/pytorch,"The discussions highlight ongoing challenges with supporting complex-valued neural networks in PyTorch, particularly regarding NCCL and DDP, with some scripts working on nightly builds suggesting fixes. There are multiple feature requests and improvements, such as adding `aten::isin.out`, handling `aten::angle`, and addressing device-specific operator implementation issues (e.g., MPS, MPS fallback errors, and placeholder storage errors). Several issues concern CI stability, such as build queues, flaky tests, and merge conflicts, often requiring reverts or force merges. Questions also arise around extending support for new data types like bfloat16, along with quality-of-life improvements like custom ops, cache management, and code refactoring to address circular dependencies. Unresolved questions include proper handling of device compatibility, fallbacks, and testing for emerging hardware support, with some discussions about potential regressions and performance impacts."
2024-07-07,pytorch/pytorch,"The discussions highlight ongoing challenges with large matrix operations in PyTorch, such as SVD and eigenvalue computations, especially regarding memory constraints and optimization workarounds, including manual block algorithms. Several issues involve hardware-specific operator support, notably with MPS and device compatibility for operations like `aten::_linalg_solve_ex`, `grid_sampler_3d`, and others, with efforts to accelerate functions like `torch.linalg.eigh`. Performance discrepancies on different hardware (Mac, CPU, GPU) are noted, with detailed benchmarking and points about how environment and hardware differences impact speedup gains. There are also concerns around bug fixes, operator implementation status, and the need for better debugging strategies, as well as tracking merge conflicts, build failures, and code stability in ongoing development. Overall, unresolved questions revolve around improving operator support across devices, optimizing performance, and managing complex codebase modifications."
2024-07-08,pytorch/pytorch,"The discussions highlight concerns about the availability and accessibility of previous libtorch and CUDA builds, with users seeking solutions to obtain older or specific versions, especially for CUDA support. Several issues relate to unexpected errors during model conversion, training, or export processes, such as shape mismatches with `adaptive_avg_pool2d`, gradient hooks, or unsupported data types like `bfloat16`. There are recurring technical questions about handling legacy hooks, improving error messaging, ensuring compatibility across hardware platforms, and managing complex dynamic shapes or cache mechanisms. Additionally, suggestions include enhancing documentation, stability of build processes, and ensuring proper environment configuration (like CUDA_HOME or NCCL) to prevent runtime failures. Unresolved questions revolve mostly around specific bug fixes, rebase conflicts, and ensuring reproducibility and stability across diverse hardware and software setups."
2024-07-09,pytorch/pytorch,"The comments reveal several recurring themes: concerns about CUDA tensor sharing in multi-process data loading warnings and collate_fn device push warnings, issues with large package sizes and Docker image bloat, and challenges in implementing device-aware features like stream control, dynamic shape support, and enhancements in PyTorch's automatic differentiation and compilation workflows. Specific technical questions include the proper handling of device-specific operations to avoid synchronization overheads, the need for more flexible APIs for checkpointing and tensor mutation, and ensuring correctness and efficiency across different hardware backends (CPU, GPU, XPU, MPS, ARM). Several suggested solutions involve better documentation, more modular code, and mechanisms like environment flags, specialized APIs, or improved compiler passes. Unresolved questions focus on how to reliably propagate custom tensor info, manage device contexts in distributed settings, and optimize for performance and correctness in evolving hardware environments."
2024-07-10,pytorch/pytorch,"The discussions highlight several key issues: The potential BC break caused by changing the signature of dummy functions like `__repr__` in symbolic graph components, and the need to support stream-based parallelism (such as in custom compiler passes and inference overlap) within PyTorch's compilation and lowering infrastructure, notably with inductor and custom ops. There's concern over current limitations in data loading for variable-length and complex transform pipelines, prompting the development of a custom multithreaded pipeline. Several user-reported bugs involve device and environment inconsistencies, such as NCCL errors, CUDA initialization failures, and mismatched tensor strides, often tied to specific versions or hardware setups. Additionally, issues around binary compatibility, API stability, and precise output/memory layout preservation (including strides and FakeTensor serialization) are recurrent topics, along with ongoing efforts to improve diagnostics and build system robustness."
2024-07-11,pytorch/pytorch,"The comments highlight several recurring issues: the need for a more robust and consistent approach to handling environment-specific and hardware-dependent features such as CUDA, MPS, and ROCm support; the challenge of supporting dynamic and discontiguous strided tensors in custom kernels and inductor, including whether to match exact strides or just order; the difficulty in correctly implementing and testing various operators (e.g., `aten::prelu`, `kthvalue`, `isin`) especially with meta tensors and during model export or compilation; questions around lazy loading and memory management in model pipelines (e.g., with FSDP) and how to reliably detect and integrate environment dependencies such as NCCL versions; and broader concerns about CI reliability, test coverage for edge cases, and the impact of code refactoring on existing behaviors and performance."
2024-07-12,pytorch/pytorch,"The comments highlight several recurring issues and questions within the PyTorch repository, including challenges with serializing non-leaf tensors requiring_grad across GPU processes, particularly in multiprocessing contexts, and the need for improved support and testing for distributed operations like all-reduce semantics and backward behaviors. There are concerns about BC-breaking changes and interface consistency, especially when handling custom deallocators or new metadata in graphs, as well as the impact of environment-specific bugs, such as those on M1 Macs or AArch64 platforms, related to operator support and floating point inconsistencies. Additionally, questions about build system updates, third-party dependencies (like pybind11), and environment configurations (e.g., for ROCm or CUDA versions) are evident. Several discussions also revolve around merge failures, test flakiness, and version management, emphasizing the ongoing need for robust CI, backward compatibility, and clear documentation when introducing new features or altering existing interfaces."
2024-07-13,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch model export and ONNX runtime compatibility, particularly regarding input naming conventions and handling dynamic shapes. There are concerns about CUDA version support, especially for CUDA 12.3, with suggestions to clarify requirements and improve installation instructions. The complexities involved in supporting mutation operations like `set_` and `resize_` within graph transformations and inductor are debated, with proposals to avoid fully functionalizing certain ops to maintain simplicity. Several PRs require additional review, approval, or coordination, notably around backward-compatible changes, build configurations (e.g., MKL linking), and ensuring tests pass across diverse environments. Unresolved questions include improving error diagnostics for distributed NCCL issues and managing dependency optionality for features anticipated in future releases."
2024-07-14,pytorch/pytorch,"The discussions highlight concerns about incomplete or unimplemented operator support on specific devices like MPS, especially for operations such as 'aten::upsample_bicubic2d.out', and the need for a fallback mechanism. There are questions about improving dataset handling in distributed setups, specifically relating to process termination via signals like SIGINT/SIGTERM, and making spawn/worker start methods more reliable. Several comments address performance issues in numerical computations, such as BF16 precision limitations and potential impact of tensor manipulations in models, with suggestions to analyze whether certain code changes are beneficial or introduce overhead. Other discussions focus on build configuration issues, such as MKL static linking on Windows, and code quality concerns like code duplication and maintainability in kernel implementations. Unresolved questions include better support for tensor and scalar uniformity in API functions, handling internal code optimizations, and ensuring robustness in multi-threaded and multi-process environments."
2024-07-15,pytorch/pytorch,"The discussions broadly raise concerns around missing features, bugs, or stability issues in PyTorch and related tooling, such as the absence of static Libtorch libraries in certain releases, and problems exporting quantized models to ONNX, particularly supporting quantized_decomposed ops. Several issues involve debugging failures or crashes (e.g., segmentation faults, GPU memory management, or build time regressions), with suggestions for workarounds like environment recompositions, code modifications, or using specific APIs (e.g., torchrun instead of mp.spawn). There are questions about the correctness of recent optimizations, such as vectorized kernels or graph transformations, and whether certain behaviors (like conjugate key handling in tensor subclasses) require stricter invariants or manual intervention. Others discuss the stability and testing of custom features (e.g., CUDA IPC, distributed backends, or specific operator support) as well as ongoing efforts to refine multi-device, cross-operator, and profiling semantics—highlighting unresolved issues such as flakiness, support for newer hardware, or correctness of complex conversions. Overall, the conversations reflect active troubleshooting, feature evolution, and validation challenges within the PyTorch ecosystem."
2024-07-16,pytorch/pytorch,"The discussions highlight several concerns, including differences in median implementation between PyTorch and NumPy, with suggestions to clarify this in the documentation or rename functions. Numerous issues address accuracy and correctness, such as implementing spectrally normalized layers for convolutional operations, fixing memory leaks, or handling tensor dtype conversions and scaling for quantization, especially on ROCm and ARM platforms. There are recurring questions about supporting or improving features like TF32 support on ROCm, CUDA pinned memory behavior, and the behavior of non-blocking data transfers. Several proposals involve refining APIs, adding validation tests, supporting specific hardware capabilities, or addressing build and compatibility issues across platforms and dependencies. Unresolved questions remain about implementation timelines, detailed API behaviors, and maintaining cross-platform, hardware, and software compatibility."
2024-07-17,pytorch/pytorch,"The discussions reveal various technical concerns such as inconsistent GPU/memory reporting, especially with PyTorch's caching mechanisms versus NVIDIA's tools, and potential errors or regressions in CUDA or ROCm device-specific code (e.g., NCCL, inductor). Several issues relate to API discrepancies (e.g., transpose/API standard differences, `torch.Size` behavior, and argument handling) and potential bugs in internal implementations (e.g., in `autograd`, `cudagraphs`, `sympy`, and the new inductor features). Some discussions focus on the need for better testing, documentation, and stability, especially regarding experimental or evolving features like dynamic shapes, cache strategies, and support for various hardware backends (XPU, ROCm, ARM). There are ongoing efforts to fix specific bugs, improve compatibility, and clarify APIs, with some proposals for refactoring or standardizing behavior (e.g., in the way metadata or caching keys are handled). Unresolved questions include ensuring consistent device handling, supporting new hardware features, and clarifying the behavior of advanced features like tensor subclassing or API standard compliance."
2024-07-18,pytorch/pytorch,"The discussions highlight ongoing concerns about the implementation details and correctness of Batch Normalization variance estimation, especially whether to use Bessel's correction during training and inference, and compatibility with frameworks like ONNX. Several issues relate to bugs or discrepancies in eigenvalue computations, especially with torch.linalg.eigh and torch.lobpcg, including sorting and correctness in small or sparse matrices. There are multiple reports of intermittent and flaky test failures, often related to distributed, JIT, dynamic shape, or tensor subclass issues, sometimes mitigated by reruns or rebaselining. Some requests focus on API behavior, such as adding support for certain quantization operations, handling data-dependent shapes, or better documentation of environment variables. Additionally, internal implementation challenges, like reworking deepcopy semantics, handling uninitialized backends, and addressing compiler-specific errors, remain active topics needing resolution."
2024-07-19,pytorch/pytorch,"The comments reflect various issues encountered after PyTorch upgrades, including web documentation index de-indexing for old docs, backward pass errors related to in-place modifications, autograd with tensor subclasses and dynamic shapes, and system-specific bugs such as incompatibilities or missing implementations for certain operators on different hardware backends (e.g., MPS, ROCm). Several discussions focus on improving the robustness of PyTorch’s automation and runtime features, such as re-entrant modes, dynamic shape handling, memory management, and compatibility of quantization with AOT compilation. There are also notes on CI flakiness and platform-specific failures, suggesting improvements in test reliability and environment handling. Common themes include fixing backward errors, better shape and operator support, and clarifying documentation or framework behaviors to avoid inconsistencies."
2024-07-20,pytorch/pytorch,"The discussions highlight issues with PyTorch's support and implementation of various operators across devices, including unimplemented operators on MPS, and the need for prioritization of such operator support. Several comments concern environment setup, particularly how to correctly specify library paths for CUDA and cuDNN, emphasizing the importance of setting `LD_LIBRARY_PATH`. There are ongoing efforts to improve error messaging and robustness, like handling dtype in `mean()` and `nanmean()` functions, and ensuring consistency across distributed compute processes to prevent NCCL timeouts. Some debates focus on developing and testing custom kernels or features as external projects before contributing to PyTorch, and the challenges posed by complex backends such as Triton and the shape handling in model components like cross-entropy or multi-head attention. Overall, unresolved questions pertain to operator support priorities, environment configuration, error handling clarity, and best practices for external kernel development."
2024-07-21,pytorch/pytorch,"The issues mainly revolve around limited or missing support for sparse tensor operations (especially batched matrix multiplications) on different devices (e.g., MPS, XPU, CUDA), with requests for enhancements like batching sparse-sparse and sparse-dense batched matmul. There are concerns about the implementation status of features such as `torch.matmul` with sparse matrices, `flash attention` support, and methods like `isin.Tensor_Tensor_out`. Some discussions address performance optimization and compatibility issues, such as upgrading dependencies or handling device-specific kernel failures. In addition, there are questions about internal infrastructure like better code generation for accelerate kernels and ensuring deterministic generated code. Overall, the discussions highlight ongoing efforts to expand sparse tensor support, improve device-specific features, and address performance and robustness concerns."
2024-07-22,pytorch/pytorch,"The discussions highlight persistent issues with experimental features and internal implementation details in PyTorch, such as handling in-place tensor modifications, CUDA stream overlaps, and NCCL communication overlaps, often tied to specific hardware or software configurations (e.g., ROCm, CUDA versions, hardware platforms). Several threads address build failures, incompatible dependencies, or CI flaky tests needing better control or monitoring mechanisms, with some requiring system or environment modifications (disabling IOMMU, updating libraries). There are also ongoing investigations into correctness and performance regressions introduced by recent PRs, especially around CUDA kernel fusion, shape handling, and the effects of symbolic shape propagation in various backends. Proposals include adding user-controllable flags, better testing, and early validation points, as well as architectural changes like explicit annotation of independence or shape properties to improve compile-time optimizations. Unresolved questions focus on compatible configurations, correct error detection, and infrastructure improvements for reproducibility and debugging."
2024-07-23,pytorch/pytorch,"The comments primarily revolve around issues with PyTorch's internal CI stability, build times, and flaky tests, especially on ROCm and trunk servers, often requiring manual retries or force merges. Several discussions involve reproducibility and correctness of features such as `torch.compile`, device handling (e.g., CPU, MPS, AMD/ROCm support), and backward compatibility concerns, including API deprecations (e.g., `upsample_rate`) and type annotations for mypy. There are technical questions about specific bugs (e.g., data-dependent tensor operations, CUDA IPC, graph breaks with specific operators, and the handling of scalar tensors in optimizer state dicts), alongside proposals for code refactors and testing improvements (e.g., including real-world data, more thorough type annotations, and more stable CI). Unresolved issues include runtime exceptions, large build durations, and ensuring proper testing and infrastructure support across heterogeneous platforms. Overall, the focus is on stability, correctness, and improving developer workflows in complex multi-platform environments."
2024-07-24,pytorch/pytorch,"The comments reveal various technical issues including code errors (e.g., incorrect use of C++ `static_cast`, torch operator not implemented on specific devices), performance regressions, and correctness concerns with models such as diffusion and large language models on different hardware (e.g., M1, MIG partitions, ROCm). Several discussions focus on build environment configurations, such as ensuring proper CUDA_VISIBLE_DEVICES setup, handling of dependencies (e.g., numpy versions), and compatibility issues across platforms (Windows, Linux, macOS). There are also efforts to fix bugs related to torch's internal mechanisms, like call of in-place operations, operator dispatch, or trace handling, and discussions about workload performance, queueing, and flakiness. Unresolved questions include ensuring consistent behavior across diverse hardware/software setups, understanding the causes of flaky tests, and improving environment management, especially for multi-GPU or MIG scenarios."
2024-07-25,pytorch/pytorch,"The discussions encompass several technical issues and feature proposals related to PyTorch, including workarounds for tensorboard logging nuances, understanding the semantics and backward pass of distributed all-reduce operations, support and performance considerations for channels-last convolutions, and various bug fixes and regressions across platforms like MPS, NCCL, and ROCm. Key suggestions involve adding more detailed profiling and testing for collective communication latency, enabling warm-up strategies in torch.compile, and improving support for dynamic shapes and non-contiguous tensors. Several regressions and flaky tests are acknowledged, with ongoing efforts to address underlying platform-specific bugs, improve stability, and enhance performance benchmarking. Unresolved questions focus on the best default behaviors for CUDA graph usage with dynamic shapes, integration timelines for new hardware like Hopper GPUs, and handling of backends or runtime configurations influencing reproducibility and stability."
2024-07-26,pytorch/pytorch,"The discussions highlight concerns about new features' implementation, especially regarding pre-padding in sequence padding and complex tensor gather operations, with suggestions for more efficient or flexible approaches. There are issues related to platform support and device enumeration, particularly for XPU/ROCm, emphasizing the need for accurate hardware detection, support lists, and user experience considerations. Several reports indicate debugging challenges, such as runtime errors, NCCL communication failures, and environment configuration problems, often needing better logging, test coverage, or workaround solutions. Merging and CI stability concerns are also noted, with multiple instances of flaky tests, reverts, and merge conflicts, underscoring the importance of stable testing and review processes. Overall, the discussions suggest ongoing efforts to improve functionality, robustness, hardware compatibility, and developer tooling, with some unresolved questions about optimal implementation strategies and environment handling."
2024-07-27,pytorch/pytorch,"The discussions predominantly revolve around optimizing CPU-GPU data transfers and pinned memory management, with suggestions to use `cudaHostRegister()` via ctypes or pre-allocating pinned tensors, despite its potential latency and complexity in multiprocess contexts. There are concerns about the efficiency and feasibility of shared pinned memory across processes, and the need for API or internal API adjustments to better support long-lived shared tensors. Several issues pertain to build environment setup on Windows, such as missing DLLs and the need for installing C++ build tools to resolve runtime errors. Ongoing questions include metrics collection (hardware counters availability on AWS), correctness of dynamic shape logic in TorchDynamo, and handling of `Literal` annotations in JIT, with some proposal to relax or bypass certain tests due to JIT limitations. Overall, the discussions include both low-level performance optimizations and higher-level API and tooling improvements."
2024-07-28,pytorch/pytorch,"The discussions primarily revolve around supporting complex or unsupported operations in PyTorch and ONNX, such as layer support (scatterNDupdate) and operator fallback behaviors, with some focusing on improving kernel performance and compatibility, particularly with GPU backends like CUDA and VPU. Several issues highlight the challenges of migrating models due to fallback op support, MPS, and hardware limitations, especially on ARM and Mac architectures. There are discussions about optimizing or replacing existing primitive operations (e.g., scan, in-place ops) and regarding visibility or accountability of certain kernel limitations (e.g., cublaslt, second derivatives). Some threads also concern build and environment setup issues, including cross-platform dependencies and the impact of external tools like Microsoft Visual Studio or toolchains. Unresolved questions include how to enhance backend support, measure performance impacts of these changes, and improve build or runtime compatibility across varied hardware and software configurations."
2024-07-29,pytorch/pytorch,"The comments primarily revolve around implementation challenges and technical nuances in PyTorch features such as CUDA kernel behaviors, NCCL backend errors, and FSDP (Fully Sharded Data Parallel) setup, particularly on complex multi-GPU or low-capability hardware. Several discussions highlight issues with in-place tensor operations, guard management, and inaccuracy or randomness in numerical computations, especially with large tensors or mixed precisions (float, bf16, int8). There is frequent mention of build and environment dependencies, including CUDA, NCCL, MKL, and device support, emphasizing the importance of proper configuration, hardware compatibility, and reproducibility. Additionally, questions about extending functionalities (like custom fields, operators, or external IO protocols) and documenting behaviors for users and developers are raised. Overall, the focus is on debugging, performance optimization, ensuring correctness, and improving usability and support for diverse hardware and software configurations."
2024-07-30,pytorch/pytorch,"The comments reflect a range of technical concerns and discussions around PyTorch development, including issues with large tensor sizes exceeding INT_MAX, potential regressions related to deterministic algorithms, and support for ARM64 Docker images on M1/M4 chips. Several reports highlight reproducibility challenges and performance regressions after updates or specific changes, with suggestions to improve test coverage, handle discontiguous tensors, or modify internal guards to better detect input changes. There are also ongoing efforts to fix build issues, enhance support for features like nested tensors, and streamline complex workflows such as module inlining and kernel patching. Overall, unresolved questions revolve around ensuring correctness and performance across diverse hardware/OS environments, improving build and runtime robustness, and refining the handling of dynamic shapes, guards, and custom operators."
2024-07-31,pytorch/pytorch,"The comments largely revolve around performance, correctness, and API consistency issues in PyTorch. Several threads highlight discrepancies between eager and compiled or inductor outputs, often involving numerical inaccuracies (e.g., with float8 or floating point rounding), or unexpected behavior such as silent silent errors or NaNs, sometimes related to CUDA/cuDNN or backend versions. Others discuss API design concerns, including the need for clearer documentation, support for custom tensor subclasses, device-agnostic interfaces, and behavior of functions like `.tolist()` or device-related attributes. There are also multiple reports of flaky tests, CI failures, and platform-specific bugs, with suggestions to improve robustness and user feedback mechanisms. Overall, the discussions focus on fixing bugs, improving tensor and model support, and clarifying how features like caching, caching, and device handling should work across different scenarios."
2024-08-01,pytorch/pytorch,"The discussion covers several technical issues and questions related to PyTorch development and support, including challenges with header inclusions in C++, CUDA/NCCL system-level connections, and nuanced behavior of symbolic shape inference, especially in the context of tracing, compilation, and nested tensor handling. Concerns are raised about correct implementation of tensor serialization/deserialization, ensuring proper locking for file generation paths, and handling dynamic shapes during export and tracing, with particular attention to the behavior of symbolic sizes and the impact of system configurations like IOMMU or specific hardware like MPS. Multiple proposals involve fixing implementation bugs, updating test coverage, and clarifying behavior around shape constraints, with unresolved questions about best practices for extending PyTorch's abstraction layers, maintaining compatibility, and system-level system connection issues affecting NCCL. Several discussions also touch on release and dependency management, including the creation of source distributions and PyPI package considerations. Overall, the conversations focus on improving robustness, correctness, and system compatibility of PyTorch's features."
2024-08-02,pytorch/pytorch,"The discussions raise concerns about improving PyTorch's support for workload-specific, efficient, and hardware-aware tensor operations, such as custom ops for image augmentation and low-level memory management, often involving complex device and layout handling. Several issues highlight the challenge of ensuring correctness and performance in heterogeneous execution environments—especially when dealing with representations like DTensors, layout inconsistencies, and device-specific behaviors (e.g., MPS, CUDA, ROCm). There is frequent mention of infrastructure and CI flakiness, build problems, and the need for clearer APIs or capabilities for profiling, debugging, and reproducibility, including handling of effects, effects tokens, and symbolic shape guards. Some conversations focus on extending PyTorch's internal APIs (hooks, custom operators, tensor representations), while others emphasize the importance of correctness and efficiency in the runtime, especially regarding re-compilation, tensor layout, and synchronization issues. Overall, the key unresolved questions relate to API design, correctness guarantees, and robustness in heterogeneous, dynamic, and graphics-assisted execution scenarios."
2024-08-03,pytorch/pytorch,"The discussions highlight several technical concerns, including incomplete or outdated feature support for MPS and Windows ARM64 in PyTorch, and the need for clearer documentation on behaviors like `model.forward()` restrictions and stride fixations. There's a focus on fixing bugs related to CUDA index limitations, improving the reproducibility and stability of checkpoint loading with FSDP/DTensor, and addressing performance regressions—sometimes with heuristics like disabling slow kernels. Additionally, there are questions about the potential for higher-order operations in exported programs, handling custom allocators, and the impact of alterations like `torch.compile()` on graph representability. Many discussions also involve merge conflicts and CI/CD failures, along with requests for new test cases and clarifications on internal mechanisms, such as IR transformations and IR IRIR.IR effects in the compiler."
2024-08-04,pytorch/pytorch,"The discussions primarily revolve around environment and compatibility issues, such as system libraries conflicting with system-installed or prior versions of PyTorch, requiring users to switch to conda or specific package versions for stability. Several users report errors related to missing operator implementations on certain devices like MPS or issues with specific library versions (e.g., torch-sparse, torchtext), often resolved through downgrades or environment adjustments. There are ongoing considerations about improving compatibility and performance, including setting CUDA architecture lists, benchmarking masked top-k operations, and managing compiler or library build configurations. Recurrent challenges include merge conflicts, CI failures, and the need for better testing, especially across different hardware platforms (e.g., Windows, Mac, ARM). Many comments also indicate active efforts to merge PRs, handle failures, and refine CI workflows to stabilize development and release processes."
2024-08-05,pytorch/pytorch,"The comments reveal ongoing discussions about PyTorch's device support, especially for MPS, CUDA, and Huawei Ascend. Major concerns include incomplete operator implementations on MPS, performance regressions, and varying behavior of device assignment and memory management across backends and hardware configurations. There are questions about the stability, correctness, and efficiency of certain kernel implementations, as well as plans for refactoring and improving device-agnostic APIs. Additionally, some issues relate to CI flakiness, test consistency, and build infrastructure, alongside proposals for API standardization and error handling improvements. Unresolved questions involve operator support, profiling accuracy, and system/system-level stability considerations."
2024-08-06,pytorch/pytorch,"The discussions primarily revolve around troubleshooting and improving support for complex tensor subclasses (e.g., sparse, nested, float8) in PyTorch's export and compilation pipelines, highlighting issues with incomplete or incorrect handling of tensor attributes (like `requires_grad`) and the need for better shape/dataset support and verification, especially on specialized hardware like MPS or within distributed contexts. Several bugs are identified related to serialization, kernel performance, and support list management for hardware backends like XPU; suggestions include stricter validation (error on non-conformant naming), explicit handling of subclasses in export, and better shape/metric management during compilation. There are interface questions about whether certain behaviors (like `tensor.numpy()`) should be extended for subclasses, especially sparse tensors, and concerns whether current infrastructure (e.g., shape duck-typing, support lists) accurately reflects hardware capabilities. Overall, the conversations indicate ongoing work to enhance robustness, correctness, and usability of advanced features (e.g., auto-parallel, cross-entropy kernels, dynamic shapes) across diverse hardware and tensor types."
2024-08-07,pytorch/pytorch,"The collected comments discuss several technical concerns, such as potential improvements and issues related to in-place batch normalization, plan and implications of BC-breaking changes (particularly around `numpy()` semantics and conjugate views), and normalization schemes for complex or difficult-to-represent data types. There is ongoing work and debate about making `views` non-mutable, handling complex type representations consistently, and ensuring forward compatibility without breaking existing APIs. Some comments highlight specific bugs, e.g., memory leaks in CUDA or issues with kernel compilation times and WGMMA support, with suggestions for workarounds or further investigation. Several issues also involve CI flakiness, build failures, and verification of regressions across different hardware and software configurations, with proposals for better testing, documentation, and handling standardization (like naming conventions or patching mechanisms). Overall, the discussion emphasizes balancing backward compatibility, performance, reliability, and standardization in feature development and testing."
2024-08-08,pytorch/pytorch,"The discussions cover a variety of technical issues including potential side effects of setting `broadcast_buffers=False` in DDP, build and compiler errors due to hardware (e.g., GCC version, AVX2, and architecture-specific code), and runtime concerns such as non-deterministic behaviors, floating-point tolerances, and device-specific support opportunities. Several comments address problems with particular operators unsupported on certain backends (e.g., MPS fallbacks, ONNX export limitations), as well as compatibility issues across different hardware and software versions (e.g., CUDA, ROCm, Apple Silicon). There are suggestions for API design improvements, such as how to manage device types and subsystem support (e.g., for `set_device`, `torch.nn.functional`, and `Tensor` device support). Many conversations involve debugging failures due to CI/flakiness, build configuration, or platform-specific bugs, with some proposing workarounds or highlighting ongoing work to fix these. Unresolved questions notably involve support for specific operators, device abstractions, and runtime behaviors (determinism, precision), often requiring further investigation or API refinements."
2024-08-09,pytorch/pytorch,"The comments encompass a range of technical concerns, including issues with sparse tensor construction and errors when constructing sparse tensors outside the `collate_fn`, as well as problems with `module 'torch' has no attribute '_C'` which were mitigated by downgrading torch versions or setting environment variables. There are questions about environment setup, particularly with libtorch, CUDA, and ROCm, and issues with build configurations, such as the need to specify `Torch_DIR` or handling out-of-date or missing dependencies. Several discussions focus on optimizing distributed communication, e.g., stream concurrency, process group initialization, and process group reuse, with questions about NCCL stream selection and group creation to improve overlap. Debugging crashes and errors on specific hardware (e.g., Apple Silicon, ARM64, GPU) and input shape/precision issues are also prevalent, with some potential fixes related to patching LLVM/ACL or tuning precision and tolerances. Overall, unresolved questions include environment configuration, compatibility, and performance optimizations across diverse hardware and software setups."
2024-08-10,pytorch/pytorch,"The discussions cover issues related to the installation and support of Caffe2, particularly the `ModuleNotFoundError` in `tools.setup_helpers`, which appears to be a package/package structure problem. Several comments suggest that this is a known packaging issue with the Caffe2 build, and users are advised to wait for fixes or workarounds like modifying the setup script. Additional concerns include build failures, compatibility issues with different architectures like M1/M3, and the need for better documentation or support for features such as quantization, complex number support, and custom type annotations in PyTorch. There are also notes about merge conflicts, build failures, and the importance of CLA signing for contributions. Overall, the main questions involve package support for cumulative system dependencies, build stability, and how to proceed with specific technical features or fixes."
2024-08-11,pytorch/pytorch,"The discussions highlight several key concerns: the need for improved device synchronization mechanisms in PyTorch, with suggestions to set internal device blocks to BlockingSync to avoid performance waste; the implementation and support of certain operations on MPS devices, such as 'aten::upsample_bicubic2d.out' and 'aten::_linalg_det.result', and the associated workarounds like setting environment variables; issues with rebase conflicts due to merge conflicts in code related to inductor and cpp files, and the potential impact on internal build stability; and questions about the accuracy and precision of floating-point computations, with some cases showing relative differences exceeding the expected 1e-6 threshold, raising concerns about numerical consistency across devices. Additionally, there are questions around code reusability, submodule handling in export, and clarifications needed for new features like process group caching."
2024-08-12,pytorch/pytorch,"The discussions primarily center around performance and efficiency issues, such as CPU core utilization in PyTorch workloads, especially when running minGPT on Windows and controlling thread counts via environment variables. There are also multiple reports of trunk flakiness across platforms and tests, with some failures related to distributed training, NCCL operations, and internal consistency checks that are sometimes mitigated with workarounds like device setting or skipping certain tests. Several questions and suggestions focus on improving multi-GPU training in C++ APIs, handling distributed memory synchronization, and addressing build or compatibility issues, including ABI support, compiler detection, and external dependencies like MKL. Some discussions highlight the need for better support and documentation around feature upgrades (e.g., oneDNN), and handling of certain internal changes (e.g., symbolic shapes, tensor subclasses) that might impact existing code and tests. Unresolved issues include flakiness in CI tests, compatibility with Windows and specific hardware configurations, and ensuring backward-compatible changes while addressing performance regressions."
2024-08-13,pytorch/pytorch,"The discussions predominantly revolve around CUDA and MPS backend behaviors and compatibility issues, including how cooperative stream concurrency works with process groups, stream selection, and the impact on performance and correctness. Several comments highlight the need for better error handling and diagnostics when unsupported features or configurations are encountered (e.g., shape guarding issues in dynamo, kernel compilation errors). There are concerns about stability and correctness of numerical results across different device types and precisions, especially in contexts like fp8, low-precision, or non-reproducible floating point operations. Workarounds involve environment configurations, re-implementation, or code restructuring, but many unresolved questions concern how to improve robustness, compatibility, and performance observation tools across diverse hardware and software setups, including Windows, ROCm, and internal infrastructure. Additionally, some discussions address the management of build and runtime environments, like ensuring correct compiler support (e.g., C++ standards, CUDA version) and adding necessary guards or checks for dynamic shapes and certain backend features."
2024-08-14,pytorch/pytorch,"The discussions mainly revolve around optimizing and debugging distributed training operations, memory management, and serialization in PyTorch, including handling specific issues like memory leaks, device placement, and the correct propagation of symbolic shapes. Several comments propose or inquire about implementation improvements such as adding better distributed collectives (e.g., grouped GEMM, p2p communication), handling auto-dynamic shapes, or refining tensor cloning to preserve strides. There are also questions about test failures, CI management, and missing dependencies (e.g., nvToolsExt), often with suggestions to skip flaky tests or improve testing strategies. Some comments highlight ongoing internal challenges like ensuring compatibility across different hardware, CUDA versions, and system configurations, especially related to Windows and ROCm. Unresolved questions include how to better support multi-accelerator setups, correctness with dynamic shapes, and adapting the code to various environment constraints."
2024-08-15,pytorch/pytorch,"The discussions highlight concerns regarding the interpretation and handling of PDF values, especially when used in log-space for optimization, clarifying that negative log densities are acceptable. Several technical issues relate to implementation details like supporting in-place normalization, handling complex data types (e.g., SymFloat), and ensuring compatibility with various backends (CUDA, HIP, MPS) and compile contexts. There are questions about the correctness and stability of shape inference, shape guards, and dynamic shape handling during export and compilation, as well as concerns about performance regressions, such as recompile overheads and kernel fusions. Some discussions also involve build system dependencies, environment configurations, and test failures on specific platforms or hardware, with suggestions to improve diagnostics, logging, and testing strategies for better robustness and maintainability. Unresolved questions include how to manage in-place operators, correctly handle float8 quantization, and verify complex bug fixes."
2024-08-16,pytorch/pytorch,"The comments highlight several recurring technical issues and discussions, including the importance of proper input types for operations like `output_size` in MaxUnpool2d exports, and the need for better support for buffers and their registration in PyTorch. There are ongoing concerns about thread management and the impact of using threads by default versus processes for performance and debugging, as well as questions surrounding the reproducibility of flaky tests in CI, especially regarding tail timeouts and test stability. Compatibility issues on different platforms, such as Windows memory management behavior, ROCm/HIPBLAS support, and macOS-related bugs, are also prominent. Additionally, there are proposals and ongoing work for improving autograd, dispatch, and device handling, including AutoPyTorch support, better autograd for custom ops, and sharding strategies in distributed training."
2024-08-17,pytorch/pytorch,"The discussions cover a range of issues related to PyTorch, including challenges with multiprocessing start methods and memory overhead when using 'spawn', and complications arising from setting the start method multiple times. Several comments discuss the implementation of type propagation and annotation strategies within `nn.Module` to improve debugging and shape inference, with suggestions for more boilerplate boilerplate code, and alternative approaches (e.g., redefining `__call__`). Issues also address device-specific operator support, particularly for MPS and CUDA, with errors indicating unsupported operations on certain backends, and potential workarounds such as environment variables or fallback mechanisms. Merge conflicts, CI failures, and version compatibility problems are frequent, highlighting ongoing maintenance and infrastructure hurdles. Finally, there are discussions about optimizing communication patterns (p2p vs collectives), sparse tensor support, and enabling features such as variadic generics and long integer indexing, with some unresolved questions about performance and implementation strategies."
2024-08-18,pytorch/pytorch,"The discussions highlight several technical concerns, including Windows-specific issues related to process spawning and pickling arguments, and challenges with MPS device support on macOS where certain operations like `aten::upsample_bicubic2d.out` are unsupported, leading to slower fallback performance. There are questions about the necessity of certain passes when setting `reorder_for_locality=False` for Compiled Autograd graphs, and unresolved support for complex number operations such as `abs`, `conj`, and `div` in exporting to ONNX and backend execution. Several issues relate to build and merge conflicts, permission errors, and CI pipeline failures, indicating ongoing development and integration challenges. Finally, some discussions involve the implementation and validation of numerical functions like beta, betaln, and gamma, with suggestions to use high-precision approximations, but no conclusive resolutions are provided."
2024-08-19,pytorch/pytorch,"The comments highlight several technical issues, mostly related to PyTorch's internal features and support across different hardware and configurations. Key concerns include bugs in CUDA kernel support, such as mismatch in expected kernels and performance regressions on MPS backends; the need for additional support of sparse and iterative linear algebra solvers; and challenges with reusing certain code-generation or compilation configurations (e.g., Triton, torch.compile) across multiple devices and backends. Several discussions emphasize fixing bugs in initialization, guard creation, and shape inference, as well as handling subclass metadata and ensuring reproducibility. Additionally, there's recurring mention of infrastructure and test sharding challenges, as well as proposals for new features like support for iterative solvers, better caching, and support for complex types, along with questions about compatibility, platform support, and regressions. Unresolved technical questions include the proper handling of subclass metadata, kernel design choices under sparsity, and the integration of new features into release branches."
2024-08-20,pytorch/pytorch,"The discussions reveal concerns around PyTorch's large and monolithic packages, especially regarding build size, modularization, and space efficiency—suggesting the need for package split options and better modularization. Several issues highlight technical complexities in distributed training, such as proper handling of device placement, cache management, and backend-specific behaviors (e.g., custom ops on ROCm vs CUDA). There are also recurring themes about flakiness and stability in CI tests due to infrastructure or environment discrepancies, as well as bugs in autograd and shape inference, which require better testing and explicit handling. Additionally, some discussions involve build system improvements, patch management, and supporting advanced features like sparse tensors, mixed precision, and custom hardware backends, emphasizing a need for clearer guidelines, better default behaviors, and consistent API semantics."
2024-08-21,pytorch/pytorch,"The discussions highlight issues with installing and building caffe2, often due to package or environment inconsistencies, with specific errors such as missing modules (`tools.setup_helpers`) or incompatible versions (e.g., libtorch ABI issues). Several comments suggest improvements for better user experience, such as replacing static warnings/votes with more efficient voting or prioritization systems, and refining the default behavior for features like `PYTORCH_ENABLE_MPS_FALLBACK` defaults. Performance testing and benchmarking on different hardware, including MPS, ROCm, and various GPUs, reveal potential regressions or performance gains, with suggestions to refactor code to generalize device-agnostic benchmarking or improve kernel efficiency. Environmental issues, such as CUDA driver incompatibilities or host memory management, and build system problems (e.g., recipe updates, build failures) are recurrent, with ongoing efforts to fix such problems. Unresolved questions include how to better support non-CUDA hardware, improve build reproducibility, and implement experimental features with minimal breakage, often tied to internal infrastructure or limited resource support."
2024-08-22,pytorch/pytorch,"The discussions highlight issues related to gradient safety when using transpose/permutations without `.contiguous()`, and in some cases, the need to add `.contiguous()` after operations like `einsum` or `einops.rearrange` to ensure proper gradient striding. There’s also a recurring concern about improving and unifying support for operations like `interp`, `torch.quantile`, and handling tensor info, especially regarding large tensor sizes and device compatibility. Several questions involve the development process—such as how to fix specific errors, how to implement or improve benchmarking, and ensuring code generates efficiently based on CPU capabilities. A few comments address the maintenance and support of older hardware/OS versions, and there are suggestions for refactoring and better API design, e.g., for tensor lookup or benchmarking interfaces. Unresolved issues include performance regressions, correctness checks, and compatibility with out-of-tree or non-traditional hardware backends like XPU."
2024-08-23,pytorch/pytorch,"The comments primarily revolve around performance issues, bug fixes, and feature requests across various aspects of PyTorch, such as distributed training, memory management, and custom device support. Several discussions identify bugs or regressions (e.g., NCCL errors, autograd inconsistencies, segmentation faults) often related to environment setup or specific hardware configurations, with some fixes merged or planned for upcoming releases. There are requests for new features, such as a unified augmentation operator, better support for sparse tensors, and enhanced autograd or device management capabilities, with some proposals involving API changes or new tools (e.g., host cache clearing). Overall, unresolved questions include environment compatibility, ensuring correct operation on various hardware and backends, and improving documentation and user guidance. Many issues are being actively addressed through bug fixes, improvements, and planned enhancements, though some remain open or are pending review."
2024-08-24,pytorch/pytorch,"The discussions highlight several technical concerns: (1) the NaN issue caused by discontinuities in custom activation functions and the potential for more ""true"" conditional operations in autograd; (2) challenges with data loading, multiprocessing, and pickling, especially when using `num_workers > 0` and setting `multiprocessing_context='spawn'`, which may cause deadlocks or errors; (3) limitations of `torch.onnx.export` with sparse tensors, with plans to upgrade to `torch.onnx.dynamo_export`; (4) compatibility issues with AMD ROCm and GFX versions, particularly on unsupported GPUs like gfx1010; and (5) workflow and CI failures due to merge conflicts, build issues, or missing release notes, with some suggestions to disable or adjust certain test environments or flags for smoother CI runs."
2024-08-25,pytorch/pytorch,"The comments highlight ongoing development and troubleshooting efforts within the PyTorch repository, including implementation of feature requests, bug fixes, and performance improvements across various modules and hardware backends such as MPS, ROCm, and CUDA. Several issues relate to hardware-specific support, such as MPS device functionality, Windows ARM64 support, and GPU architecture compatibility, emphasizing the challenges posed by diverse hardware environments. There are discussions about enhancing user experience through better API design, documentation, and debugging tools, especially regarding optimizer parameter naming, tensor memory management, and exported models. Numerous pull requests and reverts indicate active iterative development and maintenance cycles, with particular attention to correctness, performance regressions, and build/test stability. Unresolved questions include handling of complex operations, managing dependencies, and improving profiling and error diagnosis for complex workflows."
2024-08-26,pytorch/pytorch,"The discussions highlight issues related to performance regressions and instability caused by recent code changes, especially on specific devices and backends such as XPU and ROCm, with some regressions linked to particular PRs like #128922. Several comments express concerns about the correctness and stability of features like `torch.onnx.dynamo_export`, the handling of custom operators in autograd, and the implementation of models with dynamic shapes or custom fusion passes. There are recurring suggestions to improve error messages, add more and broader tests, and provide clearer documentation or API guidance, particularly around features like `torch.compile`, memory management, and custom operator support. Unresolved questions include the root causes of certain regressions, the proper handling of ephemeral sources in symbolic shape tracking, and the correct implementation of specific operators for different data types and backends. Overall, the discussions call for more thorough testing, better error diagnostics, and careful handling of device-specific and shape-related issues in ongoing PyTorch development."
2024-08-27,pytorch/pytorch,"The comments reflect multiple ongoing issues related to PyTorch's development, including difficulties with package installations (notably Caffe2), performance regressions, and compatibility problems across various backends and hardware (like MPS on Apple Silicon, ROCm on AMD GPUs, and CUDA). Several discussions highlight challenges with export functions (`torch.export`, `torch.compile`, and `aot_autograd`) especially when handling dynamic shapes, guards, or custom modules, with some users proposing workarounds or requesting better tooling/test coverage. There are also technical concerns about API consistency, internal infrastructure (e.g., handling of device meshes and distributed plans), and low-level kernel issues (such as memory, synchronization, and kernel fusion bugs). Additionally, some comments note regressions introduced by recent PRs, with questions about testing, review, and addressing edge cases like tensor strides and shape guards, often suggesting reverting or cautious incremental improvements. Overall, the discussions point to active efforts in debugging, performance optimization, and improving robustness in complex distributed and heterogeneous environments."
2024-08-28,pytorch/pytorch,"The discussion encompasses several points: (1) Enhancements to Torch FX symbolic lowering to support more function argument types, and potential higher-order ops like unroll for RNN kernel unification; (2) User interface and documentation improvements, such as unified API search and clearer organization; (3) Device-specific operator support issues, notably for MPS (macOS), CUDA (e.g., unsupported operators, performance discrepancies, out-of-memory errors), and support for new ONNX opset versions; (4) Debugging and stability concerns, including flaky tests, shape inference failures, memory leaks, and reproducibility of internal CI failures; (5) API modifications and compatibility challenges, especially regarding distributed training (FSDP), auto-grad, and support for various backends and hardware. Many unresolved questions relate to implementation details of operator support, backward compatibility testing, and platform-specific limitations."
2024-08-29,pytorch/pytorch,"The discussions highlight issues around custom autograd functions returning non-tensors, particularly with autograd not supporting passing work handles, leading to type errors. There are questions about the support and stability of distributed training setups, especially regarding environment configurations, process groups, and potential regressions across versions (e.g., 2.0.1 vs 2.4.0+). Several reports concern CI flakiness, test failures, and performance regressions due to recent changes, including kernel fusion, constant folding, and operator support for different backends such as ROCm, HIP, and ARM's SVE. Some issues relate to build configuration, integration of new features like opset support in ONNX, and ensuring consistent behavior across hardware, with suggestions for more robust testing, better error messages, and code support for various device-specific implementations. Unresolved questions include the support for non-tensor outputs in autograd, proper environment setup for distributed training, and how new core features or regressions can be systematically validated."
2024-08-30,pytorch/pytorch,"The comments highlight several recurring issues and questions related to PyTorch's functionality and stability, such as bugs in specific operators (e.g., `aten::max_pool3d_with_indices` on MPS, `aten::upsample_bicubic2d` support, and `torch.clone()` behavior), and stability concerns with features like `torch.compile()` (especially regarding caching and re-compilation). There are also discussions about infrastructure and build issues, including dependency management (NCCL, protobuf), environment setups (CUDA versions, system libraries), and platform-specific problems (Windows, macOS, Linux, ROCm). Several comments request or suggest improvements—such as better error handling, test coverage, internal support for certain features, and clarifications in documentation—while some issues are marked as fixed, pending review, or needing further work, including re-enabling flaky tests. Additionally, questions about internal implementation details (e.g., environment variable management, CUDA kernel support, and in-tree/out-of-tree device support) appear frequently, indicating ongoing efforts to improve robustness and portability."
2024-08-31,pytorch/pytorch,"The discussions mainly revolve around optimizing the concurrency between CUDA computation and communication, with some evidence of overlapping execution streams and potential performance trade-offs. Several comments discuss how to improve overlap tactics, such as using multiple process groups or adjusting coalescing behavior, though the correctness and expected behavior depend on specific implementation details. Questions also address configuration issues like setting environment variables, fixing specific bugs in functions like `mean_out`, and challenges with compatibility across different hardware (e.g., Windows 11) and software versions (PyTorch, Triton). There are unresolved issues related to precise control of collective operations, improving diagnostic tools (flight recorder, trace analysis), and handling distributed synchronization. Overall, the discussions highlight ongoing efforts to enhance CUDA-communication overlap, stability, and tooling, with some implementation questions still open."
2024-09-01,pytorch/pytorch,"The discussions highlight concerns about ABI compatibility between C++ extensions linking against different versions of libtorch, especially across platforms like MacOS and Linux, with an emphasis on packaging non-CXX11 and CXX11 libtorch variants. There is active interest in supporting native complex tensor operations in ONNX, which would simplify dispatch logic during export and potentially improve performance by native support of complex types. Some comments address build configuration issues, such as ensuring protobuf linkage is static to prevent symbol conflicts, and efforts to improve unit test shard management for better CI reliability. Additionally, ongoing investigations involve the support and integration of newer features like torch.compile, optimizer enhancements, and external toolchains like zig, along with addressing CI stability and merge conflicts. Unresolved questions include best practices for exporting complex operators, platform-specific ABI handling, and aligning build and runtime configurations to ensure compatibility and performance."
2024-09-02,pytorch/pytorch,"The discussions highlight persistent issues with CUDA tensor serialization on CPU-only or CI environments, with solutions involving careful unpickling or official fixes long overdue. Several reports concern out-of-memory errors, especially on AMD hardware, possibly due to memory fragmentation or driver-related problems, with suggestions to adjust environment variables or GPU configurations. Evaluation of PyTorch's internal implementation reveals potential problems with operator support on specific devices like MPS, and discussions suggest leveraging device-agnostic APIs such as `AcceleratorHooksInterface`. Multiple testing and CI flakiness reports illustrate challenges with stability and correctness, often linked to backend or hardware-specific bugs, with incremental fixes and rebase efforts ongoing. Lastly, questions about new features like flexible communication groups, parallel scan algorithms, and specific operator fixes indicate active development, but many issues remain unresolved or under review."
2024-09-03,pytorch/pytorch,"The comments reflect various technical concerns including limitations of double backward support in CuDNN for RNNs, issues with torch's dependency management and environment configuration (notably with numpy, CUDA, and dependencies on specific build environments), and problems related to device support and backend implementations across platforms such as MPS, ROCm, and custom accelerators like Gaudi. Several discussions also address performance bottlenecks, correctness of numerical computations (e.g., matrix inverses, eigen decompositions, and linear solves), and CI stability with flaky tests, many of which seem environment or hardware-specific. Proposals for solutions include disabling or fallback options for unsupported backends, improving build reproducibility via containerization and environment introspection, and more rigorous testing of GIL handling and autograd features. Unresolved questions involve compatibility of newer features with legacy platforms, best practices for dependency management, and ensuring correctness and performance across diverse hardware/software stacks."
2024-09-04,pytorch/pytorch,"The provided comments primarily concern issues related to PyTorch's JIT and export mechanisms, especially around compatibility, support for dynamic shapes, and peculiar behavior with certain models or operators such as RNNs, scaled dot-product attention, or complex index operations. Several comments highlight the need for better testing, documentation, and handling of corner cases like data-dependent guards, and support for specific hardware or backend features (e.g., AMD ROCm, inductor backend, or int8 quantization). There are discussions about reorganizing APIs for device-agnostic memory management and the importance of adding support for features like popcount or fixing existing operators. Many unresolved questions focus on support stability, appropriate testing, and how to handle special cases like in-place tensor modifications or models with data-dependent shapes. Overall, continuous improvements, better test coverage, and clearer documentation are emphasized to enhance robustness and usability."
2024-09-05,pytorch/pytorch,"The comments reflect ongoing challenges in PyTorch development, including incomplete support for features like AMP in C++, retaining correct tensor shape and storage info during functionalization (especially with `as_strided`), and handling complex graph break scenarios in Dynamo and torch.export, particularly with data-dependent guards and dynamic shapes. Several issues involve flakiness or instability in CI testing environments, sometimes related to platform propagation delays or environment inconsistencies. There are discussions about architectural refactors, such as flattening inheritance hierarchies for event classes, and plans to improve API consistency across devices. Several concerns focus on accurate handling of special cases (e.g., custom operations, backward hooks, complex container modules) during tracing, export, and re-compilation to ensure correctness, efficiency, and support for advanced features like distributed training and model exporting."
2024-09-06,pytorch/pytorch,"The comments reflect ongoing issues and discussions related to PyTorch's features and internal mechanisms, such as implementation bugs, compatibility concerns, and behavioral nuances across devices, platforms, and configurations. Notably, there are specific discussions on bugs in loss functions like BCEWithLogitsLoss, implementation details of operations like as_strided, and device support issues (e.g., AMD, MPS, ROCm). Many comments involve troubleshooting, workarounds, and PR reviews addressing regressions, flaky tests, or internal tooling (e.g., CI stability, rebase conflicts). Several entries raise questions about proper testing, regression detection, or correctness of behaviors in features like torch.compile, distributed setup, or specialized hardware support. Overall, these reflect maintenance, bug fixing, compatibility, and testing challenges within the PyTorch project, often focused on ensuring correctness and robustness across diverse environments."
2024-09-07,pytorch/pytorch,"The discussions highlight several technical challenges and questions related to PyTorch development. Key concerns include handling unimplemented MPS operators like `aten::unfold_backward`, with suggestions to enable fallback to CPU via `PYTORCH_ENABLE_MPS_FALLBACK=1`, and implementing missing operators such as `upsample_bicubic2d`. There are questions about improving the integration of environment variables, optimizing build layers to avoid excessive image sizes, and aligning Python API support across devices, especially for Intel GPUs. Additionally, issues with training stability and loss explosion on MPS, potential regression between PyTorch versions, and kernel/kernel launch variances are raised. Proposed solutions include environment variable settings, API unification plans, and code refactoring, but several questions remain unresolved, particularly around improving MPS support and build efficiency."
2024-09-08,pytorch/pytorch,"The discussions primarily revolve around memory management and tensor sharing, particularly in multiprocessing contexts, with suggestions to improve automatic cleanup and conversion of shared tensors. There are issues related to compatibility and conflicts between PyTorch, OpenMP, and MPI, especially with different runtime environments like Intel OpenMP, libgomp, and NVHPC, leading to crashes and instability. Concerns are also raised about proper environment variable usage (e.g., `TORCH_*`) for controlling distributed operations, as well as potential bugs in hook removal during DataParallel execution affecting hook lifecycle. Some discussions focus on bug fixes and feature improvements, such as reorganizing flash attention code, fixing tensor view alignment, and handling unsigned tensor types, with questions about best practices and detailed technical implications. Unresolved issues include reproducibility of certain errors on specific hardware, environment configuration challenges, and handling of float constants in Triton/Torch code."
2024-09-09,pytorch/pytorch,"The collected comments highlight ongoing efforts and concerns regarding PyTorch's edge feature support, especially on MPS and CUDA backends, with specific mention of missing operator implementations (e.g., unfold_backward, unfold_forward, upscale_bicubic2d, unfold). Several issues pertain to system-specific bugs such as NaNs in MPS, NaN behaviors in certain operations, and large resource leaks, with some fixes verified but others needing further debugging or architectural changes (e.g., operator registration, kernel tiling, safe kernel selection for unsigned types). Multiple discussions focus on improving test coverage, error messaging clarity (e.g., for invalid input types or data-dependent guards), and infrastructure stability, especially around trunk flakiness and CI robustness. A few comments address API design considerations, compatibility concerns (e.g., with different CUDA versions or legacy code), and process-specific warnings (e.g., with NCCL and process group initialization). Overall, unresolved questions include operator support for new data types, reliable resource management, and systematic testing to prevent flakiness; solutions proposed involve code patches, better logging, explicit API changes, or deferring fixes to more stable branches."
2024-09-10,pytorch/pytorch,"The discussions reveal several recurring themes: (1) a notable slowdown or instability during NCCL communication and process group initialization, likely system-specific or due to environment mismatches; (2) internal code bugs or limitations, such as handling dynamic shapes, operator decomposition, or specific operator support (e.g., `aten::permute.default`) in JIT/inductor, with suggestions to improve error messaging and add dedicated tests; (3) correctness issues with certain ONNX-exported models or runtime errors due to unsupported operators, which are sometimes related to the handling of specific tensor subclasses or signature parsing; (4) build and deployment challenges, including dependency management with CUDA or ROCm, and CI flakiness, often attributed to environment or infrastructure inconsistencies; (5) feature requests or follow-up tasks such as improving test coverage, managing dynamic shape support, or API design questions. Overall, the conversations focus on system stability, operator support, and testing robustness, with many issues attributed to environment/system-specific configurations that require careful debugging and incremental fixes."
2024-09-11,pytorch/pytorch,"The discussions highlight several technical concerns, including potential bugs and limitations in autograd, distributed training, and kernel support. Notably, issues with inplace variable modifications affecting gradients, the need for better handling of dynamic shapes and in-place operations in Torch autograd, and the impact of cache warmups on performance, are recurrent. There are also questions about kernel support for newer data types (like BF16 on CPU), and the stability of features such as FSDP and RPC under various hardware conditions. Many discussions propose workarounds or seek clarification on support status, with some unresolved questions about reproducibility, performance regressions, and system-specific behaviors. Overall, these conversations point to ongoing development and debugging efforts related to PyTorch’s core features, distributed training, and hardware support."
2024-09-12,pytorch/pytorch,"The comments reflect a variety of issues encountered by the PyTorch community, including bugs, performance regressions, and flaky/troublesome behavior across different hardware (GPU, MPS, CPU, ROCm, etc.). Notably, several discussions revolve around handling device-specific quirks—such as CUDA out-of-memory errors, MPS device limitations, and CPU architecture support challenges—highlighting ongoing efforts to improve backend compatibility and stability. There are also multiple reports about flaky tests and trunk flakiness that need better management, possibly involving more deliberate benchmarking strategies, better test selection, and CI environment stability fixes. Community suggestions include adding explicit device or backend checks, supporting more architecture variants (like SVE), enhancing deterministic or stable execution modes, and improving logging and diagnostics for debugging. Overall, unresolved questions concern how to streamline device-specific support, prevent flakiness, and stabilize CI tests, all while maintaining performance and correctness across diverse hardware."
2024-09-13,pytorch/pytorch,"The discussions highlight several recurring issues and questions: Support for features like FP16 training on CPU and MPS fallback remains limited or not yet implemented; support for complex operator fusion and in-place patterns via compiler passes (e.g., fusing Swish+Dropout, Conv+BatchNorm+ReLU+Residual) is ongoing and sometimes requires specifying known patterns or compiler passes; FSDP-related complexities, especially loading full state-dicts into sharded models and handling autograd/forward overlaps, pose significant challenges, with proposed workarounds including FSDP2 and meticulous parameter handling; guard and static shape inference challenges sometimes cause errors or unexpected behavior, especially with dynamic shapes or custom conditions, with suggestions to improve guard logging and stacktrace reporting; and overall, ensuring compatibility and performance across hardware backends (like ROCm, AMD, XPU) and build environments (manylinux, conda, pip) remains a concern, alongside stability issues due to trunk flakiness and infrastructure limitations."
2024-09-14,pytorch/pytorch,"The discussions highlight persistent issues with implementation inconsistencies in padding, convolution, and distribution parameter conventions, especially on MacOS and certain hardware backends like MPS, where operators like `searchsorted` and `fft` lack native support. Several comments point to the need for better handling of data-dependent operations, such as boolean checks and runtime guards, which currently raise exceptions or fail under tracing or symbolic shape evaluation. There are technical suggestions for improving operator support (e.g., implementing missing operators like `aten.permute.default`, `aten._fft_r2c.default`, or supporting unbacked symbolic conditions), as well as proposals to enhance debugging, logging, and fallback mechanisms, especially for hardware backends like HIP, AMD, and Intel GPUs. Many unresolved questions concern fixing the core support in PyTorch for operators with varying conventions, runtime shape guards, and ensuring better cross-platform consistency and compatibility, especially with newer hardware and in tracing/compilation workflows."
2024-09-15,pytorch/pytorch,"The discussions reveal concerns about device-specific behaviors and implementation details in PyTorch, such as the inclusion of device information in TorchScript models and incomplete operator support for MPS hardware, with suggestions for more device-agnostic design. Several issues involve performance discrepancies between CPU, MPS, and GPU backends, often attributed to unoptimized kernels or hardware limitations, along with requests for better tooling, testing, and documentation to clarify these behaviors. There are ongoing discussions on proper API design, such as supporting named parameters in optimizers, and ensuring backward compatibility and correct parameter conventions for distributions like the Negative Binomial. Some issues are related to build and compilation errors, including C++ compile failures or code reloading problems, with solutions involving environment setup or patching. Overall, the main concerns focus on improving hardware support, API clarity, performance, and stability across diverse architectures and use cases."
2024-09-16,pytorch/pytorch,"The issues span a variety of topics, primarily focusing on PyTorch's internal development challenges, such as unresolved bugs, experimental feature integration, and API design concerns. Several discussions highlight the need for improved documentation, test coverage, and clearer deprecation or warning strategies for features like mixed precision and complex Jacobians. Some issues involve the performance implications of custom kernels, kernel registration, and compiler behavior, with specific attention to correctness and consistency across device backends (CUDA, ROCm, MPS). There's ongoing debate about default precision policies, including whether to prioritize high accuracy or performance, and how best to communicate these decisions to users. Unresolved questions include the proper handling of data-dependent tensor shapes, the impact of certain PR changes, and system-specific configurations for advanced hardware support."
2024-09-17,pytorch/pytorch,"The discussions reflect a range of technical concerns across the PyTorch repository, including issues with bug regressions (often related to specific backends or device support, such as MPS, ROCm, and Triton), challenges with autograd and Jacobian computations (notably around complex derivatives and CUDA eager/backend differences), the need for more flexible user API support (e.g., dynamic shapes, custom device handling, and auto-tuning), and ongoing efforts to improve debugging, codegen, and build infrastructure (such as enhancing reproducibility, CI reliability, and support for non-GPU devices). Some proposals involve refactoring existing internals to better support non-standard use cases, like Triton or custom backends, and ensuring consistency between eager, compiled, and exported models. Numerous issues also highlight the importance of improving test coverage, debugging tools, and incorporating user feedback to fix flakiness and correctness bugs. Unresolved questions mainly concern how to generalize device support, improve introspection in complex derivative calculations, and manage the trade-offs between user URIs, performance, and maintainability."
2024-09-18,pytorch/pytorch,"The comments reflect various ongoing issues, workarounds, and discussions within the PyTorch repository, such as the need for proper ONNX export support for certain operations, dealing with CUDA and NCCL errors, and addressing flakiness and flaky test failures across multiple platforms. Specific concerns include improving the accuracy of error messages (e.g., for Flash Attention support), handling compatibility with different hardware and libraries (e.g., ROCm, MPS, NCCL streams), and fixing implementation details like sequence number management and shape inference in autograd functions. The discussions also highlight workflow and infrastructure challenges, such as merge failures, CI flaky tests, and dependency updates, plus internal review and sign-off processes. Suggestions include restructuring auto-merge processes, enhancing testing coverage, consolidating configuration options, and ensuring internal and external tool compatibility, especially around serialization and shape inference issues."
2024-09-19,pytorch/pytorch,"The discussions highlight ongoing challenges with incomplete operator support on Apple’s MPS device, leading users to fallback to CPU, which results in significant performance degradation. There are multiple reports of trunk flakiness affecting Linux and ROCm platforms, with issues potentially related to environment setup, environment variable propagation, and build environment inconsistencies. Several technical concerns include the proper implementation and support of specific operators (e.g., `aten::_fft_r2c`, `aten::isneginf`), ensuring correct and efficient forward/backward autograd behavior (notably for custom implementations like `cumprod_backward`), and handling out-of-tree device support with minimal disruption. Questions about improving documentation clarity, build configuration, and plugin integration (e.g., pybind11 support, CI setup for new hardware) persist. Overall, unresolved operator support, platform-specific build/configuration issues, and guidance for advanced use cases remain core concerns."
2024-09-20,pytorch/pytorch,"The discussions cover various technical concerns including how to properly remove hooks in PyTorch, particularly multiple or forward hooks, and validating the correct implementation. There are questions about CUDA device compatibility, especially building PyTorch from source for older GPUs and specific CUDA versions, with advice on selecting appropriate wheel versions. Several issues address experimental features like dynamic shape support, tensor serialization/deserialization, and the impact of specific code changes on functionality and performance, such as range analyses and exported shape constraints. Flakiness and stability of tests across different platforms, particularly in CI environments, are frequently discussed, with solutions involving reruns and disabling problematic tests. Lastly, there are concerns about correct build procedures, environment setup, and compliance with external standards like manylinux, as well as suggestions for API improvements and better documentation."
2024-09-21,pytorch/pytorch,"The discussions highlight several key issues: support for MPS device operators like `torch.isneginf()` and operators such as `aten::_fft_r2c` are unimplemented or only fallback-enabled, raising questions about future support plans; there are performance concerns with certain operators like `cumprod_backward` involving quadratic complexity, with proposals for custom, more efficient implementations; compatibility and correctness issues are noted with fake tensor operators and certain tensor subclasses, especially in distributed or JIT-execution contexts, leading to suggested code adjustments and workarounds; there are ongoing discussions about the correct approach to debugging, API changes, and ensuring proper guard conditions, especially for dynamic shapes and inference guards; finally, there is concern over CI/test failures due to external factors, support for new data types (e.g., signed `intx`), and the need for clear documentation and proper API support in both eager and compiled modes."
2024-09-22,pytorch/pytorch,"The discussions highlight several technical issues, notably the non-native implementation of `itertools.cycle` affecting memory and OOM issues in data loading and webdataset workflows, suggesting a potential native `repeat=True` feature for `DataLoader`. Multiple users report failures or limitations with MPS device operations, such as unimplemented operators like `aten::_upsample_bilinear2d_aa.out`, especially on Mac and Apple Silicon, with workarounds like fallback env variables and environment-specific fixes. CUDA-related errors, including launch failures, unspecified launch failures, and device poisoning after NaN injections, raise concerns about proper error handling, allocator resets, and compatibility, especially on H100 and ROCm hardware, with some fixes pending upstream. Several issues involve deep debugging of autograd, lazy cloning, and gradient checks, often complicated by divergent behavior, irregular warnings, and custom registration or dispatch problems. Unresolved questions include how to effectively handle CUDA errors during shutdown, proper registration of custom ops to ensure consistent behavior, and the status or timeline of feature support like `repeat=True`, as well as the impact of specific PRs on these problems."
2024-09-23,pytorch/pytorch,"The discussions predominantly revolve around handling edge cases and platform-specific limitations in PyTorch, including issues with NCCL collectives, GPU memory constraints on newer architectures like H100 and MIG, and compiler/architecture support (e.g., SVE support and debugging modes). There's concern about source code caching, build configurations (e.g., GCC versions, CUDA build differences), and load failures caused by unsupported or misdetected runtime features. Several issues highlight the need for better diagnostics, warnings, and environment validation (e.g., runtime shape evaluations, debug vs release discrepancies, and environment compatibility checks). Additionally, questions about test flakiness, CI stability, reproducibility of errors, and the handling of complex serialization/deserialization scenarios are discussed, with suggestions to improve error messaging and testing robustness."
2024-09-24,pytorch/pytorch,"The discussions highlight several technical concerns, including the need for better handling and support of dynamic shapes, especially in the context of PyTorch's export and compilation workflows. There are recurring issues with incomplete or outdated support for certain operators and features (e.g., CUDA, ROCm, SVE, ONNX conversion, numpy compatibility), often related to dependencies or platform-specific limitations. Several PRs aim to address performance, correctness, or compatibility regressions but sometimes require reworking guard semantics, API support, or cross-platform testing. Flaky test failures and CI infrastructure issues are frequently mentioned, emphasizing the importance of robust testing for various configurations. Overall, unresolved questions revolve around improving support for dynamic shapes, ensuring cross-platform consistency, and making the infrastructure more resilient to flaky or environment-dependent failures."
2024-09-25,pytorch/pytorch,"The discussions primarily revolve around issues with PyTorch's build and runtime support on various platforms, including Windows, macOS, and Linux, with particular focus on environment configuration, compiler compatibility, and support for advanced features like matrix inversion in ONNX export and dynamic shape handling. Several reports highlight specific errors such as unsupported operations in ONNX (e.g., `linalg.inv`, `lu_solve`), inconsistencies in performance measurements (e.g., CPU vs GPU norm computation times), and runtime crashes (e.g., illegal memory accesses, GPU hangs). There's concern about the correctness and stability of symbolic shape inference, especially regarding bounds and evaluation of shape expressions in inductor, and about the impact of sharding on test coverage. Many discussions suggest mitigations like bug fixes, guard checks, runtime guards, or better tooling for validation and debugging, with some questions about compatibility, the scope of future support, and the best strategies for incremental improvements. Overall, unresolved issues include ensuring full operation support in ONNX, robust environment and dependency management, and reliable performance and correctness on diverse hardware and configurations."
2024-09-26,pytorch/pytorch,"The comments reflect ongoing efforts to resolve various issues related to PyTorch's features, performance, and compatibility. Key concerns include performance regressions with sparse tensor indexing, creating more robust and optimized custom operators (especially for different hardware backends like ROCm and XPU), supporting dynamic shapes and complex patterns (such as slices or nested tensors) in TorchDynamo and FX, and ensuring binary compatibility and proper serialization/deserialization across versions and environments. Several discussions focus on improving code robustness, fixing bugs (e.g., invalid tensor sizes, uninitialized memory reads, and tensor assignment issues), and enhancing user API clarity (like deprecated functions and support for unsigned/signed integer types). Additionally, there are infrastructure and CI-related topics, such as handling flaky tests, managing resource limits, and ensuring proper rebase and merge workflows. Overall, the discussions aim to improve PyTorch's stability, efficiency, and usability across diverse hardware, models, and user scenarios."
2024-09-27,pytorch/pytorch,"The discussions mainly revolve around addressing specific technical challenges in PyTorch features and integrations, such as namely: (1) improving error handling and support for matrix inversion and linear algebra operations in ONNX export due to limited native support; (2) enhancing the robustness of variable deallocation and garbage collection in autograd tensors, especially involving fake tensors and reference cycles; (3) implementing device-specific compilation routing for distributed or in-context execution, including handling shape inference and communication dependencies without unnecessary overhead; (4) managing compatibility issues and runtime performance regressions arising from various model adjustments, third-party integration quirks, and framework changes (e.g., inductor, Triton, XNNPACK); and (5) API design and user experience concerns such as whether to expose advanced functionalities (e.g., `parallelize_module`, `DTensor` sharding utilities, or new configuration flags) as opt-in, opt-out, or transparent defaults, while ensuring proper validation, clarity, and integration with existing APIs and workflows. Unresolved questions include the exact support horizon for complex linear algebra in ONNX, handling of certain object lifetimes and garbage collection intricacies in custom autograd, and how best to balance default behavior versus user-controlled flags in complex distributed or tensor subclass scenarios."
2024-09-28,pytorch/pytorch,"The comments cover a range of topics across multiple issues in the PyTorch repository, with key concerns including the lack of nan-operations (like nanstd, nanquantile, nanmedian) and their utility, especially compared to NumPy, and the need for better support for stream and device context management in graph tracing and autograd. Several issues relate to device support and implementation gaps, such as 'aten::binomial' and 'aten::upsample_bicubic2d' on MPS, and the support for different backends (CUDA, XPU, ROCm), with discussions on device-neutral APIs and automatic backend selection. There are questions regarding the integration of distributed training components, particularly around distributed state_dict loading, with suggestions to simplify user workflows through functions like `set_model_state_dict()` and better documentation. Miscellaneous issues involve CI failures, dependency version management for different architectures, and technical details of specific features like in-place normalization, memory optimization, and graph partitioning for heterogeneous devices. Overall, the discussions focus on improving device compatibility, operator coverage, user experience, and the robustness of the distributed and graph tracing infrastructure."
2024-09-29,pytorch/pytorch,"The discussion covers a range of technical issues, primarily related to PyTorch's support for specific data types (like torch.half) on CUDA devices and OneDNN backend performance discrepancies between AMD and Intel CPUs, potentially caused by differences in instruction set requirements such as AVX512 or VNNI. Concerns are raised about implementation completeness and potential bugs in functions like `StorageNewSharedDevice` and `StorageShareDevice`, as well as the impact of certain changes on existing extensions and the need for reliable CI signals. There are questions about the handling of stream semantics in graph transformations and the behavior of custom attributes on tensors during sharding, indicating some API inconsistencies or unclear behaviors. A recurring theme is the necessity for clearer build logic documentation, guidance on environment setup (e.g., environment variables like `PYTORCH_ENABLE_MPS_FALLBACK`), and ensuring third-party dependencies are properly maintained for consistent performance and support. Lastly, some development workflows, such as rebasing and merging PRs or handling CI failures, are discussed with an emphasis on collaboration, policy adherence, and proper issue escalation."
2024-09-30,pytorch/pytorch,"The discussions primarily revolve around handling complex, low-level, hardware-specific, and performance-critical features in PyTorch, such as CUDA kernel selection, NCCL communication, and custom operator support, often highlighting the need for better API abstractions or clearer documentation. Several issues involve debugging, error handling, and ensuring compatibility across different hardware (e.g., AMD, NVIDIA, ARM), with specific attention to kernel support and communication bottlenecks. There are also ongoing debates about how to manage experimental or internal APIs (e.g., `force_uncompiled`, `parallelize_module`, `dynamo`) in ways that are user-friendly, layered within the ecosystem, and non-breaking, alongside concerns about flaky tests and CI stability. Additionally, some discussions touch on the challenges of extending support for new hardware features (e.g., FP8, symints, hardware counters) and ensuring the internal pipeline’s robustness, especially in distributed and multi-device scenarios. Overall, the key unresolved questions involve API design, compatibility, and debugging strategies for advanced features and heterogeneous hardware integration."
2024-10-01,pytorch/pytorch,"The collected discussions from GitHub issues highlight several persistent technical concerns:

1. Implementation Gaps and Support Limitations: Several issues involve missing operators (e.g., 'aten::__rshift__.Scalar', 'aten::scatter_reduce.two_out') on specific devices (e.g., MPS, CUDA), or incomplete support for features like float8, dynamic shapes, and subclass handling in autograd and compilation. Temporary workarounds like environment variables or manual operator replacements are commonly used.

2. Flakiness in CI Tests: Multiple threads discuss trunk flakiness affecting platforms like Linux, macOS, and inductor tests, often attributed to flaky tests that pass after numerous reruns, or external platform issues. There's mention of potentially disabling or rerunning tests, with some suggesting further stabilization.

3. Compatibility and Reproducibility Challenges: Issues around build reproducibility (e.g., fbgemm_gpu dependency), support for new hardware (XPU), and inconsistent behavior of functions like `.item()` for float8, indicate ongoing difficulties ensuring platform compatibility and deterministic behavior across versions.

4. Feature Development and API Evolution: Several conversations pertain to developing new features like named parameters, custom operator support, and improving the debugging/dev infra experience, emphasizing the need for better support, documentation, and tooling improvements to handle complex use cases (e.g., nested tensors, compiler flags).

5. Maintenance and Test Stability: Some issues involve ambiguous or flaky test failures, involving internal compiler passes, internal errors in the JIT/inductor, and efforts to stabilize tests (e.g., rebasings, disabling flaky tests) for reliable CI/CD workflows."
2024-10-02,pytorch/pytorch,"The discussions primarily revolve around addressing technical challenges with padding modes in PyTorch, debugging failures in various compiler and runtime components (especially related to Triton kernels, dynamic shape handling, and memory fragmentation), and improving internal features like `torch.compile` and `torchvision` compatibility. Several issues involve ensuring correct behavior when handling non-contiguous tensors, views, and layout conversions, as well as fixing bugs that cause crashes or mismatches in codegen or guard logic. There are also ongoing efforts to enhance the build and testing infrastructure, including handling flaky CI, support for new operator sets (e.g., ONNX opset 21), and internal tool integrations. Many suggestions focus on refining kernel parameter propagation, memory management (e.g., expandable segments, fragmentation), and ensuring internal functions and APIs are correctly exposed and tested. Unresolved questions include how to handle layout and view preservation during serialization, how to reliably test complex compilation paths, and how to improve overall robustness in assembly and runtime code."
2024-10-03,pytorch/pytorch,"The discussions mainly revolve around nuanced details of PyTorch's ONNX export compatibility, especially regarding `align_corners` behavior and opset versions, with efforts to ensure consistency across exports and runtime conversions (e.g., CoreML). Several issues highlight the challenges of ensuring deterministic and comparable results between PyTorch, ONNX, and runtime backends, especially with different backends or hardware (like MPS, ROCm, and CUDA). There's ongoing work to fix trunk flakiness, improve backends for hardware like AMD gfx1100, and enhance the device-agnostic API design via the `torch.acc` namespace, with considerations about the interface's design and consistency. Additionally, a recurring theme is handling dynamic shapes, kernel compilation failures, and ensuring correct memory management and introspection during graph transformations and caching; some of these are addressed through refactoring, bug fixes, and better tooling. Unresolved questions involve ensuring correct handling of kernel arguments, memory offsets, and backend-specific features, as well as debugging robustness, especially around kernel compilation and graph break points."
2024-10-04,pytorch/pytorch,"The comments reveal ongoing challenges with PyTorch's operator support on different backends (e.g., MPS, ROCm, CUDA), especially for unimplemented operators in specific devices, leading to fallbacks or workarounds. Several discussions focus on refining the export and tracing process, such as handling custom autograd functions, ensuring proper guard conditions, and managing symbolic shapes, guards, and mutations to avoid unnecessary recompilations or incorrect optimizations. There's also concern over build stability, flaky tests, CI failures, and the impact of changes on performance tuning and model conversion workflows, including interactions with third-party libraries and external tools like FBGEMM or cuSPARSLt. Additionally, some comments address code correctness and API design, including error handling, the importance of proper synchronization, and potential improvements in code structure and documentation. Overall, unresolved issues involve operator coverage across devices, correctness and efficiency of symbolic shape handling, and stability of CI and build processes."
2024-10-05,pytorch/pytorch,"The discussions primarily revolve around ongoing development and support for Vulkan and ROCm backends in PyTorch, with questions about their current status on desktop and Android, and the challenges of achieving feature parity with CUDA. Several comments highlight issues with internal build configurations, such as incorrect handling of dependencies like hipBLASLt, and suggest simplifications or refactoring of CMake files to improve maintainability. There are concerns about regression fixes, particularly the importance of temporary patches versus more robust root cause resolutions, such as interface changes impacting Intel Gaudi support and potential side effects on distributed and autograd code. Additionally, internal testing and merge failures indicate active efforts to stabilize recent changes amidst external constraints like CLA signing and internal workflows."
2024-10-06,pytorch/pytorch,"The discussions highlight several key technical concerns: the current implementation of batched QR in PyTorch uses a sequential, for-loop-based approach leveraging MAGMA, which results in slow performance on large batches and lacks GPU-level parallelism; there are calls for more parallelized and GPU-efficient implementations. Several users report MPS device operation limitations, such as missing operator support and resulting fallback to CPU, which impacts performance and stability in GPU-accelerated workflows, especially with graph neural networks and object detection models. Also, issues related to rebase failures, incomplete feature support, and build system quirks are noted, alongside questions about implementation details and plans for feature deprecation or enhancement (e.g., dynamic padding, pytrees, and serialization behaviors). Overall, there's a focus on improving algorithmic efficiency, expanding hardware support, and clarifying internal implementation strategies to support better performance and usability."
2024-10-07,pytorch/pytorch,"The comments reveal several recurring technical concerns: implementation gaps for MPS device operations like scatter_reduce, slow or missing support of `aten::isin.Tensor_Tensor_out`, and performance regressions or inefficiencies caused by certain code transformations such as in-place reshaping, aliasing, and subgraph handling. There are issues with trunk flakiness that hinder CI stability, often mitigated by rerunning tests or adjusting environment variables, and some discussions revolve around whether to introduce configurable options (e.g., optimization/decomposition options or post-processing attributes) to improve flexibility and maintainability. Several users inquire about specific feature support, correctness of behaviors (e.g., NaN handling, dtype inference), and compatibility with external libraries or hardware (e.g., CUDA, ROCm, numpy). On the automation side, there are questions about CI merging workflows, test management, and proper handling of reverts and force-merges to ensure stability. Unresolved questions include how to better support complex functions with annotations or attribute objects, how to improve test coverage and reproducibility, and whether design choices like flattening state dictionaries or throwing errors are optimal."
2024-10-08,pytorch/pytorch,"The discussions highlight ongoing challenges in implementing robust exception handling across multiprocessing and distributed contexts in PyTorch, with particular concerns about kernel and operator support on specific hardware (e.g., MPS, XPU, ROCm), and the need for clearer API semantics and documentation for features like `torch.compile` and device management. Some conversations address potential API design improvements, such as enabling support for ""all backends"" in context managers, and clarifying behavior for out-of-place or in-place tensor operations with different data types. There are also issues related to build and environment compatibility, especially with external libraries like oneDNN or CUDA, and the necessity for better test flakiness handling, error diagnostics, and user notifications. Several discussions involve reverts, rebasing, and merging CI failures that indicate ongoing stability and integration challenges. Overall, unresolved technical questions concern operator support, cross-hardware sensitivities, API clarity, and robust error handling in complex distributed and hardware-specific scenarios."
2024-10-09,pytorch/pytorch,"The comments encompass a wide range of issues and discussions related to the PyTorch project, including debugging tensor memory issues, implementation details of APIs for different hardware backends (such as XPU and CUDA), ongoing bug fixes, and performance regressions. Several threads focus on compatibility concerns (e.g., with TorchScript, ONNX, or different hardware architectures like ARM, ROCm, and NVIDIA GPUs), as well as handling specific operator support (e.g., MPS, Mkl, int8 kernels). There are also discussions on infrastructural challenges, test flakiness, CI stability, and procedures for landings, revert strategies, or code refactoring. Some threads involve clarifications about internal behaviors, like tensor identity preservation, kernel activation, or memory management, and suggest future improvements or additional tests. Overall, the discussions highlight active debugging, feature development, backends support, and tooling refinement for PyTorch's evolving ecosystem."
2024-10-10,pytorch/pytorch,"The collected discussions cover a range of issues including environment-specific bugs, feature requests, and implementation clarifications. Several comments address the need for better API consistency across different backends (e.g., XPU, CUDA, CPU) and suggest refactoring to unify tensor and device interface calls, often proposing hierarchical or namespace-based API designs. There are technical concerns about performance implications of new API designs, such as the potential slowdown of certain tensor operations or impact on existing models, alongside suggestions for conditional gating based on hardware capabilities. Discussions also highlight process improvements, such as adding tests or reworking CI configurations to prevent regressions, and some issues related to build system inconsistencies or stale PR states. Overall, unresolved questions focus on balancing API usability and extensibility with performance stability and the proper handling of environment-specific or version-dependent behavior."
2024-10-11,pytorch/pytorch,"The discussions touch on multiple key issues: the potential for background thread and process-related hang-ups caused by forking, which may be mitigated by running PyTorch with `spawn` method instead of `fork`, especially in server environments like Gunicorn; concerns about CPU-based optimization algorithms (notably L2 regularization and its impact on sparsity and acceleration benefits) which seem to degrade performance over time, possibly due to model sparsity changes; the need to support certain operations (e.g., `aten.view.dtype`, reinterpret_cast, and descriptors pre-allocation) in various backends like ONNX, ROCm, and Triton, with some being handled as special cases or requiring future implementation; and ongoing efforts to improve device and kernel support, including low-level GPU/CPU-specific optimizations, handling of new instruction sets like SVE/SVE2, and ensuring compatibility in multi-device setups. Several unresolved questions include whether certain features will be backported, how to best integrate support for pre-allocated descriptors, and how to stabilize runtime behaviors amid process forking and asynchronous communication."
2024-10-12,pytorch/pytorch,"The comments encompass multiple technical concerns, primarily centered around the challenges of installing and proxying PyTorch and related packages (e.g., PyTorch with external repositories, integration with external package managers like Poetry, Pipenv, Nexus). Several discussions address CUDA and memory management issues, such as CUDA graph sharing, allocator semantics, and performance regressions on specific hardware (e.g., AMD, Intel, Apple M1), often involving proposed solutions or workarounds. There are issues related to PyTorch's internal features, including autograd, graph capture, symbolic shape reasoning, and reproducibility, with questions about correctness, API design, and compatibility (e.g., across frameworks or hardware). Unresolved questions include environmental setup, API behavior, and the impact of recent changes on stability and performance, especially for GPU-specific features and cross-platform support. Overall, the discussions reflect ongoing efforts to improve installation, hardware compatibility, memory efficiency, debugging tools, and correctness of advanced features in PyTorch."
2024-10-13,pytorch/pytorch,"The comments highlight ongoing issues with test failures and build compatibility, especially related to linking problems on older versions of Clang's linker (LLD), and the need to appropriately enable Link Time Optimization (LTO) across different compiler versions. There are concerns about flaky or flaky-appearing tests, such as those depending on specific hardware support (e.g., ROCm, AMD/Intel GPUs) or certain features (e.g., boolean support in XNNPACK), with suggestions to skip or disable certain tests where necessary. Additionally, discussions involve mechanisms for managing PR merges (e.g., force merges, reverts, progress checks) and ensuring CI stability, as well as clarifications on testing procedures like end-to-end validation before releases. Overall, the key issues revolve around build environment compatibility, test stability, and proper CI/CD workflows for large-scale code integration."
2024-10-14,pytorch/pytorch,"The comments highlight several recurring themes in the PyTorch repository: a need for improved type and API support, such as adding `nn.Residual` and better `ModuleDict` typing; handling discontiguous tensors and memory performance issues during device transfers, especially on GPUs where non-contiguous data causes slowdowns; and longstanding bugs related to autograd, `torch.compile`, and export correctness, some of which involve intricate interactions with mode stacks, tensor subclassing, and graph tracing. There are also discussions about enabling features like LTO, improving cache metrics, and fixing regressions after interface changes. Many issues remain ambiguous or incomplete, with questions about proper API design, test stability, and compatibility across hardware and software versions. Overall, the community seeks both functional improvements and clearer, more robust debugging and testing frameworks to address these deep-seated stability and performance concerns."
2024-10-15,pytorch/pytorch,"The comments across various GitHub threads highlight several technical concerns, including issues with dataloader speed and stability on Windows 10/11 with multiple workers, which newer PyTorch versions attempt to address with flags like `persistent_workers=True`. There are recurring problems related to CUDA/cuBLAS/cuSolver DLLs and device compatibility, especially on Windows and ROCm/Linux, sometimes requiring environment or dependency adjustments. Some discussions involve refining PyTorch's internal API and functionality, such as improving how view operations, tensor slicing, and caching mechanisms work, with specific attention to correctness and performance trade-offs. Several threads also mention incomplete or flaky tests, CI failures, and the need for proper documentation, bug fixes, or patches—often with collaboration indicated between contributors and internal teams. Lastly, there are ongoing efforts to improve build processes, integrate new features like bf16 support, and address hardware-specific issues like those related to AMD GPUs and compiler optimizations."
2024-10-16,pytorch/pytorch,"The discussions highlight several ongoing issues and development considerations within the PyTorch repository:

1. Compatibility and support for new hardware, such as XPU and ROCm, including refactoring backend code for device-agnostic execution and ensuring proper build/configuration workflows.
2. Edge cases and correctness in the export and compilation process, especially around handling default parameters, dynamic sizes, and specific operator behaviors (e.g., `aten.conv2d`, `aten.index_put_`, `aten.upsample_nearest2d`), with attention to potential BC-breaking changes and test coverage.
3. Performance and memory management concerns, including tracking and verifying cache behaviors, optimizing pass propose routines, and ensuring consistency and reproducibility across different hardware and software environments.
4. Addressing internal/trunk flakiness by stabilizing CI tests and refining the tracing, guard, and de-duplication logic to prevent false positives or flaky test failures.
5. Implementation plans for features like supporting dynamic ranges, better peripheral support (e.g., `torch.distributed.recv_object_list` semantics), and adjusting compiler flags, with a focus on incremental solutions and thorough testing before major releases."
2024-10-17,pytorch/pytorch,"The discussions reveal ongoing challenges and issues related to PyTorch's distributed training, CUDA, and ROCm support, including problems with multi-GPU memory sharing, kernel support, and drivers. Multiple threads focus on handling specific failures in tests, CI flakiness, and build failures, often requiring conditional guards based on hardware capabilities or software versions. Several suggestions aim to improve robustness and maintainability, such as adding feature guards, refactoring code for device-agnostic support, and enhancing test coverage and diagnostics. For some issues, the root causes involve driver and hardware incompatibilities, like unsupported architectures on AMD GPUs, or incomplete support for new features like bf16 or low-bit arithmetic. Unresolved questions include better tooling for managing hardware-specific code paths, handling dynamic shape inference in conditionals, and establishing more stable regression testing without false positives."
2024-10-18,pytorch/pytorch,"The discussions highlight several technical challenges: enabling testing of privateuse1 device fallback mechanisms in CI across platforms like Mac and Linux, which is currently limited or unsupported, and require examples for correct registration or fallback handling; addressing regressions where certain operators or bugs persist or have re-emerged despite previous fixes, often linked to specific hardware, driver, or library versions (e.g., cudnn, cuDNN, libcufile); managing flaky or unstable tests across CI platforms, with multiple issues involving flaky trunk failures, platform-specific failures, or long rerun loops; and compatibility concerns with custom kernels, dynamic shape handling, memory management, and device features (e.g., cudagraphs, autocast, fsspec integration, and new API configurations). Many discussions involve regressions due to recent PRs, workarounds through environment variable or code modifications, and considerations for adding new APIs or fixing existing inconsistencies—particularly in terms of proper registration, testing, and platform support—to ensure robustness and usability across diverse hardware/software setups."
2024-10-19,pytorch/pytorch,"The comments reveal recurring issues related to multiprocessing start methods, notably the RuntimeError when set_start_method('spawn') is called after the context is already set. Several users describe solving training or data loading problems by adjusting parameters like num_workers, pin_memory, or setting map_location in torch.load, as well as environmental variables like LD_LIBRARY_PATH or CUDA version downgrades. There are ongoing discussions on CUDA graph optimizations, including dynamic memory management, kernel patching, and how to turning CUDA Graphs on by default in torch.compile, with concerns about performance overhead, compatibility, and implementation complexity. Some issues pertain to specific hardware support, such as AMD HIPBLAS, ARM, and Intel's approaches, highlighting the need for platform-specific configurations and support. Overall, the conversations focus on resolving environment compatibility, debugging multiprocessing and CUDA graph features, and improving robustness across diverse hardware and software setups."
2024-10-20,pytorch/pytorch,"The discussions highlight several technical concerns, including the incomplete implementation of operators like `aten::grid_sampler_3d` on specific devices such as MPS, with suggestions to add fallback options and improve device support consistency through standardized APIs. There are ongoing questions about enhancing multi-stream support and passing stream info through FakeTensor for better compatibility with Inductor and graph fusion. Additionally, challenges related to experimental features like torch.cond, with debates on implementation complexity, and issues with API behavior inconsistencies or test failures due to recent changes or deprecations are prominent. Several discussions also focus on build failures, merge conflicts, and the need for better testing, documentation, and error handling, especially around operator support and device abstractions. Unresolved questions mainly revolve around handling incomplete operator support, stream management, and how to safely introduce new features without breaking existing functionality."
2024-10-21,pytorch/pytorch,"The discussions primarily focus on handling tensors not being leaf nodes in DataLoader, especially when loaded from pretrained models, and how to properly set `requires_grad` or detach tensors during training. Several issues highlight the need for clearer API support for device abstraction, including handling non-CUDA devices and unified device management. Additionally, some technical challenges involve improving error messages for model exports, supporting FP8 tensor operations (like `cat`), and addressing specific backend bugs (e.g., with MPS or cuDNN). Regressions, flaky tests, and build environment inconsistencies also feature prominently, with suggestions for better error handling, environment validation, and test stability. Unresolved questions include how best to instrument memory testing, extend device APIs, and support complex data types during export or in custom operators."
2024-10-22,pytorch/pytorch,"The comments reflect ongoing discussions about mismatched padding schemes in PyTorch's 'same' convolution implementations across frameworks and frameworks' padding conventions, with a focus on making padding calculations compatible with TensorRT and SciPy. Several issues involve correctness and performance regressions, including handling of uneven batches, dynamic shapes, and specific hardware/software bugs (e.g., on MPS, ROCm, CUDA). Some concerns revolve around improving API consistency, explicitness, and automation—such as replacing implicit conditionals with `torch.cond`, better handling of nested/unaligned tensors, and managing multi-device setups or custom CUDA graphs. Users also raise questions about the evolution of support for sparse tensors, torch.linalg functions, and backend/device abstraction, alongside handling environment-specific build/test failures and distribution overhead. Overall, unresolved questions focus on fixing bugs, optimizing performance, improving API usability, and coordinating hardware-specific support for newer features."
2024-10-23,pytorch/pytorch,"The comments reflect ongoing discussions and issues related to PyTorch's support for various hardware targets, especially ARM64, CUDA, and ROCm. There are questions about building and deploying cross-compiled or sharded models, and the need for better abstractions to handle capabilities across diverse devices rather than device types. Several issues highlight flaky tests, CI infrastructure concerns, and regressions in performance or correctness, especially when using features like torch.compile, FSDP, or exporting to ONNX. Some comments concern internal changes, such as the handling of environment variables, native function support, and patching infra bugs, with calls for improved testing and documentation. Unresolved questions include how to properly manage mixed hardware capabilities, enforce support for new hardware, and organize code for custom low-bit or hardware-specific kernels in a way that simplifies future extension and maintains reliability."
2024-10-24,pytorch/pytorch,"The comments primarily revolve around complex technical issues and feature requests in PyTorch, including challenges with Python's multiprocessing memory behavior, integration of nan operators like `nanstd`, support for RISC-V and low-bit quantization kernels, as well as debugging and reproducibility concerns with torch.compile, AOTAutograd, and inductor. Several discussions address kernel-specific bugs, CUDA and hardware support regressions, and the need for better documentation, test coverage, and more generic API designs that abstract device capabilities instead of device types. Notably, there are ongoing efforts to improve ONNX conversion, handle complex and dynamic shapes, extend support for distributed collectives, and fix performance regressions or flaky tests in CI. Unresolved questions include how to balance API improvements with backward compatibility, the best approaches for integrating low-bit kernels, and strategies for managing CI flakiness and hardware-specific bugs."
2024-10-25,pytorch/pytorch,"The comments mainly highlight ongoing efforts to address various flaky tests, CI infrastructure issues, and compatibility concerns within the PyTorch repository. Several reports involve disabling and re-enabling tests based on flakiness, with some discussions about improving reliability or adjusting test configurations (e.g., handling of dynamic shapes, memory guards, and specific platform failures). There are also questions regarding the correct handling of features like `export` versus `export_for_training`, and compatibility issues with third-party dependencies such as MKL or oneAPI. Efforts to integrate new features, fix regressions, and ensure ABI compatibility (e.g., for `sycl`) are also evident. Unresolved questions include the impact of certain fixes on performance or correctness, and the need for better testing, documentation, and stability enhancements for new or existing functionalities."
2024-10-26,pytorch/pytorch,"The discussions highlight various technical concerns including the need for improved handling of shared objects in nested tensors, with workarounds involving `offsets()` and shape mismatch issues during concatenation and backward passes. There are questions about tool compatibility, such as using Triton on Windows, and challenges with operator support in custom backends (e.g., `aten::empty_strided` with `PrivateUse1`). Several merge failures and build/test issues due to CI timeouts, flaky tests, or external dependencies (e.g., libcudnn, oneAPI). Debates around SDK updates, performance regressions, and the impact of changing internal data structures (like `lazy_property`) on autograd and graph capturing are also prominent. Unresolved questions include how to properly integrate certain features (like dual inference with `torchrun`) and handling environment-specific build/link errors."
2024-10-27,pytorch/pytorch,"The comments reflect ongoing challenges with handling uninitialized or shape-unknown buffers in PyTorch, including difficulties with `UninitializedBuffer`, lazy modules, and buffer registration nuances. Multiple discussions address limitations in shape inference during ONNX export, especially for tensors with dynamic or unknown dimensions, and potential workarounds such as explicit reshaping or shape annotations. There are concerns about the support for specific features like RISC-V CI testing, complex FP16 computations on GPUs, and backend-specific optimizations, with suggestions for workarounds or infrastructure enhancements. Some issues relate to merge conflicts, CI failures, or rebase challenges, indicating ongoing integration and testing hurdles. Overall, there's a focus on improving robustness for shape inference, lazy initialization, cross-platform testing, and comprehensive CI coverage."
2024-10-28,pytorch/pytorch,"The discussions highlight several technical issues and proposals related to PyTorch development. There is a recurring concern about inaccuracy and performance regressions introduced in recent code, often addressed by reverting changes, adding tests, or improving code correctness—particularly around compiler behavior, code generation, and dynamic shape handling. Some threads focus on clarifying API behaviors, such as residual modules, scatter operations, and autocast support, with suggestions for better API design and documentation. Numerous issues involve build and compatibility problems, especially with CUDA, ROCm, and external libraries like oneAPI and ACL, requiring version upgrades or environment adjustments. Overall, the discussions emphasize the need for more robust testing, clearer API semantics, and careful handling of environment-specific and hardware-specific constraints to ensure stability and performance."
2024-10-29,pytorch/pytorch,"The discussion covers various issues encountered in the PyTorch repository, primarily related to build system updates, compatibility, and feature regressions. Several comments highlight the need for proper version management (e.g., CUDA, oneDNN, sccache, package dependencies) and handling of platform-specific or hardware-specific issues (e.g., AMD ROCm, SymInt, nested tensor support). There are recurring concerns about CI failures due to protocool changes, build configuration mismatches, and performance regressions, with suggested workarounds like environment variable adjustments or code modifications. Some threads focus on functional correctness, such as ensuring operator compatibility (e.g., average pooling on CPU, convolutional equivalences) and handling edge cases in autograd or compile-time transformations. Several areas remain unresolved, notably compatibility updates, performance optimization, and environment consistency, requiring further review or targeted fixes."
2024-10-30,pytorch/pytorch,"The discussions highlight several key technical concerns, such as questions about the behavior of `amp` with master FP32 parameters and activation memory usage, and issues with reproducibility of models involving torch.compile, lazy initialization, and lazy/lazy guards. Multiple threads also discuss complex build and compatibility issues with CUDA, ROCm, and specific hardware (e.g., AMD GPUs, MacOS), as well as flakiness in CI tests attributed to system variability or outdated environments. There are concerns about the correctness and performance implications of model compilation, especially around fusion, reduction fusion, and kernel code generation, often coupled with questions on test robustness, reproducibility, and performance measurement. Some threads also address API design questions, such as the handling of `zero_grad()` with FSDP, whether to introduce new APIs (e.g., `torch_testing`), and the potential for reusing communication backends across codebases. Unresolved questions include the impact of specific compiler flags, hardware support, and build/test environment consistency, particularly in distributed or GPU-accelerated contexts."
2024-10-31,pytorch/pytorch,"The comments span a range of technical issues and discussions related to the PyTorch repository. Common concerns include experimental or unstable features (e.g., Triton kernel support, autocast extensions), backend and hardware compatibility (e.g., ROCm, XPU, MPS, CUDA versions), and build or test flakiness on various platforms. In several cases, there are requests for additional diagnostics, better error messaging, or performance improvements, such as reducing autotuning overhead or supporting more flexible shape and device configurations. Some comments reference ongoing or pending PRs that aim to address these issues, with questions about release plans, bug fixes, or feature support. Overall, key unresolved questions involve ensuring hardware compatibility, improving debugging and test stability, and clarifying feature expectations across different backends and hardware platforms."
2024-11-01,pytorch/pytorch,"The discussion broadly concerns improving PyTorch's behavior regarding model tracing, serialization, and runtime compatibility. Key issues include handling discrepancies between traced and Python models, especially with floating-point outputs (`Tensor-likes are not close` warnings), and the support for core linear algebra operations in ONNX exports, which currently lack certain operators like `linalg.inv`. There's also a focus on the stability and performance of NCCL networking on various hardware, especially on multi-GPU and mixed-network setups, and how to optimize distributed initialization processes (e.g., serialized vs. parallel init). Additional concerns involve ensuring correct memory and guard management in Dynamo and model bookkeeping, fixing bugs that cause regression or failures in multi-device setups, and improving tooling and testing for new backend features such as XPU support and custom kernel integration. Lastly, some suggestions aim toward more explicit API design, better profiling, and better handling of debugging and validation strategies in complex training/inference pipelines."
2024-11-02,pytorch/pytorch,"The discussions highlight concerns about the behavior of `BCEWithLogitsLoss`, especially regarding its use of the log-sum-exp trick and handling of extreme logits, which can lead to `nan` values, and the implications for autocasting and numerical stability. There are questions about whether this behavior is a feature or a bug, and how native support for certain input ranges could be improved. Several issues relate to merge failures due to conflicts or internal checks, including the handling of internal test failures in the CI pipeline, and the need to support features like `aten::scatter_reduce.two_out`. Additionally, some conversations focus on specific platform or hardware compatibility problems (e.g., M1/M2 Macs, XPU, ROCm), performance regressions between PyTorch versions, and the appropriate design of operator signatures or guard logic for mutable tensors. Overall, unresolved questions remain about best practices for certain edge cases, handling of in-place tensor mutation, and ensuring stability across diverse hardware configurations."
2024-11-03,pytorch/pytorch,"The discussions involve a variety of technical concerns, including the need for additional support and features in PyTorch such as scatter_reduce, padding support with customizable values, and handling of special kwargs in Triton kernel configurations. Several issues highlight problems with existing limitations, such as the performance and correctness implications on MacOS with MPS, and the handling of dynamic shapes and guards in TorchDynamo. There are also ongoing efforts to improve inlining, guard evaluation, and serialization processes, alongside updates to CI/CD workflows and merge protocols to address build failures and conflicts. Some discussions suggest potential enhancements like avoiding non-integral symbols in tensor analysis, and clarifying interactions between AOT and JIT modes, especially concerning guard evaluation and model specialization."
2024-11-04,pytorch/pytorch,"The discussions encompass several key topics: improving the use of the `__name__ == ""__main__""` idiom in scripts to prevent errors like BrokenPipeError during multiprocessing (Issue #5858), and ensuring proper placement of such guards in main scripts and spawn contexts; clarifying support for arbitrary leading dimensions and the non-limiting of the third-to-last tensor dimension in torchvision transforms, possibly requiring PRs and API adjustments; addressing device support concerns, particularly for AMD (ROCm) and Intel GPUs (XPU), including build errors, performance regressions, and CI support; and ensuring stability and correctness of onnx/trt exports, especially regarding unused inputs and warning propagation (Issue #139646). Other concerns involve proper testing and documentation updates, handling of tensor views during AOTAutograd, and the need to rebase large PRs for reliable review. Overall, questions center on compatibility, correct API usage, precise error handling, and improving tooling robustness across diverse hardware and software environments."
2024-11-05,pytorch/pytorch,"The comments highlight ongoing challenges and discussions around improving PyTorch's handling of dynamic and static shapes, especially in the context of export, tracing, and runtime optimizations. Key concerns include ensuring the correctness and consistency of graph representations during export, managing shape constraints and aliasing issues, and handling heterogenous tensor layouts or external data formats. Some discussions focus on how to better support features like event hooks, tensor parallelism, and device-specific behaviors, with suggestions for API enhancements, better diagnostics, and fixing regressions caused by recent commits. A recurring unresolved question is balancing performance, correctness, and stability across diverse hardware and use cases, often requiring detailed debugging, rebase workflows, and communication with internal teams. Overall, the discussions aim to refine the codebase, tooling, and user APIs to better support advanced model compilation, export, and execution scenarios."
2024-11-06,pytorch/pytorch,"The discussion spans multiple topics, including the robustness and corner cases of internal PyTorch implementations (e.g., backtrace failures in Chinese environments, default behavior of `weights_only`, and handling of NaN conversions across data types), as well as broader issues like trunk flaky tests, CI configurations, and build reproducibility. Specific concerns involve the correctness of certain functions (e.g., `to_symmetric` in PyG, `torch.sym_not` in ONNX), and the implications of in-place tensor modifications that impact parallel inference and distributed training (e.g., in DDP, model loading, and multi-GPU performance). There are questions about how to improve or stabilize runtime behaviors, such as handling of CUDA Graphs, Triton kernel resource limits, and dependency versioning (e.g., `mkl`, `triton`). Unresolved issues include maintaining backward compatibility with new serialization schema, ensuring correct handling of nested/dynamic shapes in ONNX export, and managing flaky CI tests across different platforms. Overall, the discussions highlight a push towards more robust, predictable, and scalable PyTorch features, along with tooling improvements for better debugging, testing, and deployment."
2024-11-07,pytorch/pytorch,"The comments reveal several recurring themes: concerns about flaky or inconsistent test results in CI (often resolved after many reruns), especially for inductor/torchinductor and trunk platforms; discussions on improving serialization, caching, and build processes (e.g., using dedicated directories for inductor caches, enabling debugging features, or understanding Out-of-Memory issues); questions about environment configuration, dependency management (e.g., CUDA, MKL, NCCL, cuDNN, system libraries), and compatibility issues across different systems (Windows, Linux, ROCm); suggestions for API improvements like better error messages, combining configs, and enabling features (e.g., for libuv, torch compile, or profiling); as well as ongoing efforts to prevent flakiness, improve reproducibility, and enhance internal tooling, with some unresolved questions regarding specific configurations, environment setups, and serialization correctness."
2024-11-08,pytorch/pytorch,"The comments reveal persistent issues with flaky CI tests, often related to trunk flakiness, platform-specific failures, or outdated dependencies, which makes debugging difficult. Several questions focus on understanding root causes, such as the impact of graph specialization, the behavior of effectful nodes with respect to node replacement, or discrepancies between nightly and build-from-source wheels. There are recurring concerns about testing infrastructure, including whether certain tests should be disabled in specific CI jobs, how to support dynamic shapes and nested subgraphs in model export, and how to better log or reproduce test failures. Additionally, issues with build configurations, missing dependencies (like libcudnn or mkl), and platform-specific quirks (Windows, ROCm, Android) are repeatedly raised, often asking for simplified repros or clearer documentation to facilitate debugging and maintenance."
2024-11-09,pytorch/pytorch,"The discussions primarily revolve around addressing specific technical issues encountered in PyTorch related to CUDA, MPS, and MPI operations, including environment setup, build configurations, and device compatibility. Several comments highlight the importance of proper linking to CUDA libraries such as cuDNN, and the challenges of ensuring consistent behavior across different OS environments, especially with WSL and Windows. There are concerns about specific internal failures, like those in inductor, FBGEMM, and codegen, often linked to compiler flags, variable length arrays in C++, and environment-dependent configuration differences. Additionally, questions about reproducibility, debugging, and future features such as in-place kernel variants and better support for dynamic shapes and branching are raised. Overall, the key issues involve improving build robustness, environment consistency, and extending capabilities for dynamic and in-place computations within PyTorch's evolving infrastructure."
2024-11-10,pytorch/pytorch,"The discussions highlight ongoing development and troubleshooting within PyTorch, including feature requests such as tensor padding based on channel-wise means and support for tensor operations on arbitrary dimensions. Several issues involve build and compatibility challenges, such as ensuring macro support for NCCL, handling device-specific logic (e.g., HPU device IDs), and supporting device-agnostic guards in graph export. Recurrent concerns relate to test failures, merge conflicts, and ensuring code changes are properly rebased and validated across different environments. Additionally, there are ongoing feature discussions around stable reparameterizations for distributions, graph capturing with dynamic and discrete shapes, and handling mixed floating-point modes in workflows like ComfyUI."
2024-11-11,pytorch/pytorch,"The discussions highlight several technical issues, including challenges with PyTorch's JIT and TorchDynamo support for complex models with hooks, the need for more comprehensive testing and coverage of operators, and device-specific operator implementation gaps on MPS and ROCm. Key concerns involve handling of unsupported or not yet implemented operators, especially on mobile and MPS devices, as well as performance regressions and resource constraints observed during compilation (e.g., shared memory limits, kernel parameter tuning). There are also questions around the integration of custom kernels, the correctness of quantization, and the handling of dynamic shapes and offloading in distributed training. Several discussions suggest workarounds, work-in-progress fixes, or future plans to address these issues, but some remain unresolved or require further investigation."
2024-11-12,pytorch/pytorch,"The comments highlight various technical issues encountered in the PyTorch repository, such as system and library incompatibilities (e.g., glibc TLS limits, ROCm support on AMD GPUs), performance regressions, and correctness bugs in both inductor and JavaScript-like operator implementations. Several discussions revolve around improving tracing and compilation reliability, supporting complex mask patterns in attention modules, and refining hyperparameter heuristics for efficient kernel selection (e.g., reduction sizes, in-place variants). There are also proposals to enhance the testing framework to better detect silent errors, validate numerical accuracy, and prevent flaky trunk failures. Additionally, some issues concern environment setup and configuration, notably how environment variables (like visible devices) interact with system support and job scheduling across diverse hardware like AMD, NVIDIA, and XPU devices."
2024-11-13,pytorch/pytorch,"The comments revolve around several technical concerns:
1. Certain gradient and backward pass behaviors (especially related to log_softmax, CTC loss, and autocast caching) are inconsistent or incorrect across implementations, often involving issues with gradient projection on log-softmax tangent space and autocast cache interactions. 
2. There are ongoing efforts to improve launch, profiling, and debugging tools, including better profiling for inductor and handling of autocast caches, auto tuning, and handling dynamic shapes.
3. Numerous experimental setups, test failures, and flaky behavior in trunk CI due to auto-recompile, flakiness, or environment inconsistencies (like NCCL, GPU variant support, and CPU/GPU interactions), with discussions about disabling/enabling specific tests or changing configurations.
4. Several issues complicate debugging, including incomplete reproductions, flaky trunk tests, and the need for better reproducibility, diagnostics, and handling of different hardware environments like AMD, Intel, and NVIDIA GPUs.
5. Some discussions also hint at architectural improvements, such as supporting auto-inplace variants, higher-level API integrations, or more robust support for specific hardware features (e.g., on XPU, ROCm, or new autocast behaviors)."
2024-11-14,pytorch/pytorch,"The discussions highlight several issues: (1) the support for nested and ragged tensors, particularly in attention mechanisms like flex_attention, is still evolving, with workarounds needed for indexing and performance; (2) certain features such as `torch.where(mask)` lack support or proper implementation on specific devices like MPS, and may require extending existing APIs or adding tests; (3) there are ongoing trunk flakiness and flaky test failures across platforms, complicating CI stability; (4) users and contributors emphasize the importance of providing minimal reproducible examples for debugging, despite the difficulty of isolating complex models; and (5) compatibility and BC-breaking concerns are raised around modifications to APIs like `pynvml`, `native_functions.yaml`, and seed management in distributed setups. Overall, the focus is on improving feature support, stability, and user guidance for advanced tensor operations and deployment scenarios."
2024-11-15,pytorch/pytorch,"The discussions reveal multiple issues including compatibility between PyTorch and NumPy 2.0, particularly regarding API support and symbol attributes, with suggestions to tie support to NumPy 1.x as a baseline. There are concerns about the impact of certain API changes and the potential for breaking compilation or runtime behavior, especially on GPUs and specialized hardware like ROCm, AMD, and XPU devices, sometimes involving kernel or driver issues. Some comments highlight the importance of build dependencies, environment configuration (e.g., threading for Triton kernels, setting environment variables), and the need for better documentation and testing strategies to handle potential failing scenarios (like CUDA device asserts, unsupported operations, or miscompiled kernels). Additionally, questions arise around pipeline dependencies in CI workflows, handling flaky tests, and the impact of internal code modifications (e.g., mocked configs, PR reverts, or codegen customization). Overall, unresolved concerns include ensuring compatibility across frameworks and hardware, maintaining stable CI processes, and clarifying the impact of specific code modifications."
2024-11-16,pytorch/pytorch,"The discussions primarily revolve around CUDA and hardware compatibility issues, including selecting the correct CUDA version for installation, device support (particularly non-GPU devices like AMD and IPUs), and environment setup for proper functioning of PyTorch with different hardware backends. Several comments address bugs and potential fixes related to CUDA kernel operations, kernel launch configurations, and compilation flags. There are also questions about improving documentation clarity, managing environment variables for device visibility, and ensuring proper handling of memory reporting APIs to distinguish between caching and driver-reported metrics. Unresolved questions include how to properly support non-GPU devices in benchmarking, how to handle environment differences (like environment variables), and the precise impact of specific code changes on performance or correctness."
2024-11-17,pytorch/pytorch,"The discussions reveal ongoing challenges related to type checking inconsistencies, specifically around `torch.fx.GraphModule` vs `torch.fx.Graph`, and the use of `# type: ignore` suppressions due to inheritance from `torch.nn.Module`. There are technical concerns about the integration and debugging of `torch.compile`, especially in relation to gradient errors and convergence issues observed in NHWC group normalization with Triton, which may stem from interactions with `torch.compile` or inductor's handling of tensor layouts. Several merge failures indicate CI infrastructure and environment discrepancies, such as mismatched CUDA/CUDNN versions, OS updates, or missing dependencies like `shm.dll`. Additionally, questions arise around process group initialization in distributed training, particularly for HPU support, and concerns over build reproducibility, compatibility, and the handling of large codebase modifications within CI pipelines. Overall, unresolved questions include how to refine type annotations, fix gradient and convergence bugs tied to `torch.compile`, manage environment updates for consistent testing, and ensure proper initialization of distributed process groups across diverse hardware backends."
2024-11-18,pytorch/pytorch,"The comments reflect ongoing investigations into PyTorch's runtime behaviors and performance issues, particularly around model serialization, autograd correctness, and compilation behaviors. Several issues highlight discrepancies in saving/loading models (full models vs. state_dict), potential bugs in autograd related to conditional guards, and performance regressions possibly caused by decompositions or backend-specific optimizations. There are also discussions about related test flakiness, platform-specific bugs, and build/environment compatibility problems, with suggestions to verify differences using recent nightly versions or environment variables. Some challenges involve reproducing issues outside of internal environments, fixing incompatibilities or missing features (e.g., in ONNX exporter, custom operator registration), and ensuring proper debugging tools are used for root cause analysis. Overall, the issues are a mix of correctness, performance, and portability concerns requiring further targeted testing, code fixes, and environment adjustments."
2024-11-19,pytorch/pytorch,"The discussions highlight several recurring technical concerns: firstly, issues with interoperability and correctness in model saving/loading practices, especially the distinction between `weights_only=True` and `False`, and best practices in serializing models with diverse architectures; secondly, support and compatibility challenges for special operator patterns (e.g., nested tensors, certain custom ops) with dynamic shapes, compilation, and ONNX export, including handling of shape invariants and guards; thirdly, network and infrastructure reliability issues, such as flaky CI tests, docker build timeouts, and environment setup problems on different platforms (ROCm, XPU, MacOS), often linked to version mismatches or hardware/software limitations; fourth, difficulties in extending support for new hardware features like AMX micro-kernels, and maintaining BC while introducing new APIs (e.g., for grouped GEMMs); lastly, ongoing performance and correctness bugs in advanced features like activation checkpointing, caching mechanisms, and newer operator implementations, with suggested workarounds or incremental development plans to enhance stability and compatibility."
2024-11-20,pytorch/pytorch,"The extracted comments reveal ongoing challenges related to ensuring compatibility and correctness when deploying PyTorch with specific hardware and software configurations. Key concerns include the stability and detection of autograd's non-deterministic behaviors when used with torch.compile, especially involving backward passes and thread handling; issues with low-level GPU kernel launches and interaction with Triton and ROCm, which can cause runtime failures such as segmentation faults or memory errors; and the management of dynamic tensor shapes and invariant checks in the context of JIT and symbolic shape analysis, including guarding and guarding failures. Additionally, there are questions about build configurations, such as the correct inclusion of libraries and GCC/clang versions, and ensuring that custom kernel implementations and graph transformations (e.g., for sparse tensors or inductor/auto-tuning) are robust across platforms. The discussions also highlight the need for better tooling, testing, and documentation around these complex interactions, especially for newer hardware and evolving features like FSDP2."
2024-11-21,pytorch/pytorch,"The discussions highlight significant concerns about package modularization and disk space efficiency in PyTorch, especially regarding the monolithic libtorch_cuda.so, as well as Docker image bloat and network transfer bandwidth. Several issues point to build and environment challenges on different hardware and OS configurations, including ROCm, ARM, and macOS, often caused by version mismatches or missing dependencies. There's a recurring concern about reproducibility and correctness of certain features, such as RNG behavior in distributed training, especially with DTensor, and the support for advanced models like nested tensors. Multiple issues involve test flakiness and CI instability, sometimes linked to external factors like GLIBC versions or Docker container configs. Unresolved questions include improving build stability, better environment management, and ensuring robustness across diverse hardware and software stacks."
2024-11-22,pytorch/pytorch,"The comments highlight ongoing technical concerns related to PyTorch development. Notably, there are issues with support and integration of new CUDA versions and hardware (e.g., CUDA 12.6 support, ROCm, XPU), with some requiring rebase or compatibility fixes. Several discussions revolve around testing stability, CI flakes, and build failures, often due to environment or dependency issues, with suggestions for better CI handling (e.g., logging, rebase strategies, conditional guards). There are also questions about functional correctness, like handling of tensor views in inductor, and specific bugs (e.g., with torch.cond(), torch.compile, torch._dynamo) and trajectories for improvements (like better serialization, custom ops, etc.). Overall, the exchanges involve diagnosing failures, improving compatibility, and planning future improvements for robustness and support for hardware/HIP APIs."
2024-11-23,pytorch/pytorch,"The discussions primarily revolve around efforts to improve PyTorch's robustness, especially related to operator support across devices such as MPS and ROCm, as well as issues with symbolic shape guards and data-dependent guards that cause runtime errors or performance degradation. Several comments highlight ongoing rebasing, merge conflicts, and CI failures, sometimes due to internal framework regressions or unsupported ops (e.g., `aten::logspace.out` on MPS, `aten::_scaled_dot_product_flash_attention_for_cpu` on ROCm). Users propose potential fixes, including driver checks, fallback options, and better user notifications for unsupported operations. There are also discussions about development workflow practices, such as proper test templates, reproducibility, and handling of unaligned or unbalanced datasets in distributed training. Overall, unresolved issues include operator support on various backends, symbolic shape guard failures, and ensuring stable, supported code paths in complex distributed or accelerated environments."
2024-11-24,pytorch/pytorch,"The discussions primarily revolve around ensuring correct and optimized behavior of PyTorch features across different hardware and software configurations. Key concerns include the proper registration and impact of operators for CUDA versus CPU—particularly how fallback operators might benefit from ARM SVE support—and the importance of setting environment variables before torch import for MPS fallback fixes. There are questions about the implications of signature changes, such as incorporating hints or flags (e.g., disabling 0/1 specialization), and how these affect type safety, semantics, and inference behavior. Additionally, issues about debugging and test stability—like the potential for long-standing loader or kernel bugs and the need for detailed logging or memory snapshots—are raised. Finally, some discussions involve code quality, merge workflows, and ensuring correctness in meta functions and dispatch behavior across platforms."
2024-11-25,pytorch/pytorch,"The collected comments from the GitHub issues highlight several recurring themes: (1) challenges and ongoing efforts to improve or fix PyTorch's support for exporting models to ONNX, especially with the Dynamo-based exporter and mixed precision (INT8/INT16) support; (2) stability concerns and flaky test flakiness mitigation, particularly for CI tests on various platforms like ROCm, Windows, and Linux, with many issues being resolved after multiple reruns; (3) problems related to specific operators, such as support for CUDA, MPS, and XPU device functionalities, including symbolic shape support and custom device handling; (4) regressions or bugs introduced by recent commits (e.g., commit 861bdf9), with diagnostic testing to identify problematic changes; (5) ongoing development around features like dynamic shape support, custom kernel registration, and distributed training behavior. Many issues involve workarounds, incremental fixes, or awaiting upstream patches, with frequent asks for review, rebase, or further feature support."
2024-11-26,pytorch/pytorch,"The comments highlight several ongoing technical concerns: in Issue #52241, there is an apparent discrepancy in gradient calculations related to log_softmax and CTC loss, with proposed workarounds to correct backpropagation. Issue #141520 discusses the need for supporting RISC-V architectures, but current infrastructure and ecosystem packages like numpy lack RISC-V binaries, and enabling full support appears non-trivial. Several other issues involve build failures, flaky tests, and CI process unavailability, often linked to environment mismatches, such as GLIBC versions, compiler regressions, or broken package dependencies. Additionally, some discussions suggest improvements in documentation, testing, and the handling of dynamic shapes, metadata mutation, and compiler recompiles, with an emphasis on careful validation of correctness and performance impacts. Overall, these discussions focus on bug fixes, platform support, build stability, and correctness validation in PyTorch's evolving ecosystem."
2024-11-27,pytorch/pytorch,"The comments primarily address challenges and inquiries related to PyTorch’s ongoing development and capabilities. Several discussions focus on the implementation details and limitations of features like `torch.export.export` versus FX symbolic tracing, especially concerning model behavior modifications and input signatures. Others pertain to hardware support issues, such as CUDA device behaviors, MPS operator support on Apple Silicon, and performance regressions involving kernel caching and High-Level API changes. There are also technical questions about recursive handling of dynamic shape variables (`SymInt`), consistency of RNG states between eager and compiled modes, and issues with backend-specific performance and compatibility, notably on ROCm and Intel architectures. Additionally, some discussions include operational aspects like CI flakiness detection, CI pipeline management, and the need for better documentation or testing strategies."
2024-11-28,pytorch/pytorch,"The comments highlight several technical concerns, notably the need for improved masking support in PyTorch to handle cases like entire row masking in attention, and the desire for clearer, more precise naming for reduction modes to avoid confusion about whether weighted sums are included. There are ongoing issues with device support, such as incomplete operator implementations on MPS and ROCm, and the need for better handling of dynamic shapes and ranges in operations like `view`, `split`, and `scaled_dot_product_attention`. Several discussions revolve around stability and correctness, including handling of NaNs in attention calculations, potential precision drops in layer normalization, and ensuring consistent behavior across eager and compiled modes, especially with symbolic or dynamic sizes. Additionally, there are concerns about CI stability due to trunk flakiness, build failures on specific platforms, and how to handle internal-only features or experimental code in a way that maintains BC and performance. Overall, the key questions involve improving device and operator coverage, clarifying API semantics, and ensuring robustness and transparency in compilation and runtime behaviors."
2024-11-29,pytorch/pytorch,"The discussions highlight several technical issues including debugging CUDA illegal memory access errors, discrepancies in numerical precision between CUDA and other backends (XPU and CPU), and the need for better handling of computational stability in normalization algorithms like LayerNorm. There are questions about default behavior changes in functions like scaled dot-product attention after PyTorch 2.5.0, and the impact of recent PR reverts on debugging `std::bad_alloc` errors, especially concerning the integration of oneDNN (ideep) and its algorithms on different architectures. Additionally, some concerns involve the proper registration and validation of new backend features (e.g., SVE support on ARM), and the importance of accurate profiling and benchmarking to verify performance improvements. Unresolved questions include the exact implementation details of certain algorithms (e.g., Welford for layer normalization), the timing for public releases of newer PyTorch versions, and ensuring CI processes correctly trigger and validate recent code changes."
2024-11-30,pytorch/pytorch,"The discussions highlight multiple technical concerns, including the implementation and support of specific operators like `aten::_linalg_solve_ex.result` and `aten::upsample_bicubic2d.out`, which are currently missing or not supported on certain devices like MPS, impacting cross-platform functionality such as video editing and SAM2. There are ongoing deprecations and modifications to decomposition tables for upsample operators, requiring coordination with backend dependencies and documentation, with some merge conflicts due to failing tests or missing approvals. Merge failures due to CI check failures and flaky tests, such as memory errors during documentation build, point to underlying issues like insufficient resources or regressions introduced by recent PRs. Additionally, concerns around environment variable caching in CUDA, device visibility, and the proper modeling of meta functions indicate deeper challenges in ensuring robust GPU support and correctness across diverse hardware configurations. Unresolved questions remain about maintaining support for unstable or experimental features post-2.6, and handling device-specific quirks without breaking user workflows."
2024-12-01,pytorch/pytorch,"The comments reveal ongoing challenges with supporting certain models and operations on Apple's MPS backend, particularly with GRU, where stability and performance are concerns. Several issues involve compatibility and implementation gaps, such as deprecated support for Intel Macs and the need for additional historical wheel support. Merge failures due to CI/CD checks, test failures, or CLA signing issues are frequent, indicating workflow or legal hurdles. There is also interest in enhancing documentation, including how to properly add or update function signatures and corresponding docs. Overall, the key concerns center around improving MPS support, resolving CI failures, expanding historical model support, and enriching the documentation process."
2024-12-02,pytorch/pytorch,"The discussions primarily revolve around issues with PyTorch's handling of symbolic/dynamic shapes, especially in the context of torch.export, torch.compile, and associated IR modeling. Concerns include ensuring consistent behavior for type checks like `isinstance(s0, int)` when `s0` is a symbolic integer (SymInt), and how to correctly model strides and sizes in meta functions, particularly when dealing with views sharing storage. Several threads suggest the need for better support for in-place operations, custom operators, and inference behavior that faithfully reproduces eager or runtime results, including correctness and performance implications. There's also focus on ensuring proper handling of hardware-specific features (e.g., MPS, ROCm) and infrastructure issues like CI flakiness, job failures, and environment configuration, which complicate debugging and validation. Overall, unresolved questions include how to reliably model and optimize dynamic shapes, improve correctness in symbolic execution, and integrate these updates into production workflows."
2024-12-03,pytorch/pytorch,"The comments reveal ongoing efforts and concerns related to deep integration of compiler infrastructure like `torch.compile`, support for dynamic and static shape handling, and fullgraph guarantees, especially when dealing with features like context managers, SymInt semantics, and in-place operations. Several discussions focus on fixing correctness issues, such as ensuring meta functions accurately model strides and sizes (particularly in complex operations like `conj_physical`), and on performance considerations, like enabling persistent kernels and improving fusion heuristics for reductions. There are recurring questions about environment management (e.g., package versions, external dependencies), compiler behavior (e.g., early compilation of backward, handling of weak references), and test coverage (e.g., ensuring new features are properly tested and documented). Many unresolved issues involve the stability and correctness of functionalization, shape inference, and runtime caching mechanisms under various configurations and hardware backends (CPU, CUDA, ROCm, MPS), with some discussions about incremental workflows for refactoring and testing. Overall, while significant progress is made, critical areas like correctness guarantees, environment consistency, and robustness in complex scenarios remain open."
2024-12-04,pytorch/pytorch,"The discussions reveal several core issues: (1) Difficulty with CMake and environment variables for linking PyTorch and libtorch, with confusion around setting `Torch_DIR` and `CMAKE_PREFIX_PATH`; (2) Challenges with PyTorch's support for complex type operations, including issues with deepcopy not preserving views and handling of in-place tensor updates; (3) Troubles with environment setup on various platforms such as Fedora, Jetson, and macOS, especially related to CUDA and Nvidia libraries; (4) Flakiness in CI testing across platforms like ROCm, Linux, and others, attributed to pipeline and environment inconsistencies; (5) Broader questions about support for features like ROCm, XPU, and dynamic kernel tuning, as well as questions on documentation, build processes, and how to contribute effectively."
2024-12-05,pytorch/pytorch,"The comments reveal ongoing issues with PyTorch's support and testing across varied hardware backends, including CUDA, ROCm, and XPU devices. Key concerns include incomplete or unstable support for specific features such as MPS fallback for operators, in-progress work with distributed backends, and performance tuning issues on different hardware (e.g., A100, AMD GPUs). There are recurring discussions about the need for better test coverage, build stability, and documentation, especially for new functionalities like torchcompile, torchrun, and custom backends like Triton. Several questions concern the correct handling of device-specific behaviors, compatibility with newer hardware/software versions, and the process for improving or validating the CI infrastructure. Unresolved issues involve support for complex operators, in-part support for features like quantization or distributed training, and the need for clearer APIs or fallback mechanisms."
2024-12-06,pytorch/pytorch,"The discussions primarily revolve around understanding and fixing specific inconsistencies and bugs in PyTorch's internal features. Notably, issues include inaccuracies in the handling of tensor strides in meta functions, difficulties with supporting offload tensors' lifetime semantics, and challenges in ensuring deterministic and numerically consistent results between eager and compiled modes on various hardware, especially with nightly builds and specific architectures like WSL or ROCm. Several conversations address the need for better test coverage, refactoring large code changes into smaller reviewable chunks, and clarifying implementation details such as support for certain configurations (e.g., `qint8`, `symint` handling, support for new NCCL versions). There are also operational concerns about CI stability, dependency management, and build reproducibility. Overall, the discussions suggest ongoing efforts to improve correctness, robustness, and clarity in PyTorch's evolving compilation, runtime, and testing infrastructure."
2024-12-07,pytorch/pytorch,"The comments cover diverse issues, including model configuration and label mismatch, support for specific tensor operations and device compatibilities, and build or environment setup problems (e.g., CUDA/lib dependencies, virtual env issues). Several discussions focus on specific bug fixes, such as stride preservation in meta functions, support for TensorFlow-like features, and improvements in model serialization or inference support. There are persistent concerns about build failures, test flakiness, and CI workflows, often related to environment mismatches or unsupported hardware (e.g., MPS, ROCm, CUDA versions). Multiple suggestions point at adding or improving error messages, unit tests, and code documentation, especially for large classes/functions or new features. Overall, unresolved questions relate to compatibility, performance impact of recent changes, and proper environment configuration for seamless operation."
2024-12-08,pytorch/pytorch,"The comments highlight ongoing challenges with freezing specific layers of models wrapped in DistributedDataParallel (DDP), where setting `param.requires_grad = False` before wrapping is a suggested solution. Several issues involve potential bugs or inconsistencies in PyTorch's internal implementations, such as string formatting delays, issues with handling tensor strides during shape copying, and errors when adding documentation for certain data types like `float`. Other discussions focus on test failures due to flaky CI environments, merge conflicts from reverts, and the need for better handling of dynamic versus static dimensions, especially in relation to FakeTensor behavior and index bounds checks. Additionally, there are questions about support for GPU quantization, detailed testing on different hardware, and handling of deprecated features like Vulkan."
2024-12-09,pytorch/pytorch,"The discussions primarily revolve around improving the integration and flexibility of custom operations in PyTorch's compilation and autograd systems, including issues with in-place modifications, decomposition tables, and operator fusion strategies, notably Triton templates and reduction kernels. Several suggestions involve adding support for user-defined Triton kernels, better handling of multiple autotunes, and supporting dynamic behaviors such as heuristics dependent on autotuning configs, with some recognizing the complexity and need for future architecture changes. There are also concerns about the stability and correctness of distributed operations, especially related to process group initialization and NCCL errors, highlighting the importance of proper API usage and testing. Additionally, multiple technical bugs and regressions caused by recent PRs—such as serialization global globals, deprecated APIs, or device-specific issues—are discussed, with some solutions involving reverts, patches, and planned enhancements. Overall, the core issues involve enhancing extensibility, ensuring compatibility, and stabilizing distributed and compilation internals amidst ongoing development efforts."
2024-12-10,pytorch/pytorch,"The comments revolve around key issues like the ABI mismatch of libtorch, build dependencies and configuration (especially on macOS, Linux, and Windows), and build stability or compatibility in CI environments. Several discussions address the ABI handling differences between macOS and Linux, including whether to package ABI-specific libtorch variants or support dual ABI configurations. There are concerns about the proper setting and propagation of build flags, environment variables, and dependencies (e.g., MKLDNN/oneDNN, CUDA, ROCm). Many comments highlight the need for clearer documentation, testing, and exception messages, as well as addressing build flakiness and CI stability, especially for platform-specific regressions. Additionally, questions about backward compatibility, API deprecation, and the interaction with autograd, compiler, and runtime features are raised but often remain unresolved, indicating ongoing efforts toward stabilization and clarity."
2024-12-11,pytorch/pytorch,"The discussions highlight several key issues: (1) The need for better support of Alpine-based Docker images for security reasons, rather than workaround solutions; (2) Trunk test flakiness, platform list management, and flaky detection, especially involving Dynamo, Dynamo platform list propagation, and platform coverage; (3) Revisions and regressions related to CUDA ops, kernel fusion, kernel limits (e.g., channels, dims), and support for macOS ARM (Apple Silicon) with specific compilation flags; (4) Handling of bugs like incorrect strides after functionalization, uninitialized buffer errors, and tensor serialization issues involving symbolic shapes and the `__cuda_array_interface__`; and (5) CI infrastructure concerns such as rebase failures, merge delays, resource failures on ROCm, and the need for better support for multiple hardware types, including rocm, MPS, and HPU, as well as infrastructure prioritization. Many questions concern support for specific platforms, support features (like quantized matmul), build stability, and scaling CI coverage efficiently."
2024-12-12,pytorch/pytorch,"The comments reflect ongoing challenges with performance regressions and compatibility issues across different hardware platforms (notably AMD CPUs, NVIDIA GPUs, ROCm support, and Apple MPS). Several discussions concern specific implementation details, such as kernel naming, memory sharing between CUDA graphs, and handling of large tensor shapes in autograd/backward passes. There are also frequent reports of build/test failures related to environment, dependency, or configuration issues, many requiring workarounds or patches. Additionally, several issues highlight the need for clearer documentation, better support for dynamic shapes, and simplifying build processes for release branches. Overall, the discussions revolve around optimizing performance, robustness, and usability amidst diverse hardware and software configurations."
2024-12-13,pytorch/pytorch,"The comments highlight several technical concerns including the implementation and compatibility of various features such as hash functions, distributed and RPC support on Windows, MPS operator support, and device-specific issues like CUDA, ROCm, and MPS backend challenges. Key questions involve ensuring cross-platform support, handling of complex tensor types (e.g., complex64 on MPS), and the stability of certain operations in eager and compiled modes, alongside API consistency and potential refactoring for more unified runtime interfaces. There are also discussions regarding performance regressions, CI failure management, and feature deprecation strategies, especially around release planning and branch maintenance. Further, some comments suggest possible code improvements and tests, including type checking and better error handling, with a need to clarify support for advanced features like autograd, quantization, and device-specific capabilities. Unresolved questions pertain to the specifics of backend support (e.g., `torch.ops.higher_order.flex_attention_backward`) and detailed debugging of runtime errors or performance impacts."
2024-12-14,pytorch/pytorch,"The primary technical concerns involve the implementation and optimization of custom functionalities in PyTorch, such as GPU-resident SHA-256 hashing, support for advanced parallel operations like associative scan and selective scan (including custom ops and autograd support), and device-specific operator support issues for MPS and ROCm backends. Several discussions address the challenges of integrating these features with JIT compilation, backend fallbacks, and hardware-specific limitations or bugs, with questions about the current development progress, stability, and potential improvements—such as cache management or fallback strategies. There are ongoing efforts to improve testing, CI workflows, and handling of edge cases across platforms (macOS, CUDA, MPS, ROCm). Unresolved questions include handling data-dependent guards, ensuring backward compatibility, and properly documenting device and compiler support changes, especially for features like custom ops or new kernels."
2024-12-15,pytorch/pytorch,"The discussions primarily revolve around development challenges and feature requests in the PyTorch repository, such as limitations in data types supported by NCCL for multi-GPU communication, and the ongoing deprecation of TorchScript with concerns about its C++ API compatibility. Several issues highlight the need for improved functionality like support for batch matrix creation, extending `eye` support, and the implementation of algorithms like Levenshtein distance. There are technical concerns about build failures, standard compliance (e.g., C++17 support on macOS), and the impact of recent code changes on performance and correctness across different hardware (e.g., ROCm, MPS, CUDA). Additionally, there are ongoing merge conflicts, CI failures, and questions about the best approaches to refactor or fix specific functionalities."
2024-12-16,pytorch/pytorch,"The consolidated discussions reveal several recurring themes: (1) There are ongoing challenges with implementing and supporting advanced features such as `reinterpretc_cast` in ONNX and across different frameworks, with community interest in expanding ONNX support for such operations. (2) Merging failures and CI/CD pipeline issues, often due to flaky tests, permission or configuration problems, and the need for better infrastructure and process management, are common obstacles hindering development progress. (3) Certain API behaviors, such as the sorting behavior of `torch.unique` or the use of `torch.nn.DataParallel` versus `DistributedDataParallel`, raise questions about backward compatibility, documentation clarity, and best practices for ecosystem adoption. (4) Hardware-specific bugs and resource limitations (e.g., shared memory constraints on specific GPU models) indicate the need for targeted fixes, environment diagnostics, and potential API or implementation adjustments. (5) Community discussions also suggest that clearer standards, documentation updates, and strategic planning (such as migration to conda-forge or better support for XPU devices) are desirable to improve usability, robustness, and developer experience."
2024-12-17,pytorch/pytorch,"The comments reveal several key technical concerns across the repository:
1. Implementation inconsistencies and potential bugs, e.g., mismatch in guard behavior for `include_last_offset`, performance regressions or bugs in CUDA, and issues with variances and tensor operations (e.g., `index_put_`, `svd`, and `qr` performance/debugging). Several patches aim to fix these, with discussions on the correctness and numerical stability of algorithms.
2. System and environment compatibility issues, such as compiler errors with Clang, system-specific library conflicts, and runtime errors on macOS or Windows, highlighting challenges in cross-platform support for builds, especially with CUDA, ROCm, and macOS.
3. Build and CI system hurdles, including flaky tests, failed workflows due to environment mismatches, or long filename issues, often requiring workarounds, re-rebases, or environment tuning.
4. Architectural and design questions, particularly around behaviors like `torch.export`, guard generation, handling of fake tensors, and whether to extend existing classes like `TransformedDistribution` versus adding new parameters, indicating ongoing discussions on code simplicity, correctness, and API consistency.
5. Overall, unresolved issues include testing coverage for new features, stability of CUDA/ROCm backends, cross-platform support, and ensuring correctness and performance, with frequent reverts and re-approvals hinting at ongoing iterative development and testing challenges."
2024-12-18,pytorch/pytorch,"The extracted comments from various GitHub issues reflect a wide range of technical concerns including implementation status updates for specific features (e.g., label smoothing for segmentation, new ops on M1+ Macs), bug reports (especially related to CUDA, MPS, and ROCm compatibility), and infrastructure or code review processes. Several issues involve the correctness and support limitations of certain operators on specific hardware (e.g., BF16 support on Mac, the need for custom kernels, and API deprecations like DataParallel). Discussions also highlight ongoing efforts to improve testing robustness, rebase challenges, and the need for clearer guidelines for error handling and user-facing diagnostics. Confidence is generally expressed in ongoing fixes and the importance of upstreaming certain features, alongside noting some internal or environment-specific problems that require further investigation."
2024-12-19,pytorch/pytorch,"The comments highlight several technical considerations: (1) discrepancies in how certain native functions, especially batch norm, are being split or consolidated across updates, with suggestions that upstreaming operations or consolidating into a single native batch norm could be beneficial, possibly delaying changes due to JIT and JIT-tracing compatibility; (2) challenges in implementing or debugging features like CUDA device handling on Macs, with OS-specific API limitations such as the 2^32 byte limit in MPS, and issues related to environment configurations and driver bugs; (3) specific bugs or regressions in PyTorch's compilation, serialization, or operator behavior, such as issues with shape inference, operator dispatch, complex dtype support, or errors from native code, often requiring Reverts, rebase adjustments, or improved error messages; (4) infrastructural and CI-related issues like merge failures, flaky tests, and build correctness, sometimes exacerbated by outdated dependencies or environment restrictions; (5) ongoing discussions about API support and user-facing error messaging, including handling of data-dependent guards, reworking inference APIs, and clarifications needed for user guidance on complex or platform-specific features."
2024-12-20,pytorch/pytorch,"The discussions highlight several key issues: difficulties in reliably loading pre-trained models due to internal serialization limitations (`pickle` dependencies, moving models across paths, absolute vs relative imports), and challenges related to cross-platform support (e.g., MPS/bfloat16 on Mac, ROCm and CUDA compatibility). There are concerns about ensuring the stability of the environment, including managing dependencies like fbgemm and cuBLAS, and ensuring reproducibility and numerical stability across hardware variations. Additionally, testing frameworks and validation processes are recognized as needing improvement to handle the complexity of evolving features, such as differentiable optimizers, JIT compilation, and autodiff support. Unresolved questions include how to improve model serialization for C++, how to verify numerical correctness, and how to better organize continuous integration for multi-platform support."
2024-12-21,pytorch/pytorch,"The discussions predominantly revolve around ongoing development and debugging challenges in PyTorch, including issues with specific operators (e.g., `aten::binomial`, `aten::upsample_bicubic2d.out`) across various hardware backends (MPS, AMD, ROCm). Concerns are raised regarding build failures, merge conflicts, and branch rebases, often linked to complex infrastructure or internal dependency issues, such as GPU target support and build-time optimizations. Additionally, there are questions about deprecating features like TorchScript, handling dataclasses with `torch.compile`, and ensuring correct metadata for package releases. Several comments focus on the status of PRs, test failures, and the need for better test coverage or build configuration adjustments, indicating a mix of active development, bug fixing, and infrastructure management efforts."
2024-12-22,pytorch/pytorch,"The discussions highlight critical issues with race conditions and stability in PyTorch, often linked to shared tensors across processes, multi-threaded operations, and certain native operator behaviors. Several comments suggest workarounds, such as cloning tensors to avoid race conditions, and efforts to improve debugging practices, including better traceback logging and code auditing. Merge failures frequently occur due to unreviewed pull requests and CI checks, indicating ongoing review bottlenecks and infrastructure challenges. Other concerns revolve around compatibility with ROCm, including build timeouts and issues with specific hardware support like AMD's gfx900, as well as the proper handling of internal NCCL APIs and environment configurations. Unresolved questions pertain to improving CI reliability, handling of internal NCCL functionalities, and ensuring multi-process safety for tensor operations."
2024-12-23,pytorch/pytorch,"The discussions highlight several technical concerns: (1) Compatibility and reproducibility issues with specific hardware, such as AMD MI250x/MI210 GPUs and ROCm, where recent changes have caused runtime errors or performance regressions, often addressed by updates or workarounds. (2) The challenge in validating batched operations against single-sample ops, particularly in the context of floating-point differences and how to define acceptable deviations. (3) The need to support or eventually deprecate certain APIs or features, such as FX graph mode quantization and the handling of scalar values within `torch.compile`, with proposals for more robust and version-compatible solutions. (4) Merge conflicts and infrastructure issues, like flaky CI failures and dependency mismatches, particularly involving internal frameworks and external tools like Triton and bitsandbytes. (5) Ongoing work to improve user-facing error messages, especially around backend compatibility, and to support new features like FP8/AMP training, with a focus on stability and correctness across diverse hardware and software configurations."
2024-12-24,pytorch/pytorch,"The comments across issues indicate ongoing efforts to improve build performance, compatibility, and integration with external tools like ONNX and third-party libraries (e.g., bitsandbytes). Several reports reflect challenges related to build system modifications (e.g., handling of suffixes in release versions, system compiler issues, and support for various architectures like ROCm, XPU, CUDA), as well as technical problems such as inconsistent eager/inductor behavior, operator support (e.g., unsupported operators in ONNX export), and test failures due to code correctness or environment mismatches. There are also discussions about extending kernel functionalities (e.g., for custom masking, tied weights, or support for dynamic shapes) and improving code modularity and testing (e.g., reusing or refactoring fallback mechanisms, handling of edge cases). Some unresolved questions involve compatibility and stability of features like dark mode UI, support for float8 types, and the handling of complex model configurations during compilation and export, which often require external contributions and environment-specific fixes. Overall, the community is addressing both infrastructural issues and advanced feature extensions, with an emphasis on stability, correctness, and compatibility."
2024-12-25,pytorch/pytorch,"The discussions highlight issues with CUDA kernel errors, such as illegal memory access, debugging suggestions (e.g., setting CUDA_LAUNCH_BLOCKING=1), and performance concerns related to inductor and graph recording, particularly on multi-GPU setups. There are questions about the behavior of `requires_grad_` in conjunction with `torch.no_grad()`, and the need for better API support for string dtypes and dynamic attribute handling (e.g., lazy_property) to improve portability and compatibility with tools like Dynamo. Some threads address build and installation challenges, especially with PyTorch on specialized hardware like XPU and MIG, focusing on dependency management, proper environment sourcing, and CI stability. Overall, unresolved issues include ensuring stable memory access during GPU computations, improving build and deployment workflows, and maintaining compatibility with evolving PyTorch features and external hardware."
2024-12-26,pytorch/pytorch,"The discussions highlight several concerns surrounding the correctness, stability, and reproducibility of PyTorch features. Key issues include the variability of numerical results due to hardware-specific behaviors (e.g., cuBLAS accuracy differences across GPU architectures), the need for proper management of `out` parameters to avoid ambiguity and correctness issues, and the handling of threading, synchronization, and default stream dependencies in distributed and GPU contexts. Some discussions point to configuration and build environment limitations, such as missing dependencies or incompatible compiler flags, impacting reproducibility and build stability. Overall, unresolved questions remain about ensuring consistent, correct, and efficient behavior across diverse hardware, software configurations, and use cases, often requiring refined API design, careful testing, or further infrastructure support."
2024-12-27,pytorch/pytorch,"The discussions highlight several technical concerns including the need for bug fixes and proper input validation in functions like `torch.sort`, improvements in dtype consistency checks such as in `tensordot`, and ensuring compatibility of operations like `baddbmm` across different dtypes. Contributors are requesting reviews and testing for specific PRs related to hardware-specific support (e.g., MPS, ROCm, XPU), as well as improvements in build systems, like switching to nightly compiler versions or enabling LTO on macOS. There are also ongoing discussions about supporting custom operators in AOTI, as well as addressing performance regressions, CUDA backend correctness, and model serialization options. Several issues relate to CI failures, merge conflicts, and build environment configurations, with some requests to add tests, improve error clarity, or consider backwards compatibility. Overall, the focus is a mix of bug fixes, hardware and backend support, and infrastructure improvements, with some unresolved questions around build stability and feature support on various platforms."
2024-12-28,pytorch/pytorch,"The discussions cover various issues in the PyTorch repository, including persistent warning messages such as suppression techniques and future fix plans; the need for fixing operation order randomness in graph transformations to ensure deterministic execution; and challenges related to hardware-specific bugs, such as ROCm GPU support, memory errors on Jetson devices, and specific compatibility issues with cuDNN and ROCm versions. Several comments propose improvements like refining API behaviors, adjusting build configurations (e.g., C++ ABI settings), and adding better documentation. There are also discussions regarding CI failures, rebase workflows, and the importance of proper label management for release workflows. Overall, the conversations highlight ongoing maintenance, compatibility, and stability concerns, with some suggestions for code improvements and better tooling practices."
2024-12-29,pytorch/pytorch,"The discussions highlight several technical considerations: there's ongoing work to improve PyTorch's support on ARM64 architectures and AWS Lambda, including handling CPU info parsing errors, with patches planned for upcoming releases. Concerns about NCCL version mismatches and their impact on distributed training performance, especially regarding PCIe GPU nodes and P2P communication, are discussed, with suggestions to build NCCL from source for specific CUDA versions. The relevance and future of `torch.nn.DataParallel` versus `DistributedDataParallel` are debated, with a consensus leaning toward deprecating DP to promote DDP for better scalability and performance. Several issues involve ensuring reproducibility and deterministic behavior in GPU computations, especially when using cuDNN or TF32, with debugging tips provided. Additionally, questions about proper labeling, signing CLA, and build/test failures are noted, alongside suggestions for code improvements and clarifications."
2024-12-30,pytorch/pytorch,"The discussions highlight ongoing issues with certain PyTorch features and external integrations: for example, a known bug in `multinomial` distribution validation not fully addressed by the current codebase, and support for custom ONNX operators like `mylib::custom_add` in AOTI, which seems incomplete and requires further work for both export and runtime execution. Several comments point out that some tests are flaky or disabled, possibly due to platform-specific bugs or CI setup problems, such as the handling of `max_pool3d` or failures on rocm. There are questions about the default behavior of `torch.optim.AdamW` regarding `decoupled_weight_decay`, and notes about necessary upgrades to recent PyTorch versions for stability and correctness. Additionally, some issues involve environment-specific problems, like support for mixed precision, dynamic shape export, and infrastructure issues related to building and testing across various platforms."
2024-12-31,pytorch/pytorch,"The discussions highlight multiple technical concerns including issues with exporting models to ONNX, particularly handling dynamic shapes, operator support, and shape inference challenges, with suggested workarounds or work-in-progress fixes. There are ongoing efforts to address platform-specific bugs and regressions, such as those affecting large tensor operations (e.g., `torch.unique`, `polygamma`), and the need for better debugging tools within the compiler and runtime stack, especially for profiling and performance verification. The need for clarifying default behaviors and user-facing documentation (e.g., weight decay, ONNX export options) is also emphasized, along with fix requests and potential improvements for build processes, static linking, and device support (ARM, XPU, ROCm). Certain issues are acknowledged as flaky or platform-dependent, requiring further validation and coordinated fixes in later releases. Unresolved questions include the support of specific hardware architectures, the scope of operator support in different backends, and ensuring compatibility with recent compiler, compiler flags, and runtime environment changes."
2025-01-01,pytorch/pytorch,"The discussions highlight ongoing concerns about the consistency and unification of the `at::Tensor` and `torch::Tensor` APIs in PyTorch's C++ interface, noting that despite efforts to unify them, inconsistencies and confusion remain, especially in the official library API documentation. Several issues involve build and merge conflicts, CI testing failures, and the need for clearer guidance and better documentation, including on custom operator implementation and device compatibility (e.g., MPS, M1 Pro, ROCm, and XPU). Some discussions also address the behavior of default streams, synchronization, and device-specific operator support, along with efforts to improve developer experience and hardware support. Additionally, questions about debugging compiled code, build tool behavior, and environment-specific quirks are raised, while issues related to system compatibility, driver support, and version differences are noted as possible sources of bugs. Overall, unresolved questions focus on ensuring consistent API/API documentation, resolving build conflicts, improving device support, and clarifying behavior for developers and users across different hardware and software configurations."
2025-01-02,pytorch/pytorch,"The comments span diverse topics including proposed enhancements like utilizing NestedTensor for more elegant tensor concatenation, discussions on masking and gradient NaNs in transformer models, and issues related to distributed training and device support. Several technical questions involve the behavior of torch.compile, correctness of communication overlaps, and improvements for specific device backends such as AMD GPUs or Intel GPUs. There are ongoing efforts to fix bugs, optimize performance, and improve API usability, with some discussions prioritizing bug fixes and stability for upcoming releases. Unresolved or pending questions include the proper handling of special operators, device-specific support, and ensuring compatibility and correctness across various hardware and execution modes."
2025-01-03,pytorch/pytorch,"The comments reveal ongoing discussions about various technical concerns within the PyTorch ecosystem. Key issues include challenges in ensuring reproducible data sampling across multiple DataLoaders, limitations with certain datatypes (e.g., bf16) in functions like `torch.histc`, and the need for support for specific operators (e.g., `matrix_exp` on MPS devices). Several discussions focus on the handling of custom operators—whether registration should occur in Python or C++, and how to make such operators compatible with AOT compilation frameworks like AOTI. Additional concerns involve graph break management, compiler optimizations (e.g., IPO, LTO), and supporting advanced use cases like uneven tensor sharding, which may require API design considerations. Overall, the discussions highlight active efforts to improve reproducibility, device support, operator registration, compiler behavior, and infrastructure robustness."
2025-01-04,pytorch/pytorch,"The discussions highlight various technical concerns including the tuning performance of `foreach_copy_` operations in CUDA graphs, challenges with rebase conflicts in PRs, and build issues related to CUDA driver compatibility, library dependencies, and internal code merge failures. Several questions pertain to the correctness and efficiency of certain tensor operations, dtype handling, and device-specific features like MPS and XPU support, often with requests for testing on nightly builds or different hardware configurations. There are also recurring topics on merge failures due to CI check failures, the need for proper documentation links, and handling of versioning and package distributions to ensure compatibility across architectures. Unresolved issues include driver installation challenges, build configuration errors, and ensuring safe tensor memory alignment, alongside planning for feature integration (like SDPA, sequence parallelism, and inductor optimizations). Overall, the discussions reflect ongoing efforts to improve stability, compatibility, and performance across diverse hardware and software environments."
2025-01-05,pytorch/pytorch,"The discussions highlight several technical concerns, including the support and correctness of boolean tensor operations on the MPS backend, especially with functions like `copy_()` and `narrow()`, with suggestions to ensure tensor contiguity or manual indexing as workarounds. There are issues related to the support of FP8 data types in the `inductor` backend, with recommendations to avoid using inoperative features or fall back to other precisions like FP16, and to update PyTorch for improved support. Some discussions focus on multi-threaded resource management and safe deletion of objects like `TCPStore` in PyBind11, emphasizing synchronization and proper lifetime management. Several PRs and build failures are related to build configuration, compiler flags, and dependency support, with an emphasis on ensuring correct build environments and infrastructure stability. Overall, unresolved questions involve improving backend support, build robustness, and safe threading practices."
2025-01-06,pytorch/pytorch,"The comments primarily revolve around various technical issues and development conversations in the PyTorch repository, including concerns about proper usage of torch.no_grad() and environment variable configurations for memory management, as well as performance and compatibility issues across different hardware, particularly related to AMD GPUs, ROCm, and Intel XPU devices. Several discussions address the proper implementation and support of features like dynamic shape handling, dynamic compilation, and multi-device (multi-GPU, multi-XPU) configurations, with a focus on ensuring correctness, performance, and stability. There are also ongoing efforts to improve documentation, testing practices, and build system configurations, along with troubleshooting specific runtime errors such as memory leaks, segfaults, and compiler incompatibilities. Additionally, issues related to codebase maintenance, including rebasing, merge conflicts, and code formatting, are addressed. Overall, the threads highlight a mixture of bug fixes, feature development, platform support challenges, and operational improvements."
2025-01-07,pytorch/pytorch,"The discussions highlight several technical issues: (1) Inconsistent type propagation and default behaviors in PyTorch's CPP backend and Triton, with suggestions to unify or improve type checking; (2) Failures and performance regressions caused by compiler incompatibilities, especially with older versions of Clang on Linux-aarch64, and the need for better compiler support, automations, or gating; (3) Potential correctness problems with FSDP's parameter resharding post-in-place modifications, and recommendations to use the `reshard()` API to maintain consistency; (4) Challenges with loading custom extensions in C++, especially with proper registration and initialization, and the recommendation to prefer `Autograd::Function`; (5) Various merge, CI, documentation, and infrastructure concerns, including the need for better testing, clear documentation, and handling specific hardware/software configurations."
2025-01-08,pytorch/pytorch,"The comments highlight a variety of ongoing issues and considerations within the PyTorch ecosystem. These include hardware-specific limitations (e.g., TCC mode support for NVIDIA GPUs, MPS operator support on Apple devices), performance discrepancies between Windows and Linux, and compatibility challenges with different CUDA, ROCm, and driver versions. Several discussions focus on bug fixes, such as ONNX export issues and operator support, as well as concerns around dependency management, build efficiency, and codebase maintenance. There are also technical suggestions for improving type safety with ParamSpec, managing device memory, and extending kernel support to new hardware features. Overall, the discussions reflect efforts to enhance portability, reliability, and performance, alongside handling platform-specific limitations and improving developer workflows."
2025-01-09,pytorch/pytorch,"The comments highlight ongoing issues and discussions around PyTorch's support for features like advanced tensor indexing, MPS device limitations, NaN propagation and numerical stability, support for complex and FP8 data types, and the behavior of in-place modifications during training (particularly with FSDP). Several discussions address build support and compatibility for different hardware (e.g., AMD CPUs, MI300 GPUs, riscv architectures), along with CI infrastructure challenges. Certain features like operator support in Triton for FP8 types, extending support for multiple device types, and improving graph break handling in torch._dynamo are also key concerns. There are questions about correctness, performance improvements, and the proper handling of edge cases, especially around in-place tensor updates, custom operators, and compiler/back-end behavior. Many unresolved questions involve ensuring support across different device architectures and configurations, managing compilation and runtime support, and improving CI testing infrastructure."
2025-01-10,pytorch/pytorch,"The comments primarily revolve around technical challenges in PyTorch related to supporting dynamic shapes and shape inference, especially for ONNX export, and handling of tensor subclasses, including issues with modeling, tracing, and exporting custom or quantized modules. Concerns include the implementation of support for `*args`/`**kwargs` in FX and TorchScript, shape guard stability in the context of symbolic tensors, and the handling of kernel heuristics and autotuning policies. Several discussions involve workarounds, API improvements, and the need for better documentation, testing, and bug fixing in compiler, CUDA, and dependency management. Questions are raised about proper support and integration of features like MPS, ROCm, and external libraries, as well as efforts to fix regressions and improve build/CI reliability. Unresolved issues include shape inference failures, export errors, and API stability, with ongoing proposals for refactoring and extending support."
2025-01-11,pytorch/pytorch,"The discussions encompass various technical concerns including the implementation of a PyTorch-based SHA256 hash function to optimize GPU tensor content verification, with questions about its correctness and potential for optimization via `torch.compile`. Several issues relate to concurrency and overlapping communication and computation in distributed setups, with suggestions to use multiple process groups and streams/async operations, and questions about their performance implications. Numerous bug reports involve regression in transformer modules, dynamic shape tracing, and errors with specific operators or model conversions, often seeking workarounds or fixes in upcoming releases. Some entries discuss improving model export and conversion workflows, especially for ONNX and TensorRT, highlighting errors related to operator support, shape inference, or unsupported features. Overall, key themes include enhancing operator support, debugging dynamic shape/tracing issues, improving framework interoperability, and optimizing distributed and GPU operations."
2025-01-12,pytorch/pytorch,"The discussions highlight ongoing challenges with implementing specific PyTorch operations (e.g., `aten::linalg_eig`, `aten::kthvalue`) on the MPS device, primarily due to lack of native support and reliance on fallbacks like CPU. Contributors express interest in contributing to the operator support, seeking guidance on development starting points. Several issues involve compatibility and performance problems with MPS/tensors, especially concerning sparse tensor operations and device synchronization, with workarounds such as converting sparse tensors to dense. There are also several merge conflicts, build issues, and CI failures that hinder development progress. Overall, a key concern is improving MPS support across diverse tensor types and operations, alongside streamlining contributions and resolving build/test stability."
2025-01-13,pytorch/pytorch,"The discussions cover several key topics: (1) Enhancing RNN kernel performance through higher-order operations like unroll and scan, with ongoing work on scan and associative_scan, and benchmarking against existing implementations such as flashrnn; (2) MPS backend issues including matching behavior with CPU, support for complex tensors, specific operator implementations (like upsampling bilinear), and potential errors or crashes related to device support and driver issues; (3) API and documentation concerns, including the need for better native function support, handling of dynamic shapes, shape guards, and the importance of clear user-facing documentation; (4) CI and internal build/test failures, merge conflicts, and the importance of stability and proper testing, especially with new hardware configurations like XPU, ROCm, and CUDA; and (5) broader architectural questions such as unifying device support, cross-platform compatibility, and the support for models exported with artifacts like shape guards or dynamic shapes, alongside handling of data-dependent conditions during export."
2025-01-14,pytorch/pytorch,"The comments reflect widespread issues with PyTorch performance on Windows and various hardware backends, notably slow data loading with `num_workers > 0`, and CUDA-related inconsistencies across versions and OSs. Several reports are about unexpected errors or mysterious slowdowns, often linked to platform-specific or version-specific bugs, such as incompatibilities with `torch.ops.image::open_file`, operator implementation gaps for newer hardware or backends, and kernel performance regressions. There are frequent discussions about improving robustness of shape and stride guards during tracing and export, handling dynamic shapes when guards cannot be evaluated, and the need for better tooling (like profiling, improved diagnostics, or unified cross-platform support). Some issues are about build configuration—like missing packages, environment mismatches, or need for better test coverage—while others concern architectural or API design trade-offs, such as support for mixed hardware, combining XPU/CUDA, or generalizing for multiple device types. Overall, unresolved questions include how to best handle platform-specific bugs, maintain consistent performance, and support new hardware with minimal regressions."
2025-01-15,pytorch/pytorch,"The comments reflect ongoing discussions on various technical issues, including build configurations and ABI compatibility, especially regarding C++ ABI and build environments on different Linux distributions and architectures. Several contributors are addressing build failures and regressions caused by updates to dependencies like Triton, cutlass, and CUDA/cuDNN, often proposing patches or workarounds like using older libraries, setting preferred BLAS backends, or adjusting build scripts. There are also discussions about developers' contributions, API stability (e.g., deprecated or unsupported functions), and the importance of comprehensive testing, including shape inference and profiling to prevent silent errors. Additionally, many issues involve flaky tests and CI stability, with frequent efforts to disable or re-enable specific tests based on flakiness detection or evaluation of regressions. Overall, the key concerns center on maintaining compatibility, fixing regressions, and improving the robustness and transparency of testing and build processes."
2025-01-16,pytorch/pytorch,"The discussions highlight ongoing challenges with handling conditional losses and parameter updates in distributed DataParallel, where skipping certain losses (e.g., zero tensors) can cause hangs or process divergence without proper `find_unused_parameters=True`, and suggest moving loss-conditional logic into model's `forward()` method with compliant outputs. There’s also concern about incomplete support for certain operators (e.g., MPS-specific ops like `aten::angle`, `aten::_upsample_bilinear2d_aa.out`, `aten::kthvalue.values`), potentially requiring either workarounds like `PYTORCH_ENABLE_MPS_FALLBACK=1` or operator implementations. Several issues relate to trunk flaky tests, CI stability, and flaky environment issues, often resolved by reruns, but some—such as those involving device-specific operators or custom C++ ops—remain unresolved or require further debugging, especially around operator support on different backends (XPU, MPS, ROCm). The need for better type support (e.g., Sequence[Tensor], typing for `torch.cat`) and support for data-dependent shape guards in export and tracing is also debated, with suggestions to simplify guard conditions and improve cross-backend consistency. Overall, key unresolved questions involve operator coverage on various devices, handling of shape dependencies during export, and stabilization of flaky tests and CI infrastructure."
2025-01-17,pytorch/pytorch,"The comments reflect several recurrent themes: issues with flaky CI tests across platforms, especially on ROCm and Linux, often attributed to network instability or environment-specific bugs; concerns about CUDA NCCL initialization, memory sharing, and potential driver or driver-library mismatches; challenges with integrating new features like autograd, autotuning, and custom operators, often requiring careful API design and error handling; difficulties with debugging tools such as shape environment tracking, DSO/ABI stability, and symbol mangling, especially around dynamic shape inference and custom backends; and miscellaneous technical questions about API semantics, lib initializations, and build configurations, often seeking clarification on current limitations or proper practices. Many suggested solutions involve improving error messages, modularizing code, adding better logging, and engineering more robust environment abstraction to stabilize platform-specific behaviors and performance."
2025-01-18,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's Metal Performance Shaders (MPS) backend on macOS, primarily due to missing implementations of certain operators like `aten::angle`, `aten::_upsample_bilinear2d_aa`, `aten::kthvalue.values`, and bilinear upsampling in `torch.nn.functional.interpolate`. There is a recurring theme of requesting these functions to be added officially, with temporary workarounds suggested, such as enabling fallback to CPU via environment variables. Additionally, there are broader concerns about device abstraction, handling of various hardware backends (e.g., ROCm, HPU), and ensuring consistent and efficient caching behavior. A few discussions also address implementation details for new operators, and how to properly structure code in the `c10::metal` namespace. Overall, the core issue revolves around expanding MPS operator coverage, improving device support, and managing CI/test infrastructure complexities."
2025-01-19,pytorch/pytorch,"The discussions highlight a desire for enhanced buffer management in PyTorch, such as implementing `nn.BufferDict` and `BufferList`, to organize buffers more flexibly and enable features like model movement across devices. There are concerns about handling non-ASCII characters in profiling data, with suggestions for robust cleaning methods to prevent JSON parsing errors. Several issues relate to build failures, merge conflicts, and CI pipeline errors, indicating ongoing infrastructure challenges. Some comments question the stability, documentation, and public API status of features like `_symmetric_memory`, emphasizing the importance of backward compatibility and clear user-facing documentation. Overall, key themes include improving buffer organization, ensuring robust data handling, and resolving CI/build stability."
2025-01-20,pytorch/pytorch,"The comments highlight ongoing challenges with cross-language model serialization, especially loading C++ trained models into Python, and discussions around appropriate saving/loading practices. Several issues focus on unsupported operators or device-specific implementation gaps, such as for MPS or rocm, which impact functionality and performance, with suggestions to update dependencies or improve fallback mechanisms. There are concerns about trunk flakiness, CI failures, and platform-specific regressions, leading to reverts and remerges, as well as discussions around testing robustness and reproducibility. Some comments touch on specific technical fixes, such as handling view-based tensors during optimization or refining API stability and correctness for features like flatten_parameters. Overall, unresolved questions remain about compatibility, performance regressions, and quality assurance for new features and platform support."
2025-01-21,pytorch/pytorch,"The comments highlight ongoing challenges with Windows support for IPC functionality, especially without TCC mode, indicating a need for platform compatibility improvements. Several issues relate to performance regressions, flaky test flakiness, and build failures—often due to environment inconsistencies, dependency mismatches (e.g., sympy versions), or infrastructure limitations. There are multiple feature requests and discussions around profiling, profiling tooling, and deployment optimizations (e.g., static build, lightweight inference). Some comments express concern about test stability, flaky CI, and missing support for specific hardware like XPU, with suggestions for better testing infrastructure, caching, and platform-specific fixes. Unresolved questions involve backing up custom code, improving build processes, and handling hardware-specific configurations, with recurring themes of CI stability, environment setup, and feature correctness."
2025-01-22,pytorch/pytorch,"The discussions highlight several technical concerns including the proper initialization and handling of static library dependencies in PyTorch builds, especially with static linking and platform-specific configurations. There are questions about maintaining forward compatibility and ABI stability, particularly regarding operator definitions and the handling of custom operators such as TorchDispatchModes, which may complicate feature integration and testing. Several entries point out potential bugs or inconsistencies in CUDA kernel behaviors, device-specific implementations, and the management of dynamically loaded libraries, suggesting a need for more robust versioning, patching, and testing strategies. Additionally, there's a recurring theme about the challenge of managing external dependencies (like Triton updates), ensuring proper testing on various hardware (CPU, GPU, XPU, MPS), and understanding platform-specific performance and correctness issues. Unresolved questions include how best to formalize and stabilize interface boundaries, dependency versioning, and the proper integration of new features across diverse system architectures."
2025-01-23,pytorch/pytorch,"The comments reflect ongoing discussions and issues related to the PyTorch repository, including workarounds for specific bugs (e.g., tensorboard scalar logging and model exporting), device-specific operator implementation gaps (particularly on MPS and ROCm), and concerns about test flakiness on trunk. Several comments focus on improving tooling, such as adding type annotations, better error handling, and more robust testing (including for nested tensors, dynamic shapes, and memory leaks). There are also discussions around CI workflows, merge conflicts, dependency management, and tracking support for hardware like Eigen, Eigen library integration, and platform-specific issues (e.g., macOS on M1 and kernel compatibilities). Suggestions include enhancing logging, error messages, and consistency of behavior across backends, as well as addressing flaky tests and ensuring proper support for new hardware and device features. Unresolved questions involve device support details, safe handling of dynamic shapes, and managing the complexity of extending native functions across multiple platforms."
2025-01-24,pytorch/pytorch,"The comments reflect ongoing discussions and concerns about expanding the PyTorch C++ API, including support for DistributedDataParallel in C++, and handling of various operators like `torch.linalg.pinv`. Several issues involve build configurations, such as compatibility with CUDA 12.8 and specific hardware architectures like SM89, as well as ensuring support across different platforms including macOS, x86, and ARM. There are questions about improving or debugging operator support (e.g., `aten::reshape`, `aten::copy`) and handling complex nested tensor scenarios (e.g., jagged tensors with rotary embeddings). Additionally, concerns about CI stability, test flakiness, and proper testing—especially for new features and edge cases—emerge repeatedly. Overall, the main technical concerns involve API support expansion, platform compatibility, build configuration, operator coverage, and rigorous testing."
2025-01-25,pytorch/pytorch,"The discussions primarily revolve around challenges and limitations in implementing certain sparse tensor operations (like batched sparse matrix multiplications, reshaping sparse tensors, and batch-dimension handling) in PyTorch, with current solutions involving loops, einsum, or workarounds. Several issues address the need for improved support for sparse tensor resizing, reshaping, and batch operations, as well as the handling of different sparse layouts (CSR vs COO). There are significant concerns about sparse tensor version compatibility, type annotations, and extension API stability, especially regarding the `torch_function` dispatch system and overloading resolution, which impact both runtime behavior and static analysis. Many discussions also mention ongoing efforts to address CI failures, version compatibility (e.g., NCCL, CUDA), and plumbing of new features like FP8 support, with some proposals suggesting reverts or API overhauls. Overall, a key unresolved theme is enhancing the native support for batched and flexible sparse tensor operations, along with ensuring API stability and compatibility across PyTorch versions and hardware configurations."
2025-01-26,pytorch/pytorch,"The discussions highlight multiple unimplemented operators for the MPS device, such as `aten::unique_dim`, `aten::_upsample_bilinear2d_aa.out`, and `aten::kthvalue.values`, with requests for their addition or fallback options, and some interest in implementing support for operators like `aten::native_dropout`. There is ongoing work to improve device-specific support, including implementing Cholesky for MPS and handling device-specific features like TF32 for Intel GPUs, with questions about how to properly extend support or abstract device capabilities. Some concerns involve ensuring consistent behavior for operations across different backends (e.g., CPU, CUDA, MPS), as well as addressing performance regressions or compatibility issues on various hardware through fallback mechanisms or operator support. Additionally, discussions revolve around build and environment setup challenges, such as dependencies (e.g., XLA, Bazel, OneAPI) and ensuring code changes do not regress existing features. Overall, these issues underline the need for extending operator support on specialized devices, managing device-specific features, and maintaining compatibility amid ongoing development efforts."
2025-01-27,pytorch/pytorch,"The discussions reveal concerns about proper documentation, usability, and compatibility. Contributors seek clarification on specific API behaviors, such as adding support for nested tensors, and query about the status of feature reimplementation (e.g., `torch.scatter()`, `torch.cat()` for nested tensors). There are questions about platform-specific support, especially for new architectures like Hopper, SM89/SM90, and CUDA versions, along with potential regressions caused by recent code changes. Several discussions revolve around improving error handling, build configurations, and ensuring stability across different hardware and software environments. Overall, unresolved issues include performance regressions, platform support, and clarifications on expected behaviors for some features."
2025-01-28,pytorch/pytorch,"The discussed comments highlight ongoing issues with build failures, flaky tests, and CI pipeline inconsistencies, especially on Linux and ROCm platforms, often due to environment misconfigurations or flaky tests hiding true failures. There is concern about the stability and correctness of features such as the `torch.export` tracing, ONNX translation, and support for specific hardware like SM89/SM90, with suggested fixes including better error handling, test augmentation, and documentation updates. Several comments address the need for clearer procedures for reproducing failures, managing static linking, and ensuring proper support for different architectures and backends, such as NVLS, inductor configs, and custom allocators. Additionally, there are recurring questions about the support status of features like quantization, BF16, and certain operator kernels, as well as comments about infrastructure and validation pipeline improvements. Overall, unresolved questions focus on test stability, feature support, and CI correctness, with many proposed technical fixes and planned refactors."
2025-01-29,pytorch/pytorch,"The collected GitHub comments from the pytorch/pytorch repository reflect ongoing efforts to address a wide range of issues, including support for features like pinned tensors, large state dict loading, and multi-backend compatibility, as well as technical challenges such as CI flaky tests, rebase conflicts, and build system inconsistencies. There are discussions on improving the reliability of functionalities like the CUDA NCCL init APIs and tensor meta information handling, including addressing latent bugs with tensor subclasses and onnx export errors related to getitem handling. Multiple comments highlight the need for better CI automation, test stability, and proper dependency management across platforms, especially for specialized hardware (ARM, ROCm, XPU). Unresolved questions involve precise behaviors at function domain edges (e.g., zeta at 1), handling of special constant functions in standards, and ensuring build artifacts are correctly used across distributed systems. Overall, the comments showcase active troubleshooting, codebase improvements, test stability efforts, and ongoing feature enhancements, with some open questions about standards compliance and build reproducibility."
2025-01-30,pytorch/pytorch,"The comments reflect a range of technical concerns in the PyTorch development process, including: issues in saving/loading model weights and ensuring consistency across various model serialization formats; challenges with trunk flakiness testing and CI stability, especially on platforms like rocm and linux, with some related to flaky test disabling/disabling logic; performance regressions and correctness of tensor operations like STFT, with discussions around padding strategies and phase accuracy; complexities in distributed training, NCCL/gloo backend errors, and the need for safer communication primitives; and considerations about API stability, code ownership, and feature support such as FAv3, cross-platform graph export, and third-party dependencies like Cutlass. Many discussions involve potential workarounds, ongoing debugging, and planning future improvements, while unresolved questions include ensuring robust cross-platform loading, performance measurements accuracy, and clearer API stability guarantees."
2025-01-31,pytorch/pytorch,"The comments reflect concerns about deprecated or abandoned components such as TensorPipe, RPC modules, TensorPipe support, and the impact of their deprecation on dependent systems like Caffe2. Several discussions address build and compilation issues, especially related to compiler flags, warning treatments, and compatibility across different environments, architectures, and hardware accelerators (CUDA, ROCm, XPU). There are multiple reports of flaky tests and CI stability issues, frequently attributed to platform-specific problems, environmental mismatches, or transient flakiness, with ongoing efforts to identify root causes, improve error messaging, and improve stability. Some discussions revolve around potential enhancements in API design, typing, and system integration (e.g., better type safety with `TypedKey`, cross-language interop, and modular extension points). Many unresolved questions pertain to compatibility, dependency management, performance regressions, and the handling of special cases like symbolic shapes, nested tensors, and low-level hardware-specific support, as well as the overall process for fixing flaky tests and build failures."
2025-02-01,pytorch/pytorch,"The discussions highlight several technical concerns, including issues with environment setup and package installation methods (e.g., conda vs. devpi), device compatibility and support (notably for MPS, ROCm, and specific GPU architectures like SM89), and runtime failures or errors such as CUDA kernel failures, ambiguous error messages, and build failures in CI workflows. There are questions about whether certain features, like bounds support for L-BFGS or specific optimization strategies, are planned or feasible, and suggestions to improve code robustness (e.g., handling singular matrices in `torch.linalg.cond`, clarifying error messages). Several discussions address the need for better testing, documentation consistency (e.g., API documentation vs. test behavior), and build stability across different hardware and software environments. Unresolved issues include proper support for newer hardware or software versions, reducing CI failures, and clarifying expected behaviors in edge cases."
2025-02-02,pytorch/pytorch,"The discussions primarily revolve around handling operator implementation gaps on specific hardware backends, particularly the missing operators 'aten::_upsample_bicubic2d_aa.out' and 'aten::kthvalue.values' on the MPS device, with suggested workarounds like fallback environment variables. There are concerns about ensuring non-hanging dump functions and accurate monitoring of asynchronous operations, which could be addressed by external looping or state management approaches. Several code improvements and bug fixes involve rebase issues, build configuration, and large page memory management, emphasizing the need for proper kernel and linker script support. Additional considerations include ensuring correctness of device-specific features like SM89 support, proper packaging of generated headers, and handling operator out-variant generation failures. Overall, unresolved questions include how to implement missing operators, improve runtime robustness, and ensure build consistency across different hardware and configurations."
2025-02-03,pytorch/pytorch,"The discussions primarily revolve around challenges in maintaining correct and efficient multi-device and multi-precision computations within PyTorch. Major concerns include inconsistent behavior and bugs in distributed communication (notably NCCL and ProcessGroup configurations), issues with operator support and correctness in ONNX conversions, and performance regressions due to hardware-specific kernels (e.g., cuDNN and CUDA support). Several comments point to bugs in internal build configurations, linking issues with libraries like gloo, and the need for better automatic tuning, cross-platform compatibility, and extensive testing to catch regressions. There is also mention of upcoming features (e.g., CUDA support in Gloo and C++20 support) and infrastructure improvements (e.g., build system modifications, CI coverage), with many unresolved questions about compatibility, stability, and proper configuration."
2025-02-04,pytorch/pytorch,"The gathered GitHub comments primarily discuss issues related to PyTorch's distributed data loading, validation, and sharding heuristics; the need for specific API support for structured tensors and their efficient representations; handling of flaky CI tests across various platforms (especially ROCm, macOS, and Linux) possibly caused by environment/configuration issues; and some internal implementation details such as model checkpointing, compiler behavior, and performance optimizations (like kernel caching and tensor sharding). Several discussions mention workarounds, proposed fixes, and the necessity of API improvements or better testing strategies. Unresolved questions include how to reliably disable flaky tests, handle shape constraints especially with symbolic shapes, and determine the exact causes for certain platform-specific failures or performance regressions. Overall, the key concerns revolve around improving robustness, correctness, and efficiency of distributed training, shape inference, and code compilation in PyTorch, while managing flaky tests and environment-specific issues."
2025-02-05,pytorch/pytorch,"The comments highlight ongoing challenges related to data-dependent control flow, particularly in the context of JIT compilation with torch.compile, where certain operations like guard on data-dependent expressions or custom C++ extensions are not fully supported, leading to failures or the need for workarounds. Several discussions also address compatibility issues across hardware platforms (ROCm, XPU, CUDA), including the support of new features (e.g., SVE, wide dtypes), and the complexities of maintaining unified build systems versus separate vendor-specific builds. There are concerns about the stability and correctness of specific operations (e.g., matrix multiplication, shape evaluation, runtime errors) especially in edge cases like singular matrices, zero-sized tensors, or out-of-bounds memory access, with suggestions to improve error handling and testing infrastructure. Additionally, some topics focus on build, deployment, and environment management (PyPI wheels, conda support, refactoring for broader hardware support), and the need for better tooling, monitoring, and messaging around flaky tests, platform support, and feature progression. Many discussions remain unresolved, often pending further testing, review, or platform-specific adjustments."
2025-02-06,pytorch/pytorch,"The comments predominantly revolve around several key issues: (1) Support for CPU offloading, mixed precision, and saving checkpoints on CPU, with some solutions like implementing policy functions and managing activation checkpoints. (2) Enhancements to Fully Sharded Data Parallel (FSDP) features support for FSDP2, including migration guides, testing strategies, and API/planning discussions for overlapping optimizer steps. (3) Platform and backend support challenges, especially relating to AMD ROCm, MPS, and ARM architectures, with discussions on workarounds, API modifications, and hardware-specific kernels. (4) ONNX export limitations for matrix inverse and linear algebra ops, with requests for workarounds and future support plans. (5) General CI stability, test flakiness, and proposed fixes for inconsistencies, timeouts, and environment-related issues, alongside discussions on maintaining and rolling out new features in a complex, multi-platform codebase."
2025-02-07,pytorch/pytorch,"The comments primarily revolve around the challenges of integrating experimental features such as type casting semantics, scan/loop functionalities, and offloading strategies into PyTorch's core infrastructure. Several discussions highlight the complexities of ensuring backward compatibility, performance optimizations, and correct behavior when extending or modifying lower-level routines like kernel implementations, memory management, and device-specific operations (e.g., ROCm, NPU). There are concerns about maintaining API stability, especially with pattern matching, graph transformations, and support for dynamic shapes or heterogeneous hardware. Many questions focus on how to best implement or leverage these features—whether through pattern rewrites, custom kernels, or API design—to maximize flexibility and performance while minimizing regressions and runtime errors. Unresolved issues include clarifying the design of new APIs, ensuring correctness across hardware, and managing the interplay of different offloading and sharding strategies in distributed training."
2025-02-08,pytorch/pytorch,"The comments reflect ongoing efforts and issues related to PyTorch's performance and hardware support, notably on macOS, MPS, and Nvidia GPU clusters, with particular attention to GPU utilization, quantization, and operator support. Key questions involve improving MPS performance for small tensors, ensuring operators like `aten::_upsample_bilinear2d_aa.out` are implemented for MPS, and resolving discrepancies in GPU timing and overlapping behavior under `CUDA_DEVICE_MAX_CONNECTIONS` and `TORCH_NCCL_ENABLE_TIMING`. There are also multiple discussions on model quantization workflows, compatibility with specific hardware (e.g., Kunpeng CPUs), and how to efficiently handle distributed model sharding, offloading, and re-resharding across FSDP, TP, and CPU memory. Some issues pertain to build/test infrastructure, merge failures, and the need for better APIs to support custom backends or hardware-specific features. Overall, unresolved questions include operator support for certain devices, performance bottlenecks, and correct configuration of distributed and mixed-precision training setups."
2025-02-09,pytorch/pytorch,"The discussions cover various technical concerns including the correctness of gradient flow behavior for `min`/`max` functions in autograd, the impact of code structure on compile-time performance, and the optimal control flow for tensor data pointers to avoid unnecessary branching. There are questions about the necessity and implementation of specialized LBFGS variants and fused operations, as well as issues related to specific GPU architectures like CUDA 9.0a and ROCm. Several merge failures and CI check failures indicate ongoing integration and stability challenges. Additionally, some comments highlight the need for better build support, explicit handling of device/device-type properties, and improvements in kernel precompilation and runtime performance."
2025-02-10,pytorch/pytorch,"The discussions highlight challenges with integrating quantization-aware training and conversion for transformer-based models like wav2vec2, with issues related to operator support for quantized backends and optimized kernels such as SDPA; users seek improved tooling and better support across different hardware (e.g., AMD ROCm, NVIDIA GPUs, XPU). Several technical concerns include the correct placement and implementation of QuantStub/DeQuantStub in eager mode, handling of operator support in static versus dynamic quantization, and stability and correctness of model serialization especially with custom or external components like torchao. Community members also raise questions on performance impacts, compatibility with different hardware architectures, and the need for standardized test suites to verify ops across data types and backends. Unresolved questions involve how to extend or adapt existing APIs for broader hardware support, improving compilation times and stability in the inductor, and ensuring that quantization and operator support work reliably in real-world use cases."
2025-02-11,pytorch/pytorch,"The comments reflect ongoing discussions about optimizing inference deployment and model deployment strategies, particularly focusing on techniques like batching inference requests, overlapping inference with model weight loading, and static/dynamic shape handling. Several issues concern the registration and linking complexities of CUDA and ROCm backends, static initializers, and ensuring correct device context management across different hardware (e.g., Gaudi, ARM SVE, ROCm). There are also numerous technical challenges related to quantization support, proper serialization (pytree registration), and performance regressions in custom kernels or codegen, especially when updating dependencies or integrating new accelerators. Some comments question the stability and reproducibility of specific profiling results and attempt to address errors or performance cliffs with workarounds or refactors. Unresolved issues include ensuring correct API extensibility, robustness during serialization/deserialization, and handling of specific hardware support with the ongoing developments in metatensor, inductor, and backend-specific custom kernels."
2025-02-12,pytorch/pytorch,"The comments reflect ongoing troubleshooting and development efforts across various aspects of PyTorch, such as fixing critical bugs, improving distributed training robustness (e.g., rendezvous behaviors and fault tolerance), enhancing ONNX export compatibility, and optimizing performance issues, especially related to hardware backends like ROCm, CUDA, and inductor. There are discussions about refactoring APIs for better extensibility, fixing type and code correctness issues, and addressing flaky tests or CI failures, often involving re-runs or workarounds. Several questions are raised about feature implementation timelines, API design choices (e.g., handling tensors/gradients, quantization), and platform-specific challenges (e.g., macOS, ARM). Many comments involve code review requests, bug reports, or attempts to streamline CI processes. Overall, the key unresolved concerns relate to stability, consistency across backends and platforms, and improving developer and user tooling/workflows."
2025-02-13,pytorch/pytorch,"The discussions highlight several technical concerns, notably the handling and extension of high-dimensional distributions in PyTorch, GPU-specific issues (including stability and performance on A10G, ROCm, or Maxwell hardware), and support for onnx export of complex operations like torch.stft, ISTFT, and dynamic shape handling during compilation. There's interest in improving distributed infrastructure (rendezvous robustness, NCCL init, UCC backend support), as well as in clarifying and standardizing API behaviors, especially regarding `torch.compile` and its interaction with runtime flags like `is_compiling`. Several proposals involve defaulting to fallback paths for unsupported ops, renaming or refactoring APIs for clarity, and addressing CI stability and flakiness across platforms. An overarching unresolved question is ensuring consistency of compile-time and eager-mode behaviors, proper error reporting, and effective support for hardware-specific features and complex workflows like checkpointing and model export."
2025-02-14,pytorch/pytorch,"The comments reflect ongoing investigations into PyTorch's distributed training robustness, such as rendezvous failures and node failure handling, with suggestions to switch to more resilient backend options like `etcd`. There are concerns about build stability and flaky tests across Linux, ROCm, and Windows platforms, with many issues marked as resolved after numerous reruns, indicating intermittent flakiness issues. Several discussions focus on correctness and compatibility issues, like handling dynamic shapes during model export, ensuring proper registration of custom types for serialization, and fixing kernel or memory bugs in Inductor and CUDA. Some comments address infrastructure or tooling challenges, including CLA signing problems, environment configuration (e.g., NCCL, CUDA versions), and build environment discrepancies. Overall, key unresolved questions include improving failure resilience (e.g., node restart logic, backend choices), clarifying runtime and export behavior for complex features, and fixing sporadic flaky tests or build failures."
2025-02-15,pytorch/pytorch,"The discussion covers several technical concerns including issues with failure to merge due to failing CI checks, notably in CUDA and inductor tests, often related to specific hardware or driver incompatibilities (e.g., unsupported GPU compute capabilities or CUDA versions). Several questions revolve around fixing runtime errors—such as protobuf size limits when exporting models, handling complex tensor operations in TorchScript, and ensuring proper behavior of distributed and asynchronous operations to avoid memory leaks. There are also suggestions to modify code for improved robustness and compatibility, including disabling certain features (like GradScaler), adding fallback mechanisms to eager execution, and updating dependencies or submodules (e.g., NCCL version, ACL routines). Unresolved questions include how to address specific errors in exporting models, compatibility issues with newer hardware (sm_120), and CLA signing problems, as well as inquiries about proper testing and benchmarking practices."
2025-02-16,pytorch/pytorch,"The discussions predominantly revolve around the implementation and integration of advanced linear algebra operations, such as SVD, as ONNX operators, with concerns about their complexity and current ONNX specifications. There are multiple issues related to distributed training, notably the misalignment of model parameters across ranks in DDP caused by arguments like 'rank', which was resolved by renaming to prevent conflicts. Several comments address bugs and performance issues, including potential hardware-specific bugs in CUDA for float64 softmax, memory management problems with sharding, and challenges with custom data types like DLFloat16 and their hardware support status. Additionally, maintenance and CI workflows are highlighted, including merge failures due to unrelated test failures and the need for proper CLA signing, alongside suggestions for code style fixes, test additions, and ensuring backward compatibility during state dict operations. Overall, unresolved questions pertain to hardware support for new data types, proper handling of model state during distributed training, and ensuring stability and performance in various deployment scenarios."
2025-02-17,pytorch/pytorch,"The discussions highlight several technical concerns including the implementation complexity and maintenance difficulty of specific features (e.g., support for certain operations in PyTorch), and the need for clearer documentation on usage, especially regarding dynamic shape support and run_decompositions. There is also concern around ensuring the correctness and performance of new backend features (such as SVE support and inductor enhancements), including handling of specific tensor types and memory management issues. Several issues relate to CI failures and merge conflicts, often due to environment mismatches like outdated libraries or platform-specific limitations, particularly for ARM and ROCm platforms. Additionally, there are questions about best practices for extension points (e.g., quantization templates, operator support) and how user modifications might be supported or documented effectively."
2025-02-18,pytorch/pytorch,"The discussions revolve around several complex issues, primarily concerning the semantics of distributed autograd and gradient reductions, especially with all-reduce operations being interpreted as identities in backward passes, and how `.backward()` handles gradients across distributed nodes. There are also technical challenges related to supporting dynamic shapes in compiler and runtime, such as during tensor slicing, view behaviors, and kernel arguments, especially for hardware-specific backends like XPU or ROCm, where inconsistencies or crashes occur. Additionally, there are issues with correctness and performance regressions after code changes, including failures in MLPerf, slow compile times, and flaky test results, compounded by platform and environment-specific factors like CUDA version or driver incompatibilities. Several proposals suggest improving robustness by adding better error handling, refining foldings, and ensuring consistent handling of in-place mutations, strong typing, or PyTree registration, while ongoing fixes include patching, rebase, and disabling flaky tests until stability is achieved."
2025-02-19,pytorch/pytorch,"The comments reflect extensive discussions on various PyTorch development issues, including implementation challenges for features like multi-GPU communication, black-box backends, and ONNX export. Key concerns include ensuring correctness and performance, such as fixing memory leaks, addressing inconsistencies with tensor shapes or gradient calculations, and improving debugging or benchmarking tools. Several comments suggest potential solutions or workarounds, such as reworking specific functions, adjusting configurations, or referring to external PRs, but many issues remain unresolved or require further validation. There is also emphasis on community contributions, testing, and careful handling of platform-specific behaviors, especially around CUDA, ROCm, and environment setup. The discussions conclude with ongoing efforts to fix bugs, improve stability, and optimize performance across the codebase."
2025-02-20,pytorch/pytorch,"The comments highlight concerns regarding deprecated or non-standard usage of `torch.nn.functional.normalize`, specifically replacing `clamp_min` with `clamp(min=eps)` to ensure compatibility and consistent behavior, as well as questioning the difference between `clamp_min` and `maximum`. Several issues involve the behavior and correctness of distributed operations such as `all_gather` and `all_reduce`, especially around gradient calculations and whether silent failures or incorrect gradient propagation are occurring—suggesting a potential need for warnings or fixes. Other discussions focus on compatibility and build issues, including ROCm, MacOS, and compiler support, often linked to specific hardware or software environments; in some cases, downgrading PyTorch versions or adjusting environment variables resolves the problems. There are also questions about the correctness and reproducibility of exported models, especially in dynamic shape scenarios, and about the replacement or support of certain libraries like cub in newer code. Finally, some comments request improvements or clarifications for internal tools, testing, and performance, and suggest possible code refactoring to improve maintainability or compatibility."
2025-02-21,pytorch/pytorch,"The discussions primarily revolve around performance regressions and correctness issues in PyTorch, especially related to MKL, CUDA kernels, and distributed communication. Several comments express concern over slowdowns with MKL/BLAS, particularly on AMD/EPYC hardware, and suggest workarounds like patching MKL or recompiling with alternative libraries. Others note bugs or regressions in specific kernel behaviors (e.g., softmax float64 handling, large kernel performance) and seek fixes or workarounds such as changing backends or fixing compiler/decomposition issues. There are also multiple questions about benchmarking discrepancies, build failures, and infrastructure or CI pipeline problems affecting test reliability. Several discussions involve potential feature proposals, bug fixes, or architecture changes to better support dynamic shapes, memory management, distributed debugging, and platform-specific bugs, with some unresolved or awaiting core review or infrastructure updates."
2025-02-22,pytorch/pytorch,"The discussions primarily revolve around handling integer outputs in custom autograd modules, particularly ignoring shape and output type checks to enable gradient passing for training, and the implications on gradient computation and optimizer steps. Several issues detail the need for explicit conversion or distribution of tensors (like DTensors) to support parallelism and synchronization across devices, with recommendations for using `distribute_module` and input/layout preparation hooks. Concerns about BC (backward compatibility) when loading optimizer states and model parameters across different sharding strategies are also prevalent, suggesting that flexibility similar to models should be provided for optimizers, potentially with a `strict` parameter. Additionally, attention is given to environment-specific DLL loading problems on Windows and the support status of certain operators on MPS devices, alongside general merge and infrastructure management challenges. Overall, unresolved questions include how to properly bypass type/value checks, ensure synchronization in distributed training, and manage environment-specific limitations."
2025-02-23,pytorch/pytorch,"The discussions highlight several critical areas: compatibility and support issues, especially regarding Metal Performance Shaders (MPS) on Apple silicon, with concerns about stability and proper bug fixes for GPU/Metal-related errors; the need to fix build and linking problems for large binaries and dependencies like setuptools in Bazel, affecting internal CI and release processes; potential BC-breaking changes in default behaviors (e.g., output dtype in sorting), suggesting more flexible options like `dynamic_indices_type` to retain backwards compatibility; ongoing issues with CUDA architecture parsing errors (e.g., `90a`) in nightly builds and the importance of maintaining support for lower SM architectures; and the overall challenge of ensuring reliable, reproducible performance testing, particularly regarding batch sampling strategies and speed comparisons."
2025-02-24,pytorch/pytorch,"The discussions highlight several technical issues, including the handling of distributed and device-specific configurations (e.g., XCCL/NCCL priorities, device registration, and environment variability), as well as compatibility and correctness of features like softmax inverse, tensor shape handling, and codegen for various hardware (ARM, ROCm, AMD, Apple M2). There are concerns about ensuring reproducibility, performance, and correctness across different hardware, software versions, and compiler configurations, especially regarding support for specialized ISA features (e.g., SVE) and backend behaviors (e.g., PyTorch’s backend registration, export and AOT compilation correctness). Some discussions focus on improving test infrastructure robustness and interface design for extensibility and cross-platform support (including device abstraction and deterministic behavior). Many issues remain unresolved or require further validation, such as ensuring stable handling of tensor shape constraints, fixing existing bugs that cause crashes or incorrect outputs, and balancing performance improvements with compatibility. Overall, the key questions are about ensuring broad hardware support, correctness, and maintainability within the complex PyTorch plugin and backend ecosystem."
2025-02-25,pytorch/pytorch,"The comments highlight ongoing issues with various PyTorch features, such as challenges with torch.tensor splitting, support for complex numbers in FFT/conv operators, and device synchronization/transfer bandwidth problems across different systems and backends (NVIDIA, AMD, XPU). Several discussions involve workarounds or temporary fixes, like manually forcing padding for kernels, using custom ops, or managing device contexts. There are multiple reports of flaky tests, mostly related to CI instability or environment-specific problems, especially on ROCm and inductor pipelines, with some issues linked to specific hardware (e.g., ROCm GPUs, ROCm/ARM builds). Many comments concern infrastructure updates, including rebasings, merging PRs, or adjusting build configurations to support newer hardware, driver versions, and software layers. Several unresolved questions remain about support for features such as complex support, dynamic shape handling, and supporting new hardware/software features, with some fixes pending review or merge."
2025-02-26,pytorch/pytorch,"The discussions primarily revolve around ongoing development and breaking changes in PyTorch, such as support for lower-precision quantization, improvements in MPI and distributed operations, and sensitivity of compile/optimization passes to certain code patterns. A recurring concern is ensuring backward compatibility and minimizing operational overhead, such as avoiding extensive kernel signature changes or build complexities that delay merging sensitive features. Several issues highlight flakiness in CI tests due to hardware or environment variability, with suggestions to extend test coverage or implement better resource management (e.g., memory release, thread caching). There are also security and usability considerations, especially related to distributed features, threading, and serialization behaviors. Unresolved key questions include the impact of new API changes on existing workflows, handling of device-specific instructions, and proper support for features like TorchDispatch modes in higher-order or complex control flows."
2025-02-27,pytorch/pytorch,"The comments span a range of complex PyTorch issues, including the challenge of implementing custom schedulers like cosine in caffe2, difficulties with gradient computation in batched vmap + functorch, and compatibility concerns with model exporting, implicit autograd, and device-specific operators. Several discussions highlight support gaps or bugs in autograd, especially with checkpointing, tensor subclassing, and certain op backends (e.g., MPS and ROCm), often due to unsupported operators or device constraints. Others focus on optimizing performance, managing CI flakiness, and ensuring reproducibility across hardware (e.g., MI300, AMD GPUs). Many questions relate to code stability, correct handling of attributes in tracing, and the impact of recent changes on legacy users or specific use cases. Unresolved issues include supporting checkpointing with vmap, improving error handling for degeneracies, and upgrading migration or export workflows, with ongoing work and guidance pointing towards future refinement."
2025-02-28,pytorch/pytorch,"The discussions highlight issues with gradient correctness for CTC loss when using softmax/normed inputs, leading to proposed functional modifications (_FixCTCGrad) that adjust the gradient calculation to match expected behavior. Several issues concern PyTorch's support for features on various backends, including MPS, ROCm, and different hardware platforms, with many feature requests and fallback suggestions. There are recurrent problems with trunk flakiness testing, build failures, and dependency support, often related to platform-specific operators and compiler behaviors. Additional topics involve code migration complexities (e.g., onnxscript migration and deprecations), as well as performance and memory concerns when using torch.compile, especially with large models or in reduced memory contexts. Many issues raise questions about support stabilization, proper API semantics (e.g., lstsq residuals and device support), and how to implement or enhance features like custom kernel integration or dynamic shape handling in the ongoing development cycle."
2025-03-01,pytorch/pytorch,"The discussion covers several topics related to PyTorch development, including issues with JIT and torchscript compatibility, supporting model export with dynamic shapes, and improving performance through compilation strategies such as torch.compile. Concerns are raised about specific operator support in Dynamo and Inductor, particularly around pattern matching and fusion, as well as runtime dtype conversions and their impact on execution consistency. Infrastructure problems like build and merge failures, environment configuration, and CUDA support on ARM64 are also discussed, with suggestions on correcting build configurations and handling platform-specific issues. Many comments address ongoing bugs, the need for better testing and validation, and development workflows like rebase and merge procedures, highlighting areas requiring further investigation or bug fixes."
2025-03-02,pytorch/pytorch,"The discussions encompass several technical concerns including the proper inclusion of CUDA and cuDNN headers in PyTorch distributions, emphasizing that PyTorch wheels or libtorch should not require full CUDA toolkit headers unless explicitly needed, to maintain simplicity for users. There are ongoing efforts to improve build configurations for different architectures and platforms, such as macOS and AMD GPUs, including considerations for CUDA versions, driver support, and support for modern hardware features like dynamic shapes and hardware-specific instructions (e.g., WGMMA). Additionally, issues around the compatibility and correctness of various APIs, in particular with regard to dtype consistency in AOTInductor, ONNX export issues, and the management of internal vs public APIs, are highlighted. Some comments also reflect the administrative aspects of the development process, including merge procedures, CI scheduling, and CLA signing, which influence the workflow but are not directly related to the technical content. Unresolved questions mainly revolve around ensuring compatibility across platforms and architectures, and clarifying behaviors for certain features like dynamic shapes and custom classes in compilation pipelines."
2025-03-03,pytorch/pytorch,"The discussions highlight multiple technical concerns: the implementation of cryptographic hash functions (like SHA256) in PyTorch using only tensor operations, with suggestions about correctness and potential compile-time optimizations; challenges with model exporting and dynamic shape handling in ONNX, including the adoption of `dynamo=True` and issues with shape inference, requiring updated symbolic functions or API adjustments; the need for more expressive and device-agnostic abstractions in PyTorch's testing infrastructure to handle cross-hardware compatibility (especially for XPU/ROCM and other accelerators), including device capability queries and capabilities in `common_device_type.py`; the handling of large tensor shapes exceeding 32-bit limits in LAPACK calls, and partial workarounds or API improvements suggested; plus various CI and flaky test issues, regression occurrences, and platform-specific build challenges that need addressing, some requiring rebase or reverting of recent PRs. Overall, unresolved questions include the proper API design for shape and device abstractions, stability of large-scale model exports, and compatibility issues across different hardware and software environments."
2025-03-04,pytorch/pytorch,"The comments highlight various technical discussions and issues encountered in the 'pytorch/pytorch' repository. Key concerns include handling of flakiness in tests across platforms, especially ROCm and in trunk, often resolved by repeated reruns or disabling tests temporarily. There's ongoing work and questions about the support for distributed training and tensor parallelism, particularly in complex scenarios like TP sharding, distributed module initialization, and mixed-precision (amp) performance, with suggestions to improve API usability and error clarity. Several issues involve build or linkage problems on specific architectures or environments (e.g., CUDA 12.8, ARM/Linux, MacOS), with proposals for workarounds or improvements like environment variables or build configuration changes. Additionally, there's active debugging around kernel performance, compiler behaviors, and specific errors e.g., eigenvalue degeneracy, which require detailed analysis and possible API enhancements, such as adding epsilon margins or better error reporting."
2025-03-05,pytorch/pytorch,"The discussions primarily revolve around issues with deterministic behavior and reproducibility in PyTorch, especially related to CuBLAS and environment variable configurations, with some mentioning the need to set `CUBLAS_WORKSPACE_CONFIG`. There are concerns about the compatibility and stability of certain features across different PyTorch versions, hardware platforms (e.g., ROCm, CUDA, XPU), and environments, with some queries about performance trade-offs of alternative backends like Flash Attention. Several threads highlight flakiness and flaky testing in CI, often related to platform-specific issues or environment configurations, prompting requests for better debugging, re-trials, and stabilization strategies. Additionally, there are questions on tooling and infrastructure, such as merging policies, code rebase procedures, and support for new hardware or backends, with some discussions about the need for clearer documentation, testing coverage, or new custom ops. Overall, the key unresolved questions focus on improving deterministic reproducibility, cross-platform stability, and performance consistency in PyTorch's evolving ecosystem."
2025-03-06,pytorch/pytorch,"The collected GitHub comments reflect a diverse set of technical concerns related to PyTorch development, including issues with CI test flakiness, build configuration challenges, and platform-specific bugs (especially on MPS, ROCm, and XPU devices). Several discussions highlight the need for better support and documentation for platform-specific features such as MPS GPU operations, HPU/XPU device handling, and dynamic shape exports in ONNX. There are recurring unresolved questions about performance discrepancies (e.g., GEMM kernel performance differences), stability of certain operators on specific hardware, and process management features like process-to-CPU core binding. Suggestions include refining CI workflows, improving runtime and build diagnostics, and adjusting default behaviors for backward compatibility, with some proposals for API or system enhancements indicated. Overall, the thread indicates ongoing troubleshooting and feature work on hardware platform support, optimization, and robustness in the PyTorch ecosystem."
2025-03-07,pytorch/pytorch,"The discussions reveal concerns about the performance and implementation details of various operators and features within PyTorch, including issues with TensorBoard file access on remote filesystems, GPU/GPU memory management, and operator support for different hardware backends like MPS and ROCm. Several questions are raised about the correctness and stability of in-progress features such as dynamic shapes handling, nested tensors, and support for specialized backends like Triton, often seeking guidance on implementation details, best practices, or upstream support. There are also recurring themes about flaky tests and build stability, with discussions on re-prioritization, CI management, and infrastructure adjustments. Additionally, underlying concerns about compatibility, auto-tuning, and data serialization (e.g., for ONNX or hybrid caches) are discussed, highlighting the ongoing effort to improve robustness, performance, and usability of the framework across diverse hardware and configurations."
2025-03-08,pytorch/pytorch,"The discussions primarily revolve around device compatibility and device management in PyTorch, such as the necessity to move models to GPU before initializing certain optimizers (e.g., Adagrad) due to device mismatches during tensor operations. There are also concerns about ensuring proper implementation and support for various operators on different backends like MPS, with many operators still unimplemented or requiring additional work. Some issues involve understanding and fixing build/deployment processes, including dependency management, build scripts, and documentation consistency. Additionally, there are routine maintenance and merge conflicts, as well as questions regarding behaviors affected by PyTorch updates, such as the `torch.unique()` sorting behavior. Overall, unresolved questions focus on device transfer order, operator support, build stability, and documentation accuracy."
2025-03-09,pytorch/pytorch,"The discussions highlight performance discrepancies when using GPUs or accelerators like MPS, especially with small tensors, due to overheads and hardware-specific behaviors. Several issues concern compatibility and support for newer architectures such as AMD gfx1102/1103, Nvidia sm_120, and the handling of architecture-specific features like PTX code and compute capabilities. There are recurring challenges with driver/driver API stability, and failures related to driver detection, driver version mismatches, or unreliability of certain hardware modules. Additionally, there are discussions around build processes, dependency management for features like Triton, and compatibility of features such as torch.compile across different platforms and nightly builds. Unresolved questions remain about proper support for new hardware, and improving diagnostic or error notifications for complex distributed and hardware-initialization issues."
2025-03-10,pytorch/pytorch,"The comments predominantly highlight issues related to expanding support for advanced features in PyTorch, such as handling dynamic shapes, support for new hardware (e.g., ARM, XPU, Blackwell GPUs), and integration of features like `torch.compile` with custom backends (e.g., TVM). Several discussions suggest architectural improvements, such as implementing a generalized inductor quantizer or referencing new API options (like `options` dict) for backend customization. Many comments also report setup, CI, and flaky test failures—often related to environment mismatches, build configurations, or hardware-specific limitations—that remain unresolved. Some comments emphasize the need for better test coverage, more stable CI workflows, and longer-term plans for features like sharding, serialization, and bug fixes, alongside operational considerations for avoiding BC breaks. Overall, the discussions reflect ongoing work to extend support, improve stability, and optimize performance, with many unresolved questions and points requiring further design review."
2025-03-11,pytorch/pytorch,"The discussions highlight several technical concerns: first, the challenge of ensuring proper sharding semantics in distributed training, with debates on whether to support per-parameter sharding, dimension-wise sharding, or duplication, and the need for clear user guidance and documentation on tradeoffs. Second, there are issues related to the stability and correctness of runtime components (e.g., support for dynamic shapes, operator support such as bi-linear resize, max unpooling, and linear algebra operations), as well as compatibility with different hardware (e.g., AMD ROCm and MPS). Third, there's an ongoing effort to stabilize build processes, update dependencies (like cuDNN versions), and address CI flakiness due to hardware or environment inconsistencies. Fourth, some discussions concern code refactoring—for instance, improving the quantizer abstractions or supporting additional compiler configurations—aiming to simplify maintenance while broadening capabilities. Lastly, questions remain about proper rebase strategies, ensuring backward compatibility, and updating documentation or test coverage to reflect new features or fixes."
2025-03-12,pytorch/pytorch,"The comments primarily reflect community interest and ongoing development efforts around PyTorch features, such as implementing parallel module execution (`nn.Parallel`), improving GPU/TPU memory management, and enhancing ONNX export support for dynamic shapes. Several issues concern performance regressions and compatibility, especially with MPS, ROCm, and XPU devices, often requesting better fallback mechanisms, clearer error messages, or support for specific operations like `linalg.solve`. Discussions also highlight the complexity of supporting custom ops, export stability, and device-specific behavior, sometimes noting regressions or flakiness in CI tests or internal build processes. Several community members offer proposed fixes, patches, or request investigation into unresolved bugs, often suggesting better error handling and more informative diagnostics to streamline troubleshooting. Overall, these conversations reveal active engagement with reliability, compatibility, and efficiency improvements in PyTorch."
2025-03-13,pytorch/pytorch,"The comments reflect a variety of issues and discussions within the PyTorch ecosystem, including support and limitations of exporting models to ONNX using Dynamo (noting that `verbose=False` can fix export issues), challenges in obtaining attention weights during Transformer training (with suggestions to modify `need_weights=True` in attention modules for hooks), and the need for support for nested and jagged tensors with functions like `scatter` and concatenation (with ongoing work on such support). There are concerns about system-specific issues such as GPU memory limitations, kernel compilation regressions, and performance degradations after upgrades—especially influences of different CUDA versions and autodiff behavior. Additionally, many discussions involve CI flakiness, build environment compatibility (e.g., ROCm, Windows, MPS), and ensuring correctness and reproducibility of results, often with suggestions for fixes, workarounds, or improvements. Unresolved questions include how to extend support for nested tensors in more operations, handling of dynamic shapes, and adjusting experimental kernel configurations for stability and performance."
2025-03-14,pytorch/pytorch,"The comments collectively highlight ongoing issues related to PyTorch's support for various hardware backends (notably MPS, ROCm, and CUDA), operator support, and performance regressions in different versions. Several discussions focus on confirming the correct behavior, improving operator coverage (e.g., supporting 'aten::unfold_backward', 'aten::_standard_gamma'), and addressing performance discrepancies seen in specific operators or models across different environments. There are also several questions about builds, dependencies, and how infrastructure changes (like rebase or merge workflows) impact development, with a need for clearer testing, documentation, and support for edge cases such as non-contiguous tensors or unsupported tensor subclasses. Additionally, multiple issues concern stability and correctness in distributed training, early operator support, or new features, with some proposals suggesting better handling of device-specific behaviors, improved test coverage, or more transparent error messaging for unsupported operations. Overall, unresolved open issues remain on hardware support coverage, operator support for new devices, and performance variability, requiring further refinement and infrastructure support."
2025-03-15,pytorch/pytorch,"The discussions highlight ongoing challenges with PyTorch's handling of out-of-bounds grid sampling, suggesting that current implementation may not explicitly zero out such locations, potentially leading to unintended behaviors in modes like 'nearest'. There are concerns about ensuring non-persistent buffers are properly reinitialized after meta-device initialization, emphasizing the need for clearer guidance and better APIs, such as `to_empty`, to manage such buffers safely. Additionally, questions pertain to compatibility issues, such as the impact of Python versions on package installation, with suggestions to update pip or use alternative sources like conda-forge. Several discussions also involve improvements in testing, code stability, and merging workflows, with some issues marked as needing further work or awaiting fixes before completion."
2025-03-16,pytorch/pytorch,"The discussions encompass several technical concerns: the need for more efficient and stable ways to generate computation graphs and visualize them (e.g., using functorch and torchviz), as well as challenges with ONNX export support for operators like fft_fft2 and aten::_sample_dirichlet, especially related to support for specific opset versions and attribute capturing. Issues with hardware compatibility and resource limitations (OOM errors, especially on MPS devices and MIG architectures) are also prominent, raising questions about memory usage, operator support, and fallback options. Additionally, there are ongoing discussions about API stability, deprecation, and implementation practices (e.g., handling subsets of datasets, usage of torch.unique with padding, and the implementation of SwiGLU), as well as merge conflicts and continuous integration errors impacting development workflows. Unresolved questions include support status of certain features (e.g., functorch's graph visualization tools, FSDP2, or dynamic shape handling in ONNX), and potential coding or design improvements to address error cases and compatibility issues."
2025-03-17,pytorch/pytorch,"The discussions highlight several issues including the need for implementing unimplemented PyTorch operators (e.g., `aten::unfold_backward`, `aten::scatter_reduce.two_out`), performance variability on MPS and different hardware configurations, and challenges with dynamic shapes in ONNX export, especially with complex models and inputs. There are questions about driver support, such as AMD ROCm, and inconsistencies in seed and RNG behavior across platforms, emphasizing the importance of proper RNG initialization for deterministic workflows. Some discussions focus on CI flakiness, test stability, and merge success, as well as internal updates and build management for releases, with certain issues being marked as resolved after multiple reruns or fix submissions. Unresolved aspects include proper implementation of operators for MPS, handling dynamic shape exports reliably, and improving hardware support and testing infrastructure, alongside ongoing efforts to stabilize CI and compatibility across platforms."
2025-03-18,pytorch/pytorch,"The comments reflect a range of issues related to PyTorch's internal and external functionalities, including GPU memory management (especially with AMD ROCm and Nvidia), the behavior of torch.compile and inductor regarding shape and device handling, and support for features like complex tensors, ragged tensors, and custom operations. Several discussions highlight challenges with ensuring reproducibility, correctness, and performance, often proposing workarounds or fixes such as changing kernel tile sizes, handling memory leaks, or adjusting serialization strategies. There are also recurring validation and CI flakiness issues, with efforts to disable flaky tests, improve test coverage, or stabilize the release process. Overall, key unresolved questions involve the compatibility of new compile modes with various backends, the handling of dynamic and uneven shapes, and ensuring robust, predictable behavior across hardware and software configurations."
2025-03-19,pytorch/pytorch,"The discussions highlight several technical concerns: (1) Memory allocation issues in Metal Performance Shaders (MPS) for large tensor sequences, suggesting dynamic chunking adjustments; (2) Mismatches between expected and observed strides in custom ops, indicating a need to properly handle non-contiguous tensors and validate stride assumptions; (3) Extensive dependency and environment management challenges, such as ensuring library compatibility, correct CUDA device targeting, and build reproducibility across diverse hardware and software configurations; (4) Incomplete or fragile test coverage, especially for edge cases like variable sequence lengths and custom kernel behaviors, requiring more robust validation strategies; and (5) Procedural questions about code review, cherry-pick processes, and integrating infrastructure improvements or backports into release branches. Overall, unresolved questions concern how to handle large tensor sequences efficiently, improve custom op correctness, and streamline CI and build workflows."
2025-03-20,pytorch/pytorch,"The collected comments cover a broad set of technical issues related to PyTorch, including problems with CUDA device management (especially with `CUDA_DEVICE_MAX_CONNECTIONS`), JIT/ONNX export incompatibilities (e.g., unsupported operators like `scaled_dot_product_attention`, `fft_fft2`), and various internal bugs or failures in distributed training, FSDP, and autograd. Several discussions suggest potential workarounds such as disabling certain graph optimizations, using `strict=False` during ONNX export, or modifying environment variables. Some issues involve deep integration with hardware-specific features (e.g., MIG, ROCm, MPS) and tooling (e.g., `fx`, `torch.compile`, `dynamo`). Persistent questions remain about reproducibility, cache key validation, and supporting dynamic shapes and multi-operator support in compiled graphs. Overall, the discussions indicate ongoing debugging efforts, feature requests, and the need for clearer testing, documentation, and support for newer hardware/backends."
2025-03-21,pytorch/pytorch,"The discussions primarily revolve around the implementation and behavior of advanced features such as FFT-based convolution integration, CUDA stream parallelism, and device-specific support for various types (e.g., HIP, ROCm, Tensor parallelism). Several comments highlight limitations in CUDA stream parallelism due to overheads and kernel launch costs, with some users demonstrating successful parallel execution with large tensors. Others focus on device-specific issues like the libgomp versus libiomp linkage problem, the need for clearer documentation of sharding semantics, and the handling of dynamic shape guards, especially with symbolic operations like `max`. There are ongoing questions about proper handling of device heterogeneity, support for newer operators (like ConvTranspose3D on MPS), and ensuring backward compatibility in features like uneven sharding. Many discussions signify active development, bug fixing, and feature planning, with some unresolved issues like device support discrepancies, synchronization of global state in compiler graphs, and code correctness in complex distributed or compiled scenarios."
2025-03-22,pytorch/pytorch,"The discussions cover several technical concerns: firstly, the lack of support for MPS (Apple GPU) in Docker containers, raising questions about potential workarounds like Podman and the broader issue of hardware driver support; secondly, questions about the persistence and transformation of code, particularly the use of 'persistent' annotations in Triton kernel code and whether these represent reversed or optimized states; thirdly, issues with unsupported operators (e.g., `aten::cholesky_inverse`, `convTranspose3D`) on MPS for specific applications like medical imaging models, and expected future support; fourth, evaluation of nested tensor support, especially around jagged tensors, their conversion, and limitations in current APIs; and finally, concerns about build failures, testing regressions, and merging issues due to conflicts or infrastructure failures. Unresolved questions include efforts around compatibility, performance optimizations, and maintaining support for advanced features like autograd in irregular tensor formats."
2025-03-23,pytorch/pytorch,"The comments highlight ongoing challenges with the C++ ABI compatibility in PyTorch, especially when building custom extensions or linking with libtorch, emphasizing the need to set the `-D_GLIBCXX_USE_CXX11_ABI=0` flag to resolve linkage errors; some users report persistent undefined references to `c10::Error::Error`. Issues related to CUDA support, particularly for dynamic parallelism, `ConvTranspose3D` on MPS, and CUDA misalignments (notably with CUDA 12.2 and 2.6.0), are also prominent, with suggestions for workarounds such as switching to `torch.nn.functional.interpolate` or addressing compiler bugs. There are discussions about improving auto-tuning, kernel configuration, and sharding strategies in DTensor to optimize performance, as well as concerns about merge failures in CI due to failing checks or flaky tests. Lastly, users seek guidance on reproducing specific issues, fixing documentation and build errors, and understanding the impact of recent changes on features like the autodiff cache or ONNX support."
2025-03-24,pytorch/pytorch,"The discussion covers various topics related to PyTorch internal development, such as improving the efficiency of tensor operations with TensorOptions (e.g., `from_blob` with device specs), porting operations like `max_pool2d_with_indices` into differentiable decompositions, and addressing performance issues with specific operators like `scaled_dot_product_attention` on different hardware (e.g., MPS and H100 GPUs). There is also concern about reducing compile times, supporting dynamic shapes, and handling tensor subclasses properly during tracing and autograd, especially in custom or external operators. Several comments mention future plans for feature enhancements (like shared autotune cache keys, support for specific operators, or including third-party libraries like NVTX), as well as troubleshooting build and integration issues across diverse hardware and software environments (CUDA versions, system libraries, etc.). Unresolved questions include how best to propagate cache keys for autotuning, handle shape restrictions in distributed settings, and improve compatibility or support for custom tensor subclasses within the compiler and autograd infrastructure."
2025-03-25,pytorch/pytorch,"The comments cover diverse PyTorch development issues, including potential bugs, feature requests, and workarounds. Major concerns involve reproducibility and correctness of complex features like nested tensors, sparse tensor support, and custom ops, often with discussions on proper handling, API design, and validation. Several reports address performance regressions, slowdowns, and CI flakiness, with suggestions to revert or fix specific changes. There are recurring questions about incremental improvements, code correctness, and best practices for features like model offloading, export, and tensor modifications. Some discussions highlight the difficulty of unsupported operations, the need for clearer tooling, and the importance of proper testing and validation before merging impactful code changes."
2025-03-26,pytorch/pytorch,"The comments collectively highlight challenges with distributed training deadlocks, especially when collective operations like `torch.distributed.reduce` are called in non-synchronized contexts, or when NCCL (NVIDIA Collective Communication Library) encounters timeouts and connection issues across multiple nodes or GPUs. Several threads discuss potential workarounds, such as ensuring all processes participate in collective calls, increasing timeouts, managing unused parameters, and diagnosing deadlocks with stack tracing tools. Issues also involve environment and version mismatches (e.g., NCCL, CUDA, PyTorch, or system configurations), sometimes leading to hangs or failures in multi-GPU setups, particularly on new hardware or with recent software upgrades. Additional concerns address disabling flaky tests, supporting out-of-tree custom extensions or backends, handling serialization of distributed states (e.g., FSDP with deepcopy), and needing improved documentation or debugging tools for these distributed and device-specific problems. Lastly, some comments reflect ongoing work, patches, and patches’ review status about fixing these problems, or about the necessity of careful environment control and synchronization to prevent deadlocks and ensure stability."
2025-03-27,pytorch/pytorch,"The comments encompass a wide range of issues in the PyTorch repository, including implementation clarifications for features like stable sorting, device and memory management bugs, and model export behaviors, with some discussions on the suitability of certain APIs and the need for improved documentation. Several technical concerns relate to the handling of CUDA, ROCm, and XPU backends, especially regarding compatibility, build issues, and performance optimization, with ongoing efforts to fix specific bugs and improve support. Notable questions address experimental features like inductor kernel fusion, in-place tensor operations, and bounds checking for tensor sharding and view operations, often with suggestions for better validation, error handling, and documentation clarity. Some comments focus on process improvements such as update procedures for external dependencies, clarifying the API behavior, and managing flaky tests, as well as internal infra issues like merge failures, CLA signing, and code review bottlenecks. Unresolved questions include details about the support and correctness of certain operator implementations, device-specific behaviors, and verification of experimental features across hardware platforms."
2025-03-28,pytorch/pytorch,"The discussions primarily revolve around the absence of a `sqrtm` function in PyTorch and how to implement a preferable backward computation method for the matrix square root, such as using Sylvester or Lyapunov equations. Contributors suggest using Newton-Schulz iterations with early stopping, and note that a batched, robust implementation of `sqrtm` is missing. There is also mention of ongoing efforts in SciPy to improve their `sqrtm` method, possibly providing a fallback. Additional questions include performance considerations, handling of dynamic shapes, and the potential for unit tests to verify new implementations. Unresolved issues concern integrating more numerically stable or efficient `sqrtm` algorithms and ensuring proper testing and support across platforms."
2025-03-29,pytorch/pytorch,"The comments cover a diverse range of issues including file handling errors, divergence in performance improvements (e.g., with fp16 GEMM), challenges with layout optimization in inductor and flex attention, complexities arising from environment-specific paths (like virtual environments), and merge conflicts across multiple PRs. Several discussions involve proposing fixes, such as modifying path determination logic for CUDA libs, improving error messaging, and ensuring compatibility of decompositions. There are also concerns about merge conflicts, test failures, and build issues that hinder progression, alongside questions about feature support (e.g., onnxscript IR, sparse tensor formats). Many comments reflect ongoing efforts for refinement and integration, with unresolved questions on performance gains, correctness, and code stability, indicating active development areas and the need for further validation."
2025-03-30,pytorch/pytorch,"The discussions center around various technical challenges and feature proposals for PyTorch, including performance benchmarking of new implementations like associative_scan and torch.scan, and assessing their potential benefits over existing methods like flashrnn. There are ongoing efforts to support complex numbers in torch.compile, with concerns about performance and compatibility, especially on different hardware backends like MPS and XPU, and related API documentation issues. Several issues relate to bug fixes, such as CUDA graph handling, dtype consistency in distributed operations, and improving BLAS/backend configurations for specific hardware like gfx950 and XPU. Additionally, some discussions involve build process problems, CI failures, and the need for code maintenance, such as adding trailing commas for style consistency and fixing deprecated or unsupported operator implementations. Many unresolved questions involve ensuring compatibility, optimizing performance across diverse hardware, and clarifying API/documentation gaps."
2025-03-31,pytorch/pytorch,"The discussions encompass a variety of issues related to PyTorch features and infrastructure. Key concerns include ensuring correct and backward-compatible behavior of exported models, especially regarding model serialization, schema upgrades, and handling of collection-based inputs; performance regressions and their benchmarking across different hardware architectures, particularly with new GPU models like RTX 5090 and Blackwell; and correctness and robustness of distributed and CUDA operations, including NCCL, NCCL timing, and multi-device communication. There are questions about supporting custom operators in autograd/ingenious workflows (e.g., Triton kernels, custom ops), fixing flaky CI failures, and improving user experience through clearer error messages and better test coverage. Some discussions also focus on experimental features like TMA, inline annotations, and handling system-specific or hardware-specific limitations. Overall, the discussions highlight ongoing efforts to improve model deployment, compatibility, performance, and stability across diverse environments."
2025-04-01,pytorch/pytorch,"The comments encompass a diverse set of discussions and bug reports related to PyTorch's development. Key issues involve performance regressions and flakiness in CI tests, often linked to specific hardware platforms (e.g., CPU, CUDA, XPU, ROCm) or internal modules like inductor, fbgemm, or tensor decompositions. Several reports highlight potential bugs or missing functionalities in core libraries (e.g., oneDNN, CUTLASS, torch_xpu-ops), with suggested workarounds or pending fixes, sometimes requiring upstream patching or rebase efforts. There are also discussions about API behaviors, such as supporting masked tensors, custom allocators, and the behavior of functions like `asarray` or `torch.compile` with certain inputs, especially regarding backward compatibility and correctness. Overall, unresolved questions focus on stability, performance optimization, platform-specific issues, and ensuring compatibility while addressing flakiness, build failures, and feature support concerns."
2025-04-02,pytorch/pytorch,"The discussion highlights several issues related to CUDA device management, model serialization, and operator implementation in PyTorch. Key concerns include the hard-coding of device-specific details (e.g., GPU/cuDNN), which complicates model export and deployment across different hardware, with suggestions for native support for device-agnostic model execution in TorchScript. There are challenges around ensuring tensor memory layouts and strides are consistent, especially when loading large tensors or working with non-standard contiguous layouts, which can lead to errors or incorrect computations. Additionally, there are discussions about implementing missing operators (like `aten::angle` for MPS or `aten::_nested_tensor_from_mask_left_aligned`), and the need for better support for tensor subclasses, custom decompositions, and distributed training workflows including FSDP. Overall, the unresolved questions involve improving device interoperability, robustness and correctness of tensor serialization, operator coverage, and the integration of advanced features like CUDA graphs, tensor subclasses, and decomp support into the core framework."
2025-04-03,pytorch/pytorch,"The comments cover a wide range of issues and ongoing efforts in the PyTorch project, including build environment challenges, bug fixes, feature requests, and support for new hardware and distributed training methods. Many discussions relate to compatibility issues (e.g., with specific CUDA, ROCm, or compiler versions), pipeline improvements (such as FSDP, DTensor, and inductor optimizations), and handling of edge cases or non-standard tensor layouts. Several comments indicate active work on supporting new features, fixing bugs, or enhancing robustness, often with workarounds or planned reworks. Some issues involve environment setup, dependencies, or system-specific behaviors that can cause failures or inconsistencies. Overall, the community continually iterates on stability, performance, and feature support, with many unresolved questions around compatibility, debugging, and system-specific quirks."
2025-04-04,pytorch/pytorch,"The discussions highlight ongoing challenges with operator support on various devices (notably MPS and ROCm), with specific reference to unimplemented operators like `aten::angle` and `aten::_nested_tensor_from_mask_left_aligned`. There are concerns about compiler bugs affecting kernel registration and resource request failures on large models or certain hardware (e.g., multiple A100s, GFX1100). Several comments mention the difficulty in testing or reproducing issues due to environmental inconsistencies or the need for version-specific workarounds, as well as performance considerations between inductor-generated code and native kernels. Others address build system limitations, such as handling multiple ONNX opset imports or the impact of code changes on binary size and performance regressions. Unresolved questions focus on device memory sharing for MPS/CPU, proper handling of unsupported operators, and ensuring stability across diverse hardware and software configurations."
2025-04-05,pytorch/pytorch,"The discussions highlight several technical issues within the PyTorch ecosystem, including the lack of optimized 3D depthwise separable convolutions and the status of related implementations. There are concerns about memory management and performance on certain hardware backends, especially regarding MPS backend errors and possible memory leaks during training. Compatibility and support issues with newer Python versions, CUDA versions, and prebuilt binaries also surface, alongside challenges with build systems (e.g., CMake) and internal infrastructure (e.g., merge failures due to CI failures). Additionally, there are questions about tensor printing conventions for dynamic or symbolic sizes, and some discussions address specific user-reported bugs or feature requests, with ongoing efforts to fix or improve these areas."
2025-04-06,pytorch/pytorch,"The discussions primarily concern enhancements and debugging related to PyTorch's linear algebra functions, complex number support, and CUDA/ROCm compatibility. There are technical questions about the implementation of `matrix_power` and its support for lower-level operations like exponentiation by squaring, as well as support for complex numbers in TorchInductor and their impact on performance. CUDA and ROCm-related issues include illegal memory access errors, potential OOM conditions, and build configurations, with suggestions to condition workarounds on device specifics or external factors such as world size. Several discussions revolve around debugging runtime errors and inconsistencies in rebase/merge workflows, indicating ongoing stability and compatibility challenges. Overall, unresolved questions focus on improving hardware support, debugging techniques, and configuration management for reliable and efficient execution."
2025-04-07,pytorch/pytorch,"The collected comments highlight several key development and troubleshooting issues in the 'pytorch/pytorch' repository: (1) Enhancements to the `torch.unique` function to support various use cases, including stable sorting and index retrieval, with complex code implementing temporary patches until backend support is available; (2) Several performance and correctness regressions or flaky tests across different hardware setups (CPU, CUDA, ROCm), often related to device-specific kernels, environment configurations, or internal compiler/driver behaviors, sometimes needing reverts or workarounds; (3) Build and environment management challenges, such as handling multiple Python interpreters, package installations, or CUDA/NCCL dependencies, affecting reproducibility and stability; (4) Discrepancies in strides and tensor memory layout that could cause subtle bugs in tensor operations and kernel launches; and (5) Ongoing efforts to generalize API behavior (like dtype handling and device-agnostic interfaces), along with test infrastructure and CI flakiness that require careful management. Many unresolved questions involve ensuring correctness across diverse hardware/software configurations and stabilizing performance and build processes."
2025-04-08,pytorch/pytorch,"The discussions highlight multiple technical concerns such as debugging ""illegal memory access"" CUDA errors that seem related to specific PyTorch versions or GPU architectures, and issues caused by dynamic shape guards leading to excessive recompilations. There are recurring references to how certain configurations, like `use_duck_shape`, can prevent unnecessary recompilations during runtime, especially for models with variable input sizes. Several developers suggest implementing environment variables or runtime flags to control shape specialization behavior, aiming to improve stability and performance consistency. Additionally, some discussions focus on potential improvements in profiling, reproducibility of errors for bug fixing, and ensuring support across different hardware (e.g., specific GPU models). The unresolved questions mainly revolve around clarifying the root causes of these runtime issues, the best practices for reliable shape handling, and how to generalize configurations for broader hardware support."
2025-04-09,pytorch/pytorch,"The discussions primarily revolve around performance and compatibility issues in PyTorch, including slow training with Pypy and GIL limitations, support for moe layers, and proper handling of GPU operations on different architectures (e.g., ROCm, NVIDIA, XPU). There are concerns about ensuring correct implementation and performance of low-level operations such as `torch.unique`, tensor serialization/deserialization, and NumPy compatibility, alongside debugging and testing challenges across diverse hardware. Specific technical questions include handling dynamic shapes, CUDA kernel launches, and device memory fragmentation. Several issues are also about build compatibility, NVCC/NCCL versions, and ensuring profiling and profiling tools yield accurate insights. Unresolved questions include the correct API semantics for certain tensor operations, proper prioritization of improvements, and coordinating support for various hardware backends and features."
2025-04-10,pytorch/pytorch,"The discussions highlight several key issues: a disagreement over naming and documentation inconsistencies surrounding `torch.median` behavior compared to numpy and statistical conventions; numerous environment and dependency issues, especially related to `libomp.dylib` loading problems on macOS and macOS-specific build and runtime errors; challenges in CUDA and XPU support, specifically handling MIG devices and the impact of `libgomp` on performance; and ongoing concerns with tracing, recompilation, and guard guards in torch.compile and FX export, including data-dependent guards and serialization concerns. Several proposals include enhancing error messages, fixing dependency issues, clarifying documentation, and refining code hooks for better compatibility and performance. Unresolved questions focus on better environment configuration, efficient fallback strategies, and precise diagnostics for complex hardware and software interactions."
2025-04-11,pytorch/pytorch,"The comments cover several topics, primarily centered on issues related to PyTorch's tensor operations, especially in the context of TorchScript, Dynamo, and custom operator dispatch. Major concerns include handling device-specific details in TorchScript models (e.g., modifying 'cuda' to 'cpu'), support for custom attributes and dispatching with torch.overrides, and the stability of guard mechanisms when detecting data-dependent operations or dictionary orderings. There is also discussion on improving profiling and debugging tools (like memory profiling), issues with FSDP and distributed communication (such as cross-mesh ops and process groups), and ensuring backward compatibility with newer CUDA versions. Many questions focus on correctness (e.g., behavior of torch.unique with dim), performance (kernel timings, throughput), or technical specifics of support in the latest PyTorch versions, often highlighting the need for clearer documentation or patching certain functionalities."
2025-04-12,pytorch/pytorch,"The discussions highlight issues with PyTorch's export and JIT tracing, particularly when handling data-dependent operators, nested graph breaks, and unsupported operator decompositions such as `aten._assert_scalar.default` and `torch.sym_not`. Several commits and PRs aim to address runtime errors, support for dynamic or data-dependent shapes, and improvements in model export completeness, often encountering rebase conflicts, merge failures, or missing support for custom or complex operations. There are ongoing challenges related to kernel performance variability (e.g., large deviations in kernel runtime), platform-specific behaviors (e.g., CUDA compute capability issues), and internal infrastructure, such as missing native functions or dependencies. Some discussions focus on the need to better handle control-flow, shape, or data-dependent operators during export, possibly by introducing graph breaks or specialized support for such cases. Overall, unresolved questions remain about how to robustly support complex, dynamic models in export workflows, especially for operators with data-dependent behavior or in heterogeneous hardware environments."
2025-04-13,pytorch/pytorch,"The discussions highlight various technical challenges in the PyTorch ecosystem, such as addressing library import order causing segmentation faults, especially with Open3D and PyTorch interactions, and potential dlopen symbol conflicts possibly related to RTLD_DEEPBIND. Several issues relate to extending or modifying existing APIs for better usability or correctness, such as the typing system of nn.Module, handling uneven sharding with DTensors (including the use of `use_local_output=False` to maintain consistency), and improving the robustness of transformer implementations and attention backends. Other concerns involve ensuring correctness in gradient and numerical computations (NaN issues in training, precise differing values in forward/backward passes), and managing CI/CD workflows with appropriate approvals, rebase actions, and CLA signatures. Overall, key unresolved questions include how to seamlessly handle uneven sharding, improve API design for complex modules, and streamline CI processes for internal and external contributions."
2025-04-14,pytorch/pytorch,"The comments reveal ongoing concerns about inconsistent API support between Python and C++ in PyTorch, especially for optimizers and certain operations like SparseAdam. Several issues report runtime errors, often on M1/MPS or ROCm, related to unsupported or unimplemented operators such as `aten::angle` and `aten::_fft_*`, with some errors possibly being hardware or driver-specific. Discussions include default epsilon values, fusing or supporting operators with different dtypes or in different backends, and challenges with distributed training, process group management, and cross-mesh operations in complex setups like FSDP and DTensors. Additionally, there are internal infra topics like merge failures, rebase conflicts, and CI flakiness, as well as questions about release policies, documentation standardization, and tooling improvements. Many unresolved questions involve feature support, stability, and consistency across platforms and APIs, indicating active, multi-faceted development and debugging efforts."
2025-04-15,pytorch/pytorch,"The comments span various topics including error fixes, feature support, and infrastructure issues. Key concerns include resolving specific operator export and implementation issues (e.g., fft, conv2d, ONNX support), addressing trunk flakiness and CI instability, ensuring proper build configurations (like CUDA_HOME), and improving code compatibility (e.g., support for different data types and tensor manipulations). Several discussions suggest potential fixes, such as updating import paths, reworking registration mechanisms, or handling tensor operations with more robust checks, but some problems (e.g., NCCL errors, CI flakiness) remain unresolved or depend on external factors. Approaches like adding support for new operations, better error handling, and enabling backward support in functional collectives are proposed but may require further development or upstream contributions. Overall, the discussions highlight ongoing maintenance challenges, bug fixes, and feature enhancements across PyTorch and its ecosystem."
2025-04-16,pytorch/pytorch,"The comments mainly revolve around troubleshooting and improving various PyTorch features, such as memory leak causes from weight_norm, operators unsupported on MPS (like broadcast, broadcast, and c10d::broadcast_), numerical discrepancies in ONNX exports, and runtime errors due to missing schema or unsupported ops. Several users report issues with CI flakiness, inconsistent test failures, and build failures across different platforms (linux, mac, rocm). Some discussions concern potential improvements like better validation of batched ops, dynamic shape support, pattern matching for attention kernels, and support for newer hardware features (like Blackwell, SM 12.0). Many comments also focus on ensuring compatibility and stability for specific backend configurations, hardware support, and updates in dependencies or build processes. Unresolved questions include handling of unsupported ops, validation tolerances, and precise operational behaviors in compilation and runtime."
2025-04-17,pytorch/pytorch,"The GitHub comments reveal multiple recurring issues including version mismatches (e.g., saving/loading models with different PyTorch versions), missing operator implementations on specific hardware backends like MPS and XPU, potential bugs and instability in trunk or nightly builds, and complex behaviors such as pattern matching, shape inference, and dynamic shape propagation in JIT compilation and model tracing. Several discussions focus on troubleshooting specific errors (e.g., `Expected Tuple but got String`, `isTuple() INTERNAL ASSERT FAILED`, operator support for various devices), often with suggestions to update PyTorch versions, adjust build configurations, or manually workaround limitations (e.g., fallback environments, custom operator registration). Certain issues pertain to package management and CI stability, indicating flakiness or environment configuration challenges. Overall, key unresolved concerns include ensuring operator coverage across hardware backends, robust version compatibility, and clearer developer guidance for workarounds and debugging."
2025-04-18,pytorch/pytorch,"The discussions highlight several technical concerns including the handling of unsupported operators like `torch.inverse()` in ONNX/ONNXRuntime, support for kwargs in hooks and lazy modules, and datatype limitations such as `histc` not working with bfloat16 on GPUs. There are also recurring issues related to trunk flakiness, stability of tests, and CI pass consistency. Some questions involve build environments (e.g., ROCm support, MacOS build limitations, CUDA driver errors, and RPATH issues on macOS). Additionally, there are suggestions for improvements such as making certain features more maintainable (e.g., native codegen for `tl.dot`, better global state management), and enhancements to documentation, build processes, or internal code transitions (e.g., moving magma builds). Unresolved questions include the support for custom ops, memory leaks, robust test re-enabling, and the correct handling of dynamic shapes and support for newer hardware or compiler versions."
2025-04-19,pytorch/pytorch,"The comments from the GitHub issues highlight several ongoing development and integration challenges in the PyTorch repository. Key concerns include the need for enhanced parameter/buffer organization tools like `nn.BufferDict`, handling of deep learning model compatibility across different hardware (e.g., MPS on Mac, ROCm on AMD GPUs), and issues related to distributed training, memory management, and environment inconsistencies. Several messages discuss the status of code merges, failed CI checks, and the impact of internal refactors or external dependencies, sometimes due to outdated environments or internal stability issues. There are also discussions on new features such as high-performance fused matmul implementations, ONNX translation challenges, and experimental backend support (like CUDA TF32) versus fallback mechanisms. Overall, the discussions reflect active efforts to improve PyTorch's flexibility, hardware support, compatibility, and performance, alongside managing ongoing CI/CD reliability."
2025-04-20,pytorch/pytorch,"The discussions highlight ongoing development in PyTorch, including enhancements to support NT slicing (limited support on `narrow()`), documentation link fixes, and improvements to custom operator support, especially around autograd.Function limitations and the integration of custom C++ kernels without Python callbacks. Several issues involve debugging distributed (NCCL) asynchronous behavior, CUDA compatibility, and tensor printing for dynamic sizes or fake tensors. There are technical challenges related to build configurations on macOS (e.g., RPATH duplication issues with Xcode 16), CUDA version mismatches affecting NCCL, and ensuring proper support for metadata in distributed and autograd contexts. Additionally, some PR merge failures are due to test failures or CLA signing, with ongoing efforts to refine performance, correctness, and build compatibility across different environments."
2025-04-21,pytorch/pytorch,"The discussions highlight broad concerns about recent changes and failures in PyTorch's CI and implementation, including flaky tests, merge issues due to dependency mismatches (e.g., LAPACK, Thrust, and driver versions), and the need for clearer documentation and test coverage, especially for new features like SVE support and custom quantizers. Several comments suggest improvements such as enabling environment variables for better dispatch control, making certain features opt-in for safety, and clarifying the behavior of vmap with kwargs and batched tensors. Additionally, there are ongoing questions about performance benchmarks, correctness of compiled models, and the need to rebase or revert problematic PRs rapidly to maintain CI stability. Overall, the main concerns involve ensuring stability, correctness, and clear communication around new code paths, dependencies, and testing strategies."
2025-04-22,pytorch/pytorch,"The comments reflect ongoing efforts to improve PyTorch's performance, correctness, and usability across diverse backends and hardware, often involving complex issues like dynamic shape support, custom kernel support, and multi-backend integration. There are recurring concerns about flakiness and instability in CI tests, especially related to trunk (main branch) execution on different platforms, which seem to require better monitoring, debugging, and infrastructure support. Several discussions involve enabling or disabling particular features, tests, or operators, often driven by hardware compatibility (e.g., RTX 5090, METAL/MPS on Mac, ROCm on Linux) or correctness issues (e.g., floating-point divergences, graph transformations). Some issues indicate the need for better developer tooling, error reporting, and documentation, particularly around dynamic shape handling, kernel prefetching, and custom operator registration. Overall, these discussions highlight the balance between rapid feature development, stability, and the complexities of supporting heterogeneous hardware and evolving compiler and runtime frameworks."
2025-04-23,pytorch/pytorch,"The discussions cover several technical concerns including the incomplete documentation and searchability of `torch.is_grad_enabled`, the need for clearer error messages around sparse gradients and momentum in sparse update scenarios, and potential improvements to PyTorch's support for CUDA and ROCm hardware, especially regarding support for older architectures like SM60. There are recurring inquiries about improving the auto-tuning, performance benchmarking, and support for custom kernels in torch.compile, along with efforts to stabilize flaky tests across different platforms. Additionally, issues such as interoperability with external frameworks, the handling of sparse tensors, and the maintenance of build and CI workflows, especially on aarch64 and Apple Silicon, are emphasized. Multiple discussions suggest ongoing work to refine APIs for distributed RMA and PGAS models, better error handling, and performance tuning, with some unresolved questions about hardware support and build systems."
